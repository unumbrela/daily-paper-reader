{
  "generated_at": "2026-02-28T04:08:40.073992+00:00",
  "updated_date": "20260219-20260228",
  "carryover_days": 9,
  "items": [
    {
      "id": "2601.22362v1",
      "title": "Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \\emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.",
      "authors": [
        "Julien Delavande",
        "Regis Pierrard",
        "Sasha Luccioni"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-29T22:16:25+00:00",
      "link": "https://arxiv.org/pdf/2601.22362v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Inference optimization and serving strategies for LLMs",
      "llm_evidence_cn": "大语言模型推理优化与服务策略",
      "llm_evidence": "大语言模型推理优化与服务策略",
      "llm_tldr_en": "Empirically studies the impact of quantization and batching on LLM inference energy and latency.",
      "llm_tldr_cn": "实证研究了量化和批处理对大语言模型推理能耗和延迟的影响。",
      "llm_tldr": "实证研究了量化和批处理对大语言模型推理能耗和延迟的影响。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.22362v1",
      "carry_days": 1
    },
    {
      "id": "2601.11863v1",
      "title": "Utilizing Metadata for Better Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.",
      "authors": [
        "Raquib Bin Yousuf",
        "Shengzhe Xu",
        "Mandar Sharma",
        "Andrew Neeser",
        "Chris Latimer",
        "Naren Ramakrishnan"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "published": "2026-01-17T01:11:03+00:00",
      "link": "https://arxiv.org/pdf/2601.11863v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Metadata-aware retrieval strategies for RAG",
      "llm_evidence_cn": "RAG中感知元数据的检索策略",
      "llm_evidence": "RAG中感知元数据的检索策略",
      "llm_tldr_en": "Systematically evaluates how embedding metadata improves retrieval accuracy in RAG systems.",
      "llm_tldr_cn": "系统地评估了在RAG系统中嵌入元数据如何提高检索准确性。",
      "llm_tldr": "系统地评估了在RAG系统中嵌入元数据如何提高检索准确性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.11863v1",
      "carry_days": 1
    },
    {
      "id": "2601.17755v1",
      "title": "ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation",
      "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.",
      "authors": [
        "Jinyoung Park",
        "Sanghyeok Lee",
        "Omar Zia Khan",
        "Hyunwoo J. Kim",
        "Joo-Kyung Kim"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-25T08:58:44+00:00",
      "link": "https://arxiv.org/pdf/2601.17755v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Reinforcement learning for Graph Retrieval Augmented Generation",
      "llm_evidence_cn": "用于图检索增强生成的强化学习框架",
      "llm_evidence": "用于图检索增强生成的强化学习框架",
      "llm_tldr_en": "Introduces ProGraph-R1, using RL to improve graph-based retrieval and reasoning in LLMs.",
      "llm_tldr_cn": "引入ProGraph-R1，利用强化学习改进大语言模型的图检索与推理能力。",
      "llm_tldr": "引入ProGraph-R1，利用强化学习改进大语言模型的图检索与推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.17755v1",
      "carry_days": 1
    },
    {
      "id": "2602.05728v1",
      "title": "CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering",
      "abstract": "Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.   In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.   Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.",
      "authors": [
        "Hao Yang",
        "Zhiyu Yang",
        "Xupeng Zhang",
        "Wei Wei",
        "Yunjie Zhang",
        "Lin Yang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-05T14:52:06+00:00",
      "link": "https://arxiv.org/pdf/2602.05728v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Reducing LLM calls and token overhead in multi-hop RAG",
      "llm_evidence_cn": "减少多跳RAG中的模型调用和Token开销",
      "llm_evidence": "减少多跳RAG中的模型调用和Token开销",
      "llm_tldr_en": "Proposes CompactRAG to improve efficiency in knowledge-intensive multi-hop question answering.",
      "llm_tldr_cn": "提出CompactRAG，提高知识密集型多跳问答的效率。",
      "llm_tldr": "提出CompactRAG，提高知识密集型多跳问答的效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.05728v1",
      "carry_days": 1
    },
    {
      "id": "2601.11443v1",
      "title": "Predict the Retrieval! Test time adaptation for Retrieval Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language models' question-answering capabilities through the integration of external knowledge. However, when adapting RAG systems to specialized domains, challenges arise from distribution shifts, resulting in suboptimal generalization performance. In this work, we propose TTARAG, a test-time adaptation method that dynamically updates the language model's parameters during inference to improve RAG system performance in specialized domains. Our method introduces a simple yet effective approach where the model learns to predict retrieved content, enabling automatic parameter adjustment to the target domain. Through extensive experiments across six specialized domains, we demonstrate that TTARAG achieves substantial performance improvements over baseline RAG systems. Code available at https://github.com/sunxin000/TTARAG.",
      "authors": [
        "Xin Sun",
        "Zhongqi Chen",
        "Qiang Liu",
        "Shu Wu",
        "Bowen Song",
        "Weiqiang Wang",
        "Zilei Wang",
        "Liang Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-16T17:07:01+00:00",
      "link": "https://arxiv.org/pdf/2601.11443v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "test-time adaptation for Retrieval Augmented Generation",
      "llm_evidence_cn": "检索增强生成的测试时自适应",
      "llm_evidence": "检索增强生成的测试时自适应",
      "llm_tldr_en": "Proposes TTARAG to dynamically update LLM parameters during inference for better RAG performance in specialized domains.",
      "llm_tldr_cn": "提出TTARAG方法，在推理过程中动态更新LLM参数以提升RAG在专业领域的表现。",
      "llm_tldr": "提出TTARAG方法，在推理过程中动态更新LLM参数以提升RAG在专业领域的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.11443v1",
      "carry_days": 1
    },
    {
      "id": "2602.02382v1",
      "title": "ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs",
      "abstract": "Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.",
      "authors": [
        "Ziyan Zhang",
        "Chao Wang",
        "Zhuo Chen",
        "Chiyi Li",
        "Kai Song"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-02T17:45:43+00:00",
      "link": "https://arxiv.org/pdf/2602.02382v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Retrieval-augmented framework with chain-of-thought reasoning",
      "llm_evidence_cn": "结合思维链推理的检索增强框架",
      "llm_evidence": "结合思维链推理的检索增强框架",
      "llm_tldr_en": "ROG combines neighborhood retrieval with CoT to solve complex logic queries over knowledge graphs.",
      "llm_tldr_cn": "ROG结合邻域检索与思维链推理，解决知识图谱上的复杂逻辑查询问题。",
      "llm_tldr": "ROG结合邻域检索与思维链推理，解决知识图谱上的复杂逻辑查询问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.02382v1",
      "carry_days": 1
    },
    {
      "id": "2601.17260v1",
      "title": "The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment",
      "abstract": "Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $β$) yields progressively \"better\" behavior. We instead treat $β$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $β\\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $β$ induces capability losses that persist even after $β$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $β$ landscape rather than reliance on margins or aggregate benchmarks.",
      "authors": [
        "Marco Pollanen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-24T02:19:20+00:00",
      "link": "https://arxiv.org/pdf/2601.17260v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Analysis of DPO alignment pressure and reasoning capability",
      "llm_evidence_cn": "DPO对齐压力与推理能力的分析",
      "llm_evidence": "DPO对齐压力与推理能力的分析",
      "llm_tldr_en": "This paper investigates how DPO alignment parameters affect the reasoning capabilities of various LLM architectures.",
      "llm_tldr_cn": "本文研究了DPO对齐参数如何影响不同架构大语言模型的推理能力。",
      "llm_tldr": "本文研究了DPO对齐参数如何影响不同架构大语言模型的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2601.17260v1",
      "carry_days": 1
    },
    {
      "id": "2601.19139v2",
      "title": "Native LLM and MLLM Inference at Scale on Apple Silicon",
      "abstract": "The growing adoption of Apple Silicon for machine learning development has created demand for efficient inference solutions that leverage its unique unified memory architecture. However, existing tools either lack native optimization (PyTorch MPS) or focus solely on text models, leaving multimodal workloads underserved. We present vllm-mlx, a framework for efficient LLM and MLLM inference on Apple Silicon built natively on MLX. For text models, we achieve 21\\% to 87\\% higher throughput than llama-cpp across models ranging from Qwen3-0.6B to Nemotron-30B, while providing continuous batching that scales to 4.3x aggregate throughput at 16 concurrent requests. For multimodal models, we introduce content-based prefix caching that eliminates redundant vision encoding by identifying identical images through content hashing, regardless of input format. Our evaluation on Apple M4 Max demonstrates throughput of up to 525 tokens per second on text models and 28x speedup on repeated image queries, reducing multimodal latency from 21.7 seconds to under 1 second. Video analysis with up to 64 frames achieves 24.7x cache speedup. We release our implementation as open source to support efficient inference on consumer Apple hardware.",
      "authors": [
        "Wayner Barrios"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.ET"
      ],
      "published": "2026-01-27T03:11:02+00:00",
      "link": "https://arxiv.org/pdf/2601.19139v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Efficient LLM and MLLM inference optimization on Apple Silicon",
      "llm_evidence_cn": "Apple Silicon上的高效LLM与MLLM推理优化",
      "llm_evidence": "Apple Silicon上的高效LLM与MLLM推理优化",
      "llm_tldr_en": "vllm-mlx optimizes LLM inference throughput and batching natively for Apple Silicon's unified memory.",
      "llm_tldr_cn": "vllm-mlx针对Apple Silicon的统一内存架构优化了LLM推理吞吐量和批处理。",
      "llm_tldr": "vllm-mlx针对Apple Silicon的统一内存架构优化了LLM推理吞吐量和批处理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.19139v2",
      "carry_days": 1
    },
    {
      "id": "2602.22593v1",
      "title": "FLYING SERVING: On-the-Fly Parallelism Switching for Large Language Model Serving",
      "abstract": "Production LLM serving must simultaneously deliver high throughput, low latency, and sufficient context capacity under non-stationary traffic and mixed request requirements. Data parallelism (DP) maximizes throughput by running independent replicas, while tensor parallelism (TP) reduces per-request latency and pools memory for long-context inference. However, existing serving stacks typically commit to a static parallelism configuration at deployment; adapting to bursts, priorities, or long-context requests is often disruptive and slow. We present Flying Serving, a vLLM-based system that enables online DP-TP switching without restarting engine workers. Flying Serving makes reconfiguration practical by virtualizing the state that would otherwise force data movement: (i) a zero-copy Model Weights Manager that exposes TP shard views on demand, (ii) a KV Cache Adaptor that preserves request KV state across DP/TP layouts, (iii) an eagerly initialized Communicator Pool to amortize collective setup, and (iv) a deadlock-free scheduler that coordinates safe transitions under execution skew. Across three popular LLMs and realistic serving scenarios, Flying Serving improves performance by up to $4.79\\times$ under high load and $3.47\\times$ under low load while supporting latency- and memory-driven requests.",
      "authors": [
        "Shouwei Gao",
        "Junqi Yin",
        "Feiyi Wang",
        "Wenqian Dong"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-26T03:55:51+00:00",
      "link": "https://arxiv.org/pdf/2602.22593v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Parallelism switching for large language model serving",
      "llm_evidence_cn": "大语言模型服务的并行切换",
      "llm_evidence": "大语言模型服务的并行切换",
      "llm_tldr_en": "Flying Serving enables on-the-fly DP-TP switching to optimize LLM inference latency and throughput.",
      "llm_tldr_cn": "Flying Serving支持动态DP-TP并行切换，以优化LLM推理的延迟和吞吐量。",
      "llm_tldr": "Flying Serving支持动态DP-TP并行切换，以优化LLM推理的延迟和吞吐量。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.22593v1",
      "carry_days": 1
    },
    {
      "id": "2602.13647v1",
      "title": "PT-RAG: Structure-Fidelity Retrieval-Augmented Generation for Academic Papers",
      "abstract": "Retrieval-augmented generation (RAG) is increasingly applied to question-answering over long academic papers, where accurate evidence allocation under a fixed token budget is critical. Existing approaches typically flatten academic papers into unstructured chunks during preprocessing, which destroys the native hierarchical structure. This loss forces retrieval to operate in a disordered space, thereby producing fragmented contexts, misallocating tokens to non-evidential regions under finite token budgets, and increasing the reasoning burden for downstream language models. To address these issues, we propose PT-RAG, an RAG framework that treats the native hierarchical structure of academic papers as a low-entropy retrieval prior. PT-RAG first inherits the native hierarchy to construct a structure-fidelity PaperTree index, which prevents entropy increase at the source. It then designs a path-guided retrieval mechanism that aligns query semantics to relevant sections and selects high relevance root-to-leaf paths under a fixed token budget, yielding compact, coherent, and low-entropy retrieval contexts. In contrast to existing RAG approaches, PT-RAG avoids entropy increase caused by destructive preprocessing and provides a native low-entropy structural basis for subsequent retrieval. To assess this design, we introduce entropy-based structural diagnostics that quantify retrieval fragmentation and evidence allocation accuracy. On three academic question-answering benchmarks, PT-RAG achieves consistently lower section entropy and evidence alignment cross entropy than strong baselines, indicating reduced context fragmentation and more precise allocation to evidential regions. These structural advantages directly translate into higher answer quality.",
      "authors": [
        "Rui Yu",
        "Tianyi Wang",
        "Ruixia Liu",
        "Yinglong Wang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-14T07:40:09+00:00",
      "link": "https://arxiv.org/pdf/2602.13647v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG framework for academic papers using hierarchical structure",
      "llm_evidence_cn": "利用学术论文层级结构的RAG框架",
      "llm_evidence": "利用学术论文层级结构的RAG框架",
      "llm_tldr_en": "Proposes PT-RAG to improve retrieval accuracy in academic papers by preserving their native hierarchical structure.",
      "llm_tldr_cn": "提出PT-RAG框架，通过保留学术论文的层级结构来优化检索增强生成的效果。",
      "llm_tldr": "提出PT-RAG框架，通过保留学术论文的层级结构来优化检索增强生成的效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.13647v1",
      "carry_days": 1
    },
    {
      "id": "2602.13628v1",
      "title": "Compact LLM Deployment and World Model Assisted Offloading in Mobile Edge Computing",
      "abstract": "This paper investigates compact large language model (LLM) deployment and world-model-assisted inference offloading in mobile edge computing (MEC) networks. We first propose an edge compact LLM deployment (ECLD) framework that jointly applies structured pruning, low-bit quantization, and knowledge distillation to construct edge-deployable LLM variants, and we evaluate these models using four complementary metrics: accessibility, energy consumption, hallucination rate, and generalization accuracy. Building on the resulting compact models, we formulate an MEC offloading optimization problem that minimizes the long-term average inference latency subject to per-device energy budgets and LLM-specific quality-of-service constraints on effective accuracy and hallucination. To solve this problem under unknown and time-varying network dynamics, we develop a world model-proximal policy optimization (PPO) algorithm, which augments an on-policy PPO algorithm with a learned recurrent world model that provides improved value targets and short imagination rollouts. Extensive experiments on Llama-3.1-8B, Qwen3-8B, and Mistral-12B show that ECLD compresses base models by about 70-80% in storage (i.e., from 15.3 GB to 3.3 GB for Llama-3.1-8B) and reduces per-query energy consumption by up to 50%, while largely preserving accuracy and often lowering hallucination compared with quantization-only or pruning-only baselines. Moreover, they also show that world model-PPO speeds up convergence by about 50%, improves the final reward by 15.8% over vanilla PPO, and reduces average inference latency by 12-30% across different user populations, while satisfying the accuracy and hallucination constraints and approaching the generation quality of always-offloading with much of the efficiency of local execution.",
      "authors": [
        "Ruichen Zhang",
        "Xiaofeng Luo",
        "Jiayi He",
        "Dusit Niyato",
        "Jiawen Kang",
        "Zehui Xiong",
        "Yonghui Li"
      ],
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI"
      ],
      "published": "2026-02-14T06:37:29+00:00",
      "link": "https://arxiv.org/pdf/2602.13628v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Compact LLM deployment and inference optimization in MEC",
      "llm_evidence_cn": "移动边缘计算中的紧凑型LLM部署与推理优化",
      "llm_evidence": "移动边缘计算中的紧凑型LLM部署与推理优化",
      "llm_tldr_en": "Explores pruning, quantization, and distillation for efficient LLM deployment and offloading in edge networks.",
      "llm_tldr_cn": "研究通过剪枝、量化和蒸馏技术在边缘网络中实现高效的LLM部署与推理卸载。",
      "llm_tldr": "研究通过剪枝、量化和蒸馏技术在边缘网络中实现高效的LLM部署与推理卸载。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.13628v1",
      "carry_days": 1
    },
    {
      "id": "2601.11255v1",
      "title": "Reasoning in Trees: Improving Retrieval-Augmented Generation for Multi-Hop Question Answering",
      "abstract": "Retrieval-Augmented Generation (RAG) has demonstrated significant effectiveness in enhancing large language models (LLMs) for complex multi-hop question answering (QA). For multi-hop QA tasks, current iterative approaches predominantly rely on LLMs to self-guide and plan multi-step exploration paths during retrieval, leading to substantial challenges in maintaining reasoning coherence across steps from inaccurate query decomposition and error propagation. To address these issues, we introduce Reasoning Tree Guided RAG (RT-RAG), a novel hierarchical framework for complex multi-hop QA. RT-RAG systematically decomposes multi-hop questions into explicit reasoning trees, minimizing inaccurate decomposition through structured entity analysis and consensus-based tree selection that clearly separates core queries, known entities, and unknown entities. Subsequently, a bottom-up traversal strategy employs iterative query rewriting and refinement to collect high-quality evidence, thereby mitigating error propagation. Comprehensive experiments show that RT-RAG substantially outperforms state-of-the-art methods by 7.0% F1 and 6.0% EM, demonstrating the effectiveness of RT-RAG in complex multi-hop QA.",
      "authors": [
        "Yuling Shi",
        "Maolin Sun",
        "Zijun Liu",
        "Mo Yang",
        "Yixiong Fang",
        "Tianran Sun",
        "Xiaodong Gu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-01-16T13:02:25+00:00",
      "link": "https://arxiv.org/pdf/2601.11255v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG for multi-hop question answering and reasoning trees",
      "llm_evidence_cn": "用于多跳问答和推理树的RAG技术",
      "llm_evidence": "用于多跳问答和推理树的RAG技术",
      "llm_tldr_en": "Introduces RT-RAG to improve multi-hop reasoning by decomposing questions into structured reasoning trees.",
      "llm_tldr_cn": "引入RT-RAG框架，通过将问题分解为结构化推理树来增强多跳问答的推理能力。",
      "llm_tldr": "引入RT-RAG框架，通过将问题分解为结构化推理树来增强多跳问答的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.11255v1",
      "carry_days": 1
    },
    {
      "id": "2602.04620v2",
      "title": "QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning",
      "abstract": "GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.",
      "authors": [
        "Doyeon Lee",
        "Eunyi Lyou",
        "Hyunsoo Cho",
        "Sookyung Kim",
        "Joonseok Lee",
        "Jaemoo Choi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T14:51:04+00:00",
      "link": "https://arxiv.org/pdf/2602.04620v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "LLM fine-tuning and policy optimization for alignment",
      "llm_evidence_cn": "用于对齐的LLM微调与策略优化",
      "llm_evidence": "用于对齐的LLM微调与策略优化",
      "llm_tldr_en": "Proposes QUATRO, a query-adaptive trust region method for more stable RL-based LLM fine-tuning.",
      "llm_tldr_cn": "提出QUATRO算法，通过查询自适应信任域优化提升强化学习微调LLM的稳定性。",
      "llm_tldr": "提出QUATRO算法，通过查询自适应信任域优化提升强化学习微调LLM的稳定性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.04620v2",
      "carry_days": 1
    },
    {
      "id": "2601.16478v1",
      "title": "DeepEra: A Deep Evidence Reranking Agent for Scientific Retrieval-Augmented Generated Question Answering",
      "abstract": "With the rapid growth of scientific literature, scientific question answering (SciQA) has become increasingly critical for exploring and utilizing scientific knowledge. Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating knowledge from external sources, thereby providing credible evidence for scientific question answering. But existing retrieval and reranking methods remain vulnerable to passages that are semantically similar but logically irrelevant, often reducing factual reliability and amplifying hallucinations.To address this challenge, we propose a Deep Evidence Reranking Agent (DeepEra) that integrates step-by-step reasoning, enabling more precise evaluation of candidate passages beyond surface-level semantics. To support systematic evaluation, we construct SciRAG-SSLI (Scientific RAG - Semantically Similar but Logically Irrelevant), a large-scale dataset comprising about 300K SciQA instances across 10 subjects, constructed from 10M scientific corpus. The dataset combines naturally retrieved contexts with systematically generated distractors to test logical robustness and factual grounding. Comprehensive evaluations confirm that our approach achieves superior retrieval performance compared to leading rerankers. To our knowledge, this work is the first to comprehensively study and empirically validate innegligible SSLI issues in two-stage RAG frameworks.",
      "authors": [
        "Haotian Chen",
        "Qingqing Long",
        "Siyu Pu",
        "Xiao Luo",
        "Wei Ju",
        "Meng Xiao",
        "Yuanchun Zhou",
        "Jianghua Zhao",
        "Xuezhi Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-23T06:19:08+00:00",
      "link": "https://arxiv.org/pdf/2601.16478v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG framework for scientific question answering with reasoning",
      "llm_evidence_cn": "用于科学问答和推理的RAG框架",
      "llm_evidence": "用于科学问答和推理的RAG框架",
      "llm_tldr_en": "Proposes DeepEra, a reranking agent that uses reasoning to improve factual reliability in scientific RAG systems.",
      "llm_tldr_cn": "提出DeepEra重排序代理，通过推理提升科学RAG系统的可靠性。",
      "llm_tldr": "提出DeepEra重排序代理，通过推理提升科学RAG系统的可靠性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.16478v1",
      "carry_days": 1
    },
    {
      "id": "2602.02988v1",
      "title": "NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks, but their deployment is often constrained by substantial memory footprints and computational costs. While prior work has achieved significant progress in compressing and accelerating linear layers, nonlinear layers-such as SiLU, RMSNorm, and Softmax-still heavily depend on high-precision floating-point operations. In this paper, we propose a calibration-free, dynamic-programming-optimal, and hardware-friendly framework called Non-uniform Linear Interpolation (NLI). NLI is capable of efficiently approximating a variety of nonlinear functions, enabling seamless integration into LLMs and other deep neural networks with almost no loss in accuracy. NLI ingeniously recasts cutpoint selection as a dynamic-programming problem, achieving the globally minimal interpolation error in O(MxN2) time via Bellman's optimality principle. Based on the NLI algorithm, we also design and implement a plug-and-play universal nonlinear computation unit. Hardware experiments demonstrate that the NLI Engine achieves more than 4x improvement in computational efficiency compared to the state-of-the-art designs.",
      "authors": [
        "Jiangyong Yu",
        "Xiaomeng Han",
        "Xing Hu",
        "Chen Xu",
        "Zhe Jiang",
        "Dawei Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-03T01:47:58+00:00",
      "link": "https://arxiv.org/pdf/2602.02988v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Efficient LLM inference via nonlinear operation approximation",
      "llm_evidence_cn": "通过非线性操作近似实现高效LLM推理",
      "llm_evidence": "通过非线性操作近似实现高效LLM推理",
      "llm_tldr_en": "Proposes NLI to approximate nonlinear layers, reducing computational costs for efficient LLM deployment.",
      "llm_tldr_cn": "提出NLI近似非线性层，降低计算开销以实现高效LLM部署。",
      "llm_tldr": "提出NLI近似非线性层，降低计算开销以实现高效LLM部署。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.02988v1",
      "carry_days": 1
    },
    {
      "id": "2602.12278v1",
      "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
      "abstract": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.",
      "authors": [
        "David Jiahao Fu",
        "Lam Thanh Do",
        "Jiayu Li",
        "Kevin Chen-Chuan Chang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-12T18:59:35+00:00",
      "link": "https://arxiv.org/pdf/2602.12278v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Long document retrieval model for RAG tasks",
      "llm_evidence_cn": "用于RAG任务的长文档检索模型",
      "llm_evidence": "用于RAG任务的长文档检索模型",
      "llm_tldr_en": "Introduces AttentionRetriever, leveraging attention mechanisms to improve retrieval for long-context RAG.",
      "llm_tldr_cn": "引入AttentionRetriever，利用注意力机制改进长文本RAG的检索效果。",
      "llm_tldr": "引入AttentionRetriever，利用注意力机制改进长文本RAG的检索效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.12278v1",
      "carry_days": 1
    },
    {
      "id": "2602.14470v1",
      "title": "HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation",
      "abstract": "Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.",
      "authors": [
        "Wen-Sheng Lien",
        "Yu-Kai Chan",
        "Hao-Lung Hsiao",
        "Bo-Kai Ruan",
        "Meng-Fen Chiang",
        "Chien-An Chen",
        "Yi-Ren Yeh",
        "Hong-Han Shuai"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16T05:15:55+00:00",
      "link": "https://arxiv.org/pdf/2602.14470v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Hypergraph-based Retrieval-Augmented Generation (RAG) framework",
      "llm_evidence_cn": "基于超图的检索增强生成 (RAG) 框架",
      "llm_evidence": "基于超图的检索增强生成 (RAG) 框架",
      "llm_tldr_en": "Proposes HyperRAG to improve reasoning and retrieval efficiency in RAG using n-ary hypergraphs.",
      "llm_tldr_cn": "提出了HyperRAG，利用n元超图提高检索增强生成中的推理和检索效率。",
      "llm_tldr": "提出了HyperRAG，利用n元超图提高检索增强生成中的推理和检索效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.14470v1",
      "carry_days": 1
    },
    {
      "id": "2601.19225v2",
      "title": "RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering",
      "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications.",
      "authors": [
        "Kaehyun Um",
        "KyuHwan Yeom",
        "Haerim Yang",
        "Minyoung Choi",
        "Hyeongjun Yang",
        "Kyong-Ho Lee"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-27T05:46:32+00:00",
      "link": "https://arxiv.org/pdf/2601.19225v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG alignment for knowledge graph question answering",
      "llm_evidence_cn": "针对知识图谱问答的检索增强生成对齐",
      "llm_evidence": "针对知识图谱问答的检索增强生成对齐",
      "llm_tldr_en": "Proposes RPO-RAG to align small LLMs with relation-aware preference optimization for better KG-based reasoning.",
      "llm_tldr_cn": "提出RPO-RAG框架，通过关系感知偏好优化提升小模型在知识图谱检索增强任务中的推理能力。",
      "llm_tldr": "提出RPO-RAG框架，通过关系感知偏好优化提升小模型在知识图谱检索增强任务中的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.19225v2",
      "carry_days": 1
    },
    {
      "id": "2602.05152v1",
      "title": "RAG without Forgetting: Continual Query-Infused Key Memory",
      "abstract": "Retrieval-augmented generation (RAG) systems commonly improve robustness via query-time adaptations such as query expansion and iterative retrieval. While effective, these approaches are inherently stateless: adaptations are recomputed for each query and discarded thereafter, precluding cumulative learning and repeatedly incurring inference-time cost. Index-side approaches like key expansion introduce persistence but rely on offline preprocessing or heuristic updates that are weakly aligned with downstream task utility, leading to semantic drift and noise accumulation. We propose Evolving Retrieval Memory (ERM), a training-free framework that transforms transient query-time gains into persistent retrieval improvements. ERM updates the retrieval index through correctness-gated feedback, selectively attributes atomic expansion signals to the document keys they benefit, and progressively evolves keys via stable, norm-bounded updates. We show that query and key expansion are theoretically equivalent under standard similarity functions and prove convergence of ERM's selective updates, amortizing optimal query expansion into a stable index with zero inference-time overhead. Experiments on BEIR and BRIGHT across 13 domains demonstrate consistent gains in retrieval and generation, particularly on reasoning-intensive tasks, at native retrieval speed.",
      "authors": [
        "Yuntong Hu",
        "Sha Li",
        "Naren Ramakrishnan",
        "Liang Zhao"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-05T00:12:45+00:00",
      "link": "https://arxiv.org/pdf/2602.05152v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Retrieval-augmented generation and persistent memory updates",
      "llm_evidence_cn": "检索增强生成与持久化记忆更新",
      "llm_evidence": "检索增强生成与持久化记忆更新",
      "llm_tldr_en": "Proposes Evolving Retrieval Memory to transform transient query gains into persistent RAG improvements.",
      "llm_tldr_cn": "提出ERM框架，将RAG系统中的瞬时查询增益转化为持久的检索性能提升。",
      "llm_tldr": "提出ERM框架，将RAG系统中的瞬时查询增益转化为持久的检索性能提升。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.05152v1",
      "carry_days": 1
    },
    {
      "id": "2602.02477v1",
      "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability",
      "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.",
      "authors": [
        "Xiao Liang",
        "Zhong-Zhi Li",
        "Zhenghao Lin",
        "Eric Hancheng Jiang",
        "Hengyuan Zhang",
        "Yelong Shen",
        "Kai-Wei Chang",
        "Ying Nian Wu",
        "Yeyun Gong",
        "Weizhu Chen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-02T18:54:54+00:00",
      "link": "https://arxiv.org/pdf/2602.02477v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Divide-and-conquer reasoning and chain-of-thought",
      "llm_evidence_cn": "分治推理与思维链",
      "llm_evidence": "分治推理与思维链",
      "llm_tldr_en": "Enhances LLM reasoning by training for divide-and-conquer strategies to improve test-time scalability.",
      "llm_tldr_cn": "通过训练分治推理能力，提升大语言模型在复杂任务中的推理扩展性。",
      "llm_tldr": "通过训练分治推理能力，提升大语言模型在复杂任务中的推理扩展性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.02477v1",
      "carry_days": 1
    },
    {
      "id": "2602.04879v1",
      "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.",
      "authors": [
        "Penghui Qi",
        "Xiangxin Zhou",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Min Lin",
        "Wee Sun Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T18:59:04+00:00",
      "link": "https://arxiv.org/pdf/2602.04879v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "LLM reinforcement learning and PPO optimization",
      "llm_evidence_cn": "大语言模型强化学习与PPO优化",
      "llm_evidence": "大语言模型强化学习与PPO优化",
      "llm_tldr_en": "Analyzes and improves the trust region mechanism in PPO for more stable LLM reinforcement learning.",
      "llm_tldr_cn": "重新审视PPO中的信任区域机制，解决大模型强化学习中的训练不稳定问题。",
      "llm_tldr": "重新审视PPO中的信任区域机制，解决大模型强化学习中的训练不稳定问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.04879v1",
      "carry_days": 1
    },
    {
      "id": "2601.21162v1",
      "title": "A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning",
      "abstract": "Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.",
      "authors": [
        "Jiate Liu",
        "Zebin Chen",
        "Shaobo Qiao",
        "Mingchen Ju",
        "Danting Zhang",
        "Bocheng Han",
        "Shuyue Yu",
        "Xin Shu",
        "Jingling Wu",
        "Dong Wen",
        "Xin Cao",
        "Guanfeng Liu",
        "Zhengyi Yang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB"
      ],
      "published": "2026-01-29T01:58:30+00:00",
      "link": "https://arxiv.org/pdf/2601.21162v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Adaptive Graph-RAG for multihop reasoning",
      "llm_evidence_cn": "用于多跳推理的自适应图检索增强生成",
      "llm_evidence": "用于多跳推理的自适应图检索增强生成",
      "llm_tldr_en": "A2RAG uses an adaptive controller and agentic retriever to improve Graph-RAG efficiency and reliability.",
      "llm_tldr_cn": "A2RAG通过自适应控制器和智能体检索器提升了图检索增强生成的效率与可靠性。",
      "llm_tldr": "A2RAG通过自适应控制器和智能体检索器提升了图检索增强生成的效率与可靠性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.21162v1",
      "carry_days": 1
    },
    {
      "id": "2601.17212v1",
      "title": "DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation",
      "abstract": "Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.",
      "authors": [
        "Saadat Hasan Khan",
        "Spencer Hong",
        "Jingyu Wu",
        "Kevin Lybarger",
        "Youbing Yin",
        "Erin Babinsky",
        "Daben Liu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-23T22:47:16+00:00",
      "link": "https://arxiv.org/pdf/2601.17212v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Diversity-focused RAG for reasoning-intensive QA",
      "llm_evidence_cn": "面向推理密集型问答的多样化检索增强生成",
      "llm_evidence": "面向推理密集型问答的多样化检索增强生成",
      "llm_tldr_en": "DF-RAG improves RAG performance on complex tasks by incorporating diversity into the retrieval step.",
      "llm_tldr_cn": "DF-RAG通过在检索步骤中引入多样性，提升了RAG在复杂推理任务中的表现。",
      "llm_tldr": "DF-RAG通过在检索步骤中引入多样性，提升了RAG在复杂推理任务中的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.17212v1",
      "carry_days": 1
    },
    {
      "id": "2602.04926v1",
      "title": "Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented Generation",
      "abstract": "Retrieval-augmented generation (RAG) is now standard for knowledge-intensive LLM tasks, but most systems still treat every query as fresh, repeatedly re-retrieving long passages and re-reasoning from scratch, inflating tokens, latency, and cost. We present AutoPrunedRetriever, a graph-style RAG system that persists the minimal reasoning subgraph built for earlier questions and incrementally extends it for later ones. AutoPrunedRetriever stores entities and relations in a compact, ID-indexed codebook and represents questions, facts, and answers as edge sequences, enabling retrieval and prompting over symbolic structure instead of raw text. To keep the graph compact, we apply a two-layer consolidation policy (fast ANN/KNN alias detection plus selective $k$-means once a memory threshold is reached) and prune low-value structure, while prompts retain only overlap representatives and genuinely new evidence. We instantiate two front ends: AutoPrunedRetriever-REBEL, which uses REBEL as a triplet parser, and AutoPrunedRetriever-llm, which swaps in an LLM extractor. On GraphRAG-Benchmark (Medical and Novel), both variants achieve state-of-the-art complex reasoning accuracy, improving over HippoRAG2 by roughly 9--11 points, and remain competitive on contextual summarize and generation. On our harder STEM and TV benchmarks, AutoPrunedRetriever again ranks first, while using up to two orders of magnitude fewer tokens than graph-heavy baselines, making it a practical substrate for long-running sessions, evolving corpora, and multi-agent pipelines.",
      "authors": [
        "Ning Wang",
        "Kuanyan Zhu",
        "Daniel Yuehwoon Yee",
        "Yitang Gao",
        "Shiying Huang",
        "Zirun Xu",
        "Sainyam Galhotra"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-04T08:48:11+00:00",
      "link": "https://arxiv.org/pdf/2602.04926v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Graph-style RAG for efficient knowledge-intensive tasks",
      "llm_evidence_cn": "用于高效知识密集型任务的图式检索增强生成",
      "llm_evidence": "用于高效知识密集型任务的图式检索增强生成",
      "llm_tldr_en": "AutoPrunedRetriever uses minimal reasoning subgraphs to reduce costs in retrieval-augmented generation.",
      "llm_tldr_cn": "AutoPrunedRetriever通过构建最小推理子图，降低了检索增强生成的计算成本。",
      "llm_tldr": "AutoPrunedRetriever通过构建最小推理子图，降低了检索增强生成的计算成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.04926v1",
      "carry_days": 1
    },
    {
      "id": "2601.17532v1",
      "title": "Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection",
      "abstract": "Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \\textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.",
      "authors": [
        "Zhipeng Song",
        "Yizhi Zhou",
        "Xiangyu Kong",
        "Jiulong Jiao",
        "Xinrui Bao",
        "Xu You",
        "Xueqing Shi",
        "Yuhang Zhou",
        "Heng Qi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-24T17:14:10+00:00",
      "link": "https://arxiv.org/pdf/2601.17532v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Information gain pruning for generator-aligned reranking in RAG",
      "llm_evidence_cn": "RAG中生成器对齐重排序的信息增益剪枝",
      "llm_evidence": "RAG中生成器对齐重排序的信息增益剪枝",
      "llm_tldr_en": "Proposes IGP to select the most useful retrieved passages for RAG, improving performance under context constraints.",
      "llm_tldr_cn": "提出IGP方法为RAG选择最有用的检索段落，在上下文限制下提升性能。",
      "llm_tldr": "提出IGP方法为RAG选择最有用的检索段落，在上下文限制下提升性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.17532v1",
      "carry_days": 1
    },
    {
      "id": "2602.16113v1",
      "title": "Evolutionary Context Search for Automated Skill Acquisition",
      "abstract": "Large Language Models cannot reliably acquire new knowledge post-deployment -- even when relevant text resources exist, models fail to transform them into actionable knowledge without retraining. Retrieval-Augmented Generation attempts to bridge this gap by surfacing relevant documents at inference time, yet similarity-based retrieval often fails to identify context that actually improves task performance. We introduce Evolutionary Context Search (ECS), an evolutionary method that searches context combinations using accuracy on a small development set, requiring only inference calls without weight updates. ECS moves beyond semantic similarity to discover non-obvious context pairings that significantly boost performance. Our empirical results show that ECS improves BackendBench by 27\\% and $τ$-bench airline by 7\\%. The evolved contexts are model-agnostic, as those evolved with Gemini-3-Flash transfer effectively to Claude Sonnet and DeepSeek. This suggests that ECS opens a path toward automated context discovery for skill acquisition -- an efficient alternative to manual prompt engineering or costly fine-tuning.",
      "authors": [
        "Qi Sun",
        "Stefan Nielsen",
        "Rio Yokota",
        "Yujin Tang"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "published": "2026-02-18T00:47:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16113v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Evolutionary search for optimal context combinations in RAG",
      "llm_evidence_cn": "RAG中最佳上下文组合的演化搜索",
      "llm_evidence": "RAG中最佳上下文组合的演化搜索",
      "llm_tldr_en": "Proposes Evolutionary Context Search to find non-obvious document pairings that improve RAG performance.",
      "llm_tldr_cn": "提出演化上下文搜索，通过发现非显式的文档组合来提升RAG性能。",
      "llm_tldr": "提出演化上下文搜索，通过发现非显式的文档组合来提升RAG性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.16113v1",
      "carry_days": 1
    },
    {
      "id": "2601.21109v1",
      "title": "ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference",
      "abstract": "Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.",
      "authors": [
        "Ketan Thakkar",
        "Maitreyi Chatterjee",
        "Ramasubramanian Balasubramanian",
        "Achyuthan Jootoo",
        "Rajendra Ugrani"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-28T22:58:28+00:00",
      "link": "https://arxiv.org/pdf/2601.21109v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Adaptive sequence partitioning for memory-efficient LoRA",
      "llm_evidence_cn": "用于内存高效LoRA的自适应序列分区",
      "llm_evidence": "用于内存高效LoRA的自适应序列分区",
      "llm_tldr_en": "Proposes ChunkWise LoRA to dynamically adjust adaptation ranks based on token complexity.",
      "llm_tldr_cn": "提出ChunkWise LoRA，根据令牌复杂度动态调整适配秩，提升微调效率。",
      "llm_tldr": "提出ChunkWise LoRA，根据令牌复杂度动态调整适配秩，提升微调效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.21109v1",
      "carry_days": 1
    },
    {
      "id": "2602.13069v1",
      "title": "Memory-Efficient Structured Backpropagation for On-Device LLM Fine-Tuning",
      "abstract": "On-device fine-tuning enables privacy-preserving personalization of large language models, but mobile devices impose severe memory constraints, typically 6--12GB shared across all workloads. Existing approaches force a trade-off between exact gradients with high memory (MeBP) and low memory with noisy estimates (MeZO). We propose Memory-efficient Structured Backpropagation (MeSP), which bridges this gap by manually deriving backward passes that exploit LoRA's low-rank structure. Our key insight is that the intermediate projection $h = xA$ can be recomputed during backward at minimal cost since rank $r \\ll d_{in}$, eliminating the need to store it. MeSP achieves 49\\% average memory reduction compared to MeBP on Qwen2.5 models (0.5B--3B) while computing mathematically identical gradients. Our analysis also reveals that MeZO's gradient estimates show near-zero correlation with true gradients (cosine similarity $\\approx$0.001), explaining its slow convergence. MeSP reduces peak memory from 361MB to 136MB for Qwen2.5-0.5B, enabling fine-tuning scenarios previously infeasible on memory-constrained devices.",
      "authors": [
        "Juneyoung Park",
        "Yuri Hong",
        "Seongwan Kim",
        "Jaeho Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-13T16:24:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13069v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Memory-efficient backpropagation for on-device LoRA fine-tuning",
      "llm_evidence_cn": "用于端侧LoRA微调的内存高效反向传播",
      "llm_evidence": "用于端侧LoRA微调的内存高效反向传播",
      "llm_tldr_en": "Reduces memory usage during fine-tuning by exploiting the low-rank structure of LoRA layers.",
      "llm_tldr_cn": "利用LoRA层的低秩结构减少微调时的内存占用，支持端侧训练。",
      "llm_tldr": "利用LoRA层的低秩结构减少微调时的内存占用，支持端侧训练。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.13069v1",
      "carry_days": 1
    },
    {
      "id": "2602.12709v1",
      "title": "ReFilter: Improving Robustness of Retrieval-Augmented Generation via Gated Filter",
      "abstract": "Retrieval-augmented generation (RAG) has become a dominant paradigm for grounding large language models (LLMs) with external evidence in knowledge-intensive question answering. A core design choice is how to fuse retrieved samples into the LLMs, where existing internal fusion approaches broadly fall into query-based fusion, parametric fusion, and latent-based fusion. Despite their effectiveness at modest retrieval scales, these methods often fail to scale gracefully as the number of retrieved candidates k increases: Larger k improves evidence coverage, yet realistic top-k retrieval inevitably contains irrelevant or redundant content and increases the inference cost.   To address these limitations, we propose ReFilter, a novel latent-based fusion framework that performs token-level filtering and fusion. ReFilter consists of three key components: a context encoder for encoding context features, a gated filter for weighting each token, and a token fusion module for integrating the weighted token feature into the LLM's hidden states. Our experiments across four general-domain QA benchmarks show that ReFilter consistently achieves the best average performance under both in-domain adaptation and out-of-domain transfer. ReFilter further generalizes to five biomedical QA benchmarks in zero-shot transfer without domain fine-tuning, reaching 70.01% average accuracy with Qwen2.5-14B-Instruct.",
      "authors": [
        "Yixin Chen",
        "Ying Xiong",
        "Shangyu Wu",
        "Xiangrui Ke",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13T08:25:26+00:00",
      "link": "https://arxiv.org/pdf/2602.12709v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Improving RAG robustness via latent-based gated filtering",
      "llm_evidence_cn": "通过基于潜空间的门控过滤提高RAG鲁棒性",
      "llm_evidence": "通过基于潜空间的门控过滤提高RAG鲁棒性",
      "llm_tldr_en": "Proposes ReFilter to improve Retrieval-Augmented Generation by filtering irrelevant content at scale.",
      "llm_tldr_cn": "提出ReFilter，通过在大规模检索中过滤无关内容来增强检索增强生成的鲁棒性。",
      "llm_tldr": "提出ReFilter，通过在大规模检索中过滤无关内容来增强检索增强生成的鲁棒性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.12709v1",
      "carry_days": 1
    },
    {
      "id": "2601.17668v1",
      "title": "Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction",
      "abstract": "Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.",
      "authors": [
        "Jang-Hyun Kim",
        "Dongyoon Han",
        "Sangdoo Yun"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-25T03:07:54+00:00",
      "link": "https://arxiv.org/pdf/2601.17668v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "efficient and accurate LLM inference with KV cache eviction",
      "llm_evidence_cn": "通过KV缓存逐出实现高效准确的LLM推理",
      "llm_evidence": "通过KV缓存逐出实现高效准确的LLM推理",
      "llm_tldr_en": "Develops a gated KV cache eviction method to reduce memory overhead during LLM inference.",
      "llm_tldr_cn": "开发了一种门控KV缓存逐出方法，以减少LLM推理过程中的内存开销。",
      "llm_tldr": "开发了一种门控KV缓存逐出方法，以减少LLM推理过程中的内存开销。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.17668v1",
      "carry_days": 1
    },
    {
      "id": "2601.13222v1",
      "title": "Incorporating Q&A Nuggets into Retrieval-Augmented Generation",
      "abstract": "RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.",
      "authors": [
        "Laura Dietz",
        "Bryan Li",
        "Gabrielle Liu",
        "Jia-Huei Ju",
        "Eugene Yang",
        "Dawn Lawrie",
        "William Walden",
        "James Mayfield"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-01-19T16:57:33+00:00",
      "link": "https://arxiv.org/pdf/2601.13222v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Nugget-Augmented Generation for RAG systems",
      "llm_evidence_cn": "用于RAG系统的金块增强生成技术",
      "llm_evidence": "用于RAG系统的金块增强生成技术",
      "llm_tldr_en": "Introduces Crucible, a RAG system using Q&A nuggets to improve citation provenance and information extraction.",
      "llm_tldr_cn": "介绍Crucible系统，通过问答金块增强RAG的引用溯源和信息提取能力。",
      "llm_tldr": "介绍Crucible系统，通过问答金块增强RAG的引用溯源和信息提取能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.13222v1",
      "carry_days": 1
    },
    {
      "id": "2602.07525v1",
      "title": "IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory",
      "abstract": "Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text associations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. However, their misaligned memory organization necessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a framework inspired by human intuition-guided reasoning. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate realistic memory structures. During querying, IGMiRAG distills intuitive strategies via a question parser to control mining depth and memory window, and activates instantaneous memories as anchors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, emulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task complexity (average 6.3k+, minimum 3.0k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness.",
      "authors": [
        "Xingliang Hou",
        "Yuyan Liu",
        "Qi Sun",
        "haoxiu wang",
        "Hao Hu",
        "Shaoyi Du",
        "Zhiqiang Tian"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-07T12:42:31+00:00",
      "link": "https://arxiv.org/pdf/2602.07525v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Intuition-guided RAG with hypergraph memory",
      "llm_evidence_cn": "基于超图记忆的直觉引导RAG框架",
      "llm_evidence": "基于超图记忆的直觉引导RAG框架",
      "llm_tldr_en": "Proposes IGMiRAG, a framework using hierarchical hypergraphs to align knowledge for better RAG retrieval.",
      "llm_tldr_cn": "提出IGMiRAG框架，利用层次化超图对齐知识以优化RAG检索。",
      "llm_tldr": "提出IGMiRAG框架，利用层次化超图对齐知识以优化RAG检索。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.07525v1",
      "carry_days": 1
    },
    {
      "id": "2602.17856v1",
      "title": "Enhancing Scientific Literature Chatbots with Retrieval-Augmented Generation: A Performance Evaluation of Vector and Graph-Based Systems",
      "abstract": "This paper investigates the enhancement of scientific literature chatbots through retrieval-augmented generation (RAG), with a focus on evaluating vector- and graph-based retrieval systems. The proposed chatbot leverages both structured (graph) and unstructured (vector) databases to access scientific articles and gray literature, enabling efficient triage of sources according to research objectives. To systematically assess performance, we examine two use-case scenarios: retrieval from a single uploaded document and retrieval from a large-scale corpus. Benchmark test sets were generated using a GPT model, with selected outputs annotated for evaluation. The comparative analysis emphasizes retrieval accuracy and response relevance, providing insight into the strengths and limitations of each approach. The findings demonstrate the potential of hybrid RAG systems to improve accessibility to scientific knowledge and to support evidence-based decision making.",
      "authors": [
        "Hamideh Ghanadian",
        "Amin Kamali",
        "Mohammad Hossein Tekieh"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-19T21:42:02+00:00",
      "link": "https://arxiv.org/pdf/2602.17856v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Enhancing chatbots with vector and graph-based RAG systems",
      "llm_evidence_cn": "利用基于向量和图的RAG系统增强聊天机器人",
      "llm_evidence": "利用基于向量和图的RAG系统增强聊天机器人",
      "llm_tldr_en": "Evaluates RAG performance using vector and graph databases for scientific literature retrieval.",
      "llm_tldr_cn": "评估了在科学文献检索中使用向量和图数据库的RAG性能。",
      "llm_tldr": "评估了在科学文献检索中使用向量和图数据库的RAG性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.17856v1",
      "carry_days": 1
    },
    {
      "id": "2602.18750v1",
      "title": "HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD",
      "abstract": "Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\\times$ speedup over baselines while preserving model accuracy.",
      "authors": [
        "He Sun",
        "Li Li",
        "Mingjun Xiao"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR"
      ],
      "published": "2026-02-21T08:19:59+00:00",
      "link": "https://arxiv.org/pdf/2602.18750v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Efficient long-context LLM inference on edge devices",
      "llm_evidence_cn": "边缘设备上高效的长上下文LLM推理",
      "llm_evidence": "边缘设备上高效的长上下文LLM推理",
      "llm_tldr_en": "Presents HillInfer, a framework for efficient long-context inference on resource-constrained edge devices.",
      "llm_tldr_cn": "提出HillInfer框架，用于在资源受限的边缘设备上进行高效的长上下文推理。",
      "llm_tldr": "提出HillInfer框架，用于在资源受限的边缘设备上进行高效的长上下文推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.18750v1",
      "carry_days": 1
    },
    {
      "id": "2602.05512v2",
      "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
      "abstract": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
      "authors": [
        "Larissa Pusch",
        "Alexandre Courtiol",
        "Tim Conrad"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-05T10:10:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05512v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "LLM-centered architecture for knowledge-graph RAG",
      "llm_evidence_cn": "以大模型为核心的知识图谱检索增强生成架构",
      "llm_evidence": "以大模型为核心的知识图谱检索增强生成架构",
      "llm_tldr_en": "Introduces a framework combining LLMs with Knowledge Graphs to improve RAG accuracy and multi-hop reasoning.",
      "llm_tldr_cn": "引入了一个结合大模型与知识图谱的框架，以提高RAG的准确性和多跳推理能力。",
      "llm_tldr": "引入了一个结合大模型与知识图谱的框架，以提高RAG的准确性和多跳推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.05512v2",
      "carry_days": 1
    },
    {
      "id": "2602.00083v1",
      "title": "SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost.",
      "authors": [
        "Yuxin Yang",
        "Gangda Deng",
        "Ömer Faruk Akgül",
        "Nima Chitsazan",
        "Yash Govilkar",
        "Akasha Tigalappanavara",
        "Shi-Xiong Zhang",
        "Sambit Sahu",
        "Viktor Prasanna"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-01-22T20:18:55+00:00",
      "link": "https://arxiv.org/pdf/2602.00083v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "adaptive scaling for RAG in multi-hop question answering",
      "llm_evidence_cn": "多跳问答中检索增强生成的自适应缩放",
      "llm_evidence": "多跳问答中检索增强生成的自适应缩放",
      "llm_tldr_en": "SPARC-RAG manages sequential and parallel scaling to improve retrieval-augmented generation for complex tasks.",
      "llm_tldr_cn": "SPARC-RAG通过管理顺序和并行缩放，改进了复杂任务下的检索增强生成。",
      "llm_tldr": "SPARC-RAG通过管理顺序和并行缩放，改进了复杂任务下的检索增强生成。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.00083v1",
      "carry_days": 1
    },
    {
      "id": "2602.06072v1",
      "title": "PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference",
      "abstract": "Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.",
      "authors": [
        "Rui Ning",
        "Wei Zhang",
        "Fan Lai"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published": "2026-02-03T01:46:34+00:00",
      "link": "https://arxiv.org/pdf/2602.06072v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "efficient attention for batched LLM inference",
      "llm_evidence_cn": "批处理大模型推理的高效注意力机制",
      "llm_evidence": "批处理大模型推理的高效注意力机制",
      "llm_tldr_en": "PackInfer optimizes GPU utilization for heterogeneous LLM inference batches to improve deployment efficiency.",
      "llm_tldr_cn": "PackInfer优化了异构大模型推理批次的GPU利用率，提升了部署效率。",
      "llm_tldr": "PackInfer优化了异构大模型推理批次的GPU利用率，提升了部署效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.06072v1",
      "carry_days": 1
    },
    {
      "id": "2602.11937v1",
      "title": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration",
      "abstract": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.",
      "authors": [
        "Akhiad Bercovich",
        "Nir Ailon",
        "Vladimir Anisimov",
        "Tomer Asida",
        "Nave Assaf",
        "Mohammad Dabbah",
        "Ido Galil",
        "Amnon Geifman",
        "Yonatan Geifman",
        "Izhak Golan",
        "Roi Koren",
        "Itay Levy",
        "Zach Moshe",
        "Pavlo Molchanov",
        "Najeeb Nabwani",
        "Mostofa Patwari",
        "Omri Puny",
        "Tomer Ronen",
        "Itamar Schen",
        "Elad Segal",
        "Ido Shahaf",
        "Oren Tropp",
        "Ran Zilberstein",
        "Ran El-Yaniv"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12T13:36:19+00:00",
      "link": "https://arxiv.org/pdf/2602.11937v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Inference optimization for reasoning-focused MoE models",
      "llm_evidence_cn": "推理型混合专家模型的推理优化",
      "llm_evidence": "推理型混合专家模型的推理优化",
      "llm_tldr_en": "Applies NAS and quantization to optimize deployment and speed up reasoning-focused MoE models.",
      "llm_tldr_cn": "利用神经架构搜索和量化技术优化部署，加速长链推理型MoE模型的推理速度。",
      "llm_tldr": "利用神经架构搜索和量化技术优化部署，加速长链推理型MoE模型的推理速度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.11937v1",
      "carry_days": 1
    },
    {
      "id": "2602.18734v1",
      "title": "Rethinking Retrieval-Augmented Generation as a Cooperative Decision-Making Problem",
      "abstract": "Retrieval-Augmented Generation (RAG) has demonstrated strong effectiveness in knowledge-intensive tasks by grounding language generation in external evidence. Despite its success, many existing RAG systems are built based on a ranking-centric, asymmetric dependency paradigm, where the generation quality of the generator is highly dependent on reranking results of the reranker. To overcome this limitation, we reformulate RAG as a cooperative multi-agent decision-making problem and propose Cooperative Retrieval-Augmented Generation (CoRAG), a framework in which the reranker and the generator act as peer decision-makers rather than being connected through an asymmetric dependency pipeline. By jointly optimizing their behaviors toward a shared task objective, the reranker and generator are encouraged to cooperate, ensuring that document reranking and generation work in concert to improve the final response. Experimental results demonstrate good generalization and improved generation stability of CoRAG, even when the model is trained on only around 10K PopQA samples. Our model released in https://anonymous.4open.science/r/CoRAG-D63F",
      "authors": [
        "Lichang Song",
        "Ting Long",
        "Yi Chang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-21T06:32:36+00:00",
      "link": "https://arxiv.org/pdf/2602.18734v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Reformulates RAG as a cooperative multi-agent decision-making problem",
      "llm_evidence_cn": "将RAG重新表述为协作多智能体决策问题",
      "llm_evidence": "将RAG重新表述为协作多智能体决策问题",
      "llm_tldr_en": "Proposes CoRAG, a framework where reranker and generator act as peer decision-makers to improve RAG performance.",
      "llm_tldr_cn": "提出CoRAG框架，通过重排器与生成器的协同决策优化检索增强生成的效果。",
      "llm_tldr": "提出CoRAG框架，通过重排器与生成器的协同决策优化检索增强生成的效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.18734v1",
      "carry_days": 1
    },
    {
      "id": "2602.14452v1",
      "title": "WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity",
      "abstract": "Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.",
      "authors": [
        "Lei Chen",
        "Yuan Meng",
        "Xiaoyu Zhan",
        "Zhi Wang",
        "Wenwu Zhu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16T04:18:36+00:00",
      "link": "https://arxiv.org/pdf/2602.14452v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Boosting LLM inference efficiency with activation sparsity",
      "llm_evidence_cn": "通过激活稀疏性提升LLM推理效率",
      "llm_evidence": "通过激活稀疏性提升LLM推理效率",
      "llm_tldr_en": "Proposes WiSparse to leverage weight-aware activation sparsity for training-free LLM inference optimization.",
      "llm_tldr_cn": "提出WiSparse方法，利用权重感知的激活稀疏性实现无需训练的LLM推理加速。",
      "llm_tldr": "提出WiSparse方法，利用权重感知的激活稀疏性实现无需训练的LLM推理加速。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.14452v1",
      "carry_days": 1
    },
    {
      "id": "2602.10210v1",
      "title": "How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge",
      "abstract": "Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.",
      "authors": [
        "Junhong Lin",
        "Bing Zhang",
        "Song Wang",
        "Ziyan Liu",
        "Dan Gutfreund",
        "Julian Shun",
        "Yada Zhu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10T19:04:01+00:00",
      "link": "https://arxiv.org/pdf/2602.10210v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Benchmarking retrieval and reasoning for knowledge-intensive tasks",
      "llm_evidence_cn": "针对知识密集型任务的检索与推理基准测试",
      "llm_evidence": "针对知识密集型任务的检索与推理基准测试",
      "llm_tldr_en": "Introduces HybridRAG-Bench to evaluate how much reasoning RAG models add beyond parametric recall.",
      "llm_tldr_cn": "推出HybridRAG-Bench，用于评估RAG模型在知识密集型任务中超越参数记忆的推理能力。",
      "llm_tldr": "推出HybridRAG-Bench，用于评估RAG模型在知识密集型任务中超越参数记忆的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.10210v1",
      "carry_days": 1
    },
    {
      "id": "2602.13571v1",
      "title": "LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems",
      "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its efficacy depends on accurate document retrieval and ranking. Although existing rerankers demonstrate effectiveness, they frequently necessitate specialized training, impose substantial computational expenses, and fail to fully exploit the semantic capabilities of LLMs, particularly their inherent confidence signals. We propose the LLM-Confidence Reranker (LCR), a training-free, plug-and-play algorithm that enhances reranking in RAG systems by leveraging black-box LLM confidence derived from Maximum Semantic Cluster Proportion (MSCP). LCR employs a two-stage process: confidence assessment via multinomial sampling and clustering, followed by binning and multi-level sorting based on query and document confidence thresholds. This approach prioritizes relevant documents while preserving original rankings for high-confidence queries, ensuring robustness. Evaluated on BEIR and TREC benchmarks with BM25 and Contriever retrievers, LCR--using only 7--9B-parameter pre-trained LLMs--consistently improves NDCG@5 by up to 20.6% across pre-trained LLM and fine-tuned Transformer rerankers, without degradation. Ablation studies validate the hypothesis that LLM confidence positively correlates with document relevance, elucidating LCR's mechanism. LCR offers computational efficiency, parallelism for scalability, and broad compatibility, mitigating hallucinations in applications like medical diagnosis.",
      "authors": [
        "Zhipeng Song",
        "Xiangyu Kong",
        "Xinrui Bao",
        "Yizhi Zhou",
        "Jiulong Jiao",
        "Sitong Liu",
        "Yuhang Zhou",
        "Heng Qi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-14T03:12:05+00:00",
      "link": "https://arxiv.org/pdf/2602.13571v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Enhancing Retrieval-Augmented Generation systems",
      "llm_evidence_cn": "增强检索增强生成系统",
      "llm_evidence": "增强检索增强生成系统",
      "llm_tldr_en": "Presents a training-free reranker for RAG systems using LLM confidence signals to reduce hallucinations.",
      "llm_tldr_cn": "提出一种无需训练的RAG重排序算法，利用LLM置信信号减少幻觉。",
      "llm_tldr": "提出一种无需训练的RAG重排序算法，利用LLM置信信号减少幻觉。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.13571v1",
      "carry_days": 1
    },
    {
      "id": "2601.20810v1",
      "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs",
      "abstract": "Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph",
      "authors": [
        "Shahd Seddik",
        "Fahd Seddik",
        "Iman Saberi",
        "Fatemeh Fard",
        "Minh Hieu Huynh",
        "Patanamon Thongtanunam"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "published": "2026-01-28T17:58:30+00:00",
      "link": "https://arxiv.org/pdf/2601.20810v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Retrieval-Augmented Generation (RAG) for code generation",
      "llm_evidence_cn": "用于代码生成的检索增强生成 (RAG)",
      "llm_evidence": "用于代码生成的检索增强生成 (RAG)",
      "llm_tldr_en": "Proposes Programming Knowledge Graphs to improve RAG precision and reduce hallucinations in code generation tasks.",
      "llm_tldr_cn": "提出编程知识图谱，以提高代码生成任务中 RAG 的检索精度并减少幻觉。",
      "llm_tldr": "提出编程知识图谱，以提高代码生成任务中 RAG 的检索精度并减少幻觉。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.20810v1",
      "carry_days": 1
    },
    {
      "id": "2602.07356v1",
      "title": "Controllable Value Alignment in Large Language Models through Neuron-Level Editing",
      "abstract": "Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.",
      "authors": [
        "Yonghui Yang",
        "Junwei Li",
        "Jilong Liu",
        "Yicheng He",
        "Fengbin Zhu",
        "Weibiao Huang",
        "Le Wu",
        "Richang Hong",
        "Tat-Seng Chua"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-07T04:35:16+00:00",
      "link": "https://arxiv.org/pdf/2602.07356v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Neuron-level editing for controllable value alignment",
      "llm_evidence_cn": "用于可控价值对齐的神经元级编辑",
      "llm_evidence": "用于可控价值对齐的神经元级编辑",
      "llm_tldr_en": "Proposes NeVA, a framework for precise value alignment in LLMs by editing specific value-relevant neurons.",
      "llm_tldr_cn": "提出 NeVA 框架，通过编辑特定的价值相关神经元实现大模型的可控价值对齐。",
      "llm_tldr": "提出 NeVA 框架，通过编辑特定的价值相关神经元实现大模型的可控价值对齐。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.07356v1",
      "carry_days": 1
    },
    {
      "id": "2601.11340v1",
      "title": "Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models",
      "abstract": "Chain-of-Thought reasoning has significantly enhanced the problem-solving capabilities of Large Language Models. Unfortunately, current models generate reasoning steps sequentially without foresight, often becoming trapped in suboptimal reasoning paths with redundant steps. In contrast, we introduce Neural Chain-of-Thought Search (NCoTS), a framework that reformulates reasoning as a dynamic search for the optimal thinking strategy. By quantitatively characterizing the solution space, we reveal the existence of sparse superior reasoning paths that are simultaneously more accurate and concise than standard outputs. Our method actively navigates towards these paths by evaluating candidate reasoning operators using a dual-factor heuristic that optimizes for both correctness and computational cost. Consequently, NCoTS achieves a Pareto improvement across diverse reasoning benchmarks, boosting accuracy by over 3.5% while reducing generation length by over 22%. Our code and data are available at https://github.com/MilkThink-Lab/Neural-CoT-Search.",
      "authors": [
        "Guoming Ling",
        "Zhongzhan Huang",
        "Yupei Lin",
        "Junxin Li",
        "Shanshan Zhong",
        "Hefeng Wu",
        "Liang Lin"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-16T14:38:18+00:00",
      "link": "https://arxiv.org/pdf/2601.11340v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Neural Chain-of-Thought Search for optimal reasoning",
      "llm_evidence_cn": "用于最优推理的神经思维链搜索",
      "llm_evidence": "用于最优推理的神经思维链搜索",
      "llm_tldr_en": "Reformulates reasoning as a dynamic search to find more accurate and concise Chain-of-Thought paths.",
      "llm_tldr_cn": "将推理重新表述为动态搜索，以寻找更准确、更简洁的思维链路径。",
      "llm_tldr": "将推理重新表述为动态搜索，以寻找更准确、更简洁的思维链路径。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2601.11340v1",
      "carry_days": 1
    },
    {
      "id": "2602.03442v1",
      "title": "A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces",
      "abstract": "Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.",
      "authors": [
        "Mingxuan Du",
        "Benfeng Xu",
        "Chiwei Zhu",
        "Shaohan Wang",
        "Pengyu Wang",
        "Xiaorui Wang",
        "Zhendong Mao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-03T12:07:21+00:00",
      "link": "https://arxiv.org/pdf/2602.03442v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "agentic RAG via hierarchical retrieval interfaces",
      "llm_evidence_cn": "通过分层检索接口实现智能体RAG",
      "llm_evidence": "通过分层检索接口实现智能体RAG",
      "llm_tldr_en": "Introduces A-RAG, an agentic framework that allows LLMs to actively participate in retrieval decisions.",
      "llm_tldr_cn": "引入A-RAG框架，使LLM能够主动参与检索决策以增强RAG性能。",
      "llm_tldr": "引入A-RAG框架，使LLM能够主动参与检索决策以增强RAG性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.03442v1",
      "carry_days": 1
    },
    {
      "id": "2602.04265v1",
      "title": "Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes \"thickening\" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to \"thinning\", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.",
      "authors": [
        "Wenze Lin",
        "Zhen Yang",
        "Xitai Jiang",
        "Pony Ma",
        "Gao Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-04T06:55:58+00:00",
      "link": "https://arxiv.org/pdf/2602.04265v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "reward shaping for LLM reasoning capabilities",
      "llm_evidence_cn": "针对LLM推理能力的奖励塑造",
      "llm_evidence": "针对LLM推理能力的奖励塑造",
      "llm_tldr_en": "Introduces a dynamic reward framework to enhance reasoning and problem-solving in LLMs via RL.",
      "llm_tldr_cn": "引入动态奖励框架，通过强化学习增强LLM的推理和问题解决能力。",
      "llm_tldr": "引入动态奖励框架，通过强化学习增强LLM的推理和问题解决能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.04265v1",
      "carry_days": 1
    },
    {
      "id": "2602.11149v1",
      "title": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning",
      "abstract": "Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.",
      "authors": [
        "Dawid J. Kopiczko",
        "Sagar Vaze",
        "Tijmen Blankevoort",
        "Yuki M. Asano"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11T18:58:54+00:00",
      "link": "https://arxiv.org/pdf/2602.11149v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "SFT on chain-of-thought data for reasoning language models",
      "llm_evidence_cn": "针对推理语言模型的思维链数据有监督微调",
      "llm_evidence": "针对推理语言模型的思维链数据有监督微调",
      "llm_tldr_en": "Shows that repeating small CoT datasets during SFT can outperform single-epoch training on larger datasets.",
      "llm_tldr_cn": "研究表明在SFT过程中重复使用小型CoT数据集的效果优于单次训练大型数据集。",
      "llm_tldr": "研究表明在SFT过程中重复使用小型CoT数据集的效果优于单次训练大型数据集。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.11149v1",
      "carry_days": 1
    },
    {
      "id": "2601.21698v1",
      "title": "Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics",
      "abstract": "Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.",
      "authors": [
        "Mohamed Elgaar",
        "Hadi Amiri"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-29T13:30:18+00:00",
      "link": "https://arxiv.org/pdf/2601.21698v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Curriculum learning for LLM pretraining dynamics",
      "llm_evidence_cn": "大语言模型预训练中的课程学习动态",
      "llm_evidence": "大语言模型预训练中的课程学习动态",
      "llm_tldr_en": "Analyzes how data ordering affects the learning trajectory and efficiency during LLM pre-training.",
      "llm_tldr_cn": "分析了数据排序如何影响大语言模型预训练期间的学习轨迹和效率。",
      "llm_tldr": "分析了数据排序如何影响大语言模型预训练期间的学习轨迹和效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2601.21698v1",
      "carry_days": 1
    },
    {
      "id": "2602.14696v1",
      "title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)",
      "abstract": "Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.",
      "authors": [
        "Nihal V. Nayak",
        "Paula Rodriguez-Diaz",
        "Neha Hulkund",
        "Sara Beery",
        "David Alvarez-Melis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16T12:33:05+00:00",
      "link": "https://arxiv.org/pdf/2602.14696v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Instruction fine-tuning and data selection algorithms",
      "llm_evidence_cn": "指令微调与数据选择算法",
      "llm_evidence": "指令微调与数据选择算法",
      "llm_tldr_en": "Analyzes data representation and selection algorithms for targeted instruction fine-tuning of LLMs.",
      "llm_tldr_cn": "分析了大语言模型针对性指令微调中的数据表示和选择算法。",
      "llm_tldr": "分析了大语言模型针对性指令微调中的数据表示和选择算法。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.14696v1",
      "carry_days": 1
    },
    {
      "id": "2602.17831v1",
      "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels",
      "abstract": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.",
      "authors": [
        "Simon Henniger",
        "Gabriel Poesia"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-19T20:49:15+00:00",
      "link": "https://arxiv.org/pdf/2602.17831v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Evaluating language model reasoning with puzzle duels",
      "llm_evidence_cn": "通过谜题对决评估语言模型的推理能力",
      "llm_evidence": "通过谜题对决评估语言模型的推理能力",
      "llm_tldr_en": "Proposes a framework where models generate puzzles to evaluate genuine reasoning capabilities.",
      "llm_tldr_cn": "提出了一个模型生成谜题以评估真实推理能力的框架。",
      "llm_tldr": "提出了一个模型生成谜题以评估真实推理能力的框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.17831v1",
      "carry_days": 1
    },
    {
      "id": "2602.00482v1",
      "title": "AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models",
      "abstract": "Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\\times$ in $τ^2$-bench higher training throughput.",
      "authors": [
        "Jiarui Zhang",
        "Yuchen Yang",
        "Ran Yan",
        "Zhiyu Mei",
        "Liyuan Zhang",
        "Daifeng Li",
        "Wei Fu",
        "Jiaxuan Gao",
        "Shusheng Xu",
        "Yi Wu",
        "Binhang Yuan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-31T03:05:34+00:00",
      "link": "https://arxiv.org/pdf/2602.00482v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient reinforcement learning for LLM post-training",
      "llm_evidence_cn": "大语言模型后训练的高效强化学习",
      "llm_evidence": "大语言模型后训练的高效强化学习",
      "llm_tldr_en": "Introduces dynamic tree attention to optimize prefix sharing during LLM reinforcement learning.",
      "llm_tldr_cn": "引入动态树注意力机制，以优化LLM强化学习过程中的前缀共享。",
      "llm_tldr": "引入动态树注意力机制，以优化LLM强化学习过程中的前缀共享。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.00482v1",
      "carry_days": 1
    },
    {
      "id": "2602.05695v2",
      "title": "SweetSpot: An Analytical Model for Predicting Energy Efficiency of LLM Inference",
      "abstract": "Large Language Models (LLMs) inference is central to modern AI applications, dominating worldwide datacenter workloads, making it critical to predict its energy footprint. Existing approaches estimate energy consumption as a simple linear function of input and output sequence. However, by analyzing the autoregressive structure of Transformers, which implies a fundamentally non-linear relationship between input and output sequence lengths and energy consumption, we demonstrate the existence of a generation energy minima. Peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs. Consequently, we propose SweetSpot, an analytical model derived from the computational and memory-access complexity of the Transformer architecture, which accurately characterizes the efficiency curve as a function of input and output lengths. To assess accuracy, we measure energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite. We test input and output lengths from 64 to 4096 tokens and achieve a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"sweet spots\" reduce energy usage, up to 33.41x, enabling informed truncation, summarization, and adaptive generation strategies in production systems.",
      "authors": [
        "Hiari Pizzini Cavagna",
        "Andrea Proia",
        "Giacomo Madella",
        "Giovanni B. Esposito",
        "Francesco Antici",
        "Daniele Cesarini",
        "Zeynep Kiziltan",
        "Andrea Bartolini"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.PF"
      ],
      "published": "2026-02-05T14:21:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05695v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Analytical model for predicting energy efficiency of LLM inference",
      "llm_evidence_cn": "预测大语言模型推理能效的分析模型",
      "llm_evidence": "预测大语言模型推理能效的分析模型",
      "llm_tldr_en": "Proposes SweetSpot to predict and optimize the energy consumption of LLM inference based on sequence lengths.",
      "llm_tldr_cn": "提出SweetSpot模型，通过分析序列长度来预测和优化大语言模型推理的能效。",
      "llm_tldr": "提出SweetSpot模型，通过分析序列长度来预测和优化大语言模型推理的能效。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.05695v2",
      "carry_days": 1
    },
    {
      "id": "2602.04248v1",
      "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
      "abstract": "Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.",
      "authors": [
        "Hao Lu",
        "Haoyuan Huang",
        "Yulin Zhou",
        "Chen Li",
        "Ningxin Zhu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T06:14:55+00:00",
      "link": "https://arxiv.org/pdf/2602.04248v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "MCTS for enhancing reasoning capabilities of LLMs",
      "llm_evidence_cn": "利用蒙特卡洛树搜索增强大模型的推理能力",
      "llm_evidence": "利用蒙特卡洛树搜索增强大模型的推理能力",
      "llm_tldr_en": "Introduces Empirical-MCTS to transform stateless search into continuous learning for LLM reasoning.",
      "llm_tldr_cn": "引入Empirical-MCTS，将无状态搜索转化为大模型推理的持续学习过程。",
      "llm_tldr": "引入Empirical-MCTS，将无状态搜索转化为大模型推理的持续学习过程。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.04248v1",
      "carry_days": 1
    },
    {
      "id": "2601.11676v1",
      "title": "HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network",
      "abstract": "The deployment of large language models' (LLMs) inference at the edge can facilitate prompt service responsiveness while protecting user privacy. However, it is critically challenged by the resource constraints of a single edge node. Distributed inference has emerged to aggregate and leverage computational resources across multiple devices. Yet, existing methods typically require strict synchronization, which is often infeasible due to the unreliable network conditions. In this paper, we propose HALO, a novel framework that can boost the distributed LLM inference in lossy edge network. The core idea is to enable a relaxed yet effective synchronization by strategically allocating less critical neuron groups to unstable devices, thus avoiding the excessive waiting time incurred by delayed packets. HALO introduces three key mechanisms: (1) a semantic-aware predictor to assess the significance of neuron groups prior to activation. (2) a parallel execution scheme of neuron group loading during the model inference. (3) a load-balancing scheduler that efficiently orchestrates multiple devices with heterogeneous resources. Experimental results from a Raspberry Pi cluster demonstrate that HALO achieves a 3.41x end-to-end speedup for LLaMA-series LLMs under unreliable network conditions. It maintains performance comparable to optimal conditions and significantly outperforms the state-of-the-art in various scenarios.",
      "authors": [
        "Peirong Zheng",
        "Wenchao Xu",
        "Haozhao Wang",
        "Jinyu Chen",
        "Xuemin Shen"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "published": "2026-01-16T07:37:23+00:00",
      "link": "https://arxiv.org/pdf/2601.11676v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Distributed LLM inference in lossy edge networks",
      "llm_evidence_cn": "有损边缘网络中的分布式大模型推理",
      "llm_evidence": "有损边缘网络中的分布式大模型推理",
      "llm_tldr_en": "Proposes HALO framework to optimize distributed LLM inference under unreliable network conditions.",
      "llm_tldr_cn": "提出HALO框架，优化不可靠网络环境下的分布式大模型推理。",
      "llm_tldr": "提出HALO框架，优化不可靠网络环境下的分布式大模型推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.11676v1",
      "carry_days": 1
    },
    {
      "id": "2602.03652v1",
      "title": "RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.",
      "authors": [
        "Süha Kağan Köse",
        "Mehmet Can Baytekin",
        "Burak Aktaş",
        "Bilge Kaan Görür",
        "Evren Ayberk Munis",
        "Deniz Yılmaz",
        "Muhammed Yusuf Kartal",
        "Çağrı Toraman"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2026-02-03T15:35:11+00:00",
      "link": "https://arxiv.org/pdf/2602.03652v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Best practices for Retrieval Augmented Generation in Turkish",
      "llm_evidence_cn": "土耳其语检索增强生成的最佳实践",
      "llm_evidence": "土耳其语检索增强生成的最佳实践",
      "llm_tldr_en": "Benchmarks RAG pipeline stages for morphologically rich languages like Turkish.",
      "llm_tldr_cn": "针对土耳其语等形态丰富语言的RAG流水线各阶段进行基准测试。",
      "llm_tldr": "针对土耳其语等形态丰富语言的RAG流水线各阶段进行基准测试。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.03652v1",
      "carry_days": 1
    },
    {
      "id": "2601.05503v1",
      "title": "Over-Searching in Search-Augmented Large Language Models",
      "abstract": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
      "authors": [
        "Roy Xie",
        "Deepak Gopinath",
        "David Qiu",
        "Dong Lin",
        "Haitian Sun",
        "Saloni Potdar",
        "Bhuwan Dhingra"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-09T03:24:46+00:00",
      "link": "https://arxiv.org/pdf/2601.05503v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Evaluation of over-searching in search-augmented LLMs",
      "llm_evidence_cn": "搜索增强大模型中过度搜索的评估",
      "llm_evidence": "搜索增强大模型中过度搜索的评估",
      "llm_tldr_en": "Analyzes the efficiency and hallucination risks of unnecessary retrieval in search-augmented LLMs.",
      "llm_tldr_cn": "分析搜索增强大模型中不必要检索带来的效率和幻觉风险。",
      "llm_tldr": "分析搜索增强大模型中不必要检索带来的效率和幻觉风险。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.05503v1",
      "carry_days": 1
    },
    {
      "id": "2601.12323v2",
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.",
      "authors": [
        "Yin Cai",
        "Zhouhong Gu",
        "Juntao Zhang",
        "Ping Chen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-18T09:10:08+00:00",
      "link": "https://arxiv.org/pdf/2601.12323v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Learning reasoning from multi-agent social interaction",
      "llm_evidence_cn": "通过多智能体社会交互学习推理能力",
      "llm_evidence": "通过多智能体社会交互学习推理能力",
      "llm_tldr_en": "Proposes MARO to enhance LLM reasoning through multi-agent social interaction and reward optimization.",
      "llm_tldr_cn": "提出MARO方法，通过多智能体社交环境中的交互和奖励优化来增强大模型的推理能力。",
      "llm_tldr": "提出MARO方法，通过多智能体社交环境中的交互和奖励优化来增强大模型的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.12323v2",
      "carry_days": 1
    },
    {
      "id": "2601.17918v1",
      "title": "Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models",
      "abstract": "Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.",
      "authors": [
        "Dain Kim",
        "Jiwoo Lee",
        "Jaehoon Yun",
        "Yong Hoe Koo",
        "Qingyu Chen",
        "Hyunjae Kim",
        "Jaewoo Kang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "published": "2026-01-25T17:36:53+00:00",
      "link": "https://arxiv.org/pdf/2601.17918v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Benchmarking DPO variants for medical LVLMs",
      "llm_evidence_cn": "在医疗多模态大模型上评估DPO变体",
      "llm_evidence": "在医疗多模态大模型上评估DPO变体",
      "llm_tldr_en": "Evaluates Direct Preference Optimization (DPO) variants for aligning medical vision-language models.",
      "llm_tldr_cn": "在医疗多模态大模型领域对多种直接偏好优化（DPO）变体进行了全面的基准测试和评估。",
      "llm_tldr": "在医疗多模态大模型领域对多种直接偏好优化（DPO）变体进行了全面的基准测试和评估。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2601.17918v1",
      "carry_days": 1
    },
    {
      "id": "2602.05708v1",
      "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
      "abstract": "Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.",
      "authors": [
        "Chuangtao Ma",
        "Zeyu Zhang",
        "Arijit Khan",
        "Sebastian Schelter",
        "Paul Groth"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.CL"
      ],
      "published": "2026-02-05T14:33:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05708v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cost-efficient RAG for entity matching tasks",
      "llm_evidence_cn": "用于实体匹配任务的高效RAG架构",
      "llm_evidence": "用于实体匹配任务的高效RAG架构",
      "llm_tldr_en": "Introduces CE-RAG4EM to reduce computation costs in RAG-based entity matching for knowledge-intensive tasks.",
      "llm_tldr_cn": "引入CE-RAG4EM架构，通过分块批处理降低RAG在知识密集型实体匹配任务中的计算开销。",
      "llm_tldr": "引入CE-RAG4EM架构，通过分块批处理降低RAG在知识密集型实体匹配任务中的计算开销。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.05708v1",
      "carry_days": 1
    },
    {
      "id": "2602.22090v1",
      "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
      "abstract": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20\\% to 40\\%. When applied to GPT-4o API calls, it reduces token usage by approximately 60\\%, further improving cost efficiency. These findings indicate the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications.",
      "authors": [
        "Bo-Wei Chen",
        "Chung-Chi Chen",
        "An-Zi Yen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-25T16:38:03+00:00",
      "link": "https://arxiv.org/pdf/2602.22090v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Dynamic model selection for cost-efficient inference",
      "llm_evidence_cn": "用于高效推理的动态模型选择策略",
      "llm_evidence": "用于高效推理的动态模型选择策略",
      "llm_tldr_en": "Proposes a confidence-driven strategy to select between small and large models for efficient LLM inference.",
      "llm_tldr_cn": "提出一种基于置信度的动态模型选择策略，通过在大小模型间切换来优化推理效率和成本。",
      "llm_tldr": "提出一种基于置信度的动态模型选择策略，通过在大小模型间切换来优化推理效率和成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.22090v1",
      "carry_days": 1
    },
    {
      "id": "2601.09402v1",
      "title": "Structured Knowledge Representation through Contextual Pages for Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating external knowledge. Recently, some works have incorporated iterative knowledge accumulation processes into RAG models to progressively accumulate and refine query-related knowledge, thereby constructing more comprehensive knowledge representations. However, these iterative processes often lack a coherent organizational structure, which limits the construction of more comprehensive and cohesive knowledge representations. To address this, we propose PAGER, a page-driven autonomous knowledge representation framework for RAG. PAGER first prompts an LLM to construct a structured cognitive outline for a given question, which consists of multiple slots representing a distinct knowledge aspect. Then, PAGER iteratively retrieves and refines relevant documents to populate each slot, ultimately constructing a coherent page that serves as contextual input for guiding answer generation. Experiments on multiple knowledge-intensive benchmarks and backbone models show that PAGER consistently outperforms all RAG baselines. Further analyses demonstrate that PAGER constructs higher-quality and information-dense knowledge representations, better mitigates knowledge conflicts, and enables LLMs to leverage external knowledge more effectively. All code is available at https://github.com/OpenBMB/PAGER.",
      "authors": [
        "Xinze Li",
        "Zhenghao Liu",
        "Haidong Xin",
        "Yukun Yan",
        "Shuo Wang",
        "Zheni Zeng",
        "Sen Mei",
        "Ge Yu",
        "Maosong Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-14T11:44:31+00:00",
      "link": "https://arxiv.org/pdf/2601.09402v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Structured knowledge representation framework for RAG",
      "llm_evidence_cn": "用于RAG的结构化知识表示框架",
      "llm_evidence": "用于RAG的结构化知识表示框架",
      "llm_tldr_en": "PAGER improves RAG by using LLMs to construct structured cognitive outlines for better knowledge representation.",
      "llm_tldr_cn": "提出PAGER框架，通过构建结构化认知大纲来改进RAG系统中的知识表示和检索效果。",
      "llm_tldr": "提出PAGER框架，通过构建结构化认知大纲来改进RAG系统中的知识表示和检索效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.09402v1",
      "carry_days": 1
    },
    {
      "id": "2601.13697v1",
      "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
      "abstract": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.",
      "authors": [
        "Zhihang Yuan",
        "Chengyu Yue",
        "Long Huang",
        "Litu Ou",
        "Lei Shi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-20T07:51:32+00:00",
      "link": "https://arxiv.org/pdf/2601.13697v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Uncertainty-aware data selection for instruction tuning",
      "llm_evidence_cn": "用于指令微调的不确定性感知数据筛选",
      "llm_evidence": "用于指令微调的不确定性感知数据筛选",
      "llm_tldr_en": "Proposes GRADFILTERING to select high-quality data for efficient LLM instruction tuning using gradient signals.",
      "llm_tldr_cn": "提出GRADFILTERING框架，利用梯度信噪比进行数据筛选，以实现更高效的指令微调。",
      "llm_tldr": "提出GRADFILTERING框架，利用梯度信噪比进行数据筛选，以实现更高效的指令微调。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.13697v1",
      "carry_days": 1
    },
    {
      "id": "2602.05145v1",
      "title": "TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference",
      "abstract": "Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptation directly into high-performance LLM inference systems. TIDE reuses target model hidden states generated during inference as training signals, enabling zero-overhead draft adaptation without reloading the target model, and employs adaptive runtime control to activate speculation and training only when beneficial. TIDE exploits heterogeneous clusters by mapping decoupled inference and training to appropriate GPU classes. Across diverse real-world workloads, TIDE achieves up to 1.15x throughput improvement over static speculative decoding while reducing draft training time by 1.67x compared to approaches that recompute training signals.",
      "authors": [
        "Jiyoung Park",
        "Hankyu Jang",
        "Changseok Song",
        "Wookeun Jung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T00:06:12+00:00",
      "link": "https://arxiv.org/pdf/2602.05145v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Self-improving framework for LLM inference acceleration",
      "llm_evidence_cn": "用于大模型推理加速的自改进框架",
      "llm_evidence": "用于大模型推理加速的自改进框架",
      "llm_tldr_en": "TIDE accelerates LLM inference through an online adaptive speculative decoding engine.",
      "llm_tldr_cn": "提出TIDE框架，通过在线自适应的投机采样技术显著加速大语言模型的推理过程。",
      "llm_tldr": "提出TIDE框架，通过在线自适应的投机采样技术显著加速大语言模型的推理过程。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.05145v1",
      "carry_days": 1
    },
    {
      "id": "2602.09109v1",
      "title": "Distributed Hybrid Parallelism for Large Language Models: Comparative Study and System Design Guide",
      "abstract": "With the rapid growth of large language models (LLMs), a wide range of methods have been developed to distribute computation and memory across hardware devices for efficient training and inference. While existing surveys provide descriptive overviews of these techniques, systematic analysis of their benefits and trade offs and how such insights can inform principled methodology for designing optimal distributed systems remain limited. This paper offers a comprehensive review of collective operations and distributed parallel strategies, complemented by mathematical formulations to deepen theoretical understanding. We further examine hybrid parallelization designs, emphasizing communication computation overlap across different stages of model deployment, including both training and inference. Recent advances in automated search for optimal hybrid parallelization strategies using cost models are also discussed. Moreover, we present case studies with mainstream architecture categories to reveal empirical insights to guide researchers and practitioners in parallelism strategy selection. Finally, we highlight open challenges and limitations of current LLM training paradigms and outline promising directions for the next generation of large scale model development.",
      "authors": [
        "Hossam Amer",
        "Rezaul Karim",
        "Ali Pourranjbar",
        "Weiwei Zhang",
        "Walid Ahmed",
        "Boxing Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC"
      ],
      "published": "2026-02-09T19:01:13+00:00",
      "link": "https://arxiv.org/pdf/2602.09109v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "distributed parallel strategies for efficient LLM training and inference",
      "llm_evidence_cn": "用于高效LLM训练和推理的分布式并行策略",
      "llm_evidence": "用于高效LLM训练和推理的分布式并行策略",
      "llm_tldr_en": "Provides a system design guide for distributed hybrid parallelism to optimize LLM computation and memory usage.",
      "llm_tldr_cn": "提供分布式混合并行系统设计指南，以优化大语言模型的计算和内存使用。",
      "llm_tldr": "提供分布式混合并行系统设计指南，以优化大语言模型的计算和内存使用。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.09109v1",
      "carry_days": 1
    },
    {
      "id": "2601.07475v1",
      "title": "ARCQuant: Boosting NVFP4 Quantization with Augmented Residual Channels for LLMs",
      "abstract": "The emergence of fine-grained numerical formats like NVFP4 presents new opportunities for efficient Large Language Model (LLM) inference. However, it is difficult to adapt existing Post-Training Quantization (PTQ) strategies to these formats: rotation-based methods compromise fine-grained block isolation; smoothing techniques struggle with significant 4-bit quantization errors; and mixed-precision approaches often conflict with hardware constraints on unified-precision computation. To address these challenges, we propose ARCQuant, a framework that boosts NVFP4 performance via Augmented Residual Channels. Distinct from methods that compromise block isolation or hardware uniformity, ARCQuant maintains a strictly unified NVFP4 format by augmenting the activation matrix with quantized residual channels. This design integrates the error compensation process directly into the matrix reduction dimension, enabling the use of standard, highly optimized GEMM kernels with minimal overhead. Theoretical analysis confirms that the worst-case error bound of our dual-stage NVFP4 quantization is comparable to that of standard 8-bit formats such as MXFP8. Extensive experiments on LLaMA and Qwen models demonstrate that ARCQuant achieves state-of-the-art accuracy, comparable to full-precision baselines in perplexity and downstream tasks. Furthermore, deployment on RTX 5090 and RTX PRO 6000 GPUs confirms practical benefits, achieving up to 3x speedup over FP16. Our code is available at https://github.com/actypedef/ARCQuant .",
      "authors": [
        "Haoqian Meng",
        "Yilun Luo",
        "Yafei Zhao",
        "Wenyuan Liu",
        "Peng Zhang",
        "Xindian Ma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-12T12:27:22+00:00",
      "link": "https://arxiv.org/pdf/2601.07475v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "boosting NVFP4 quantization for efficient LLM inference",
      "llm_evidence_cn": "提升NVFP4量化性能以实现高效LLM推理",
      "llm_evidence": "提升NVFP4量化性能以实现高效LLM推理",
      "llm_tldr_en": "Presents ARCQuant, a framework that uses augmented residual channels to improve 4-bit quantization for LLM deployment.",
      "llm_tldr_cn": "提出ARCQuant框架，通过增强残差通道提升LLM的4位量化推理性能。",
      "llm_tldr": "提出ARCQuant框架，通过增强残差通道提升LLM的4位量化推理性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.07475v1",
      "carry_days": 1
    },
    {
      "id": "2602.14490v1",
      "title": "Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts",
      "abstract": "Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.",
      "authors": [
        "Buze Zhang",
        "Jinkai Tao",
        "Zilang Zeng",
        "Neil He",
        "Ali Maatouk",
        "Menglin Yang",
        "Rex Ying"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "published": "2026-02-16T06:07:32+00:00",
      "link": "https://arxiv.org/pdf/2602.14490v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "parameter-efficient fine-tuning using mixture of geometric spaces",
      "llm_evidence_cn": "使用混合几何空间的参数高效微调",
      "llm_evidence": "使用混合几何空间的参数高效微调",
      "llm_tldr_en": "Proposes Mixture of Space (MoS) to enhance PEFT by leveraging multiple geometric manifolds for LLM adaptation.",
      "llm_tldr_cn": "提出Mixture of Space (MoS)框架，利用多种几何流形增强LLM的参数高效微调效果。",
      "llm_tldr": "提出Mixture of Space (MoS)框架，利用多种几何流形增强LLM的参数高效微调效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.14490v1",
      "carry_days": 1
    },
    {
      "id": "2602.03075v1",
      "title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution",
      "abstract": "Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.",
      "authors": [
        "Junjie Huang",
        "Jiarui Qin",
        "Di Yin",
        "Weiwen Liu",
        "Yong Yu",
        "Xing Sun",
        "Weinan Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-03T04:04:41+00:00",
      "link": "https://arxiv.org/pdf/2602.03075v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "RL-guided mid-training to improve pre-trained foundation",
      "llm_evidence_cn": "强化学习引导的中期训练以改进预训练基础",
      "llm_evidence": "强化学习引导的中期训练以改进预训练基础",
      "llm_tldr_en": "ReMiT introduces a bidirectional training flywheel where RL-tuned models enhance the base model during annealing.",
      "llm_tldr_cn": "ReMiT引入双向训练机制，通过强化学习微调模型在退火阶段增强基础模型。",
      "llm_tldr": "ReMiT引入双向训练机制，通过强化学习微调模型在退火阶段增强基础模型。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.03075v1",
      "carry_days": 1
    },
    {
      "id": "2602.17520v1",
      "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
      "abstract": "Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local definitions. In this work, we study a systematic failure mode we term semantic override, in which an LLM reverts to its pretrained default interpretation of operators or gate behavior despite explicit redefinition in the prompt. We also identify a related class of errors, assumption injection, where models commit to unstated hardware semantics when critical details are underspecified, rather than requesting clarification. We introduce a compact micro-benchmark of 30 logic and digital-circuit reasoning tasks designed as verifier-style traps, spanning Boolean algebra, operator overloading, redefined gates, and circuit-level semantics. Evaluating three frontier LLMs, we observe persistent noncompliance with local specifications, confident but incompatible assumptions, and dropped constraints even in elementary settings. Our findings highlight a gap between surface-level correctness and specification-faithful reasoning, motivating evaluation protocols that explicitly test local unlearning and semantic compliance in formal domains.",
      "authors": [
        "Yogeswar Reddy Thota",
        "Setareh Rafatirad",
        "Homayoun Houman",
        "Tooraj Nikoubin"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR"
      ],
      "published": "2026-02-19T16:33:46+00:00",
      "link": "https://arxiv.org/pdf/2602.17520v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Measuring semantic override hallucinations in LLM reasoning",
      "llm_evidence_cn": "衡量LLM推理中的语义覆盖幻觉",
      "llm_evidence": "衡量LLM推理中的语义覆盖幻觉",
      "llm_tldr_en": "Studies failure modes where LLMs ignore prompt-local definitions in favor of pre-trained conventions during reasoning.",
      "llm_tldr_cn": "研究LLM在推理过程中忽略提示词定义而倾向于预训练惯例的失效模式。",
      "llm_tldr": "研究LLM在推理过程中忽略提示词定义而倾向于预训练惯例的失效模式。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.17520v1",
      "carry_days": 1
    },
    {
      "id": "2602.15156v1",
      "title": "Panini: Continual Learning in Token Space via Structured Memory",
      "abstract": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.",
      "authors": [
        "Shreyas Rajesh",
        "Pavan Holur",
        "Mehmet Yigit Turali",
        "Chenda Duan",
        "Vwani Roychowdhury"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-16T19:58:03+00:00",
      "link": "https://arxiv.org/pdf/2602.15156v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Non-parametric continual learning framework for RAG",
      "llm_evidence_cn": "用于RAG的非参数持续学习框架",
      "llm_evidence": "用于RAG的非参数持续学习框架",
      "llm_tldr_en": "Panini improves RAG efficiency by integrating new information into a structured external semantic memory.",
      "llm_tldr_cn": "Panini通过将新信息整合到结构化外部语义内存中，提高了RAG的推理效率。",
      "llm_tldr": "Panini通过将新信息整合到结构化外部语义内存中，提高了RAG的推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.15156v1",
      "carry_days": 1
    },
    {
      "id": "2601.12538v1",
      "title": "Agentic Reasoning for Large Language Models",
      "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
      "authors": [
        "Tianxin Wei",
        "Ting-Wei Li",
        "Zhining Liu",
        "Xuying Ning",
        "Ze Yang",
        "Jiaru Zou",
        "Zhichen Zeng",
        "Ruizhong Qiu",
        "Xiao Lin",
        "Dongqi Fu",
        "Zihao Li",
        "Mengting Ai",
        "Duo Zhou",
        "Wenxuan Bao",
        "Yunzhe Li",
        "Gaotang Li",
        "Cheng Qian",
        "Yu Wang",
        "Xiangru Tang",
        "Yin Xiao",
        "Liri Fang",
        "Hui Liu",
        "Xianfeng Tang",
        "Yuji Zhang",
        "Chi Wang",
        "Jiaxuan You",
        "Heng Ji",
        "Hanghang Tong",
        "Jingrui He"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-18T18:58:23+00:00",
      "link": "https://arxiv.org/pdf/2601.12538v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Survey of agentic reasoning and planning in LLMs",
      "llm_evidence_cn": "LLM智能体推理与规划综述",
      "llm_evidence": "LLM智能体推理与规划综述",
      "llm_tldr_en": "A comprehensive survey on agentic reasoning, covering planning, tool use, and self-evolution in LLMs.",
      "llm_tldr_cn": "关于智能体推理的全面综述，涵盖了LLM的规划、工具使用和自我演化。",
      "llm_tldr": "关于智能体推理的全面综述，涵盖了LLM的规划、工具使用和自我演化。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2601.12538v1",
      "carry_days": 1
    },
    {
      "id": "2601.07354v1",
      "title": "Semantic Compression of LLM Instructions via Symbolic Metalanguages",
      "abstract": "We introduce MetaGlyph, a symbolic language for compressing prompts by encoding instructions as mathematical symbols rather than prose. Unlike systems requiring explicit decoding rules, MetaGlyph uses symbols like $\\in$ (membership) and $\\Rightarrow$ (implication) that models already understand from their training data. We test whether these symbols work as ''instruction shortcuts'' that models can interpret without additional teaching.   We evaluate eight models across two dimensions relevant to practitioners: scale (3B-1T parameters) and accessibility (open-source for local deployment vs. proprietary APIs). MetaGlyph achieves 62-81% token reduction across all task types. For API-based deployments, this translates directly to cost savings; for local deployments, it reduces latency and memory pressure.   Results vary by model. Gemini 2.5 Flash achieves 75% semantic equivalence between symbolic and prose instructions on selection tasks, with 49.9% membership operator fidelity. Kimi K2 reaches 98.1% fidelity for implication ($\\Rightarrow$) and achieves perfect (100%) accuracy on selection tasks with symbolic prompts. GPT-5.2 Chat shows the highest membership fidelity observed (91.3%), though with variable parse success across task types. Claude Haiku 4.5 achieves 100% parse success with 26% membership fidelity. Among mid-sized models, Qwen 2.5 7B shows 62% equivalence on extraction tasks. Mid-sized open-source models (7B-12B) show near-zero operator fidelity, suggesting a U-shaped relationship where sufficient scale overcomes instruction-tuning biases.",
      "authors": [
        "Ernst van Gassen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-12T09:26:46+00:00",
      "link": "https://arxiv.org/pdf/2601.07354v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Token reduction and inference optimization via symbolic metalanguages",
      "llm_evidence_cn": "通过符号元语言减少Token消耗并优化推理",
      "llm_evidence": "通过符号元语言减少Token消耗并优化推理",
      "llm_tldr_en": "Introduces MetaGlyph to compress LLM instructions into symbols, significantly reducing token costs and latency.",
      "llm_tldr_cn": "引入MetaGlyph符号语言压缩指令，显著降低Token消耗并提升推理效率。",
      "llm_tldr": "引入MetaGlyph符号语言压缩指令，显著降低Token消耗并提升推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.07354v1",
      "carry_days": 1
    },
    {
      "id": "2601.16444v2",
      "title": "Exploring the Effects of Alignment on Numerical Bias in Large Language Models",
      "abstract": "\"LLM-as-a-judge,\" which utilizes large language models (LLMs) as evaluators, has proven effective in many evaluation tasks. However, evaluator LLMs exhibit numerical bias, a phenomenon where certain evaluation scores are generated disproportionately often, leading reduced evaluation performance. This study investigates the cause of this bias. Given that most evaluator LLMs are aligned through instruction tuning and preference tuning, and that prior research suggests alignment reduces output diversity, we hypothesize that numerical bias arises from alignment. To test this, we compare outputs from pre- and post-alignment LLMs, and observe that alignment indeed increases numerical bias. We also explore mitigation strategies for post-alignment LLMs, including temperature scaling, distribution calibration, and score range adjustment. Among these, score range adjustment is most effective in reducing bias and improving performance, though still heuristic. Our findings highlight the need for further work on optimal score range selection and more robust mitigation strategies.",
      "authors": [
        "Ayako Sato",
        "Hwichan Kim",
        "Zhousi Chen",
        "Masato Mita",
        "Mamoru Komachi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-23T04:45:35+00:00",
      "link": "https://arxiv.org/pdf/2601.16444v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Effects of instruction tuning and alignment on LLM bias",
      "llm_evidence_cn": "指令微调与对齐对LLM偏见的影响",
      "llm_evidence": "指令微调与对齐对LLM偏见的影响",
      "llm_tldr_en": "Investigates how instruction and preference tuning contribute to numerical bias in LLM-as-a-judge scenarios.",
      "llm_tldr_cn": "研究指令微调和偏好对齐如何导致LLM在作为评估者时产生数值偏见。",
      "llm_tldr": "研究指令微调和偏好对齐如何导致LLM在作为评估者时产生数值偏见。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.16444v2",
      "carry_days": 1
    },
    {
      "id": "2602.12916v2",
      "title": "Reliable Thinking with Images",
      "abstract": "As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.",
      "authors": [
        "Haobin Li",
        "Yutong Yang",
        "Yijie Lin",
        "Xiang Dai",
        "Mouxing Yang",
        "Xi Peng"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-13T13:22:04+00:00",
      "link": "https://arxiv.org/pdf/2602.12916v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Multimodal Chain-of-Thought reasoning and noisy thinking",
      "llm_evidence_cn": "多模态思维链推理与噪声思维研究",
      "llm_evidence": "多模态思维链推理与噪声思维研究",
      "llm_tldr_en": "Studies the reliability of interleaved image-text Chain-of-Thought in multimodal large language models.",
      "llm_tldr_cn": "研究多模态大语言模型中图文交织思维链推理的可靠性及噪声问题。",
      "llm_tldr": "研究多模态大语言模型中图文交织思维链推理的可靠性及噪声问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.12916v2",
      "carry_days": 1
    },
    {
      "id": "2601.22108v1",
      "title": "Value-Based Pre-Training with Downstream Feedback",
      "abstract": "Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.",
      "authors": [
        "Shuqi Ke",
        "Giulia Fanti"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-29T18:38:09+00:00",
      "link": "https://arxiv.org/pdf/2601.22108v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Self-supervised pre-training with downstream feedback",
      "llm_evidence_cn": "结合下游反馈的自监督预训练",
      "llm_evidence": "结合下游反馈的自监督预训练",
      "llm_tldr_en": "Introduces V-Pretraining to steer self-supervised pre-training using downstream task information.",
      "llm_tldr_cn": "引入V-Pretraining方法，利用下游任务反馈引导自监督预训练过程。",
      "llm_tldr": "引入V-Pretraining方法，利用下游任务反馈引导自监督预训练过程。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2601.22108v1",
      "carry_days": 1
    },
    {
      "id": "2601.23155v1",
      "title": "SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training",
      "abstract": "Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost.",
      "authors": [
        "Powei Chang",
        "Jinpeng Zhang",
        "Bowen Chen",
        "Chenyu Wang",
        "Chenlu Guo",
        "Yixing Zhang",
        "Yukang Gao",
        "JianXiang Xiang",
        "Yue Gao",
        "Chaoqun Sun",
        "Yiyi Chen",
        "Dongying Kong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-30T16:40:48+00:00",
      "link": "https://arxiv.org/pdf/2601.23155v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Information-based data selection for instruction tuning",
      "llm_evidence_cn": "基于信息的指令微调数据选择",
      "llm_evidence": "基于信息的指令微调数据选择",
      "llm_tldr_en": "Introduces SPICE, a submodular optimization method to select high-quality data for efficient LLM instruction tuning.",
      "llm_tldr_cn": "引入SPICE子模优化方法，为高效LLM指令微调选择高质量数据。",
      "llm_tldr": "引入SPICE子模优化方法，为高效LLM指令微调选择高质量数据。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.23155v1",
      "carry_days": 1
    },
    {
      "id": "2602.20091v1",
      "title": "How Retrieved Context Shapes Internal Representations in RAG",
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.",
      "authors": [
        "Samuel Yeh",
        "Sharon Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-23T18:02:04+00:00",
      "link": "https://arxiv.org/pdf/2602.20091v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Analysis of retrieved context in RAG internal representations",
      "llm_evidence_cn": "RAG内部表示中检索上下文的分析",
      "llm_evidence": "RAG内部表示中检索上下文的分析",
      "llm_tldr_en": "Investigates how different types of retrieved documents influence the internal hidden states of LLMs in RAG.",
      "llm_tldr_cn": "研究RAG中不同类型的检索文档如何影响LLM的内部隐藏状态。",
      "llm_tldr": "研究RAG中不同类型的检索文档如何影响LLM的内部隐藏状态。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.20091v1",
      "carry_days": 1
    },
    {
      "id": "2601.12241v1",
      "title": "Power Aware Dynamic Reallocation For Inference",
      "abstract": "Disaggregation has emerged as a powerful strategy for optimizing large language model (LLM) inference by separating compute-intensive prefill and memory-bound decode phases across specialized GPUs. This separation improves utilization and throughput under fixed hardware capacity. However, as model and cluster scales grow, power, rather than compute, has become the dominant limiter of overall performance and cost efficiency. In this paper, we propose RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to sustain goodput within strict power caps. RAPID utilizes static and dynamic power reallocation in addition to GPU reallocation to improve performance under fixed power bounds. RAPID improves overall performance and application consistency beyond what is achievable in current disaggregation solutions, resulting in up to a 2x improvement in SLO attainment at peak load when compared to a static assignment without an increase in complexity or cost.",
      "authors": [
        "Yiwei Jiang",
        "Sangeeta Chowdhary",
        "Nathaniel Morris",
        "Rutwik Jain",
        "Srilatha Manne",
        "Sam Bayliss"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-01-18T03:30:38+00:00",
      "link": "https://arxiv.org/pdf/2601.12241v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Power-aware disaggregated inference framework for LLMs",
      "llm_evidence_cn": "LLM的功耗感知解耦推理框架",
      "llm_evidence": "LLM的功耗感知解耦推理框架",
      "llm_tldr_en": "Presents RAPID, a framework that manages GPU power budgets to optimize LLM inference throughput.",
      "llm_tldr_cn": "提出RAPID框架，通过管理GPU功耗预算来优化LLM推理吞吐量。",
      "llm_tldr": "提出RAPID框架，通过管理GPU功耗预算来优化LLM推理吞吐量。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.12241v1",
      "carry_days": 1
    },
    {
      "id": "2601.11652v1",
      "title": "WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching",
      "abstract": "As Large Language Models (LLMs) become increasingly accessible to end users, an ever-growing number of inference requests are initiated from edge devices and computed on centralized GPU clusters. However, the resulting exponential growth in computation workload is placing significant strain on data centers, while edge devices remain largely underutilized, leading to imbalanced workloads and resource inefficiency across the network. Integrating edge devices into the LLM inference process via speculative decoding helps balance the workload between the edge and the cloud, while maintaining lossless prediction accuracy. In this paper, we identify and formalize two critical bottlenecks that limit the efficiency and scalability of distributed speculative LLM serving: Wasted Drafting Time and Verification Interference. To address these challenges, we propose WISP, an efficient and SLO-aware distributed LLM inference system that consists of an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components collaboratively enhance drafting efficiency and optimize verification request scheduling on the server. Extensive numerical results show that WISP improves system capacity by up to 2.1x and 4.1x, and increases system goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED, respectively.",
      "authors": [
        "Xiangchen Li",
        "Jiakun Fan",
        "Qingyuan Wang",
        "Dimitrios Spatharakis",
        "Saeid Ghafouri",
        "Hans Vandierendonck",
        "Deepu John",
        "Bo Ji",
        "Ali R. Butt",
        "Dimitrios S. Nikolopoulos"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "published": "2026-01-15T16:46:01+00:00",
      "link": "https://arxiv.org/pdf/2601.11652v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Distributed speculative LLM serving and inference optimization",
      "llm_evidence_cn": "分布式投机LLM服务与推理优化",
      "llm_evidence": "分布式投机LLM服务与推理优化",
      "llm_tldr_en": "Optimizes LLM serving at the edge using dynamic drafting and speculative decoding to balance workloads.",
      "llm_tldr_cn": "利用动态草稿和投机解码优化边缘端LLM服务，平衡计算负载。",
      "llm_tldr": "利用动态草稿和投机解码优化边缘端LLM服务，平衡计算负载。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.11652v1",
      "carry_days": 1
    },
    {
      "id": "2601.20147v1",
      "title": "Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization",
      "abstract": "Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.",
      "authors": [
        "Saima Afrin",
        "Zaiyu Cheng",
        "Tushar Sharma",
        "Alexander Serebrenik",
        "Massimiliano Di Penta",
        "Antonio Mastropaolo"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-01-28T00:45:28+00:00",
      "link": "https://arxiv.org/pdf/2601.20147v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Instruction-tuned Language Models and explicit task instructions during fine-tuning",
      "llm_evidence_cn": "指令微调语言模型以及微调过程中的显式任务指令",
      "llm_evidence": "指令微调语言模型以及微调过程中的显式任务指令",
      "llm_tldr_en": "Explores data-centric optimization for instruction-tuned code language models to improve summarization efficiency.",
      "llm_tldr_cn": "探讨了针对指令微调代码语言模型的数据中心优化，以提高摘要效率。",
      "llm_tldr": "探讨了针对指令微调代码语言模型的数据中心优化，以提高摘要效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.20147v1",
      "carry_days": 1
    },
    {
      "id": "2602.22261v1",
      "title": "Sustainable LLM Inference using Context-Aware Model Switching",
      "abstract": "Large language models have become central to many AI applications, but their growing energy consumption raises serious sustainability concerns. A key limitation in current AI deployments is the reliance on a one-size-fits-all inference strategy where most systems route every request to the same large model, regardless of task complexity, leading to substantial and unnecessary energy waste. To address this issue, we propose a context-aware model switching approach that dynamically selects an appropriate language model based on query complexity. The proposed system uses a Context-Aware Model Switching for Energy-Efficient LLM Inference that combines caching for repeated queries, rulebased complexity scoring for fast and explainable decisions, machine learning classification to capture semantic intent, and a user-adaptive component that learns from interaction patterns over time. The proposed architecture was evaluated using real conversation workloads and three open-source language models (Gemma3 1B, Gemma3 4B and Qwen3 4B) with different computational costs, measuring energy consumption (via NVML GPU power telemetry), response latency, routing accuracy, and output quality (BERTScore F1) to reflect real-world usage conditions. Experimental results show that the model switching approach can reduce energy consumption by up to 67.5% compared to always using the largest model while maintaining a response quality of 93.6%. In addition, the response time for simple queries also improved significantly by approximately 68%. These results show that model switching inference offers a practical and scalable path toward more energy-efficient and sustainable AI systems, demonstrating that significant efficiency gains can be achieved without major sacrifices in response quality.",
      "authors": [
        "Yuvarani",
        "Akashdeep Singh",
        "Zahra Fathanah",
        "Salsabila Harlen",
        "Syeikha Syafura Al-Zahra binti Zahari",
        "Hema Subramaniam"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-25T03:42:12+00:00",
      "link": "https://arxiv.org/pdf/2602.22261v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Energy-efficient LLM inference and context-aware model switching",
      "llm_evidence_cn": "节能的大语言模型推理和上下文感知模型切换",
      "llm_evidence": "节能的大语言模型推理和上下文感知模型切换",
      "llm_tldr_en": "Proposes a dynamic model switching framework to optimize LLM inference energy consumption based on query complexity.",
      "llm_tldr_cn": "提出了一种动态模型切换框架，根据查询复杂度优化大语言模型推理能耗。",
      "llm_tldr": "提出了一种动态模型切换框架，根据查询复杂度优化大语言模型推理能耗。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.22261v1",
      "carry_days": 1
    },
    {
      "id": "2601.13995v1",
      "title": "From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning",
      "abstract": "Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \\textbf{+5.84\\%} using only \\textbf{5\\%} of the data, while our aligned sampling strategy further boosts average performance by \\textbf{+4.24\\%}.",
      "authors": [
        "Zihan Niu",
        "Wenping Hu",
        "Junmin Chen",
        "Xiyue Wang",
        "Tong Xu",
        "Ruiming Tang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-20T14:06:51+00:00",
      "link": "https://arxiv.org/pdf/2601.13995v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Controllable data selection for LLM instruction tuning using knowledge trees",
      "llm_evidence_cn": "使用知识树进行大语言模型指令微调的可控数据选择",
      "llm_evidence": "使用知识树进行大语言模型指令微调的可控数据选择",
      "llm_tldr_en": "Introduces TAGS, a framework for fine-grained, hierarchical data selection to improve LLM instruction tuning.",
      "llm_tldr_cn": "引入了TAGS框架，通过细粒度的分层数据选择来改进大语言模型的指令微调。",
      "llm_tldr": "引入了TAGS框架，通过细粒度的分层数据选择来改进大语言模型的指令微调。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.13995v1",
      "carry_days": 1
    },
    {
      "id": "2602.19509v1",
      "title": "Pyramid MoA: A Probabilistic Framework for Cost-Optimized Anytime Inference",
      "abstract": "Large Language Models (LLMs) face a persistent trade-off between inference cost and reasoning capability. While \"Oracle\" models (e.g., Llama-3-70B) achieve state-of-the-art accuracy, they are prohibitively expensive for high-volume deployment. Smaller models (e.g., 8B parameters) are cost-effective but struggle with complex tasks. In this work, we propose \"Pyramid MoA\", a hierarchical Mixture-of-Agents architecture that uses a lightweight Router to dynamically escalate queries only when necessary. By leveraging semantic agreement and confidence calibration among an ensemble of small models, our Router identifies \"hard\" problems with high precision. On the GSM8K benchmark, our system achieves 93.0% accuracy, effectively matching the Oracle baseline (98.0%) while reducing compute costs by 61%. We demonstrate that the system introduces negligible latency overhead (+0.82s) and allows for a tunable trade-off between performance and budget.",
      "authors": [
        "Arindam Khaled"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-23T04:47:47+00:00",
      "link": "https://arxiv.org/pdf/2602.19509v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cost-optimized anytime inference and Mixture-of-Agents architecture",
      "llm_evidence_cn": "成本优化的随时推理和代理混合架构",
      "llm_evidence": "成本优化的随时推理和代理混合架构",
      "llm_tldr_en": "Develops a hierarchical Mixture-of-Agents framework to balance inference cost and reasoning capability.",
      "llm_tldr_cn": "开发了一个分层代理混合框架，以平衡推理成本和推理能力。",
      "llm_tldr": "开发了一个分层代理混合框架，以平衡推理成本和推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.19509v1",
      "carry_days": 1
    },
    {
      "id": "2602.06493v1",
      "title": "Adaptive Uncertainty-Aware Tree Search for Robust Reasoning",
      "abstract": "Inference-time reasoning scaling has significantly advanced the capabilities of Large Language Models (LLMs) in complex problem-solving. A prevalent approach involves external search guided by Process Reward Models (PRMs). However, a fundamental limitation of this framework is the epistemic uncertainty of PRMs when evaluating reasoning paths that deviate from their training distribution. In this work, we conduct a systematic analysis of this challenge. We first provide empirical evidence that PRMs exhibit high uncertainty and unreliable scoring on out-of-distribution (OOD) samples. We then establish a theoretical framework proving that while standard search incurs linear regret accumulation, an uncertainty-aware strategy can achieve sublinear regret. Motivated by these findings, we propose Uncertainty-Aware Tree Search (UATS), a unified method that estimates uncertainty via Monte Carlo Dropout and dynamically allocates compute budget using a reinforcement learning-based controller. Extensive experiments demonstrate that our approach effectively mitigates the impact of OOD errors.",
      "authors": [
        "Zeen Song",
        "Zihao Ma",
        "Wenwen Qiang",
        "Changwen Zheng",
        "Gang Hua"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-06T08:40:05+00:00",
      "link": "https://arxiv.org/pdf/2602.06493v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Inference-time reasoning scaling and tree search for complex problem-solving",
      "llm_evidence_cn": "推理阶段的推理扩展和用于复杂问题解决的树搜索",
      "llm_evidence": "推理阶段的推理扩展和用于复杂问题解决的树搜索",
      "llm_tldr_en": "Analyzes and improves the robustness of LLM reasoning using uncertainty-aware tree search strategies.",
      "llm_tldr_cn": "通过不确定性感知树搜索策略分析并提高了大语言模型推理的鲁棒性。",
      "llm_tldr": "通过不确定性感知树搜索策略分析并提高了大语言模型推理的鲁棒性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.06493v1",
      "carry_days": 1
    },
    {
      "id": "2602.21144v1",
      "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism",
      "abstract": "Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path.   This paper presents a communication-efficient TP design for selective SSM inference that addresses three practical engineering challenges: enabling TTFT improvements via an SSM state cache across prefill and decode, partitioning the mixer's packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing TP aggregation overhead with quantized AllReduce. We evaluate on three representative SSM-based LLMs spanning pure-SSM and hybrid architectures - Mamba, Falcon-Mamba, and Zamba - on NVIDIA A6000 and A100 clusters. Our experiments show substantial throughput gains from tensor-parallel SSM inference, improving batch-request throughput by ~1.6-2.1x on 2 GPUs and ~2.6-4.0x on 4 GPUs for Mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead.",
      "authors": [
        "Anurag Dutt",
        "Nimit Shah",
        "Hazem Masarani",
        "Anshul Gandhi"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published": "2026-02-24T17:47:54+00:00",
      "link": "https://arxiv.org/pdf/2602.21144v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient deployment and inference optimization of SSM-based LLMs",
      "llm_evidence_cn": "基于状态空间模型的大语言模型的高效部署和推理优化",
      "llm_evidence": "基于状态空间模型的大语言模型的高效部署和推理优化",
      "llm_tldr_en": "Presents a tensor parallelism design to scale selective State-Space Model inference across multiple GPUs.",
      "llm_tldr_cn": "提出了一种张量并行设计，以在多个GPU上扩展选择性状态空间模型的推理。",
      "llm_tldr": "提出了一种张量并行设计，以在多个GPU上扩展选择性状态空间模型的推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.21144v1",
      "carry_days": 1
    },
    {
      "id": "2602.05512v1",
      "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
      "abstract": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
      "authors": [
        "Larissa Pusch",
        "Alexandre Courtiol",
        "Tim Conrad"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-05T10:10:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05512v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM-centered architecture for Knowledge-Graph RAG",
      "llm_evidence_cn": "以LLM为中心的知识图谱检索增强生成架构",
      "llm_evidence": "以LLM为中心的知识图谱检索增强生成架构",
      "llm_tldr_en": "Introduces a framework combining LLMs with Knowledge Graphs to improve RAG accuracy and multi-hop reasoning.",
      "llm_tldr_cn": "引入了一个结合LLM与知识图谱的框架，以提高RAG的准确性和多跳推理能力。",
      "llm_tldr": "引入了一个结合LLM与知识图谱的框架，以提高RAG的准确性和多跳推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.05512v1",
      "carry_days": 1
    },
    {
      "id": "2602.20338v1",
      "title": "Emergent Manifold Separability during Reasoning in Large Language Models",
      "abstract": "Chain-of-Thought (CoT) prompting significantly improves reasoning in Large Language Models, yet the temporal dynamics of the underlying representation geometry remain poorly understood. We investigate these dynamics by applying Manifold Capacity Theory (MCT) to a compositional Boolean logic task, allowing us to quantify the linear separability of latent representations without the confounding factors of probe training. Our analysis reveals that reasoning manifests as a transient geometric pulse, where concept manifolds are untangled into linearly separable subspaces immediately prior to computation and rapidly compressed thereafter. This behavior diverges from standard linear probe accuracy, which remains high long after computation, suggesting a fundamental distinction between information that is merely retrievable and information that is geometrically prepared for processing. We interpret this phenomenon as \\emph{Dynamic Manifold Management}, a mechanism where the model dynamically modulates representational capacity to optimize the bandwidth of the residual stream throughout the reasoning chain.",
      "authors": [
        "Alexandre Polo",
        "Chanwoo Chun",
        "SueYeon Chung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-23T20:36:17+00:00",
      "link": "https://arxiv.org/pdf/2602.20338v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Geometric analysis of Chain-of-Thought reasoning dynamics",
      "llm_evidence_cn": "思维链推理动力学的几何分析",
      "llm_evidence": "思维链推理动力学的几何分析",
      "llm_tldr_en": "Investigates the latent representation geometry of LLMs during Chain-of-Thought reasoning tasks.",
      "llm_tldr_cn": "研究了LLM在执行思维链推理任务时的潜层表示几何特性。",
      "llm_tldr": "研究了LLM在执行思维链推理任务时的潜层表示几何特性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.20338v1",
      "carry_days": 1
    },
    {
      "id": "2601.06002v2",
      "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
      "abstract": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
      "authors": [
        "Qiguang Chen",
        "Yantao Du",
        "Ziniu Li",
        "Jinhao Liu",
        "Songyao Duan",
        "Jiarui Guo",
        "Minghao Liu",
        "Jiaheng Liu",
        "Tong Yang",
        "Ge Zhang",
        "Libo Qin",
        "Wanxiang Che",
        "Wenhao Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-09T18:39:01+00:00",
      "link": "https://arxiv.org/pdf/2601.06002v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Mapping the topology of Long Chain-of-Thought reasoning",
      "llm_evidence_cn": "映射长思维链推理的拓扑结构",
      "llm_evidence": "映射长思维链推理的拓扑结构",
      "llm_tldr_en": "Proposes a structural view of Long CoT trajectories to improve reasoning learning in LLMs.",
      "llm_tldr_cn": "提出了长思维链轨迹的结构化视角，以改进LLM的推理学习。",
      "llm_tldr": "提出了长思维链轨迹的结构化视角，以改进LLM的推理学习。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2601.06002v2",
      "carry_days": 1
    },
    {
      "id": "2602.03073v1",
      "title": "TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT",
      "abstract": "Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) are the two dominant paradigms for enhancing Large Language Model (LLM) performance on downstream tasks. While RL generally preserves broader model capabilities (retention) better than SFT, it comes with significant costs: complex reward engineering, instability, and expensive on-policy sampling. In contrast, SFT is efficient but brittle, often suffering from catastrophic forgetting due to $\\textbf{Supervision Mismatch}$: the divergence between the model's evolving policy and static training labels. We address this trade-off with $\\textbf{Trajectory-Mixed Supervision (TMS)}$, a reward-free framework that approximates the on-policy benefits of RL by creating a dynamic curriculum from the model's own historical checkpoints. TMS minimizes $\\textit{Policy-Label Divergence (PLD)}$, preventing the mode collapse that drives forgetting in standard SFT. Experiments across reasoning (MATH, GSM8K) and instruction-following benchmarks demonstrate that TMS effectively shifts the accuracy--retention Pareto frontier. While RL remains the gold standard for retention, TMS significantly outperforms standard and iterative SFT, bridging the gap to RL without requiring reward models or verifiers. Mechanistic analysis confirms that PLD drift accurately predicts forgetting and that TMS successfully mitigates this drift.",
      "authors": [
        "Rana Muhammad Shahroz Khan",
        "Zijie Liu",
        "Zhen Tan",
        "Charles Fleming",
        "Tianlong Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03T04:01:26+00:00",
      "link": "https://arxiv.org/pdf/2602.03073v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Reward-free on-policy SFT for LLM performance enhancement",
      "llm_evidence_cn": "用于提升LLM性能的无奖励在线有监督微调",
      "llm_evidence": "用于提升LLM性能的无奖励在线有监督微调",
      "llm_tldr_en": "Proposes Trajectory-Mixed Supervision to bridge the gap between SFT and RL for efficient LLM tuning.",
      "llm_tldr_cn": "提出了轨迹混合监督框架，弥合了SFT与RL之间的差距，实现高效的LLM微调。",
      "llm_tldr": "提出了轨迹混合监督框架，弥合了SFT与RL之间的差距，实现高效的LLM微调。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.03073v1",
      "carry_days": 1
    },
    {
      "id": "2602.04900v3",
      "title": "Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization",
      "abstract": "As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the benefits of container orchestration, such as scalability and resource efficiency, to complex AI workflows. We implement and evaluate an illustrative, multi-stage use case consisting of automatic speech recognition and summarization. First, we address batch inference by using Kueue to manage jobs that transcribe audio files with Whisper models and Dynamic Accelerator Slicer (DAS) to increase parallel job execution. Second, we address a discrete online inference scenario by feeding the transcripts to a Large Language Model for summarization hosted using llm-d, a novel solution utilizing the recent developments around the Kubernetes Gateway API Inference Extension (GAIE) for optimized routing of inference requests. Our findings illustrate that these complementary components (Kueue, DAS, and GAIE) form a cohesive, high-performance platform, proving Kubernetes' capability to serve as a unified foundation for demanding GenAI workloads: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36\\%; and GAIE working in conjunction with llm-d improved tail Time to First Token latency by up to 90% even under high loads.",
      "authors": [
        "Sai Sindhur Malleni",
        "Raúl Sevilla",
        "Aleksei Vasilevskii",
        "José Castillo Lema",
        "André Bauer"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-02-03T15:36:08+00:00",
      "link": "https://arxiv.org/pdf/2602.04900v3",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Kubernetes performance for GenAI and LLM inference optimization",
      "llm_evidence_cn": "用于生成式AI和LLM推理优化的Kubernetes性能评估",
      "llm_evidence": "用于生成式AI和LLM推理优化的Kubernetes性能评估",
      "llm_tldr_en": "Evaluates Kubernetes-native tools for managing and optimizing LLM inference workloads at scale.",
      "llm_tldr_cn": "评估了用于大规模管理和优化LLM推理工作负载的Kubernetes原生工具。",
      "llm_tldr": "评估了用于大规模管理和优化LLM推理工作负载的Kubernetes原生工具。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.04900v3",
      "carry_days": 1
    },
    {
      "id": "2602.07616v1",
      "title": "SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models",
      "abstract": "Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.",
      "authors": [
        "Juntong Wu",
        "Jialiang Cheng",
        "Fuyu Lv",
        "Ou Dan",
        "Li Yuan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-07T16:51:16+00:00",
      "link": "https://arxiv.org/pdf/2602.07616v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient batch decoding for Mixture-of-Experts LLMs",
      "llm_evidence_cn": "混合专家模型LLM的高效批量解码",
      "llm_evidence": "混合专家模型LLM的高效批量解码",
      "llm_tldr_en": "Presents SERE to optimize inference efficiency and expert sparsity in MoE-based large language models.",
      "llm_tldr_cn": "提出了SERE方法，旨在优化基于MoE的大语言模型的推理效率和专家稀疏性。",
      "llm_tldr": "提出了SERE方法，旨在优化基于MoE的大语言模型的推理效率和专家稀疏性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.07616v1",
      "carry_days": 1
    },
    {
      "id": "2601.10254v1",
      "title": "NoReGeo: Non-Reasoning Geometry Benchmark",
      "abstract": "We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.",
      "authors": [
        "Irina Abdullaeva",
        "Anton Vasiliuk",
        "Elizaveta Goncharova",
        "Temurbek Rahmatullaev",
        "Zagorulko Ivan",
        "Maxim Kurkin",
        "Andrey Kuznetsov"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-15T10:22:55+00:00",
      "link": "https://arxiv.org/pdf/2601.10254v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "evaluating geometric understanding and reasoning in LLMs",
      "llm_evidence_cn": "评估大语言模型的几何理解与推理能力",
      "llm_evidence": "评估大语言模型的几何理解与推理能力",
      "llm_tldr_en": "Introduces NoReGeo to evaluate LLMs' intrinsic spatial understanding without relying on algebraic reasoning.",
      "llm_tldr_cn": "提出NoReGeo基准，用于评估大模型在不依赖代数推理情况下的内在空间几何理解能力。",
      "llm_tldr": "提出NoReGeo基准，用于评估大模型在不依赖代数推理情况下的内在空间几何理解能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.10254v1",
      "carry_days": 1
    },
    {
      "id": "2602.02313v2",
      "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient",
      "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.",
      "authors": [
        "Changming Li",
        "Kaixing Zhang",
        "Haoyun Xu",
        "Yingdong Shi",
        "Zheng Zhang",
        "Kaitao Song",
        "Kan Ren"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-02T16:43:09+00:00",
      "link": "https://arxiv.org/pdf/2602.02313v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "interpreting and controlling internal reasoning mechanisms",
      "llm_evidence_cn": "解释并控制大模型的内部推理机制",
      "llm_evidence": "解释并控制大模型的内部推理机制",
      "llm_tldr_en": "Uses Integrated Policy Gradient to identify and control model components contributing to sequential reasoning.",
      "llm_tldr_cn": "利用集成策略梯度识别并控制影响大模型序列推理输出的内部组件。",
      "llm_tldr": "利用集成策略梯度识别并控制影响大模型序列推理输出的内部组件。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.02313v2",
      "carry_days": 1
    },
    {
      "id": "2602.08948v1",
      "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute",
      "abstract": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.",
      "authors": [
        "Chen Jin",
        "Ryutaro Tanno",
        "Tom Diethe",
        "Philip Teare"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-09T17:44:41+00:00",
      "link": "https://arxiv.org/pdf/2602.08948v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "efficient inference and adaptive test-time compute",
      "llm_evidence_cn": "高效推理与自适应测试时计算",
      "llm_evidence": "高效推理与自适应测试时计算",
      "llm_tldr_en": "Introduces CoRefine, a lightweight controller to optimize LLM reasoning efficiency and reduce token usage.",
      "llm_tldr_cn": "提出CoRefine方法，通过轻量级控制器实现自适应推理，大幅降低推理时的计算开销。",
      "llm_tldr": "提出CoRefine方法，通过轻量级控制器实现自适应推理，大幅降低推理时的计算开销。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.08948v1",
      "carry_days": 1
    },
    {
      "id": "2602.07086v1",
      "title": "Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation",
      "abstract": "Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.",
      "authors": [
        "Michael Marketsmüller",
        "Simon Martin",
        "Tim Schlippe"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-06T08:37:06+00:00",
      "link": "https://arxiv.org/pdf/2602.07086v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "evaluating RAG variants for code and API generation",
      "llm_evidence_cn": "评估用于代码和API生成的RAG变体",
      "llm_evidence": "评估用于代码和API生成的RAG变体",
      "llm_tldr_en": "Evaluates standard RAG, Self-RAG, and CoRAG for enterprise tasks like SQL and API call generation.",
      "llm_tldr_cn": "对比评估了多种RAG变体在企业级SQL和API生成任务中的实际表现。",
      "llm_tldr": "对比评估了多种RAG变体在企业级SQL和API生成任务中的实际表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.07086v1",
      "carry_days": 1
    },
    {
      "id": "2602.12222v1",
      "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training",
      "abstract": "Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \\textbf{\\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \\textbf{\\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT",
      "authors": [
        "Miaosen Zhang",
        "Yishan Liu",
        "Shuxia Lin",
        "Xu Yang",
        "Qi Dai",
        "Chong Luo",
        "Weihao Jiang",
        "Peng Hou",
        "Anxiang Zeng",
        "Xin Geng",
        "Baining Guo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-12T17:59:58+00:00",
      "link": "https://arxiv.org/pdf/2602.12222v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "improving supervised fine-tuning generalization",
      "llm_evidence_cn": "提升有监督微调的泛化能力",
      "llm_evidence": "提升有监督微调的泛化能力",
      "llm_tldr_en": "Proposes Distribution Discriminant Theory to enable On-Policy SFT, bridging the gap between SFT and RL.",
      "llm_tldr_cn": "提出分布判别理论以实现同策略SFT，缩小有监督微调与强化学习之间的泛化差距。",
      "llm_tldr": "提出分布判别理论以实现同策略SFT，缩小有监督微调与强化学习之间的泛化差距。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.12222v1",
      "carry_days": 1
    },
    {
      "id": "2602.12196v1",
      "title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education",
      "abstract": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.",
      "authors": [
        "Mohamed Huti",
        "Alasdair Mackintosh",
        "Amy Waldock",
        "Dominic Andrews",
        "Maxime Lelièvre",
        "Moritz Boos",
        "Tobias Murray",
        "Paul Atherton",
        "Robin A. A. Ince",
        "Oliver G. B. Garrod"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-12T17:29:03+00:00",
      "link": "https://arxiv.org/pdf/2602.12196v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Evaluating Multimodal LLMs on visual reasoning problems",
      "llm_evidence_cn": "评估多模态大模型在视觉推理问题上的表现",
      "llm_evidence": "评估多模态大模型在视觉推理问题上的表现",
      "llm_tldr_en": "Introduces a benchmark to evaluate the spatial and relational reasoning capabilities of Multimodal LLMs.",
      "llm_tldr_cn": "发布VRB基准测试，用于评估多模态大语言模型在真实教育场景中的视觉推理能力。",
      "llm_tldr": "发布VRB基准测试，用于评估多模态大语言模型在真实教育场景中的视觉推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.12196v1",
      "carry_days": 1
    },
    {
      "id": "2601.13288v1",
      "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
      "abstract": "Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.",
      "authors": [
        "Gonzalo Ariel Meyoyan",
        "Luciano Del Corro"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-19T18:40:29+00:00",
      "link": "https://arxiv.org/pdf/2601.13288v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient inference and layer-selective probes for LLMs",
      "llm_evidence_cn": "大模型高效推理与层选择性探测",
      "llm_evidence": "大模型高效推理与层选择性探测",
      "llm_tldr_en": "Reuses LLM hidden states for efficient single-pass classification to reduce inference latency and VRAM.",
      "llm_tldr_cn": "通过复用LLM隐藏状态进行单次前向传播分类，降低推理延迟和显存占用。",
      "llm_tldr": "通过复用LLM隐藏状态进行单次前向传播分类，降低推理延迟和显存占用。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.13288v1",
      "carry_days": 1
    },
    {
      "id": "2602.04816v2",
      "title": "Horizon-LM: A RAM-Centric Architecture for LLM Training",
      "abstract": "The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2$\\times$ higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.",
      "authors": [
        "Zhengqing Yuan",
        "Lichao Sun",
        "Yanfang Ye"
      ],
      "primary_category": "cs.OS",
      "categories": [
        "cs.OS",
        "cs.CL",
        "cs.DC"
      ],
      "published": "2026-02-04T18:04:46+00:00",
      "link": "https://arxiv.org/pdf/2602.04816v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "RAM-centric architecture for LLM instruction tuning",
      "llm_evidence_cn": "以内存为中心的大模型指令微调架构",
      "llm_evidence": "以内存为中心的大模型指令微调架构",
      "llm_tldr_en": "Presents Horizon-LM, a memory-centric training system to lower barriers for LLM post-training and tuning.",
      "llm_tldr_cn": "提出Horizon-LM内存中心架构，降低大模型指令微调和对齐的硬件门槛。",
      "llm_tldr": "提出Horizon-LM内存中心架构，降低大模型指令微调和对齐的硬件门槛。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.04816v2",
      "carry_days": 1
    },
    {
      "id": "2602.11137v1",
      "title": "Weight Decay Improves Language Model Plasticity",
      "abstract": "The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.",
      "authors": [
        "Tessa Han",
        "Sebastian Bordt",
        "Hanlin Zhang",
        "Sham Kakade"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-11T18:49:26+00:00",
      "link": "https://arxiv.org/pdf/2602.11137v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM pre-training regularization and plasticity",
      "llm_evidence_cn": "大语言模型预训练正则化与可塑性",
      "llm_evidence": "大语言模型预训练正则化与可塑性",
      "llm_tldr_en": "Demonstrates that weight decay during pre-training significantly improves LLM fine-tuning adaptability.",
      "llm_tldr_cn": "研究发现预训练中的权重衰减能显著提升大语言模型在下游微调中的可塑性。",
      "llm_tldr": "研究发现预训练中的权重衰减能显著提升大语言模型在下游微调中的可塑性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2602.11137v1",
      "carry_days": 1
    },
    {
      "id": "2602.17691v1",
      "title": "Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering",
      "abstract": "Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.   On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.   Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.",
      "authors": [
        "Craig Atkinson"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-06T06:24:37+00:00",
      "link": "https://arxiv.org/pdf/2602.17691v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Decoupling entropy from hallucination in quantized LLMs",
      "llm_evidence_cn": "量化大模型中熵与幻觉的解耦",
      "llm_evidence": "量化大模型中熵与幻觉的解耦",
      "llm_tldr_en": "HELIX uses manifold steering to reduce hallucinations in quantized LLMs during inference.",
      "llm_tldr_cn": "HELIX通过流形引导技术减少了量化大模型在推理过程中的幻觉问题。",
      "llm_tldr": "HELIX通过流形引导技术减少了量化大模型在推理过程中的幻觉问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.17691v1",
      "carry_days": 1
    },
    {
      "id": "2602.08329v1",
      "title": "Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference",
      "abstract": "A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.",
      "authors": [
        "Yifei Gao",
        "Lei Wang",
        "Rong-Cheng Tu",
        "Qixin Zhang",
        "Jun Cheng",
        "Dacheng Tao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "published": "2026-02-09T07:05:23+00:00",
      "link": "https://arxiv.org/pdf/2602.08329v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "KV selection for efficient long-context LLM inference",
      "llm_evidence_cn": "长文本大模型推理中的高效KV选择",
      "llm_evidence": "长文本大模型推理中的高效KV选择",
      "llm_tldr_en": "PrHS optimizes LLM inference by selecting KV entries before attention scoring to reduce computation.",
      "llm_tldr_cn": "PrHS通过在注意力评分前选择KV条目来优化大模型推理，降低计算开销。",
      "llm_tldr": "PrHS通过在注意力评分前选择KV条目来优化大模型推理，降低计算开销。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.08329v1",
      "carry_days": 1
    },
    {
      "id": "2602.01990v1",
      "title": "SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning",
      "abstract": "Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.",
      "authors": [
        "Zhen-Hao Xie",
        "Jun-Tao Tang",
        "Yu-Cheng Shi",
        "Han-Jia Ye",
        "De-Chuan Zhan",
        "Da-Wei Zhou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02T11:47:06+00:00",
      "link": "https://arxiv.org/pdf/2602.01990v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Multimodal continual instruction tuning with MoE",
      "llm_evidence_cn": "基于混合专家的多模态持续指令微调",
      "llm_evidence": "基于混合专家的多模态持续指令微调",
      "llm_tldr_en": "SAME addresses router drift in MoE-based multimodal models during continual instruction tuning.",
      "llm_tldr_cn": "SAME解决了混合专家多模态模型在持续指令微调过程中的路由偏移问题。",
      "llm_tldr": "SAME解决了混合专家多模态模型在持续指令微调过程中的路由偏移问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.01990v1",
      "carry_days": 1
    },
    {
      "id": "2601.17261v2",
      "title": "AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning",
      "abstract": "Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.",
      "authors": [
        "Wei Lin",
        "Yining Jiang",
        "Qingyu Song",
        "Qiao Xiang",
        "Hong Xu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-24T02:28:15+00:00",
      "link": "https://arxiv.org/pdf/2601.17261v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Zeroth-order optimization for memory-efficient LLM fine-tuning",
      "llm_evidence_cn": "用于内存高效大模型微调的零阶优化",
      "llm_evidence": "用于内存高效大模型微调的零阶优化",
      "llm_tldr_en": "AGZO enables memory-efficient LLM fine-tuning by leveraging activation-guided zeroth-order optimization.",
      "llm_tldr_cn": "AGZO利用激活引导的零阶优化实现了内存受限环境下的高效大模型微调。",
      "llm_tldr": "AGZO利用激活引导的零阶优化实现了内存受限环境下的高效大模型微调。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.17261v2",
      "carry_days": 1
    },
    {
      "id": "2602.20973v1",
      "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving",
      "abstract": "To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.",
      "authors": [
        "Yuliang Ji",
        "Fuchen Shen",
        "Jian Wu",
        "Qiujie Xie",
        "Yue Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-24T14:53:34+00:00",
      "link": "https://arxiv.org/pdf/2602.20973v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Evaluating LLM reasoning and problem solving in FOL",
      "llm_evidence_cn": "评估大模型在一阶逻辑中的推理与问题解决能力",
      "llm_evidence": "评估大模型在一阶逻辑中的推理与问题解决能力",
      "llm_tldr_en": "This paper introduces PC-FOL to evaluate LLMs' capabilities in non-linear, case-based mathematical reasoning.",
      "llm_tldr_cn": "该研究引入了PC-FOL数据集，用于评估大模型在非线性分类讨论数学推理中的能力。",
      "llm_tldr": "该研究引入了PC-FOL数据集，用于评估大模型在非线性分类讨论数学推理中的能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.20973v1",
      "carry_days": 1
    },
    {
      "id": "2601.10823v2",
      "title": "Mugi: Value Level Parallelism For Efficient LLMs",
      "abstract": "Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\\times$ and $668\\times$ for nonlinear softmax operations, and $2.07\\times$ and $3.11\\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\\times$ and embodied carbon by $1.48\\times$.",
      "authors": [
        "Daniel Price",
        "Prabhu Vellaisamy",
        "John Shen",
        "Di Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "published": "2026-01-15T19:48:21+00:00",
      "link": "https://arxiv.org/pdf/2601.10823v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Value level parallelism for efficient LLM inference",
      "llm_evidence_cn": "用于高效大模型推理的值级并行",
      "llm_evidence": "用于高效大模型推理的值级并行",
      "llm_tldr_en": "Mugi optimizes LLM efficiency through value level parallelism for nonlinear approximations and GEMMs.",
      "llm_tldr_cn": "Mugi通过值级并行优化了非线性近似和矩阵乘法，提升了大模型的推理效率。",
      "llm_tldr": "Mugi通过值级并行优化了非线性近似和矩阵乘法，提升了大模型的推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.10823v2",
      "carry_days": 1
    },
    {
      "id": "2602.08060v2",
      "title": "Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices",
      "abstract": "LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\\times$ speedup for translation tasks, closely matching analytic expectations.",
      "authors": [
        "Alejandro Ruiz y Mesa",
        "Guilherme Korol",
        "Moritz Riesterer",
        "João Paulo Cardoso de Lima",
        "Jeronimo Castrillon"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-08T17:09:51+00:00",
      "link": "https://arxiv.org/pdf/2602.08060v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Accelerated LLM inference on edge devices using speculative sampling",
      "llm_evidence_cn": "使用投机采样加速边缘设备上的LLM推理",
      "llm_evidence": "使用投机采样加速边缘设备上的LLM推理",
      "llm_tldr_en": "Proposes a compiler-assisted speculative sampling framework to optimize LLM inference latency on heterogeneous edge SoCs.",
      "llm_tldr_cn": "提出一种编译辅助的投机采样框架，旨在优化异构边缘芯片上的LLM推理延迟。",
      "llm_tldr": "提出一种编译辅助的投机采样框架，旨在优化异构边缘芯片上的LLM推理延迟。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.08060v2",
      "carry_days": 1
    },
    {
      "id": "2601.23006v1",
      "title": "InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning",
      "abstract": "Supervised fine-tuning (SFT) is fundamental to adapting large language models, yet training on complete datasets incurs prohibitive costs with diminishing returns. Existing data selection methods suffer from severe domain specificity: techniques optimized for general instruction-following fail on reasoning tasks, and vice versa. We observe that measuring entropy differences between base models and minimally instruction-tuned calibrated models reveals a pattern -- samples with the lowest differential entropy consistently yield optimal performance across domains, yet this principle manifests domain-adaptively: reasoning tasks favor entropy increase (cognitive expansion), while general tasks favor entropy decrease (cognitive compression). We introduce InstructDiff, a unified framework that operationalizes differential entropy as a domain-adaptive selection criterion through warmup calibration, bi-directional NLL filtering, and entropy-based ranking. Extensive experiments show that InstructDiff achieves 17\\% relative improvement over full data training on mathematical reasoning and 52\\% for general instruction-following, outperforming prior baselines while using only 10\\% of the data.",
      "authors": [
        "Junyou Su",
        "He Zhu",
        "Xiao Luo",
        "Liyu Zhang",
        "Hong-Yu Zhou",
        "Yun Chen",
        "Peng Li",
        "Yang Liu",
        "Guanhua Chen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-30T14:15:44+00:00",
      "link": "https://arxiv.org/pdf/2601.23006v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Domain-adaptive data selection for efficient LLM fine-tuning",
      "llm_evidence_cn": "用于高效LLM微调的领域自适应数据选择",
      "llm_evidence": "用于高效LLM微调的领域自适应数据选择",
      "llm_tldr_en": "Presents InstructDiff, a method using differential entropy to select optimal data for efficient instruction fine-tuning.",
      "llm_tldr_cn": "提出InstructDiff，一种利用微分熵选择最优数据以实现高效指令微调的方法。",
      "llm_tldr": "提出InstructDiff，一种利用微分熵选择最优数据以实现高效指令微调的方法。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.23006v1",
      "carry_days": 1
    },
    {
      "id": "2602.22538v1",
      "title": "RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format",
      "abstract": "Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.",
      "authors": [
        "Zhehao Huang",
        "Yuhang Liu",
        "Baijiong Lin",
        "Yixin Lou",
        "Zhengbao He",
        "Hanling Tian",
        "Tao Li",
        "Xiaolin Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-26T02:26:45+00:00",
      "link": "https://arxiv.org/pdf/2602.22538v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Enhancing instruction following in large reasoning models via model merging",
      "llm_evidence_cn": "通过模型合并增强大型推理模型的指令遵循能力",
      "llm_evidence": "通过模型合并增强大型推理模型的指令遵循能力",
      "llm_tldr_en": "Introduces RAIN-Merging to combine reasoning capabilities with instruction following while preserving thinking formats.",
      "llm_tldr_cn": "引入RAIN-Merging，在保留思维格式的同时将推理能力与指令遵循相结合。",
      "llm_tldr": "引入RAIN-Merging，在保留思维格式的同时将推理能力与指令遵循相结合。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.22538v1",
      "carry_days": 1
    },
    {
      "id": "2602.14516v1",
      "title": "Efficient Multi-round LLM Inference over Disaggregated Serving",
      "abstract": "With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases.   In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines.",
      "authors": [
        "Wenhao He",
        "Youhe Jiang",
        "Penghao Zhao",
        "Quanqing Xu",
        "Eiko Yoneki",
        "Bin Cui",
        "Fangcheng Fu"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-16T07:07:30+00:00",
      "link": "https://arxiv.org/pdf/2602.14516v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient multi-round LLM inference over disaggregated serving",
      "llm_evidence_cn": "解耦服务架构下的高效多轮LLM推理",
      "llm_evidence": "解耦服务架构下的高效多轮LLM推理",
      "llm_tldr_en": "Presents AMPD, a framework to optimize prefill-decode coordination for multi-round LLM inference workflows.",
      "llm_tldr_cn": "提出AMPD框架，旨在优化多轮LLM推理工作流中的预填充与解码协作。",
      "llm_tldr": "提出AMPD框架，旨在优化多轮LLM推理工作流中的预填充与解码协作。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.14516v1",
      "carry_days": 1
    },
    {
      "id": "2601.17112v1",
      "title": "Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.",
      "authors": [
        "A. El Ichi",
        "K. Jbilou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-23T18:37:17+00:00",
      "link": "https://arxiv.org/pdf/2601.17112v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Low-rank tensor approximation for LLM weight compression",
      "llm_evidence_cn": "用于大语言模型权重压缩的低秩张量近似",
      "llm_evidence": "用于大语言模型权重压缩的低秩张量近似",
      "llm_tldr_en": "Introduces a tensor compression framework to reduce LLM memory footprint and computational costs.",
      "llm_tldr_cn": "引入张量压缩框架，旨在减少大语言模型的内存占用和计算成本。",
      "llm_tldr": "引入张量压缩框架，旨在减少大语言模型的内存占用和计算成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.17112v1",
      "carry_days": 1
    },
    {
      "id": "2602.00161v1",
      "title": "Block removal for large language models through constrained binary optimization",
      "abstract": "Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.",
      "authors": [
        "David Jansen",
        "Roman Rausch",
        "David Montero",
        "Roman Orus"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "quant-ph"
      ],
      "published": "2026-01-29T19:46:39+00:00",
      "link": "https://arxiv.org/pdf/2602.00161v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Compressing LLMs by removing transformer blocks via optimization",
      "llm_evidence_cn": "通过优化算法移除Transformer层以压缩大模型",
      "llm_evidence": "通过优化算法移除Transformer层以压缩大模型",
      "llm_tldr_en": "Formulates block removal as a binary optimization problem to efficiently compress large language models.",
      "llm_tldr_cn": "将层移除建模为二元优化问题，以实现大语言模型的高效压缩。",
      "llm_tldr": "将层移除建模为二元优化问题，以实现大语言模型的高效压缩。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.00161v1",
      "carry_days": 1
    },
    {
      "id": "2602.01237v1",
      "title": "Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models",
      "abstract": "Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.",
      "authors": [
        "Katrina Brown",
        "Aneesh Muppidi",
        "Rana Shahout"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-01T13:58:23+00:00",
      "link": "https://arxiv.org/pdf/2602.01237v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Predictive scheduling for efficient Chain-of-Thought reasoning",
      "llm_evidence_cn": "针对高效思维链推理的预测调度",
      "llm_evidence": "针对高效思维链推理的预测调度",
      "llm_tldr_en": "Optimizes inference-time reasoning by dynamically allocating token budgets for Chain-of-Thought traces.",
      "llm_tldr_cn": "通过动态分配思维链生成的令牌预算，优化推理阶段的推理效率。",
      "llm_tldr": "通过动态分配思维链生成的令牌预算，优化推理阶段的推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.01237v1",
      "carry_days": 1
    },
    {
      "id": "2602.11688v1",
      "title": "GORGO: Maximizing KV-Cache Reuse While Minimizing Network Latency in Cross-Region LLM Load Balancing",
      "abstract": "Distributing LLM inference across geographical regions can improve Time-to-First-Token (TTFT) by regionalizing service deployments. While existing multi-region load balancers save prefill computation by prioritizing Key--Value (KV) Cache hit rate, they ignore cluster networking latency, a critical factor in routing decisions. We introduce GORGO, a method for minimizing TTFT by optimizing a total serving cost as a function of available compute, network latency, and prefix caching. Using extensive profiling on custom infrastructure, we analyze component-level latency bottlenecks and benchmark GORGO against three baselines: (1) naive least-load routing, which ignores prefix-cache overlap; (2) prefix-similarity routing, which selectively pushes requests to the replica with the highest cached-prefix overlap; and (3) a centralized HTTP proxy that runs the GORGO policy while tracking requests across all nodes. We demonstrate that GORGO reduces P99 TTFT through network-aware routing and improves average TTFT by preventing pathological cross-region forwarding. Additionally, we find that GORGO-proxy overcomes synchronization overhead in previous methods and is 2.5x faster on median TTFT, demonstrating the success of a centralized router.",
      "authors": [
        "Alessio Ricci Toniolo",
        "Abinaya Dinesh",
        "Rome Thorstenson"
      ],
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI",
        "cs.DC"
      ],
      "published": "2026-02-12T08:09:14+00:00",
      "link": "https://arxiv.org/pdf/2602.11688v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Optimizing KV-cache reuse and latency for LLM inference",
      "llm_evidence_cn": "优化大模型推理中的KV缓存复用与延迟",
      "llm_evidence": "优化大模型推理中的KV缓存复用与延迟",
      "llm_tldr_en": "Minimizes TTFT in cross-region LLM deployments by balancing compute, latency, and prefix caching.",
      "llm_tldr_cn": "通过平衡计算、延迟和前缀缓存，最小化跨区域大模型部署的首字延迟。",
      "llm_tldr": "通过平衡计算、延迟和前缀缓存，最小化跨区域大模型部署的首字延迟。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.11688v1",
      "carry_days": 1
    },
    {
      "id": "2602.06127v1",
      "title": "Compressing LLMs with MoP: Mixture of Pruners",
      "abstract": "The high computational demands of Large Language Models (LLMs) motivate methods that reduce parameter count and accelerate inference. In response, model pruning emerges as an effective strategy, yet current methods typically focus on a single dimension-depth or width. We introduce MoP (Mixture of Pruners), an iterative framework that unifies these dimensions. At each iteration, MoP generates two branches-pruning in depth versus pruning in width-and selects a candidate to advance the path. On LLaMA-2 and LLaMA-3, MoP advances the frontier of structured pruning, exceeding the accuracy of competing methods across a broad set of compression regimes. It also consistently outperforms depth-only and width-only pruning. Furthermore, MoP translates structural pruning into real speedup, reducing end-to-end latency by 39% at 40% compression. Finally, extending MoP to the vision-language model LLaVA-1.5, we notably improve computational efficiency and demonstrate that text-only recovery fine-tuning can restore performance even on visual tasks.",
      "authors": [
        "Bruno Lopes Yamamoto",
        "Lucas Lauton de Alcantara",
        "Victor Zacarias",
        "Leandro Giusti Mugnaini",
        "Keith Ando Ogawa",
        "Lucas Pellicer",
        "Rosimeire Pereira Costa",
        "Edson Bollis",
        "Anna Helena Reali Costa",
        "Artur Jordao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T19:01:06+00:00",
      "link": "https://arxiv.org/pdf/2602.06127v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Structured pruning framework for LLM inference acceleration",
      "llm_evidence_cn": "用于大模型推理加速的结构化剪枝框架",
      "llm_evidence": "用于大模型推理加速的结构化剪枝框架",
      "llm_tldr_en": "Introduces Mixture of Pruners to unify depth and width pruning for efficient LLM compression.",
      "llm_tldr_cn": "引入混合剪枝器（MoP），统一深度和宽度剪枝以实现高效的大模型压缩。",
      "llm_tldr": "引入混合剪枝器（MoP），统一深度和宽度剪枝以实现高效的大模型压缩。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.06127v1",
      "carry_days": 1
    },
    {
      "id": "2602.13942v1",
      "title": "A Theoretical Framework for LLM Fine-tuning Using Early Stopping for Non-random Initialization",
      "abstract": "In the era of large language models (LLMs), fine-tuning pretrained models has become ubiquitous. Yet the theoretical underpinning remains an open question. A central question is why only a few epochs of fine-tuning are typically sufficient to achieve strong performance on many different tasks. In this work, we approach this question by developing a statistical framework, combining rigorous early stopping theory with the attention-based Neural Tangent Kernel (NTK) for LLMs, offering new theoretical insights on fine-tuning practices. Specifically, we formally extend classical NTK theory [Jacot et al., 2018] to non-random (i.e., pretrained) initializations and provide a convergence guarantee for attention-based fine-tuning. One key insight provided by the theory is that the convergence rate with respect to sample size is closely linked to the eigenvalue decay rate of the empirical kernel matrix induced by the NTK. We also demonstrate how the framework can be used to explain task vectors for multiple tasks in LLMs. Finally, experiments with modern language models on real-world datasets provide empirical evidence supporting our theoretical insights.",
      "authors": [
        "Zexuan Sun",
        "Garvesh Raskutti"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-15T00:43:21+00:00",
      "link": "https://arxiv.org/pdf/2602.13942v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Theoretical framework for attention-based LLM fine-tuning",
      "llm_evidence_cn": "基于注意力的语言模型微调理论框架",
      "llm_evidence": "基于注意力的语言模型微调理论框架",
      "llm_tldr_en": "Provides theoretical insights into why few-epoch fine-tuning works for pretrained attention models.",
      "llm_tldr_cn": "为预训练注意力模型仅需少量轮次微调即可奏效提供了理论见解。",
      "llm_tldr": "为预训练注意力模型仅需少量轮次微调即可奏效提供了理论见解。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.13942v1",
      "carry_days": 1
    },
    {
      "id": "2601.06431v2",
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "abstract": "Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.",
      "authors": [
        "Qingyu Ren",
        "Qianyu He",
        "Jingwen Chang",
        "Jie Zeng",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Han Xia",
        "Zeye Sun",
        "Fei Yu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-10T05:11:38+00:00",
      "link": "https://arxiv.org/pdf/2601.06431v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Logic-structured instruction following and structure-aware rewarding",
      "llm_evidence_cn": "逻辑结构化指令遵循与结构感知奖励",
      "llm_evidence": "逻辑结构化指令遵循与结构感知奖励",
      "llm_tldr_en": "Proposes LSRIF to improve LLM instruction following by explicitly modeling logical dependencies in training.",
      "llm_tldr_cn": "提出LSRIF框架，通过在训练中显式建模逻辑依赖来提升大模型的指令遵循能力。",
      "llm_tldr": "提出LSRIF框架，通过在训练中显式建模逻辑依赖来提升大模型的指令遵循能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.06431v2",
      "carry_days": 1
    },
    {
      "id": "2602.08005v1",
      "title": "DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity",
      "abstract": "The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.",
      "authors": [
        "Jitai Hao",
        "Qiang Huang",
        "Yaowei Wang",
        "Min Zhang",
        "Jun Yu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-08T15:14:36+00:00",
      "link": "https://arxiv.org/pdf/2602.08005v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "KV cache compression for efficient LLM deployment",
      "llm_evidence_cn": "用于高效大模型部署的KV缓存压缩",
      "llm_evidence": "用于高效大模型部署的KV缓存压缩",
      "llm_tldr_en": "Introduces DeltaKV, a residual-based compression method to reduce memory and speed up long-context LLM inference.",
      "llm_tldr_cn": "引入DeltaKV，一种基于残差的压缩方法，旨在减少内存占用并加速长文本大模型推理。",
      "llm_tldr": "引入DeltaKV，一种基于残差的压缩方法，旨在减少内存占用并加速长文本大模型推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.08005v1",
      "carry_days": 1
    },
    {
      "id": "2601.06562v1",
      "title": "Mosaic: Unlocking Long-Context Inference for Diffusion LLMs via Global Memory Planning and Dynamic Peak Taming",
      "abstract": "Diffusion-based large language models (dLLMs) have emerged as a promising paradigm, utilizing simultaneous denoising to enable global planning and iterative refinement. While these capabilities are particularly advantageous for long-context generation, deploying such models faces a prohibitive memory capacity barrier stemming from severe system inefficiencies. We identify that existing inference systems are ill-suited for this paradigm: unlike autoregressive models constrained by the cumulative KV-cache, dLLMs are bottlenecked by transient activations recomputed at every step. Furthermore, general-purpose memory reuse mechanisms lack the global visibility to adapt to dLLMs' dynamic memory peaks, which toggle between logits and FFNs. To address these mismatches, we propose Mosaic, a memory-efficient inference system that shifts from local, static management to a global, dynamic paradigm. Mosaic integrates a mask-only logits kernel to eliminate redundancy, a lazy chunking optimizer driven by an online heuristic search to adaptively mitigate dynamic peaks, and a global memory manager to resolve fragmentation via virtual addressing. Extensive evaluations demonstrate that Mosaic achieves an average 2.71$\\times$ reduction in the memory peak-to-average ratio and increases the maximum inference sequence length supportable on identical hardware by 15.89-32.98$\\times$. This scalability is achieved without compromising accuracy and speed, and in fact reducing latency by 4.12%-23.26%.",
      "authors": [
        "Liang Zheng",
        "Bowen Shi",
        "Yitao Hu",
        "Jiawei Zhang",
        "Ruofan Li",
        "Sheng Chen",
        "Wenxin Li",
        "Keqiu Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-10T13:17:08+00:00",
      "link": "https://arxiv.org/pdf/2601.06562v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Inference optimization and memory planning for Diffusion LLMs",
      "llm_evidence_cn": "扩散大模型的推理优化与内存规划",
      "llm_evidence": "扩散大模型的推理优化与内存规划",
      "llm_tldr_en": "Presents Mosaic to unlock long-context inference for diffusion-based LLMs via dynamic memory management.",
      "llm_tldr_cn": "提出Mosaic框架，通过动态内存管理解决扩散大模型在长文本推理中的内存瓶颈。",
      "llm_tldr": "提出Mosaic框架，通过动态内存管理解决扩散大模型在长文本推理中的内存瓶颈。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.06562v1",
      "carry_days": 1
    },
    {
      "id": "2602.10729v1",
      "title": "BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization",
      "abstract": "The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-cost models and complex queries to higher-cost models (i.e., heterogeneous query routing); and (ii) a systems perspective that utilizes heterogeneous GPU resources as cost-effective alternatives to homogeneous high-end GPUs (i.e., heterogeneous model deployment). However, algorithm-system co-design for cost-efficient LLM serving necessitates sophisticated management: (i) Determining optimal query routing strategies under latency and quality requirements, (ii) configuring model deployment across heterogeneous GPUs with appropriate resource allocation and parallelism strategies, and (iii) co-optimizing routing and deployment decisions to maximize overall system performance. To address these challenges, we present BOute, a quality-aware scheduling system that jointly exploits heterogeneous model and GPU capabilities for cost-efficient LLM serving. BOute employs a multi-objective Bayesian optimization (MOBO) framework to co-optimize the routing strategy and model deployment, thereby maximizing the cost-efficiency of the serving system while guaranteeing response quality. Evaluation results demonstrate that BOute outperforms state-of-the-art LLM serving systems by up to 157% and 59% on average under identical cost budgets and quality requirements, or reducing serving costs by 15%-61% (38% on average) while maintaining the same performance targets, validating its effectiveness in achieving cost-efficient LLM serving.",
      "authors": [
        "Youhe Jiang",
        "Fangcheng Fu",
        "Eiko Yoneki"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-11T10:44:12+00:00",
      "link": "https://arxiv.org/pdf/2602.10729v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cost-efficient LLM serving and inference optimization",
      "llm_evidence_cn": "高性价比的大模型服务与推理优化",
      "llm_evidence": "高性价比的大模型服务与推理优化",
      "llm_tldr_en": "Optimizes LLM serving costs using Bayesian optimization for heterogeneous model routing and deployment.",
      "llm_tldr_cn": "利用贝叶斯优化对异构模型路由和部署进行协同设计，降低大模型服务成本。",
      "llm_tldr": "利用贝叶斯优化对异构模型路由和部署进行协同设计，降低大模型服务成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.10729v1",
      "carry_days": 1
    },
    {
      "id": "2601.21623v1",
      "title": "LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models",
      "abstract": "Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.",
      "authors": [
        "Stanislav Budzinskiy",
        "Marian Gloser",
        "Tolunay Yilmaz",
        "Ying Hong Tham",
        "Yuanyi Lin",
        "Wenyi Fang",
        "Fan Wu",
        "Philipp Petersen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-01-29T12:26:00+00:00",
      "link": "https://arxiv.org/pdf/2601.21623v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "mixed-precision inference for transformer models",
      "llm_evidence_cn": "针对Transformer模型的混合精度推理",
      "llm_evidence": "针对Transformer模型的混合精度推理",
      "llm_tldr_en": "Proposes an adaptive mixed-precision strategy to optimize transformer inference efficiency.",
      "llm_tldr_cn": "提出一种自适应混合精度策略，旨在优化Transformer模型的推理效率。",
      "llm_tldr": "提出一种自适应混合精度策略，旨在优化Transformer模型的推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.21623v1",
      "carry_days": 1
    },
    {
      "id": "2601.22705v1",
      "title": "CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control",
      "abstract": "Batch inference for agentic workloads stresses the GPU key-value (KV) cache in a sustained and cumulative manner, often causing severe throughput degradation well before memory capacity is exhausted. We identify this phenomenon as middle-phase thrashing, a previously under-characterized pathology in which cache efficiency collapses as long-lived agents accumulate state over time.   We argue that mitigating this pathology requires moving beyond reactive, request-level cache management to proactive, agent-level admission control. Drawing inspiration from congestion control in distributed systems, we view the KV cache as a shared resource whose efficient utilization depends on feedback-driven regulation. Based on this insight, we present CONCUR, a lightweight control layer that regulates agent admission to bound aggregate cache pressure while preserving execution continuity. CONCUR adapts a cache-aware control algorithm to dynamically adjust the number of active agents using runtime cache signals.   Across large models and real-world agent workloads, CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.",
      "authors": [
        "Qiaoling Chen",
        "Zhisheng Ye",
        "Tian Tang",
        "Peng Sun",
        "Boyu Tian",
        "Guoteng Wang",
        "Shenggui Li",
        "Yonggang Wen",
        "Zhenhua Han",
        "Tianwei Zhang"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-01-30T08:27:20+00:00",
      "link": "https://arxiv.org/pdf/2601.22705v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "high-throughput agentic batch inference and KV cache management",
      "llm_evidence_cn": "高吞吐智能体批处理推理与KV缓存管理",
      "llm_evidence": "高吞吐智能体批处理推理与KV缓存管理",
      "llm_tldr_en": "Introduces congestion control for KV cache to improve throughput in agentic LLM workloads.",
      "llm_tldr_cn": "引入KV缓存拥塞控制机制，提升智能体工作负载下的LLM推理吞吐量。",
      "llm_tldr": "引入KV缓存拥塞控制机制，提升智能体工作负载下的LLM推理吞吐量。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.22705v1",
      "carry_days": 1
    },
    {
      "id": "2602.21140v1",
      "title": "ReviveMoE: Fast Recovery for Hardware Failures in Large-Scale MoE LLM Inference Deployments",
      "abstract": "As LLM deployments scale over more hardware, the probability of a single failure in a system increases significantly, and cloud operators must consider robust countermeasures to handle these inevitable failures. A common recovery approach is to simply restart the LLM serving instance; however, this is costly in model-as-a-service (MaaS) inference settings, where reloading model weights and recompiling computation graphs can introduce significant delays to incoming requests. We propose ReviveMoE, a method for rapid failure recovery in large-scale LLM deployments without restarting the serving instance. ReviveMoE is designed to support both the traditional LLM architecture, which collocates MoE and attention on the same hardware, and the disaggregated architectures, which separate MoE from attention. Integrated into Huawei Cloud's MaaS, ReviveMoE is built on top of Huawei's xDeepServe serving platform and the XCCL communications library.",
      "authors": [
        "Haley Li",
        "Xinglu Wang",
        "Cong Feng",
        "Chunxu Zuo",
        "Yanan Wang",
        "Hei Lo",
        "Yufei Cui",
        "Bingji Wang",
        "Duo Cui",
        "Shuming Jing",
        "Yizhou Shan",
        "Ying Xiong",
        "Jiannan Wang",
        "Yong Zhang",
        "Zhenan Fan"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-24T17:39:41+00:00",
      "link": "https://arxiv.org/pdf/2602.21140v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "fast recovery for large-scale MoE LLM inference deployments",
      "llm_evidence_cn": "大规模MoE LLM推理部署的快速故障恢复",
      "llm_evidence": "大规模MoE LLM推理部署的快速故障恢复",
      "llm_tldr_en": "Presents ReviveMoE for rapid failure recovery in large-scale LLM deployments without restarts.",
      "llm_tldr_cn": "提出ReviveMoE方法，实现大规模LLM部署中无需重启的快速故障恢复。",
      "llm_tldr": "提出ReviveMoE方法，实现大规模LLM部署中无需重启的快速故障恢复。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.21140v1",
      "carry_days": 1
    },
    {
      "id": "2602.00276v1",
      "title": "Localizing and Correcting Errors for LLM-based Planners",
      "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.",
      "authors": [
        "Aditya Kumar",
        "William W. Cohen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-30T19:56:15+00:00",
      "link": "https://arxiv.org/pdf/2602.00276v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "reasoning capabilities and symbolic classical planning tasks",
      "llm_evidence_cn": "推理能力与符号经典规划任务",
      "llm_evidence": "推理能力与符号经典规划任务",
      "llm_tldr_en": "Improves LLM reasoning in planning tasks through localized in-context learning corrections.",
      "llm_tldr_cn": "通过局部上下文学习修正，提升LLM在规划任务中的推理能力。",
      "llm_tldr": "通过局部上下文学习修正，提升LLM在规划任务中的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.00276v1",
      "carry_days": 1
    },
    {
      "id": "2602.01587v1",
      "title": "Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment",
      "abstract": "Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.",
      "authors": [
        "Zehua Cheng",
        "Jianwei Yang",
        "Wei Dai",
        "Jiahao Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-02T03:26:45+00:00",
      "link": "https://arxiv.org/pdf/2602.01587v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "noise-augmented alignment tuning for LLM safety",
      "llm_evidence_cn": "用于LLM安全的噪声增强对齐微调",
      "llm_evidence": "用于LLM安全的噪声增强对齐微调",
      "llm_tldr_en": "Proposes a noise-augmented alignment framework to defend LLMs against jailbreak attacks.",
      "llm_tldr_cn": "提出一种噪声增强对齐框架，用于防御针对LLM的越狱攻击。",
      "llm_tldr": "提出一种噪声增强对齐框架，用于防御针对LLM的越狱攻击。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.01587v1",
      "carry_days": 1
    },
    {
      "id": "2602.01137v1",
      "title": "Self-Generative Adversarial Fine-Tuning for Large Language Models",
      "abstract": "Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.",
      "authors": [
        "Shiguang Wu",
        "Yaqing Wang",
        "Quanming Yao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-01T10:20:27+00:00",
      "link": "https://arxiv.org/pdf/2602.01137v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Self-generative adversarial fine-tuning for LLM alignment",
      "llm_evidence_cn": "用于LLM对齐的自生成对抗微调",
      "llm_evidence": "用于LLM对齐的自生成对抗微调",
      "llm_tldr_en": "Presents SGALM, a framework for LLM alignment using generative adversarial games without external reward models.",
      "llm_tldr_cn": "提出SGALM框架，通过生成对抗博弈实现LLM对齐，无需外部奖励模型。",
      "llm_tldr": "提出SGALM框架，通过生成对抗博弈实现LLM对齐，无需外部奖励模型。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.01137v1",
      "carry_days": 1
    },
    {
      "id": "2601.21803v1",
      "title": "RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes",
      "abstract": "Retrieval-Augmented Generation (RAG) systems combine dense retrievers and language models to ground LLM outputs in retrieved documents. However, the opacity of how these components interact creates challenges for deployment in high-stakes domains. We present RAG-E, an end-to-end explainability framework that quantifies retriever-generator alignment through mathematically grounded attribution methods. Our approach adapts Integrated Gradients for retriever analysis, introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value approximation, for generator attribution, and introduces the Weighted Attribution-Relevance Gap (WARG) metric to measure how well a generator's document usage aligns with a retriever's ranking. Empirical analysis on TREC CAsT and FoodSafeSum reveals critical misalignments: for 47.4% to 66.7% of queries, generators ignore the retriever's top-ranked documents, while 48.1% to 65.9% rely on documents ranked as less relevant. These failure modes demonstrate that RAG output quality depends not solely on individual component performance but on their interplay, which can be audited via RAG-E.",
      "authors": [
        "Korbinian Randl",
        "Guido Rocchietti",
        "Aron Henriksson",
        "Ziawasch Abedjan",
        "Tony Lindgren",
        "John Pavlopoulos"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-29T14:47:00+00:00",
      "link": "https://arxiv.org/pdf/2601.21803v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Explainability framework for retriever-generator alignment in RAG",
      "llm_evidence_cn": "RAG中检索器与生成器对齐的可解释性框架",
      "llm_evidence": "RAG中检索器与生成器对齐的可解释性框架",
      "llm_tldr_en": "Introduces RAG-E to quantify and explain the interaction between retrievers and generators in RAG systems.",
      "llm_tldr_cn": "引入RAG-E框架，用于量化和解释RAG系统中检索器与生成器之间的交互。",
      "llm_tldr": "引入RAG-E框架，用于量化和解释RAG系统中检索器与生成器之间的交互。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.21803v1",
      "carry_days": 1
    },
    {
      "id": "2602.05695v1",
      "title": "Determining Energy Efficiency Sweet Spots in Production LLM Inference",
      "abstract": "Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"Sweet Spots\" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.",
      "authors": [
        "Hiari Pizzini Cavagna",
        "Andrea Proia",
        "Giacomo Madella",
        "Giovanni B. Esposito",
        "Francesco Antici",
        "Daniele Cesarini",
        "Zeynep Kiziltan",
        "Andrea Bartolini"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.PF"
      ],
      "published": "2026-02-05T14:21:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05695v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Energy efficiency optimization for LLM inference",
      "llm_evidence_cn": "LLM推理的能效优化研究",
      "llm_evidence": "LLM推理的能效优化研究",
      "llm_tldr_en": "Proposes an analytical model to determine energy efficiency sweet spots in production LLM inference.",
      "llm_tldr_cn": "提出一种分析模型，用于确定生产环境中LLM推理的最佳能效区间。",
      "llm_tldr": "提出一种分析模型，用于确定生产环境中LLM推理的最佳能效区间。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.05695v1",
      "carry_days": 1
    },
    {
      "id": "2601.17768v2",
      "title": "LLM-42: Enabling Determinism in LLM Inference with Verified Speculation",
      "abstract": "In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.   Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.",
      "authors": [
        "Raja Gond",
        "Aditya K Kamath",
        "Ramachandran Ramjee",
        "Ashish Panwar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-01-25T09:58:57+00:00",
      "link": "https://arxiv.org/pdf/2601.17768v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Deterministic LLM inference with verified speculation",
      "llm_evidence_cn": "通过验证投机实现确定性LLM推理",
      "llm_evidence": "通过验证投机实现确定性LLM推理",
      "llm_tldr_en": "Presents LLM-42, a scheduling approach to enable deterministic outputs in LLM inference without sacrificing throughput.",
      "llm_tldr_cn": "提出LLM-42调度方法，在不损失吞吐量的情况下实现LLM推理的确定性输出。",
      "llm_tldr": "提出LLM-42调度方法，在不损失吞吐量的情况下实现LLM推理的确定性输出。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.17768v2",
      "carry_days": 1
    },
    {
      "id": "2601.21909v1",
      "title": "From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning",
      "abstract": "Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\\% and 4.63\\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.",
      "authors": [
        "Shaojie Wang",
        "Liang Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-29T16:00:48+00:00",
      "link": "https://arxiv.org/pdf/2601.21909v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cognitively aligned post-training for LLM reasoning",
      "llm_evidence_cn": "用于提升LLM推理能力的认知对齐后训练",
      "llm_evidence": "用于提升LLM推理能力的认知对齐后训练",
      "llm_tldr_en": "Develops a post-training method that aligns LLM reasoning with human cognitive strategies for better generalization.",
      "llm_tldr_cn": "开发了一种后训练方法，将LLM推理与人类认知策略对齐以提高泛化能力。",
      "llm_tldr": "开发了一种后训练方法，将LLM推理与人类认知策略对齐以提高泛化能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.21909v1",
      "carry_days": 1
    },
    {
      "id": "2602.05472v1",
      "title": "ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation",
      "abstract": "The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \\textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \\textbf{costly} to scale, \\textbf{brittle} across domains, and \\textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \\textbf{ALIVE} (\\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \\emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.",
      "authors": [
        "Yiwen Duan",
        "Jing Ye",
        "Xinpei Zhao"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T09:20:23+00:00",
      "link": "https://arxiv.org/pdf/2602.05472v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Adversarial learning for intrinsic reasoning acquisition",
      "llm_evidence_cn": "通过对抗学习获取内在推理能力",
      "llm_evidence": "通过对抗学习获取内在推理能力",
      "llm_tldr_en": "Introduces ALIVE, a framework using verbal evaluation to move beyond scalar rewards for LLM reasoning alignment.",
      "llm_tldr_cn": "引入ALIVE框架，利用言语评估超越标量奖励，实现LLM推理能力的对齐。",
      "llm_tldr": "引入ALIVE框架，利用言语评估超越标量奖励，实现LLM推理能力的对齐。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.05472v1",
      "carry_days": 1
    },
    {
      "id": "2601.08146v2",
      "title": "Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning",
      "abstract": "Adapting LLMs to low-resource languages is difficult: labeled data is scarce, full-model fine-tuning is unstable, and continued cross-lingual tuning can cause catastrophic forgetting. We propose Circuit-Targeted Supervised Fine-Tuning (CT-SFT): a counterfactual-free adaptation of CD-T (Contextual Decomposition Transformer) that uses a label-balanced mean baseline and task-directional relevance scoring to identify a sparse set of task-relevant attention heads in a proxy-language checkpoint, then transfer learns to a target language by updating only those heads (plus LayerNorm) via head-level gradient masking. Across NusaX-Senti and XNLI, CT-SFT improves cross-lingual accuracy over continued full fine-tuning while updating only a small subset of model parameters. We find an editing-preserving trade-off: harder transfers favor editing circuit heads, while easier transfers often favor near-zero (i.e., low-relevance heads) updates, preserving the source mechanism. CT-SFT also substantially reduces catastrophic forgetting, preserving proxy/source-language competence during transfer.",
      "authors": [
        "Khumaisa Nur'aini",
        "Ayu Purwarianti",
        "Alham Fikri Aji",
        "Derry Wijaya"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-13T02:20:53+00:00",
      "link": "https://arxiv.org/pdf/2601.08146v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Circuit-Targeted Supervised Fine-Tuning for low-resource adaptation",
      "llm_evidence_cn": "针对低资源适应的电路目标监督微调",
      "llm_evidence": "针对低资源适应的电路目标监督微调",
      "llm_tldr_en": "Proposes CT-SFT to adapt LLMs to low-resource languages by updating sparse task-relevant attention heads.",
      "llm_tldr_cn": "提出CT-SFT，通过更新稀疏的任务相关注意力头，将LLM适配到低资源语言。",
      "llm_tldr": "提出CT-SFT，通过更新稀疏的任务相关注意力头，将LLM适配到低资源语言。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.08146v2",
      "carry_days": 1
    },
    {
      "id": "2602.17937v1",
      "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification",
      "abstract": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.",
      "authors": [
        "Xiaotang Du",
        "Giwon Hong",
        "Wai-Chung Kwan",
        "Rohit Saxena",
        "Ivan Titov",
        "Pasquale Minervini",
        "Emily Allaway"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.PL"
      ],
      "published": "2026-02-20T01:56:27+00:00",
      "link": "https://arxiv.org/pdf/2602.17937v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Instruction optimization for Chain-of-Thought and reasoning performance",
      "llm_evidence_cn": "针对思维链和推理性能的指令优化",
      "llm_evidence": "针对思维链和推理性能的指令优化",
      "llm_tldr_en": "Compares instruction optimization techniques like CoT and ReAct for tabular fact verification using DSPy.",
      "llm_tldr_cn": "使用DSPy比较了CoT和ReAct等指令优化技术在表格事实验证中的应用。",
      "llm_tldr": "使用DSPy比较了CoT和ReAct等指令优化技术在表格事实验证中的应用。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.17937v1",
      "carry_days": 1
    },
    {
      "id": "2602.07215v1",
      "title": "Multi-Agentic AI for Fairness-Aware and Accelerated Multi-modal Large Model Inference in Real-world Mobile Edge Networks",
      "abstract": "Generative AI (GenAI) has transformed applications in natural language processing and content creation, yet centralized inference remains hindered by high latency, limited customizability, and privacy concerns. Deploying large models (LMs) in mobile edge networks emerges as a promising solution. However, it also poses new challenges, including heterogeneous multi-modal LMs with diverse resource demands and inference speeds, varied prompt/output modalities that complicate orchestration, and resource-limited infrastructure ill-suited for concurrent LM execution. In response, we propose a Multi-Agentic AI framework for latency- and fairness-aware multi-modal LM inference in mobile edge networks. Our solution includes a long-term planning agent, a short-term prompt scheduling agent, and multiple on-node LM deployment agents, all powered by foundation language models. These agents cooperatively optimize prompt routing and LM deployment through natural language reasoning over runtime telemetry and historical experience. To evaluate its performance, we further develop a city-wide testbed that supports network monitoring, containerized LM deployment, intra-server resource management, and inter-server communications. Experiments demonstrate that our solution reduces average latency by over 80% and improves fairness (Normalized Jain index) to 0.90 compared to other baselines. Moreover, our solution adapts quickly without fine-tuning, offering a generalizable solution for optimizing GenAI services in edge environments.",
      "authors": [
        "Haiyuan Li",
        "Hari Madhukumar",
        "Shuangyi Yan",
        "Yulei Wu",
        "Dimitra Simeonidou"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-02-06T21:52:49+00:00",
      "link": "https://arxiv.org/pdf/2602.07215v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Multi-agentic framework for accelerated LLM inference in edge networks",
      "llm_evidence_cn": "用于边缘网络中加速LLM推理的多智能体框架",
      "llm_evidence": "用于边缘网络中加速LLM推理的多智能体框架",
      "llm_tldr_en": "Proposes a multi-agent framework to optimize latency and fairness for LLM inference at the mobile edge.",
      "llm_tldr_cn": "提出多智能体框架，优化移动边缘LLM推理的延迟和公平性。",
      "llm_tldr": "提出多智能体框架，优化移动边缘LLM推理的延迟和公平性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.07215v1",
      "carry_days": 1
    },
    {
      "id": "2601.17551v1",
      "title": "GreenServ: Energy-Efficient Context-Aware Dynamic Routing for Multi-Model LLM Inference",
      "abstract": "Large language models (LLMs) demonstrate remarkable capabilities, but their broad deployment is limited by significant computational resource demands, particularly energy consumption during inference. Static, one-model-fits-all inference strategies are often inefficient, as they do not exploit the diverse range of available models or adapt to varying query requirements.   This paper presents GreenServ, a dynamic, context-aware routing framework that optimizes the trade-off between inference accuracy and energy efficiency. GreenServ extracts lightweight contextual features from each query, including task type, semantic cluster, and text complexity, and routes queries to the most suitable model from a heterogeneous pool, based on observed accuracy and energy usage. We employ a multi-armed bandit approach to learn adaptive routing policies online. This approach operates under partial feedback, eliminates the need for extensive offline calibration, and streamlines the integration of new models into the inference pipeline.   We evaluated GreenServ across five benchmark tasks and a pool of 16 contemporary open-access LLMs. Experimental results show that GreenServ consistently outperforms static (single-model) and random baselines. In particular, compared to random routing, GreenServ achieved a 22% increase in accuracy while reducing cumulative energy consumption by 31%. Finally, we evaluated GreenServ with RouterBench, achieving an average accuracy of 71.7% with a peak accuracy of 75.7%. All artifacts are open-source and available as an anonymous repository for review purposes here: https://anonymous.4open.science/r/llm-inference-router-EBEA/README.md",
      "authors": [
        "Thomas Ziller",
        "Shashikant Ilager",
        "Alessandro Tundo",
        "Ezio Bartocci",
        "Leonardo Mariani",
        "Ivona Brandic"
      ],
      "primary_category": "cs.PF",
      "categories": [
        "cs.PF",
        "cs.LG"
      ],
      "published": "2026-01-24T18:42:16+00:00",
      "link": "https://arxiv.org/pdf/2601.17551v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Energy-efficient dynamic routing for multi-model LLM inference",
      "llm_evidence_cn": "多模型LLM推理的节能动态路由",
      "llm_evidence": "多模型LLM推理的节能动态路由",
      "llm_tldr_en": "Introduces GreenServ to optimize the trade-off between inference accuracy and energy consumption via routing.",
      "llm_tldr_cn": "引入GreenServ，通过路由优化推理准确性与能耗之间的权衡。",
      "llm_tldr": "引入GreenServ，通过路由优化推理准确性与能耗之间的权衡。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.17551v1",
      "carry_days": 1
    },
    {
      "id": "2601.20417v2",
      "title": "SpeechMapper: Speech-to-text Embedding Projector for LLMs",
      "abstract": "Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.",
      "authors": [
        "Biswesh Mohapatra",
        "Marcely Zanon Boito",
        "Ioan Calapodescu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-28T09:22:58+00:00",
      "link": "https://arxiv.org/pdf/2601.20417v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "instruction tuning for speech-to-LLM embedding",
      "llm_evidence_cn": "语音到大语言模型嵌入的指令微调",
      "llm_evidence": "语音到大语言模型嵌入的指令微调",
      "llm_tldr_en": "SpeechMapper uses a two-stage training process to efficiently connect speech models to LLMs via instruction tuning.",
      "llm_tldr_cn": "SpeechMapper通过两阶段训练，利用指令微调高效地将语音模型连接到大语言模型。",
      "llm_tldr": "SpeechMapper通过两阶段训练，利用指令微调高效地将语音模型连接到大语言模型。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.20417v2",
      "carry_days": 1
    },
    {
      "id": "2601.07898v1",
      "title": "Large Language Models and Algorithm Execution: Application to an Arithmetic Function",
      "abstract": "Large Language Models (LLMs) have recently developed new advanced functionalities. Their effectiveness relies on statistical learning and generalization capabilities. However, they face limitations in internalizing the data they process and struggle, for instance, to autonomously execute algorithms. In this paper, we investigate the possibility of extending these models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. We introduce a training model called LLM-DAL (Large Language Model - Decompositional Algorithmic Learning), through which we demonstrate that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.",
      "authors": [
        "Farah Ben Slama",
        "Frédéric Armetta"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-12T12:27:59+00:00",
      "link": "https://arxiv.org/pdf/2601.07898v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "improving LLM reasoning and algorithm execution",
      "llm_evidence_cn": "提升大语言模型的推理与算法执行能力",
      "llm_evidence": "提升大语言模型的推理与算法执行能力",
      "llm_tldr_en": "Proposes LLM-DAL to enhance algorithmic reasoning and generalization through decompositional supervised training.",
      "llm_tldr_cn": "提出LLM-DAL，通过分解式监督训练增强大模型的算法推理和泛化能力。",
      "llm_tldr": "提出LLM-DAL，通过分解式监督训练增强大模型的算法推理和泛化能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.07898v1",
      "carry_days": 1
    },
    {
      "id": "2602.05235v1",
      "title": "FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.",
      "authors": [
        "Zhilin Liang",
        "Yuxiang Wang",
        "Zimu Zhou",
        "Hainan Zhang",
        "Boyi Liu",
        "Yongxin Tong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T02:52:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05235v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "federated retrieval-augmented generation via adapters",
      "llm_evidence_cn": "通过适配器实现联邦检索增强生成",
      "llm_evidence": "通过适配器实现联邦检索增强生成",
      "llm_tldr_en": "FedMosaic enables privacy-preserving RAG by using parametric adapters instead of sharing raw documents.",
      "llm_tldr_cn": "FedMosaic通过使用参数化适配器而非共享原始文档，实现了保护隐私的检索增强生成。",
      "llm_tldr": "FedMosaic通过使用参数化适配器而非共享原始文档，实现了保护隐私的检索增强生成。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.05235v1",
      "carry_days": 1
    },
    {
      "id": "2602.03103v1",
      "title": "Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision",
      "abstract": "Instruction tuning is now the default way to train and adapt large language models, but many instruction--input--output pairs are only weakly specified: for a given input, the same output can remain plausible under several alternative instructions. This raises a simple question: \\emph{does the instruction uniquely determine the target output?}   We propose the \\textbf{Task--Specificity Score (TSS)} to quantify how much an instruction matters for predicting its output, by contrasting the true instruction against plausible alternatives for the same input. We further introduce \\textbf{TSS++}, which uses hard alternatives and a small quality term to mitigate easy-negative effects. Across three instruction datasets (\\textsc{Alpaca}, \\textsc{Dolly-15k}, \\textsc{NI-20}) and three open LLMs (Gemma, Llama, Qwen), we show that selecting task-specific examples improves downstream performance under tight token budgets and complements quality-based filters such as perplexity and IFD.",
      "authors": [
        "Pritam Kadasi",
        "Abhishek Upperwal",
        "Mayank Singh"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-03T04:57:47+00:00",
      "link": "https://arxiv.org/pdf/2602.03103v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "measuring instruction importance in instruction tuning",
      "llm_evidence_cn": "衡量指令微调中指令的重要性",
      "llm_evidence": "衡量指令微调中指令的重要性",
      "llm_tldr_en": "Introduces Task-Specificity Score to quantify how much instructions influence output in LLM instruction tuning.",
      "llm_tldr_cn": "引入任务特定得分（TSS），量化指令在大模型指令微调中对输出的影响程度。",
      "llm_tldr": "引入任务特定得分（TSS），量化指令在大模型指令微调中对输出的影响程度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.03103v1",
      "carry_days": 1
    },
    {
      "id": "2602.19240v1",
      "title": "Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks.",
      "authors": [
        "Sen Zhao",
        "Lincheng Zhou",
        "Yue Chen",
        "Ding Zou"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-22T15:44:53+00:00",
      "link": "https://arxiv.org/pdf/2602.19240v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "graph-augmented generation for reasoning over relational loops",
      "llm_evidence_cn": "用于关系环路推理的图增强生成",
      "llm_evidence": "用于关系环路推理的图增强生成",
      "llm_tldr_en": "Enhances RAG for textual graphs by integrating higher-dimensional structures to improve reasoning capabilities.",
      "llm_tldr_cn": "通过集成高维结构增强文本图的RAG，以提升大模型的推理能力。",
      "llm_tldr": "通过集成高维结构增强文本图的RAG，以提升大模型的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.19240v1",
      "carry_days": 1
    },
    {
      "id": "2602.22642v1",
      "title": "Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning",
      "abstract": "Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.",
      "authors": [
        "Qin-Wen Luo",
        "Sheng Ren",
        "Xiang Chen",
        "Rui Liu",
        "Jun Fang",
        "Naiqiang Tan",
        "Sheng-Jun Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-26T05:47:30+00:00",
      "link": "https://arxiv.org/pdf/2602.22642v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "efficient LLM reasoning and Chain-of-Thought compression",
      "llm_evidence_cn": "高效的大模型推理与思维链压缩",
      "llm_evidence": "高效的大模型推理与思维链压缩",
      "llm_tldr_en": "Proposes entropy regularization to compress Chain-of-Thought reasoning without sacrificing model capability.",
      "llm_tldr_cn": "提出熵正则化方法，在不牺牲模型能力的前提下压缩思维链推理过程。",
      "llm_tldr": "提出熵正则化方法，在不牺牲模型能力的前提下压缩思维链推理过程。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.22642v1",
      "carry_days": 1
    },
    {
      "id": "2602.01075v2",
      "title": "ConvexBench: Can LLMs Recognize Convex Functions?",
      "abstract": "Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \\cb, a scalable and mechanically verifiable benchmark for testing \\textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \\textit{parsing failure} and \\textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).",
      "authors": [
        "Yepeng Liu",
        "Yu Huang",
        "Yu-Xiang Wang",
        "Yingbin Liang",
        "Yuheng Bu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-01T07:41:17+00:00",
      "link": "https://arxiv.org/pdf/2602.01075v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Reasoning and problem solving capabilities of LLMs in mathematics",
      "llm_evidence_cn": "大语言模型在数学领域的推理与问题解决能力",
      "llm_evidence": "大语言模型在数学领域的推理与问题解决能力",
      "llm_tldr_en": "Evaluates LLMs on a new benchmark for convex analysis to test deep compositional reasoning limits.",
      "llm_tldr_cn": "通过凸分析基准测试评估大模型在深度组合推理方面的性能瓶颈。",
      "llm_tldr": "通过凸分析基准测试评估大模型在深度组合推理方面的性能瓶颈。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.01075v2",
      "carry_days": 1
    },
    {
      "id": "2601.18999v1",
      "title": "Randomization Boosts KV Caching, Learning Balances Query Load: A Joint Perspective",
      "abstract": "KV caching is a fundamental technique for accelerating Large Language Model (LLM) inference by reusing key-value (KV) pairs from previous queries, but its effectiveness under limited memory is highly sensitive to the eviction policy. The default Least Recently Used (LRU) eviction algorithm struggles with dynamic online query arrivals, especially in multi-LLM serving scenarios, where balancing query load across workers and maximizing cache hit rate of each worker are inherently conflicting objectives. We give the first unified mathematical model that captures the core trade-offs between KV cache eviction and query routing. Our analysis reveals the theoretical limitations of existing methods and leads to principled algorithms that integrate provably competitive randomized KV cache eviction with learning-based methods to adaptively route queries with evolving patterns, thus balancing query load and cache hit rate. Our theoretical results are validated by extensive experiments across 4 benchmarks and 3 prefix-sharing settings, demonstrating improvements of up to 6.92$\\times$ in cache hit rate, 11.96$\\times$ reduction in latency, 14.06$\\times$ reduction in time-to-first-token (TTFT), and 77.4% increase in throughput over the state-of-the-art methods. Our code is available at https://github.com/fzwark/KVRouting.",
      "authors": [
        "Fangzhou Wu",
        "Sandeep Silwal",
        "Qiuyi",
        "Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-26T22:20:59+00:00",
      "link": "https://arxiv.org/pdf/2601.18999v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient deployment and KV cache inference optimization",
      "llm_evidence_cn": "高效部署与KV缓存推理优化",
      "llm_evidence": "高效部署与KV缓存推理优化",
      "llm_tldr_en": "Analyzes KV cache eviction and query routing to optimize LLM inference under memory constraints.",
      "llm_tldr_cn": "分析KV缓存剔除与查询路由，旨在优化内存受限下的大模型推理效率。",
      "llm_tldr": "分析KV缓存剔除与查询路由，旨在优化内存受限下的大模型推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.18999v1",
      "carry_days": 1
    },
    {
      "id": "2601.08584v1",
      "title": "Ministral 3",
      "abstract": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",
      "authors": [
        "Alexander H. Liu",
        "Kartik Khandelwal",
        "Sandeep Subramanian",
        "Victor Jouault",
        "Abhinav Rastogi",
        "Adrien Sadé",
        "Alan Jeffares",
        "Albert Jiang",
        "Alexandre Cahill",
        "Alexandre Gavaudan",
        "Alexandre Sablayrolles",
        "Amélie Héliou",
        "Amos You",
        "Andy Ehrenberg",
        "Andy Lo",
        "Anton Eliseev",
        "Antonia Calvi",
        "Avinash Sooriyarachchi",
        "Baptiste Bout",
        "Baptiste Rozière",
        "Baudouin De Monicault",
        "Clémence Lanfranchi",
        "Corentin Barreau",
        "Cyprien Courtot",
        "Daniele Grattarola",
        "Darius Dabert",
        "Diego de las Casas",
        "Elliot Chane-Sane",
        "Faruk Ahmed",
        "Gabrielle Berrada",
        "Gaëtan Ecrepont",
        "Gauthier Guinet",
        "Georgii Novikov",
        "Guillaume Kunsch",
        "Guillaume Lample",
        "Guillaume Martin",
        "Gunshi Gupta",
        "Jan Ludziejewski",
        "Jason Rute",
        "Joachim Studnia",
        "Jonas Amar",
        "Joséphine Delas",
        "Josselin Somerville Roberts",
        "Karmesh Yadav",
        "Khyathi Chandu",
        "Kush Jain",
        "Laurence Aitchison",
        "Laurent Fainsin",
        "Léonard Blier",
        "Lingxiao Zhao",
        "Louis Martin",
        "Lucile Saulnier",
        "Luyu Gao",
        "Maarten Buyl",
        "Margaret Jennings",
        "Marie Pellat",
        "Mark Prins",
        "Mathieu Poirée",
        "Mathilde Guillaumin",
        "Matthieu Dinot",
        "Matthieu Futeral",
        "Maxime Darrin",
        "Maximilian Augustin",
        "Mia Chiquier",
        "Michel Schimpf",
        "Nathan Grinsztajn",
        "Neha Gupta",
        "Nikhil Raghuraman",
        "Olivier Bousquet",
        "Olivier Duchenne",
        "Patricia Wang",
        "Patrick von Platen",
        "Paul Jacob",
        "Paul Wambergue",
        "Paula Kurylowicz",
        "Pavankumar Reddy Muddireddy",
        "Philomène Chagniot",
        "Pierre Stock",
        "Pravesh Agrawal",
        "Quentin Torroba",
        "Romain Sauvestre",
        "Roman Soletskyi",
        "Rupert Menneer",
        "Sagar Vaze",
        "Samuel Barry",
        "Sanchit Gandhi",
        "Siddhant Waghjale",
        "Siddharth Gandhi",
        "Soham Ghosh",
        "Srijan Mishra",
        "Sumukh Aithal",
        "Szymon Antoniak",
        "Teven Le Scao",
        "Théo Cachet",
        "Theo Simon Sorg",
        "Thibaut Lavril",
        "Thiziri Nait Saada",
        "Thomas Chabal",
        "Thomas Foubert",
        "Thomas Robert",
        "Thomas Wang",
        "Tim Lawson",
        "Tom Bewley",
        "Tom Bewley",
        "Tom Edwards",
        "Umar Jamil",
        "Umberto Tomasini",
        "Valeriia Nemychnikova",
        "Van Phung",
        "Vincent Maladière",
        "Virgile Richard",
        "Wassim Bouaziz",
        "Wen-Ding Li",
        "William Marshall",
        "Xinghui Li",
        "Xinyu Yang",
        "Yassine El Ouahidi",
        "Yihan Wang",
        "Yunhao Tang",
        "Zaccharie Ramzi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-13T14:06:03+00:00",
      "link": "https://arxiv.org/pdf/2601.08584v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Instruction finetuned and parameter-efficient dense models",
      "llm_evidence_cn": "指令微调与参数高效的稠密模型",
      "llm_evidence": "指令微调与参数高效的稠密模型",
      "llm_tldr_en": "Introduces the Ministral 3 series of efficient models with instruction tuning and reasoning variants.",
      "llm_tldr_cn": "发布Ministral 3系列高效模型，包含指令微调和推理增强等多个版本。",
      "llm_tldr": "发布Ministral 3系列高效模型，包含指令微调和推理增强等多个版本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.08584v1",
      "carry_days": 1
    },
    {
      "id": "2601.15434v2",
      "title": "ManuRAG: Multi-modal Retrieval Augmented Generation for Manufacturing Question Answering (Early Version)",
      "abstract": "The evolution of digital manufacturing requires intelligent Question Answering (QA) systems that can seamlessly integrate and analyze complex multi-modal data, such as text, images, formulas, and tables. Conventional Retrieval Augmented Generation (RAG) methods often fall short in handling this complexity, resulting in subpar performance. We introduce ManuRAG, an innovative multi-modal RAG framework designed for manufacturing QA, incorporating specialized techniques to improve answer accuracy, reliability, and interpretability. To benchmark performance, we evaluate ManuRAG on three datasets comprising a total of 1,515 QA pairs, corresponding to mathematical, multiple-choice, and review-based questions in manufacturing principles and practices. Experimental results show that ManuRAG consistently outperforms existing methods across all evaluated datasets. Furthermore, ManuRAG's adaptable design makes it applicable to other domains, including law, healthcare, and finance, positioning it as a versatile tool for domain-specific QA.",
      "authors": [
        "Yunqing Li",
        "Zihan Dong",
        "Farhad Ameri",
        "Jianbang Zhang"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE"
      ],
      "published": "2026-01-21T19:59:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15434v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Multi-modal Retrieval Augmented Generation framework",
      "llm_evidence_cn": "多模态检索增强生成框架",
      "llm_evidence": "多模态检索增强生成框架",
      "llm_tldr_en": "Proposes ManuRAG, a multi-modal RAG framework for complex question answering in manufacturing.",
      "llm_tldr_cn": "提出ManuRAG框架，利用多模态检索增强技术解决制造业中的复杂问答问题。",
      "llm_tldr": "提出ManuRAG框架，利用多模态检索增强技术解决制造业中的复杂问答问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.15434v2",
      "carry_days": 1
    },
    {
      "id": "2602.13680v1",
      "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
      "abstract": "Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that integrates Sliding Window Attention (SWA) with non-linear Test-Time Training (TTT) memory networks. \\textsc{AllMem} enables models to effectively scale to ultra-long contexts while mitigating catastrophic forgetting. This approach not only overcomes the representation constraints typical of linear memory models but also significantly reduces the computational and memory footprint during long-sequence inference. Furthermore, we implement a Memory-Efficient Fine-Tuning strategy to replace standard attention layers in pre-trained models with memory-augmented sliding window layers. This framework facilitates the efficient transformation of any off-the-shelf pre-trained LLM into an \\textsc{AllMem}-based architecture. Empirical evaluations confirm that our 4k window model achieves near-lossless performance on 37k LongBench with a marginal 0.83 drop compared to full attention. Furthermore, on InfiniteBench at a 128k context, our 8k window variant outperforms full attention, which validates the effectiveness of our parameterized memory in mitigating noise and maintaining robust long-range modeling without the prohibitive costs of global attention.",
      "authors": [
        "Ziming Wang",
        "Xiang Wang",
        "Kailong Peng",
        "Lang Qin",
        "Juan Gabriel Kostelec",
        "Christos Sourmpis",
        "Axel Laborieux",
        "Qinghai Guo"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-14T09:04:28+00:00",
      "link": "https://arxiv.org/pdf/2602.13680v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient long-context modeling and memory-efficient inference optimization",
      "llm_evidence_cn": "高效长文本建模与内存高效的推理优化",
      "llm_evidence": "高效长文本建模与内存高效的推理优化",
      "llm_tldr_en": "Introduces AllMem to reduce computational and memory overhead during long-sequence LLM inference.",
      "llm_tldr_cn": "推出AllMem架构，通过混合注意力与内存网络降低LLM长文本推理的计算与内存开销。",
      "llm_tldr": "推出AllMem架构，通过混合注意力与内存网络降低LLM长文本推理的计算与内存开销。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.13680v1",
      "carry_days": 1
    },
    {
      "id": "2602.14302v1",
      "title": "Floe: Federated Specialization for Real-Time LLM-SLM Inference",
      "abstract": "Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.",
      "authors": [
        "Chunlin Tian",
        "Kahou Tam",
        "Yebo Wu",
        "Shuaihang Zhong",
        "Li Li",
        "Nicholas D. Lane",
        "Chengzhong Xu"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published": "2026-02-15T20:28:38+00:00",
      "link": "https://arxiv.org/pdf/2602.14302v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient deployment and real-time LLM-SLM inference optimization",
      "llm_evidence_cn": "高效部署与实时LLM-SLM推理优化",
      "llm_evidence": "高效部署与实时LLM-SLM推理优化",
      "llm_tldr_en": "A federated framework combining cloud LLMs and edge SLMs for low-latency, privacy-preserving inference.",
      "llm_tldr_cn": "一种结合云端LLM与边缘SLM的联邦框架，旨在实现低延迟、保护隐私的推理部署。",
      "llm_tldr": "一种结合云端LLM与边缘SLM的联邦框架，旨在实现低延迟、保护隐私的推理部署。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.14302v1",
      "carry_days": 1
    },
    {
      "id": "2602.17809v1",
      "title": "Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning",
      "abstract": "Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.",
      "authors": [
        "Ibne Farabi Shihab",
        "Sanjeda Akter",
        "Anuj Sharma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-19T20:17:54+00:00",
      "link": "https://arxiv.org/pdf/2602.17809v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Bayesian parameter-efficient fine-tuning framework for LLM adaptation",
      "llm_evidence_cn": "用于LLM适配的贝叶斯参数高效微调框架",
      "llm_evidence": "用于LLM适配的贝叶斯参数高效微调框架",
      "llm_tldr_en": "Introduces Stiefel-Bayes Adapters for reliable and calibrated parameter-efficient fine-tuning of LLMs.",
      "llm_tldr_cn": "引入Stiefel-Bayes适配器，为LLM的参数高效微调提供可靠的置信度估计。",
      "llm_tldr": "引入Stiefel-Bayes适配器，为LLM的参数高效微调提供可靠的置信度估计。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.17809v1",
      "carry_days": 1
    },
    {
      "id": "2601.19924v1",
      "title": "OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \\textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.",
      "authors": [
        "Yitian Chen",
        "Cheng Cheng",
        "Yinan Sun",
        "Zi Ling",
        "Dongdong Ge"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-09T09:22:33+00:00",
      "link": "https://arxiv.org/pdf/2601.19924v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Evaluating LLMs' reasoning capabilities in optimization modeling",
      "llm_evidence_cn": "评估LLM在优化建模中的推理能力",
      "llm_evidence": "评估LLM在优化建模中的推理能力",
      "llm_tldr_en": "Benchmarks LLMs on complex optimization tasks to understand their automated formulation and problem-solving limits.",
      "llm_tldr_cn": "通过OPT-ENGINE基准测试评估LLM在复杂优化建模任务中的推理与问题解决能力。",
      "llm_tldr": "通过OPT-ENGINE基准测试评估LLM在复杂优化建模任务中的推理与问题解决能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.19924v1",
      "carry_days": 1
    },
    {
      "id": "2601.05616v1",
      "title": "Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks",
      "abstract": "In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.",
      "authors": [
        "ShaoZhen Liu",
        "Xinting Huang",
        "Houwen Peng",
        "Xin Chen",
        "Xinyang Song",
        "Qi Li",
        "Zhenan Sun"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-09T08:19:11+00:00",
      "link": "https://arxiv.org/pdf/2601.05616v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Enhancing reasoning with self-generated long chain-of-thought data",
      "llm_evidence_cn": "通过自生成的长思维链数据增强推理",
      "llm_evidence": "通过自生成的长思维链数据增强推理",
      "llm_tldr_en": "A two-stage training framework using self-evolved CoT data to improve LLM mathematical problem-solving.",
      "llm_tldr_cn": "提出双阶段训练框架，利用自演化的思维链数据提升LLM的数学推理与自纠错能力。",
      "llm_tldr": "提出双阶段训练框架，利用自演化的思维链数据提升LLM的数学推理与自纠错能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2601.05616v1",
      "carry_days": 1
    },
    {
      "id": "2602.11609v1",
      "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
      "abstract": "We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.   To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.   Code, data, and package are available at https://github.com/maitrix-org/scPilot",
      "authors": [
        "Yiming Gao",
        "Zhen Wang",
        "Jefferson Chen",
        "Mark Antkowiak",
        "Mengzhou Hu",
        "JungHo Kong",
        "Dexter Pratt",
        "Jieyuan Liu",
        "Enze Ma",
        "Zhiting Hu",
        "Eric P. Xing"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "q-bio.GN"
      ],
      "published": "2026-02-12T06:04:11+00:00",
      "link": "https://arxiv.org/pdf/2602.11609v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM reasoning capability for step-by-step problem solving in bioinformatics",
      "llm_evidence_cn": "大语言模型在生物信息学中逐步解决问题的推理能力",
      "llm_evidence": "大语言模型在生物信息学中逐步解决问题的推理能力",
      "llm_tldr_en": "Presents scPilot, a framework for LLM-based reasoning and automated analysis in single-cell RNA-seq data.",
      "llm_tldr_cn": "提出了 scPilot 框架，利用大语言模型的推理能力实现单细胞 RNA-seq 数据的自动化分析。",
      "llm_tldr": "提出了 scPilot 框架，利用大语言模型的推理能力实现单细胞 RNA-seq 数据的自动化分析。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.11609v1",
      "carry_days": 1
    },
    {
      "id": "2601.22132v1",
      "title": "Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference",
      "abstract": "Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.",
      "authors": [
        "Ziming Dong",
        "Hardik Sharma",
        "Evan O'Toole",
        "Jaya Prakash Champati",
        "Kui Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-29T18:52:54+00:00",
      "link": "https://arxiv.org/pdf/2601.22132v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cost-efficient inference and deployment of LLMs",
      "llm_evidence_cn": "大语言模型的高效推理与部署",
      "llm_evidence": "大语言模型的高效推理与部署",
      "llm_tldr_en": "Introduces LLM Shepherding to reduce inference costs by using LLM hints to guide smaller models.",
      "llm_tldr_cn": "引入LLM Shepherding框架，通过LLM提示引导小模型以降低推理成本。",
      "llm_tldr": "引入LLM Shepherding框架，通过LLM提示引导小模型以降低推理成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.22132v1",
      "carry_days": 1
    },
    {
      "id": "2601.11464v1",
      "title": "MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models",
      "abstract": "As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.",
      "authors": [
        "Xiaoran Fan",
        "Zhichao Sun",
        "Tao Ji",
        "Lixing Shen",
        "Tao Gui"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-01-16T17:45:34+00:00",
      "link": "https://arxiv.org/pdf/2601.11464v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient inference optimization and KV cache compression",
      "llm_evidence_cn": "高效推理优化与KV缓存压缩",
      "llm_evidence": "高效推理优化与KV缓存压缩",
      "llm_tldr_en": "Develops MHA2MLA-VLM to compress KV cache and accelerate vision-language model inference.",
      "llm_tldr_cn": "开发MHA2MLA-VLM框架，通过压缩KV缓存加速视觉语言模型的推理。",
      "llm_tldr": "开发MHA2MLA-VLM框架，通过压缩KV缓存加速视觉语言模型的推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.11464v1",
      "carry_days": 1
    },
    {
      "id": "2602.05393v1",
      "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better",
      "abstract": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \\textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\\times$ speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\\times$ fewer parameters than the target model.",
      "authors": [
        "Ji Zhao",
        "Yufei Gu",
        "Shitong Shao",
        "Xun Zhou",
        "Liang Xiang",
        "Zeke Xie"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-05T07:19:34+00:00",
      "link": "https://arxiv.org/pdf/2602.05393v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Accelerating LLM pre-training using existing models",
      "llm_evidence_cn": "利用现有模型加速大语言模型预训练",
      "llm_evidence": "利用现有模型加速大语言模型预训练",
      "llm_tldr_en": "Proposes Late-to-Early Training to accelerate large model pre-training using smaller pretrained models.",
      "llm_tldr_cn": "提出LET训练范式，利用现有小模型加速大模型的预训练过程。",
      "llm_tldr": "提出LET训练范式，利用现有小模型加速大模型的预训练过程。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2602.05393v1",
      "carry_days": 1
    },
    {
      "id": "2601.13244v1",
      "title": "Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks",
      "abstract": "Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.",
      "authors": [
        "Prateek Munjal",
        "Clement Christophe",
        "Ronnie Rajan",
        "Praveenkumar Kanithi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-19T17:26:49+00:00",
      "link": "https://arxiv.org/pdf/2601.13244v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Evaluation of instruction-tuned models versus base models",
      "llm_evidence_cn": "指令微调模型与基座模型的评估对比",
      "llm_evidence": "指令微调模型与基座模型的评估对比",
      "llm_tldr_en": "Investigates the limitations of instruction tuning on reasoning and domain-shifted benchmarks.",
      "llm_tldr_cn": "研究了指令微调在推理任务和领域迁移基准测试中的局限性。",
      "llm_tldr": "研究了指令微调在推理任务和领域迁移基准测试中的局限性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.13244v1",
      "carry_days": 1
    },
    {
      "id": "2601.14546v1",
      "title": "Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation",
      "abstract": "The quality of answers generated by large language models (LLMs) in retrieval-augmented generation (RAG) is largely influenced by the contextual information contained in the retrieved documents. A key challenge for improving RAG is to predict both the utility of retrieved documents -- quantified as the performance gain from using context over generation without context -- and the quality of the final answers in terms of correctness and relevance. In this paper, we define two prediction tasks within RAG. The first is retrieval performance prediction (RPP), which estimates the utility of retrieved documents. The second is generation performance prediction (GPP), which estimates the final answer quality. We hypothesise that in RAG, the topical relevance of retrieved documents correlates with their utility, suggesting that query performance prediction (QPP) approaches can be adapted for RPP and GPP. Beyond these retriever-centric signals, we argue that reader-centric features, such as the LLM's perplexity of the retrieved context conditioned on the input query, can further enhance prediction accuracy for both RPP and GPP. Finally, we propose that features reflecting query-agnostic document quality and readability can also provide useful signals to the predictions. We train linear regression models with the above categories of predictors for both RPP and GPP. Experiments on the Natural Questions (NQ) dataset show that combining predictors from multiple feature categories yields the most accurate estimates of RAG performance.",
      "authors": [
        "Fangzheng Tian",
        "Debasis Ganguly",
        "Craig Macdonald"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-01-20T23:59:54+00:00",
      "link": "https://arxiv.org/pdf/2601.14546v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Retrieval-augmented generation for knowledge intensive tasks",
      "llm_evidence_cn": "知识密集型任务的检索增强生成",
      "llm_evidence": "知识密集型任务的检索增强生成",
      "llm_tldr_en": "Defines tasks to predict retrieval utility and answer quality in RAG systems.",
      "llm_tldr_cn": "定义了预测RAG系统中检索效用和回答质量的任务，以优化生成效果。",
      "llm_tldr": "定义了预测RAG系统中检索效用和回答质量的任务，以优化生成效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.14546v1",
      "carry_days": 1
    },
    {
      "id": "2602.07213v1",
      "title": "Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used",
      "abstract": "Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.",
      "authors": [
        "Srijan Shakya",
        "Anamaria-Roberta Hartl",
        "Sepp Hochreiter",
        "Korbinian Pöppel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-06T21:48:26+00:00",
      "link": "https://arxiv.org/pdf/2602.07213v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Adaptive retrieval for reasoning and problem solving",
      "llm_evidence_cn": "用于推理和问题解决的自适应检索",
      "llm_evidence": "用于推理和问题解决的自适应检索",
      "llm_tldr_en": "Evaluates adaptive retrieval strategies for enhancing complex reasoning in large language models.",
      "llm_tldr_cn": "评估了自适应检索策略在增强大语言模型复杂推理能力方面的作用。",
      "llm_tldr": "评估了自适应检索策略在增强大语言模型复杂推理能力方面的作用。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.07213v1",
      "carry_days": 1
    },
    {
      "id": "2602.14536v1",
      "title": "Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets",
      "abstract": "Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.",
      "authors": [
        "Yuchen Yang",
        "Wenze Lin",
        "Enhao Huang",
        "Zhixuan Chu",
        "Hongbin Zhou",
        "Lan Tao",
        "Yiming Li",
        "Zhan Qin",
        "Kui Ren"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-16T07:49:33+00:00",
      "link": "https://arxiv.org/pdf/2602.14536v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Token-level noise filtering for LLM fine-tuning",
      "llm_evidence_cn": "大语言模型微调中的令牌级噪声过滤",
      "llm_evidence": "大语言模型微调中的令牌级噪声过滤",
      "llm_tldr_en": "Introduces XTF, a framework to filter token-level noise in datasets to improve LLM fine-tuning performance.",
      "llm_tldr_cn": "引入 XTF 框架，通过过滤数据集中的令牌级噪声来优化大语言模型的微调性能。",
      "llm_tldr": "引入 XTF 框架，通过过滤数据集中的令牌级噪声来优化大语言模型的微调性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.14536v1",
      "carry_days": 1
    },
    {
      "id": "2601.14603v1",
      "title": "Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum",
      "abstract": "Large Language Models (LLMs) achieve competitive performance across diverse natural language processing (NLP) tasks, yet pretraining is computationally demanding, making optimizer efficiency an important practical consideration. Muon accelerates LLM pretraining via orthogonal momentum updates that serve as a matrix analogue of the element-wise sign operator. Motivated by the recent perspective that Adam is a variance-adaptive sign update algorithm, we propose two variants of Muon, Muon-NSR and Muon-VS, which apply variance-adaptive normalization to momentum before orthogonalization. Muon-NSR applies noise-to-signal ratio (NSR) modulation, while Muon-VS performs variance-based scaling without introducing additional hyperparameters. Experiments on GPT-2 and LLaMA pretraining demonstrate that our proposed methods accelerate convergence and consistently achieve lower validation loss than both competitive, well-tuned AdamW and Muon baselines. For example, on the LLaMA-1.2B model, Muon-NSR and Muon-VS reduce the iterations required to reach the target validation loss by $1.36\\times$ relative to the well-tuned Muon following the recent benchmark.",
      "authors": [
        "Jingru Li",
        "Yibo Fan",
        "Huan Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-21T02:41:56+00:00",
      "link": "https://arxiv.org/pdf/2601.14603v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Accelerating LLM pretraining with variance-adaptive optimizers",
      "llm_evidence_cn": "利用方差自适应优化器加速大模型预训练",
      "llm_evidence": "利用方差自适应优化器加速大模型预训练",
      "llm_tldr_en": "Presents Muon variants to improve the efficiency and speed of Large Language Model pretraining.",
      "llm_tldr_cn": "提出 Muon 变体优化器，旨在提高大语言模型预训练的效率和速度。",
      "llm_tldr": "提出 Muon 变体优化器，旨在提高大语言模型预训练的效率和速度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2601.14603v1",
      "carry_days": 1
    },
    {
      "id": "2602.01734v1",
      "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration",
      "abstract": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.",
      "authors": [
        "Lianhai Ren",
        "Yucheng Ding",
        "Xiao Liu",
        "Qianxiao Li",
        "Peng Cheng",
        "Yeyun Gong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-02T07:18:45+00:00",
      "link": "https://arxiv.org/pdf/2602.01734v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Preventing training instability in LLM pretraining",
      "llm_evidence_cn": "防止大语言模型预训练中的训练不稳定",
      "llm_evidence": "防止大语言模型预训练中的训练不稳定",
      "llm_tldr_en": "Introduces MSign optimizer to restore weight matrix rank and prevent gradient explosions during LLM pretraining.",
      "llm_tldr_cn": "引入 MSign 优化器，通过恢复权重矩阵秩来防止大模型预训练中的梯度爆炸。",
      "llm_tldr": "引入 MSign 优化器，通过恢复权重矩阵秩来防止大模型预训练中的梯度爆炸。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.01734v1",
      "carry_days": 1
    },
    {
      "id": "2602.02987v1",
      "title": "Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control",
      "abstract": "Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \\emph{prefill} phase that processes user input, followed by a memory-bound \\emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.",
      "authors": [
        "Ruihan Lin",
        "Zezhen Ding",
        "Zean Han",
        "Jiheng Zhang"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "math.OC"
      ],
      "published": "2026-02-03T01:47:37+00:00",
      "link": "https://arxiv.org/pdf/2602.02987v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Inference optimization and prefill-decode contention",
      "llm_evidence_cn": "推理优化与预填充-解码竞争",
      "llm_evidence": "推理优化与预填充-解码竞争",
      "llm_tldr_en": "Develops a control framework to manage GPU resource contention between prefill and decode phases in LLM inference.",
      "llm_tldr_cn": "开发了一个随机控制框架，用于管理大模型推理中预填充和解码阶段的 GPU 资源竞争。",
      "llm_tldr": "开发了一个随机控制框架，用于管理大模型推理中预填充和解码阶段的 GPU 资源竞争。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.02987v1",
      "carry_days": 1
    },
    {
      "id": "2602.01410v1",
      "title": "SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training",
      "abstract": "Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.",
      "authors": [
        "Yunjie Pan",
        "Yongyi Yang",
        "Hanmei Yang",
        "Scott Mahlke"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "published": "2026-02-01T19:34:27+00:00",
      "link": "https://arxiv.org/pdf/2602.01410v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "adaptive mixed-precision training for LLM pretraining",
      "llm_evidence_cn": "LLM预训练的自适应混合精度训练",
      "llm_evidence": "LLM预训练的自适应混合精度训练",
      "llm_tldr_en": "Introduces SNIP, a framework for efficient subbyte precision LLM training to improve convergence and stability.",
      "llm_tldr_cn": "引入SNIP框架，通过子字节精度优化LLM预训练的效率与稳定性。",
      "llm_tldr": "引入SNIP框架，通过子字节精度优化LLM预训练的效率与稳定性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.01410v1",
      "carry_days": 1
    },
    {
      "id": "2602.17697v1",
      "title": "Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters",
      "abstract": "Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.",
      "authors": [
        "Nada Zine",
        "Clément Quinton",
        "Romain Rouvoy"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.SE"
      ],
      "published": "2026-02-06T16:18:22+00:00",
      "link": "https://arxiv.org/pdf/2602.17697v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "tuning inference hyperparameters for LLM optimization",
      "llm_evidence_cn": "调整推理超参数以优化LLM",
      "llm_evidence": "调整推理超参数以优化LLM",
      "llm_tldr_en": "Applies variability modeling to optimize LLM inference configuration for energy efficiency and performance.",
      "llm_tldr_cn": "利用可变性建模优化LLM推理配置，提升能效与性能。",
      "llm_tldr": "利用可变性建模优化LLM推理配置，提升能效与性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.17697v1",
      "carry_days": 1
    },
    {
      "id": "2601.07892v1",
      "title": "Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification",
      "abstract": "The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .",
      "authors": [
        "Hong Huang",
        "Decheng Wu",
        "Qiangqiang Hu",
        "Guanghua Yu",
        "Jinhai Yang",
        "Jianchen Zhu",
        "Xue Liu",
        "Dapeng Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-12T08:49:34+00:00",
      "link": "https://arxiv.org/pdf/2601.07892v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "hardware-efficient quantization for LLM deployment",
      "llm_evidence_cn": "用于LLM部署的硬件高效量化",
      "llm_evidence": "用于LLM部署的硬件高效量化",
      "llm_tldr_en": "Presents Sherry, a 1.25-bit ternary quantization framework for efficient LLM deployment on edge devices.",
      "llm_tldr_cn": "提出Sherry框架，通过1.25比特三值量化实现LLM在边缘设备的高效部署。",
      "llm_tldr": "提出Sherry框架，通过1.25比特三值量化实现LLM在边缘设备的高效部署。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.07892v1",
      "carry_days": 1
    },
    {
      "id": "2601.15710v1",
      "title": "FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design",
      "abstract": "We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.",
      "authors": [
        "Jiahao Zhang",
        "Zifan He",
        "Nicholas Fraser",
        "Michaela Blott",
        "Yizhou Sun",
        "Jason Cong"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22T07:31:51+00:00",
      "link": "https://arxiv.org/pdf/2601.15710v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "composable library for LLM inference acceleration",
      "llm_evidence_cn": "用于LLM推理加速的可组合库",
      "llm_evidence": "用于LLM推理加速的可组合库",
      "llm_tldr_en": "Develops FlexLLM, an HLS library for rapid creation of domain-specific, quantized LLM inference accelerators.",
      "llm_tldr_cn": "开发FlexLLM库，支持快速构建量化的领域特定LLM推理加速器。",
      "llm_tldr": "开发FlexLLM库，支持快速构建量化的领域特定LLM推理加速器。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.15710v1",
      "carry_days": 1
    },
    {
      "id": "2601.22297v1",
      "title": "Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning",
      "abstract": "The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.",
      "authors": [
        "Chenxi Liu",
        "Yanshuo Chen",
        "Ruibo Chen",
        "Tianyi Xiong",
        "Tong Zheng",
        "Heng Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-29T20:21:44+00:00",
      "link": "https://arxiv.org/pdf/2601.22297v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "self-debate reinforcement learning for LLM alignment",
      "llm_evidence_cn": "用于LLM对齐的自我辩论强化学习",
      "llm_evidence": "用于LLM对齐的自我辩论强化学习",
      "llm_tldr_en": "Proposes SDRL to improve LLM reasoning and multi-agent collaboration through reinforcement learning.",
      "llm_tldr_cn": "提出SDRL框架，通过强化学习提升LLM的推理与多智能体协作能力。",
      "llm_tldr": "提出SDRL框架，通过强化学习提升LLM的推理与多智能体协作能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2601.22297v1",
      "carry_days": 1
    },
    {
      "id": "2601.18195v1",
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.",
      "authors": [
        "Linhan Cao",
        "Wei Sun",
        "Weixia Zhang",
        "Xiangyang Zhu",
        "Kaiwei Zhang",
        "Jun Jia",
        "Dandan Zhu",
        "Guangtao Zhai",
        "Xiongkuo Min"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-26T06:27:03+00:00",
      "link": "https://arxiv.org/pdf/2601.18195v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "training-free Retrieval-Augmented Generation (RAG) framework",
      "llm_evidence_cn": "无需训练的检索增强生成 (RAG) 框架",
      "llm_evidence": "无需训练的检索增强生成 (RAG) 框架",
      "llm_tldr_en": "Introduces QualiRAG, a training-free RAG framework for visual quality assessment using multimodal models.",
      "llm_tldr_cn": "引入 QualiRAG，一种利用多模态模型进行视觉质量评估的无需训练的 RAG 框架。",
      "llm_tldr": "引入 QualiRAG，一种利用多模态模型进行视觉质量评估的无需训练的 RAG 框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.18195v1",
      "carry_days": 1
    },
    {
      "id": "2601.16462v1",
      "title": "Graph-Anchored Knowledge Indexing for Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a dominant paradigm for mitigating hallucinations in Large Language Models (LLMs) by incorporating external knowledge. Nevertheless, effectively integrating and interpreting key evidence scattered across noisy documents remains a critical challenge for existing RAG systems. In this paper, we propose GraphAnchor, a novel Graph-Anchored Knowledge Indexing approach that reconceptualizes graph structures from static knowledge representations into active, evolving knowledge indices. GraphAnchor incrementally updates a graph during iterative retrieval to anchor salient entities and relations, yielding a structured index that guides the LLM in evaluating knowledge sufficiency and formulating subsequent subqueries. The final answer is generated by jointly leveraging all retrieved documents and the final evolved graph. Experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of GraphAnchor, and reveal that GraphAnchor modulates the LLM's attention to more effectively associate key information distributed in retrieved documents. All code and data are available at https://github.com/NEUIR/GraphAnchor.",
      "authors": [
        "Zhenghao Liu",
        "Mingyan Wu",
        "Xinze Li",
        "Yukun Yan",
        "Shuo Wang",
        "Cheng Yang",
        "Minghe Yu",
        "Zheni Zeng",
        "Maosong Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-23T05:41:05+00:00",
      "link": "https://arxiv.org/pdf/2601.16462v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Graph-Anchored Knowledge Indexing for RAG",
      "llm_evidence_cn": "用于 RAG 的图锚定知识索引",
      "llm_evidence": "用于 RAG 的图锚定知识索引",
      "llm_tldr_en": "Proposes GraphAnchor to improve RAG by using evolving graph structures to index and retrieve key evidence.",
      "llm_tldr_cn": "提出 GraphAnchor，通过使用演化的图结构来索引和检索关键证据，从而改进 RAG。",
      "llm_tldr": "提出 GraphAnchor，通过使用演化的图结构来索引和检索关键证据，从而改进 RAG。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.16462v1",
      "carry_days": 1
    },
    {
      "id": "2601.12658v2",
      "title": "Augmenting Question Answering with A Hybrid RAG Approach",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.",
      "authors": [
        "Tianyi Yang",
        "Nashrah Haque",
        "Vaishnave Jonnalagadda",
        "Yuya Jeremy Ong",
        "Zhehui Chen",
        "Yanzhao Wu",
        "Lei Yu",
        "Divyesh Jadav",
        "Wenqi Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-19T02:08:47+00:00",
      "link": "https://arxiv.org/pdf/2601.12658v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "hybrid RAG architecture for Question-Answering",
      "llm_evidence_cn": "用于问答的混合 RAG 架构",
      "llm_evidence": "用于问答的混合 RAG 架构",
      "llm_tldr_en": "Introduces SSRAG, a hybrid architecture combining vector and graph retrieval to enhance QA accuracy.",
      "llm_tldr_cn": "引入 SSRAG，一种结合向量和图检索的混合架构，以增强问答准确性。",
      "llm_tldr": "引入 SSRAG，一种结合向量和图检索的混合架构，以增强问答准确性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.12658v2",
      "carry_days": 1
    },
    {
      "id": "2601.18116v1",
      "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning",
      "abstract": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.   We present \\textbf{FABLE}, a \\textbf{F}orest-based \\textbf{A}daptive \\textbf{B}i-path \\textbf{L}LM-\\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.   Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.",
      "authors": [
        "Lin Sun",
        "Linglin Zhang",
        "Jingang Huang",
        "Change Jia",
        "Zhengwei Cheng",
        "Xiangzheng Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-26T04:00:56+00:00",
      "link": "https://arxiv.org/pdf/2601.18116v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM-enhanced retrieval for multi-document reasoning",
      "llm_evidence_cn": "LLM 增强的多文档推理检索",
      "llm_evidence": "LLM 增强的多文档推理检索",
      "llm_tldr_en": "Presents FABLE, a forest-based adaptive retrieval framework to overcome long-context LLM limitations in RAG.",
      "llm_tldr_cn": "提出 FABLE，一种基于森林的自适应检索框架，以克服 RAG 中长上下文 LLM 的局限性。",
      "llm_tldr": "提出 FABLE，一种基于森林的自适应检索框架，以克服 RAG 中长上下文 LLM 的局限性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.18116v1",
      "carry_days": 1
    },
    {
      "id": "2602.03359v1",
      "title": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
      "abstract": "Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at https://github.com/ningding-o/MeKi.",
      "authors": [
        "Ning Ding",
        "Fangcheng Liu",
        "Kyungrae Kim",
        "Linji Hao",
        "Kyeng-Hun Lee",
        "Hyeonmok Ko",
        "Yehui Tang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-03T10:32:04+00:00",
      "link": "https://arxiv.org/pdf/2602.03359v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "efficient LLM scaling and edge device deployment",
      "llm_evidence_cn": "高效的 LLM 缩放与边缘设备部署",
      "llm_evidence": "高效的 LLM 缩放与边缘设备部署",
      "llm_tldr_en": "Proposes MeKi to scale LLM capacity via storage-based expert knowledge injection for efficient edge inference.",
      "llm_tldr_cn": "提出 MeKi，通过基于存储的专家知识注入来扩展 LLM 容量，实现高效的边缘推理。",
      "llm_tldr": "提出 MeKi，通过基于存储的专家知识注入来扩展 LLM 容量，实现高效的边缘推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.03359v1",
      "carry_days": 1
    },
    {
      "id": "2602.04620v1",
      "title": "QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning",
      "abstract": "GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.",
      "authors": [
        "Doyeon Lee",
        "Eunyi Lyou",
        "Hyunsoo Cho",
        "Sookyung Kim",
        "Joonseok Lee",
        "Jaemoo Choi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T14:51:04+00:00",
      "link": "https://arxiv.org/pdf/2602.04620v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "trust region policy optimization for LLM fine-tuning",
      "llm_evidence_cn": "用于 LLM 微调的置信域策略优化",
      "llm_evidence": "用于 LLM 微调的置信域策略优化",
      "llm_tldr_en": "Proposes QUATRO, a principled RL-based fine-tuning method to stabilize LLM alignment and optimization.",
      "llm_tldr_cn": "提出 QUATRO，一种基于强化学习的原则性微调方法，以稳定 LLM 的对齐和优化。",
      "llm_tldr": "提出 QUATRO，一种基于强化学习的原则性微调方法，以稳定 LLM 的对齐和优化。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.04620v1",
      "carry_days": 1
    },
    {
      "id": "2602.22812v1",
      "title": "Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching",
      "abstract": "Since local LLM inference on resource-constrained edge devices imposes a severe performance bottleneck, this paper proposes distributed prompt caching to enhance inference performance by cooperatively sharing intermediate processing states across multiple low-end edge devices. To fully utilize prompt similarity, our distributed caching mechanism also supports partial matching. As this approach introduces communication overhead associated with state sharing over a wireless network, we introduce a Bloom-filter-based data structure, referred to as a catalog, to determine whether a remote server possesses the desired internal states, thereby suppressing unnecessary communication. Experiments using the Gemma-3 270M model and the MMLU dataset on the Raspberry Pi Zero 2W platform demonstrate that the proposed approach reduces TTFT (Time to First Token) and TTLT (Time to Last Token) by 93.12% and 50.07% on average, respectively.",
      "authors": [
        "Hiroki Matsutani",
        "Naoki Matsuda",
        "Naoto Sugiura"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published": "2026-02-26T09:53:17+00:00",
      "link": "https://arxiv.org/pdf/2602.22812v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "distributed prompt caching for edge device inference",
      "llm_evidence_cn": "边缘设备推理的分布式提示缓存",
      "llm_evidence": "边缘设备推理的分布式提示缓存",
      "llm_tldr_en": "Proposes distributed prompt caching to accelerate LLM inference on resource-constrained edge devices.",
      "llm_tldr_cn": "提出分布式提示缓存技术，旨在加速资源受限边缘设备上的LLM推理性能。",
      "llm_tldr": "提出分布式提示缓存技术，旨在加速资源受限边缘设备上的LLM推理性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.22812v1",
      "carry_days": 1
    },
    {
      "id": "2602.06932v1",
      "title": "When RL Meets Adaptive Speculative Training: A Unified Training-Serving System",
      "abstract": "Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before deployment; (2) delayed utility feedback, since the true end-to-end decoding speedup is only known after training and cannot be inferred reliably from acceptance rate alone due to model-architecture and system-level overheads; and (3) domain-drift degradation, as the target model is repurposed to new domains and the speculator becomes stale and less effective.   To address these issues, we present Aurora, a unified training-serving system that closes the loop by continuously learning a speculator directly from live inference traces. Aurora reframes online speculator learning as an asynchronous reinforcement-learning problem: accepted tokens provide positive feedback, while rejected speculator proposals provide implicit negative feedback that we exploit to improve sample efficiency. Our design integrates an SGLang-based inference server with an asynchronous training server, enabling hot-swapped speculator updates without service interruption. Crucially, Aurora supports day-0 deployment: a speculator can be served immediately and rapidly adapted to live traffic, improving system performance while providing immediate utility feedback. Across experiments, Aurora achieves a 1.5x day-0 speedup on recently released frontier models (e.g., MiniMax M2.1 229B and Qwen3-Coder-Next 80B). Aurora also adapts effectively to distribution shifts in user traffic, delivering an additional 1.25x speedup over a well-trained but static speculator on widely used models (e.g., Qwen3 and Llama3).",
      "authors": [
        "Junxiong Wang",
        "Fengxiang Bie",
        "Jisen Li",
        "Zhongzhu Zhou",
        "Zelei Shao",
        "Yubo Wang",
        "Yinghui Liu",
        "Qingyang Wu",
        "Avner May",
        "Sri Yanamandra",
        "Yineng Zhang",
        "Ce Zhang",
        "Tri Dao",
        "Percy Liang",
        "Ben Athiwaratkun",
        "Shuaiwen Leon Song",
        "Chenfeng Xu",
        "Xiaoxia Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-06T18:28:54+00:00",
      "link": "https://arxiv.org/pdf/2602.06932v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "adaptive speculative training for LLM serving",
      "llm_evidence_cn": "LLM服务的自适应投机训练",
      "llm_evidence": "LLM服务的自适应投机训练",
      "llm_tldr_en": "Integrates RL with speculative decoding to accelerate LLM inference and reduce deployment lag.",
      "llm_tldr_cn": "将强化学习与投机解码结合，加速LLM推理并减少部署延迟。",
      "llm_tldr": "将强化学习与投机解码结合，加速LLM推理并减少部署延迟。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.06932v1",
      "carry_days": 1
    },
    {
      "id": "2601.18350v3",
      "title": "When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs",
      "abstract": "Large language models can exhibit surprising adapter interference when combining domain adaptation and instruction alignment in safety-critical settings. We study a two-stage LoRA pipeline for medical LLMs, where domain-oriented pre-training (PT) and supervised fine-tuning (SFT) are trained separately and later merged through weighted adapter merging. We observe that introducing PT signal can systematically alter model behavior and produce reasoning-style outputs, even when evaluation templates explicitly attempt to suppress such behavior. This interference leads to a divergence between surface metrics and reasoning or alignment behavior: BLEU/ROUGE scores drop significantly, while multiple-choice accuracy improves. We further show that small pipeline mistakes can easily misattribute SFT-only behavior to merged models, and provide a lightweight merge-verification routine to ensure correctness and reproducibility. Our findings highlight an interaction between knowledge injection and instruction alignment in adapter-based fine-tuning, with important implications for safety-critical model deployment.",
      "authors": [
        "Junyi Zou"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-26T10:54:06+00:00",
      "link": "https://arxiv.org/pdf/2601.18350v3",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "adapter merging and instruction alignment in LLMs",
      "llm_evidence_cn": "LLM中的适配器合并与指令对齐",
      "llm_evidence": "LLM中的适配器合并与指令对齐",
      "llm_tldr_en": "Studies how domain pre-training and instruction fine-tuning interact during adapter merging.",
      "llm_tldr_cn": "研究领域预训练与指令微调在适配器合并过程中的相互影响。",
      "llm_tldr": "研究领域预训练与指令微调在适配器合并过程中的相互影响。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.18350v3",
      "carry_days": 1
    },
    {
      "id": "2602.11513v1",
      "title": "Differentially Private and Communication Efficient Large Language Model Split Inference via Stochastic Quantization and Soft Prompt",
      "abstract": "Large Language Models (LLMs) have achieved remarkable performance and received significant research interest. The enormous computational demands, however, hinder the local deployment on devices with limited resources. The current prevalent LLM inference paradigms require users to send queries to the service providers for processing, which raises critical privacy concerns. Existing approaches propose to allow the users to obfuscate the token embeddings before transmission and utilize local models for denoising. Nonetheless, transmitting the token embeddings and deploying local models may result in excessive communication and computation overhead, preventing practical implementation. In this work, we propose \\textbf{DEL}, a framework for \\textbf{D}ifferentially private and communication \\textbf{E}fficient \\textbf{L}LM split inference. More specifically, an embedding projection module and a differentially private stochastic quantization mechanism are proposed to reduce the communication overhead in a privacy-preserving manner. To eliminate the need for local models, we adapt soft prompt at the server side to compensate for the utility degradation caused by privacy. To the best of our knowledge, this is the first work that utilizes soft prompt to improve the trade-off between privacy and utility in LLM inference, and extensive experiments on text generation and natural language understanding benchmarks demonstrate the effectiveness of the proposed method.",
      "authors": [
        "Yujie Gu",
        "Richeng Jin",
        "Xiaoyu Ji",
        "Yier Jin",
        "Wenyuan Xu"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published": "2026-02-12T03:13:16+00:00",
      "link": "https://arxiv.org/pdf/2602.11513v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "efficient and private LLM split inference",
      "llm_evidence_cn": "高效且私密的LLM拆分推理",
      "llm_evidence": "高效且私密的LLM拆分推理",
      "llm_tldr_en": "Proposes a private and communication-efficient split inference framework for LLMs.",
      "llm_tldr_cn": "提出一种针对LLM的隐私保护且通信高效的拆分推理框架。",
      "llm_tldr": "提出一种针对LLM的隐私保护且通信高效的拆分推理框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.11513v1",
      "carry_days": 1
    },
    {
      "id": "2602.08387v1",
      "title": "Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research",
      "abstract": "Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.",
      "authors": [
        "Max Lübbering",
        "Timm Ruland",
        "Richard Rutmann",
        "Felix Stollenwerk",
        "David Fitzek",
        "Michael Fromm",
        "Alexander Weber",
        "Rafet Sifa",
        "Nicolas Flores-Herr",
        "Joachim Köhler",
        "Mehdi Ali"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published": "2026-02-09T08:39:41+00:00",
      "link": "https://arxiv.org/pdf/2602.08387v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Framework for large-scale LLM training and research",
      "llm_evidence_cn": "大规模大语言模型训练与研究框架",
      "llm_evidence": "大规模大语言模型训练与研究框架",
      "llm_tldr_en": "Introduces Modalities, a PyTorch-native framework designed for efficient LLM pre-training and ablation studies.",
      "llm_tldr_cn": "介绍了Modalities，一个专为高效LLM预训练和消融实验设计的PyTorch原生框架。",
      "llm_tldr": "介绍了Modalities，一个专为高效LLM预训练和消融实验设计的PyTorch原生框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Large Language Models research papers and surveys",
      "matched_requirement_id": "req-1",
      "paper_id": "2602.08387v1",
      "carry_days": 1
    },
    {
      "id": "2602.13419v1",
      "title": "Protect$^*$: Steerable Retrosynthesis through Neuro-Symbolic State Encoding",
      "abstract": "Large Language Models (LLMs) have shown remarkable potential in scientific domains like retrosynthesis; yet, they often lack the fine-grained control necessary to navigate complex problem spaces without error. A critical challenge is directing an LLM to avoid specific, chemically sensitive sites on a molecule - a task where unconstrained generation can lead to invalid or undesirable synthetic pathways. In this work, we introduce Protect$^*$, a neuro-symbolic framework that grounds the generative capabilities of Large Language Models (LLMs) in rigorous chemical logic. Our approach combines automated rule-based reasoning - using a comprehensive database of 55+ SMARTS patterns and 40+ characterized protecting groups - with the generative intuition of neural models. The system operates via a hybrid architecture: an ``automatic mode'' where symbolic logic deterministically identifies and guards reactive sites, and a ``human-in-the-loop mode'' that integrates expert strategic constraints. Through ``active state tracking,'' we inject hard symbolic constraints into the neural inference process via a dedicated protection state linked to canonical atom maps. We demonstrate this neuro-symbolic approach through case studies on complex natural products, including the discovery of a novel synthetic pathway for Erythromycin B, showing that grounding neural generation in symbolic logic enables reliable, expert-level autonomy.",
      "authors": [
        "Shreyas Vinaya Sathyanarayana",
        "Shah Rahil Kirankumar",
        "Sharanabasava D. Hiremath",
        "Bharath Ramsundar"
      ],
      "primary_category": "q-bio.QM",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "q-bio.BM"
      ],
      "published": "2026-02-13T19:41:55+00:00",
      "link": "https://arxiv.org/pdf/2602.13419v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Neuro-symbolic framework for LLM reasoning in retrosynthesis",
      "llm_evidence_cn": "用于逆合成推理的神经符号框架",
      "llm_evidence": "用于逆合成推理的神经符号框架",
      "llm_tldr_en": "Combines chemical logic with LLMs to improve reasoning and control in scientific problem solving.",
      "llm_tldr_cn": "将化学逻辑与大模型结合，提升科学问题解决中的推理与控制能力。",
      "llm_tldr": "将化学逻辑与大模型结合，提升科学问题解决中的推理与控制能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.13419v1",
      "carry_days": 1
    },
    {
      "id": "2601.22735v1",
      "title": "MM-THEBench: Do Reasoning MLLMs Think Reasonably?",
      "abstract": "Recent advances in multimodal large language models (MLLMs) mark a shift from non-thinking models to post-trained reasoning models capable of solving complex problems through thinking. However, whether such thinking mitigates hallucinations in multimodal perception and reasoning remains unclear. Self-reflective reasoning enhances robustness but introduces additional hallucinations, and subtle perceptual errors still result in incorrect or coincidentally correct answers. Existing benchmarks primarily focus on models before the emergence of reasoning MLLMs, neglecting the internal thinking process and failing to measure the hallucinations that occur during thinking. To address these challenges, we introduce MM-THEBench, a comprehensive benchmark for assessing hallucinations of intermediate CoTs in reasoning MLLMs. MM-THEBench features a fine-grained taxonomy grounded in cognitive dimensions, diverse data with verified reasoning annotations, and a multi-level automated evaluation framework. Extensive experiments on mainstream reasoning MLLMs reveal insights into how thinking affects hallucination and reasoning capability in various multimodal tasks.",
      "authors": [
        "Zhidian Huang",
        "Zijun Yao",
        "Ji Qi",
        "Shangqing Tu",
        "Junxian Ma",
        "Jinxin Liu",
        "Weichuan Liu",
        "Xiaoyin Che",
        "Lei Hou",
        "Juanzi Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-30T09:17:50+00:00",
      "link": "https://arxiv.org/pdf/2601.22735v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Assessing hallucinations during the thinking process of reasoning MLLMs",
      "llm_evidence_cn": "评估推理型多模态大模型思考过程中的幻觉",
      "llm_evidence": "评估推理型多模态大模型思考过程中的幻觉",
      "llm_tldr_en": "Benchmarks hallucinations in the internal thinking and reasoning processes of advanced MLLMs.",
      "llm_tldr_cn": "针对高级多模态大模型的内部思考和推理过程中的幻觉进行基准测试。",
      "llm_tldr": "针对高级多模态大模型的内部思考和推理过程中的幻觉进行基准测试。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.22735v1",
      "carry_days": 1
    },
    {
      "id": "2601.07464v1",
      "title": "IFDNS: An Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning",
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across a wide range of reasoning tasks, including logical and mathematical problem-solving. While prompt-based methods like Chain-of-Thought (CoT) can enhance LLM reasoning abilities to some extent, they often suffer from a lack of faithfulness, where the derived conclusions may not align with the generated reasoning chain. To address this issue, researchers have explored neuro-symbolic approaches to bolster LLM logical reasoning capabilities. However, existing neuro-symbolic methods still face challenges with information loss during the process. To overcome these limitations, we introduce Iterative Feedback-Driven Neuro-Symbolic (IFDNS), a novel prompt-based method that employs a multi-round feedback mechanism to address LLM limitations in handling complex logical relationships. IFDNS utilizes iterative feedback during the logic extraction phase to accurately extract causal relationship statements and translate them into propositional and logical implication expressions, effectively mitigating information loss issues. Furthermore, IFDNS is orthogonal to existing prompt methods, allowing for seamless integration with various prompting approaches. Empirical evaluations across six datasets demonstrate the effectiveness of IFDNS in significantly improving the performance of CoT and Chain-of-Thought with Self-Consistency (CoT-SC). Specifically, IFDNS achieves a +9.40% accuracy boost for CoT on the LogiQA dataset and a +11.70% improvement for CoT-SC on the PrOntoQA dataset.",
      "authors": [
        "Xiaoheng Wang",
        "Tongxuan Liu",
        "Zi Gong",
        "Xianzhe Dong",
        "Yuting Zeng",
        "Minhan Hu",
        "Weizhe Huang",
        "Jing Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-12T12:20:19+00:00",
      "link": "https://arxiv.org/pdf/2601.07464v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Neuro-symbolic method for faithful logical reasoning",
      "llm_evidence_cn": "用于忠实逻辑推理的神经符号方法",
      "llm_evidence": "用于忠实逻辑推理的神经符号方法",
      "llm_tldr_en": "Introduces IFDNS, a neuro-symbolic approach to improve the faithfulness of LLM reasoning beyond standard CoT.",
      "llm_tldr_cn": "引入IFDNS神经符号方法，旨在解决CoT推理中结论与过程不一致的忠实度问题。",
      "llm_tldr": "引入IFDNS神经符号方法，旨在解决CoT推理中结论与过程不一致的忠实度问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2601.07464v1",
      "carry_days": 1
    },
    {
      "id": "2602.10965v1",
      "title": "MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs",
      "abstract": "Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable LLMs. Although MoEs offer strong efficiency and capacity scaling, naively adapting dense-model editors is both computationally costly and prone to routing distribution shifts that undermine stability and consistency. To address these challenges, we introduce MoEEdit, the first routing-stable framework for parameter-modifying knowledge editing in MoE LLMs. Our method reparameterizes expert updates via per-expert null-space projections that keep router inputs invariant and thereby suppress routing shifts. The resulting block-structured optimization is solved efficiently with a block coordinate descent (BCD) solver. Experiments show that MoEEdit attains state-of-the-art efficacy and generalization while preserving high specificity and routing stability, with superior compute and memory efficiency. These results establish a robust foundation for scalable, precise knowledge editing in sparse LLMs and underscore the importance of routing-stable interventions.",
      "authors": [
        "Yupu Gu",
        "Rongzhe Wei",
        "Andy Zhu",
        "Pan Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11T15:56:30+00:00",
      "link": "https://arxiv.org/pdf/2602.10965v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "parameter-modifying knowledge editing for Mixture-of-Experts LLMs",
      "llm_evidence_cn": "针对混合专家模型LLM的参数修改知识编辑",
      "llm_evidence": "针对混合专家模型LLM的参数修改知识编辑",
      "llm_tldr_en": "Introduces MoEEdit, a framework for stable and efficient knowledge editing in sparse MoE large language models.",
      "llm_tldr_cn": "引入MoEEdit框架，实现稀疏混合专家大语言模型中稳定且高效的知识编辑。",
      "llm_tldr": "引入MoEEdit框架，实现稀疏混合专家大语言模型中稳定且高效的知识编辑。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.10965v1",
      "carry_days": 1
    },
    {
      "id": "2602.02855v1",
      "title": "When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models",
      "abstract": "Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.",
      "authors": [
        "Gibbs Nwemadji",
        "Bruno Loureiro",
        "Jean Barbier"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "math.ST"
      ],
      "published": "2026-02-02T22:02:52+00:00",
      "link": "https://arxiv.org/pdf/2602.02855v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "dynamical analysis of LoRA fine-tuning after pre-training",
      "llm_evidence_cn": "预训练后LoRA微调的动力学分析",
      "llm_evidence": "预训练后LoRA微调的动力学分析",
      "llm_tldr_en": "Analyzes how excessive pre-training can unexpectedly slow down the convergence of LoRA fine-tuning optimization.",
      "llm_tldr_cn": "分析了过度预训练如何意外地减慢LoRA微调优化的收敛速度。",
      "llm_tldr": "分析了过度预训练如何意外地减慢LoRA微调优化的收敛速度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.02855v1",
      "carry_days": 1
    },
    {
      "id": "2602.20770v1",
      "title": "Pipeline for Verifying LLM-Generated Mathematical Solutions",
      "abstract": "With the growing popularity of Large Reasoning Models and their results in solving mathematical problems, it becomes crucial to measure their capabilities. We introduce a pipeline for both automatic and interactive verification as a more accurate alternative to only checking the answer which is currently the most popular approach for benchmarks. The pipeline can also be used as a generator of correct solutions both in formal and informal languages. 3 AI agents, which can be chosen for the benchmark accordingly, are included in the structure. The key idea is the use of prompts to obtain the solution in the specific form which allows for easier verification using proof assistants and possible use of small models ($\\le 8B$). Experiments on several datasets suggest low probability of False Positives. The open-source implementation with instructions on setting up a server is available at https://github.com/LogicEnj/lean4_verification_pipeline.",
      "authors": [
        "Varvara Sazonova",
        "Dmitri Shmelkin",
        "Stanislav Kikot",
        "Vasily Motolygin"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-24T11:01:25+00:00",
      "link": "https://arxiv.org/pdf/2602.20770v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "verifying LLM-generated mathematical solutions and reasoning",
      "llm_evidence_cn": "验证LLM生成的数学解法与推理",
      "llm_evidence": "验证LLM生成的数学解法与推理",
      "llm_tldr_en": "Introduces a pipeline for automatic and interactive verification of mathematical reasoning in large language models.",
      "llm_tldr_cn": "引入了一套用于自动和交互式验证大语言模型数学推理能力的流水线。",
      "llm_tldr": "引入了一套用于自动和交互式验证大语言模型数学推理能力的流水线。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.20770v1",
      "carry_days": 1
    },
    {
      "id": "2601.09981v2",
      "title": "DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models",
      "abstract": "Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to mitigate overthinking and the associated attention dispersion. Extensive experiments conducted on 3B and 7B variants of Qwen2.5-VL, as well as on both SAM2 and SAM3, demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation accuracy.",
      "authors": [
        "Yulin He",
        "Wei Chen",
        "Zhikang Jian",
        "Tianhang Guo",
        "Wenjuan Zhou",
        "Minglong Li",
        "Shaowu Yang",
        "Wenjing Yang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-15T01:48:45+00:00",
      "link": "https://arxiv.org/pdf/2601.09981v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Efficient reasoning and problem solving in MLLMs",
      "llm_evidence_cn": "多模态大模型中的高效推理与问题解决",
      "llm_evidence": "多模态大模型中的高效推理与问题解决",
      "llm_tldr_en": "Proposes a two-stage rollout strategy to improve reasoning efficiency and accuracy in multimodal tasks.",
      "llm_tldr_cn": "提出一种两阶段展开策略，旨在提高多模态任务中的推理效率和分割准确度。",
      "llm_tldr": "提出一种两阶段展开策略，旨在提高多模态任务中的推理效率和分割准确度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.09981v2",
      "carry_days": 1
    },
    {
      "id": "2601.13892v1",
      "title": "Multi-Objective Hierarchical Optimization with Large Language Models",
      "abstract": "Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.",
      "authors": [
        "Andrej Schwanke",
        "Lyubomir Ivanov",
        "David Salinas",
        "Frank Hutter",
        "Arber Zela"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-20T12:10:13+00:00",
      "link": "https://arxiv.org/pdf/2601.13892v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Leveraging LLM reasoning for multi-objective optimization",
      "llm_evidence_cn": "利用LLM推理能力进行多目标优化",
      "llm_evidence": "利用LLM推理能力进行多目标优化",
      "llm_tldr_en": "Uses LLMs as surrogate models and samplers within a hierarchical strategy for complex optimization tasks.",
      "llm_tldr_cn": "在分层策略中将LLM用作代理模型和采样器，处理复杂优化任务。",
      "llm_tldr": "在分层策略中将LLM用作代理模型和采样器，处理复杂优化任务。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.13892v1",
      "carry_days": 1
    },
    {
      "id": "2601.18554v1",
      "title": "Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities",
      "abstract": "Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.",
      "authors": [
        "Alberto Purpura",
        "Li Wang",
        "Sahil Badyal",
        "Eugenio Beaufrand",
        "Adam Faulkner"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-26T15:02:15+00:00",
      "link": "https://arxiv.org/pdf/2601.18554v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Evaluation of LLM instruction compliance and following abilities",
      "llm_evidence_cn": "评估大语言模型的指令合规性和遵循能力",
      "llm_evidence": "评估大语言模型的指令合规性和遵循能力",
      "llm_tldr_en": "Introduces a benchmark for granular evaluation of how well LLMs follow complex, multi-constraint instructions.",
      "llm_tldr_cn": "引入了一个基准，用于细粒度评估大语言模型遵循复杂多约束指令的能力。",
      "llm_tldr": "引入了一个基准，用于细粒度评估大语言模型遵循复杂多约束指令的能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.18554v1",
      "carry_days": 1
    },
    {
      "id": "2602.04634v1",
      "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
      "abstract": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
      "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Ruize Zhang",
        "Chunyang Zhu",
        "Shi Yu",
        "Weilin Liu",
        "Quanlu Zhang",
        "Wenbo Ding",
        "Chao Yu",
        "Yu Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2026-02-04T15:05:12+00:00",
      "link": "https://arxiv.org/pdf/2602.04634v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Multi-agent reinforcement learning for broad information seeking",
      "llm_evidence_cn": "用于广泛信息检索的多智能体强化学习",
      "llm_evidence": "用于广泛信息检索的多智能体强化学习",
      "llm_tldr_en": "Explores width scaling via multi-agent systems to enhance LLM reasoning and information seeking capabilities.",
      "llm_tldr_cn": "探索通过多智能体系统进行宽度扩展，以增强LLM的推理和信息检索能力。",
      "llm_tldr": "探索通过多智能体系统进行宽度扩展，以增强LLM的推理和信息检索能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.04634v1",
      "carry_days": 1
    },
    {
      "id": "2601.09285v1",
      "title": "Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction",
      "abstract": "Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.",
      "authors": [
        "Mianzhi Pan",
        "JianFei Li",
        "Peishuo Liu",
        "Botian Wang",
        "Yawen Ouyang",
        "Yiming Rong",
        "Hao Zhou",
        "Jianbing Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-01-14T08:45:07+00:00",
      "link": "https://arxiv.org/pdf/2601.09285v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Spatial-aware continual pre-training and instruction tuning for MOFs",
      "llm_evidence_cn": "针对MOF结构的空觉感知持续预训练与指令微调",
      "llm_evidence": "针对MOF结构的空觉感知持续预训练与指令微调",
      "llm_tldr_en": "Adapts LLMs for specialized structure prediction through domain-specific pre-training and instruction tuning.",
      "llm_tldr_cn": "通过领域特定的预训练和指令微调，使LLM适应专门的结构预测任务。",
      "llm_tldr": "通过领域特定的预训练和指令微调，使LLM适应专门的结构预测任务。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2601.09285v1",
      "carry_days": 1
    },
    {
      "id": "2602.19058v1",
      "title": "Do LLMs and VLMs Share Neurons for Inference? Evidence and Mechanisms of Cross-Modal Transfer",
      "abstract": "Large vision-language models (LVLMs) have rapidly advanced across various domains, yet they still lag behind strong text-only large language models (LLMs) on tasks that require multi-step inference and compositional decision-making. Motivated by their shared transformer architectures, we investigate whether the two model families rely on common internal computation for such inference. At the neuron level, we uncover a surprisingly large overlap: more than half of the top-activated units during multi-step inference are shared between representative LLMs and LVLMs, revealing a modality-invariant inference subspace.   Through causal probing via activation amplification, we further show that these shared neurons encode consistent and interpretable concept-level effects, demonstrating their functional contribution to inference. Building on this insight, we propose Shared Neuron Low-Rank Fusion (SNRF), a parameter-efficient framework that transfers mature inference circuitry from LLMs to LVLMs. SNRF profiles cross-model activations to identify shared neurons, computes a low-rank approximation of inter-model weight differences, and injects these updates selectively within the shared-neuron subspace. This mechanism strengthens multimodal inference performance with minimal parameter changes and requires no large-scale multimodal fine-tuning.   Across diverse mathematics and perception benchmarks, SNRF consistently enhances LVLM inference performance while preserving perceptual capabilities. Our results demonstrate that shared neurons form an interpretable bridge between LLMs and LVLMs, enabling low-cost transfer of inference ability into multimodal models. Our code is available at [https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons](https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons).",
      "authors": [
        "Chenhang Cui",
        "An Zhang",
        "Yuxin Chen",
        "Gelei Deng",
        "Jingnan Zheng",
        "Zhenkai Liang",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-22T06:04:05+00:00",
      "link": "https://arxiv.org/pdf/2602.19058v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Shared neurons for inference in LLMs and VLMs",
      "llm_evidence_cn": "LLM与VLM在推理中的共享神经元",
      "llm_evidence": "LLM与VLM在推理中的共享神经元",
      "llm_tldr_en": "Discovers shared internal computation mechanisms for multi-step inference across different model modalities.",
      "llm_tldr_cn": "发现了跨不同模态模型进行多步推理时共享的内部计算机制。",
      "llm_tldr": "发现了跨不同模态模型进行多步推理时共享的内部计算机制。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.19058v1",
      "carry_days": 1
    },
    {
      "id": "2601.22623v1",
      "title": "SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly",
      "abstract": "Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.",
      "authors": [
        "Wei Zhu",
        "Zhiwen Tang",
        "Kun Yue"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-01-30T06:26:34+00:00",
      "link": "https://arxiv.org/pdf/2601.22623v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "multi-agent planning for complex problem solving",
      "llm_evidence_cn": "用于复杂问题解决的多智能体规划",
      "llm_evidence": "用于复杂问题解决的多智能体规划",
      "llm_tldr_en": "Proposes SYMPHONY, a multi-agent framework using heterogeneous LLMs to improve MCTS planning and exploration.",
      "llm_tldr_cn": "提出SYMPHONY框架，通过异构大模型多智能体协作提升蒙特卡洛树搜索的规划性能。",
      "llm_tldr": "提出SYMPHONY框架，通过异构大模型多智能体协作提升蒙特卡洛树搜索的规划性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.22623v1",
      "carry_days": 1
    },
    {
      "id": "2602.11731v1",
      "title": "Thinking with Drafting: Optical Decompression via Logical Reconstruction",
      "abstract": "Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.",
      "authors": [
        "Jingxuan Wei",
        "Honghao He",
        "Caijun Jia",
        "Siyuan Li",
        "Zheng Sun",
        "Yuhang Xu",
        "Yuanyuan Lin",
        "Linzhuang Sun",
        "Yuchen Wu",
        "Bihui Yu",
        "Xiangxiang Zhang",
        "Cheng Tan"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12T08:54:02+00:00",
      "link": "https://arxiv.org/pdf/2602.11731v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "reasoning over visual inputs via logical reconstruction",
      "llm_evidence_cn": "通过逻辑重构进行视觉输入推理",
      "llm_evidence": "通过逻辑重构进行视觉输入推理",
      "llm_tldr_en": "Proposes Thinking with Drafting (TwD) to bridge visual perception and logical reasoning using a DSL.",
      "llm_tldr_cn": "提出TwD方法，利用领域特定语言作为中间表示，弥合视觉感知与逻辑推理之间的鸿沟。",
      "llm_tldr": "提出TwD方法，利用领域特定语言作为中间表示，弥合视觉感知与逻辑推理之间的鸿沟。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.11731v1",
      "carry_days": 1
    },
    {
      "id": "2602.17680v1",
      "title": "BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs",
      "abstract": "Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.",
      "authors": [
        "Yujia Wang",
        "Jihong Guan",
        "Wengen Li",
        "Shuigeng Zhou",
        "Xuhong Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T13:24:49+00:00",
      "link": "https://arxiv.org/pdf/2602.17680v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Domain-adaptive continual pretraining for biological reasoning",
      "llm_evidence_cn": "面向生物推理的领域自适应持续预训练",
      "llm_evidence": "面向生物推理的领域自适应持续预训练",
      "llm_tldr_en": "BioBridge uses continual pretraining to align protein models with LLMs for enhanced biological reasoning.",
      "llm_tldr_cn": "BioBridge通过持续预训练将蛋白质模型与大模型对齐，增强了生物语义推理能力。",
      "llm_tldr": "BioBridge通过持续预训练将蛋白质模型与大模型对齐，增强了生物语义推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.17680v1",
      "carry_days": 1
    },
    {
      "id": "2602.04634v2",
      "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
      "abstract": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
      "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Ruize Zhang",
        "Chunyang Zhu",
        "Shi Yu",
        "Weilin Liu",
        "Quanlu Zhang",
        "Wenbo Ding",
        "Chao Yu",
        "Yu Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2026-02-04T15:05:12+00:00",
      "link": "https://arxiv.org/pdf/2602.04634v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Multi-agent reinforcement learning for broad information seeking and reasoning",
      "llm_evidence_cn": "用于广泛信息寻求和推理的多智能体强化学习",
      "llm_evidence": "用于广泛信息寻求和推理的多智能体强化学习",
      "llm_tldr_en": "Explores width scaling via multi-agent reinforcement learning to enhance LLM problem-solving for broad tasks.",
      "llm_tldr_cn": "通过多智能体强化学习探索宽度扩展，以增强LLM处理广泛任务的解决能力。",
      "llm_tldr": "通过多智能体强化学习探索宽度扩展，以增强LLM处理广泛任务的解决能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.04634v2",
      "carry_days": 1
    },
    {
      "id": "2601.10131v2",
      "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
      "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \\textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
      "authors": [
        "Yizhan Li",
        "Florence Cloutier",
        "Sifan Wu",
        "Ali Parviz",
        "Boris Knyazev",
        "Yan Zhang",
        "Glen Berseth",
        "Bang Liu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-01-15T07:18:05+00:00",
      "link": "https://arxiv.org/pdf/2601.10131v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Retrieval-augmented multi-agent framework for molecule generation",
      "llm_evidence_cn": "用于分子生成的检索增强多智能体框架",
      "llm_evidence": "用于分子生成的检索增强多智能体框架",
      "llm_tldr_en": "Uses a retrieval-augmented multi-agent system and RL to solve complex, knowledge-intensive molecular design tasks.",
      "llm_tldr_cn": "利用检索增强的多智能体系统和强化学习来解决复杂的知识密集型分子设计任务。",
      "llm_tldr": "利用检索增强的多智能体系统和强化学习来解决复杂的知识密集型分子设计任务。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.10131v2",
      "carry_days": 1
    },
    {
      "id": "2602.01956v1",
      "title": "Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation",
      "abstract": "Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.",
      "authors": [
        "Seonghyeon Park",
        "Jewon Yeom",
        "Jaewon Sok",
        "Jeongjae Park",
        "Heejun Kim",
        "Taesup Kim"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02T11:03:37+00:00",
      "link": "https://arxiv.org/pdf/2602.01956v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Efficient uncertainty estimation for risk-aware LLM deployment",
      "llm_evidence_cn": "用于风险感知LLM部署的高效不确定性估计",
      "llm_evidence": "用于风险感知LLM部署的高效不确定性估计",
      "llm_tldr_en": "Develops a knowledge distillation framework to efficiently estimate LLM uncertainty for safer inference and deployment.",
      "llm_tldr_cn": "开发了一个知识蒸馏框架，以高效估计LLM不确定性，实现更安全的推理和部署。",
      "llm_tldr": "开发了一个知识蒸馏框架，以高效估计LLM不确定性，实现更安全的推理和部署。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.01956v1",
      "carry_days": 1
    },
    {
      "id": "2602.19938v1",
      "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
      "abstract": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.   We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.   We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference.",
      "authors": [
        "Zijie Liu",
        "Jie Peng",
        "Jinhao Duan",
        "Zirui Liu",
        "Kaixiong Zhou",
        "Mingfu Liang",
        "Luke Simon",
        "Xi Liu",
        "Zhaozhuo Xu",
        "Tianlong Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-23T15:11:16+00:00",
      "link": "https://arxiv.org/pdf/2602.19938v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Load balancing for sparse Mixture-of-Experts LLM inference",
      "llm_evidence_cn": "稀疏专家混合LLM推理的负载均衡",
      "llm_evidence": "稀疏专家混合LLM推理的负载均衡",
      "llm_tldr_en": "Analyzes and optimizes expert routing in SMoE models to improve inference efficiency and deployment performance.",
      "llm_tldr_cn": "分析并优化SMoE模型中的专家路由，以提高推理效率和部署性能。",
      "llm_tldr": "分析并优化SMoE模型中的专家路由，以提高推理效率和部署性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.19938v1",
      "carry_days": 1
    },
    {
      "id": "2602.10138v1",
      "title": "Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement",
      "abstract": "Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragmented and lacks systematic organization. This survey provides a comprehensive roadmap of this nascent frontier by structuring the domain's core components. We begin by analyzing the fundamental challenges of fusing visual and linguistic information in charts. We then categorize downstream tasks and datasets, introducing a novel taxonomy of canonical and non-canonical benchmarks to highlight the field's expanding scope. Subsequently, we present a comprehensive evolution of methodologies, tracing the progression from classic deep learning techniques to state-of-the-art MLLM paradigms that leverage sophisticated fusion strategies. By critically examining the limitations of current models, particularly their perceptual and reasoning deficits, we identify promising future directions, including advanced alignment techniques and reinforcement learning for cognitive enhancement. This survey aims to equip researchers and practitioners with a structured understanding of how MLLMs are transforming chart information fusion and to catalyze progress toward more robust and reliable systems.",
      "authors": [
        "Zhihang Yi",
        "Jian Zhao",
        "Jiancheng Lv",
        "Tao Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-08T12:59:50+00:00",
      "link": "https://arxiv.org/pdf/2602.10138v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Survey and taxonomy of Multimodal Large Language Models",
      "llm_evidence_cn": "多模态大语言模型的综述与分类",
      "llm_evidence": "多模态大语言模型的综述与分类",
      "llm_tldr_en": "Provides a comprehensive survey and taxonomy of MLLMs specifically for chart understanding and information fusion.",
      "llm_tldr_cn": "针对图表理解和信息融合领域，提供了多模态大语言模型的全面综述与分类。",
      "llm_tldr": "针对图表理解和信息融合领域，提供了多模态大语言模型的全面综述与分类。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "State-of-the-art large language models survey and taxonomy",
      "matched_requirement_id": "req-7",
      "paper_id": "2602.10138v1",
      "carry_days": 1
    },
    {
      "id": "2602.02465v1",
      "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
      "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
      "authors": [
        "Jana Zeller",
        "Thaddäus Wiedemer",
        "Fanfei Li",
        "Thomas Klein",
        "Prasanna Mayilvahanan",
        "Matthias Bethge",
        "Felix Wichmann",
        "Ryan Cotterell",
        "Wieland Brendel"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-02T18:49:06+00:00",
      "link": "https://arxiv.org/pdf/2602.02465v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Evaluating reasoning capabilities of multimodal frontier models",
      "llm_evidence_cn": "评估多模态前沿模型的推理能力",
      "llm_evidence": "评估多模态前沿模型的推理能力",
      "llm_tldr_en": "Evaluates whether intermediate visual imagery helps multimodal models solve complex multi-step reasoning problems.",
      "llm_tldr_cn": "评估中间视觉图像是否能帮助多模态模型解决复杂的跨步推理问题。",
      "llm_tldr": "评估中间视觉图像是否能帮助多模态模型解决复杂的跨步推理问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.02465v1",
      "carry_days": 1
    },
    {
      "id": "2601.13384v1",
      "title": "From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning",
      "abstract": "The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.",
      "authors": [
        "Jiajun Zhang",
        "Zeyu Cui",
        "Jiaxi Yang",
        "Lei Zhang",
        "Yuheng Jing",
        "Zeyao Ma",
        "Tianyi Bai",
        "Zilei Wang",
        "Qiang Liu",
        "Liang Wang",
        "Binyuan Hui",
        "Junyang Lin"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "published": "2026-01-19T20:33:53+00:00",
      "link": "https://arxiv.org/pdf/2601.13384v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Search-and-replace instruction tuning for code infilling",
      "llm_evidence_cn": "用于代码填充的搜索替换指令微调",
      "llm_evidence": "用于代码填充的搜索替换指令微调",
      "llm_tldr_en": "Proposes a search-and-replace framework to improve code infilling through instruction-following priors.",
      "llm_tldr_cn": "提出一种搜索并替换框架，通过指令遵循先验知识改进代码填充任务。",
      "llm_tldr": "提出一种搜索并替换框架，通过指令遵循先验知识改进代码填充任务。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.13384v1",
      "carry_days": 1
    },
    {
      "id": "2602.16100v1",
      "title": "LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum",
      "abstract": "With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).",
      "authors": [
        "Zijie Su",
        "Muhammed Tawfiqul Islam",
        "Mohammad Goudarzi",
        "Adel N. Toosi"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-18T00:09:09+00:00",
      "link": "https://arxiv.org/pdf/2602.16100v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "efficiently serving LLM inference under limited GPU resources",
      "llm_evidence_cn": "在有限GPU资源下高效提供LLM推理服务",
      "llm_evidence": "在有限GPU资源下高效提供LLM推理服务",
      "llm_tldr_en": "Explores intent-based orchestration for LLM serving in heterogeneous cloud-edge environments.",
      "llm_tldr_cn": "探索在异构云边环境下的意图驱动LLM推理编排方案。",
      "llm_tldr": "探索在异构云边环境下的意图驱动LLM推理编排方案。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.16100v1",
      "carry_days": 1
    },
    {
      "id": "2601.21522v1",
      "title": "More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)",
      "abstract": "The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.",
      "authors": [
        "Sagi Meir",
        "Tommer D. Keidar",
        "Noam Levi",
        "Shlomi Reuveni",
        "Barak Hirshberg"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-01-29T10:37:32+00:00",
      "link": "https://arxiv.org/pdf/2601.21522v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "improving inference of LLMs at a fixed budget",
      "llm_evidence_cn": "在固定预算下改进LLM推理",
      "llm_evidence": "在固定预算下改进LLM推理",
      "llm_tldr_en": "Introduces the Reset-and-Discard method to optimize LLM query coverage under cost constraints.",
      "llm_tldr_cn": "引入“重置与丢弃”方法，在成本限制下优化LLM查询的覆盖率。",
      "llm_tldr": "引入“重置与丢弃”方法，在成本限制下优化LLM查询的覆盖率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.21522v1",
      "carry_days": 1
    },
    {
      "id": "2601.18699v1",
      "title": "Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
      "abstract": "Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.",
      "authors": [
        "Olaf Yunus Laitinen Imanov"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-26T17:15:10+00:00",
      "link": "https://arxiv.org/pdf/2601.18699v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Mechanistic analysis of forgetting during LLM fine-tuning",
      "llm_evidence_cn": "LLM微调过程中灾难性遗忘的机制分析",
      "llm_evidence": "LLM微调过程中灾难性遗忘的机制分析",
      "llm_tldr_en": "Analyzes the mechanisms of catastrophic forgetting in Transformer-based LLMs during sequential fine-tuning.",
      "llm_tldr_cn": "分析了基于Transformer的LLM在连续微调过程中产生灾难性遗忘的机制。",
      "llm_tldr": "分析了基于Transformer的LLM在连续微调过程中产生灾难性遗忘的机制。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.18699v1",
      "carry_days": 1
    },
    {
      "id": "2601.19673v1",
      "title": "A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models",
      "abstract": "The present benchmarks for testing the audio modality of multimodal large language models concentrate on testing various audio tasks such as speaker diarization or gender identification in isolation. Whether a multimodal model can answer the questions that require reasoning skills to combine audio tasks of different categories, cannot be verified with their use. To address this issue, we propose Audio Reasoning Tasks (ART), a new benchmark for assessing the ability of multimodal models to solve problems that require reasoning over audio signal.",
      "authors": [
        "Iwona Christop",
        "Mateusz Czyżnikiewicz",
        "Paweł Skórzewski",
        "Łukasz Bondaruk",
        "Jakub Kubiak",
        "Marcin Lewandowski",
        "Marek Kubis"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "published": "2026-01-27T14:54:10+00:00",
      "link": "https://arxiv.org/pdf/2601.19673v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Benchmark for audio reasoning capabilities of multimodal LLMs",
      "llm_evidence_cn": "多模态大语言模型音频推理能力的基准测试",
      "llm_evidence": "多模态大语言模型音频推理能力的基准测试",
      "llm_tldr_en": "Introduces ART benchmark to assess reasoning skills in multimodal models across various audio tasks.",
      "llm_tldr_cn": "引入ART基准，评估多模态模型在各种音频任务中的推理能力。",
      "llm_tldr": "引入ART基准，评估多模态模型在各种音频任务中的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.19673v1",
      "carry_days": 1
    },
    {
      "id": "2602.04254v1",
      "title": "Scaling Agentic Verifier for Competitive Coding",
      "abstract": "Large language models (LLMs) have demonstrated strong coding capabilities but still struggle to solve competitive programming problems correctly in a single attempt. Execution-based re-ranking offers a promising test-time scaling strategy, yet existing methods are constrained by either difficult test case generation or inefficient random input sampling. To address this limitation, we propose Agentic Verifier, an execution-based agent that actively reasons about program behaviors and searches for highly discriminative test inputs that expose behavioral discrepancies among candidate solutions. Through multi-turn interaction with code execution environments, the verifier iteratively refines the candidate input generator and produces targeted counterexamples rather than blindly sampling inputs. We train the verifier to acquire this discriminative input generation capability via a scalable pipeline combining large-scale data synthesis, rejection fine-tuning, and agentic reinforcement learning. Extensive experiments across five competitive programming benchmarks demonstrate consistent improvements over strong execution-based baselines, achieving up to +10-15% absolute gains in Best@K accuracy. Further analysis reveals clear test-time scaling behavior and highlights the verifier's broader potential beyond reranking.",
      "authors": [
        "Zeyao Ma",
        "Jing Zhang",
        "Xiaokang Zhang",
        "Jiaxi Yang",
        "Zongmeng Zhang",
        "Jiajun Zhang",
        "Yuheng Jing",
        "Lei Zhang",
        "Hao Zheng",
        "Wenting Zhao",
        "Junyang Lin",
        "Binyuan Hui"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T06:30:40+00:00",
      "link": "https://arxiv.org/pdf/2602.04254v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Agentic verifier for reasoning and problem solving in coding",
      "llm_evidence_cn": "用于编程推理和问题解决的智能体验证器",
      "llm_evidence": "用于编程推理和问题解决的智能体验证器",
      "llm_tldr_en": "Proposes an execution-based agent to improve LLM coding performance through active reasoning and verification.",
      "llm_tldr_cn": "提出一种基于执行的智能体，通过主动推理和验证提高LLM的编程性能。",
      "llm_tldr": "提出一种基于执行的智能体，通过主动推理和验证提高LLM的编程性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.04254v1",
      "carry_days": 1
    },
    {
      "id": "2601.18924v1",
      "title": "RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures",
      "abstract": "Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.",
      "authors": [
        "Andrew Jaffe",
        "Noah Reicin",
        "Jinho D. Choi"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-26T19:52:42+00:00",
      "link": "https://arxiv.org/pdf/2601.18924v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Evaluating instruction following in complex prompt structures",
      "llm_evidence_cn": "评估复杂提示结构中的指令遵循能力",
      "llm_evidence": "评估复杂提示结构中的指令遵循能力",
      "llm_tldr_en": "Introduces RIFT to test how prompt topology and structural ordering affect LLM instruction following.",
      "llm_tldr_cn": "引入RIFT，测试提示拓扑和结构顺序如何影响LLM的指令遵循。",
      "llm_tldr": "引入RIFT，测试提示拓扑和结构顺序如何影响LLM的指令遵循。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.18924v1",
      "carry_days": 1
    },
    {
      "id": "2602.01541v1",
      "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.",
      "authors": [
        "Boyi Li",
        "Yifan Shen",
        "Yuanzhe Liu",
        "Yifan Xu",
        "Jiateng Liu",
        "Xinzhuo Li",
        "Zhengyuan Li",
        "Jingyuan Zhu",
        "Yunhan Zhong",
        "Fangzhou Lan",
        "Jianguo Cao",
        "James M. Rehg",
        "Heng Ji",
        "Ismini Lourentzou",
        "Xu Cao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-02T02:19:50+00:00",
      "link": "https://arxiv.org/pdf/2602.01541v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Scaling Chain-of-Thought reasoning in multimodal LLMs",
      "llm_evidence_cn": "多模态大语言模型中的思维链推理扩展",
      "llm_evidence": "多模态大语言模型中的思维链推理扩展",
      "llm_tldr_en": "Introduces Cognitive Supersensing to enhance MLLM reasoning via visual imagery and latent prediction.",
      "llm_tldr_cn": "引入认知超感官训练范式，通过视觉想象增强多模态大模型的推理能力。",
      "llm_tldr": "引入认知超感官训练范式，通过视觉想象增强多模态大模型的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.01541v1",
      "carry_days": 1
    },
    {
      "id": "2602.03773v1",
      "title": "Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL",
      "abstract": "Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.",
      "authors": [
        "Ian Wu",
        "Yuxiao Qu",
        "Amrith Setlur",
        "Aviral Kumar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03T17:34:04+00:00",
      "link": "https://arxiv.org/pdf/2602.03773v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Constructing reasoning chains for continual improvement",
      "llm_evidence_cn": "构建推理链以实现持续改进",
      "llm_evidence": "构建推理链以实现持续改进",
      "llm_tldr_en": "Introduces an iterative decoding algorithm that uses reasoning chains to improve LLM performance at test time.",
      "llm_tldr_cn": "引入一种迭代解码算法，通过推理链使大模型在测试时具备持续改进能力。",
      "llm_tldr": "引入一种迭代解码算法，通过推理链使大模型在测试时具备持续改进能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.03773v1",
      "carry_days": 1
    },
    {
      "id": "2601.07372v1",
      "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.",
      "authors": [
        "Xin Cheng",
        "Wangding Zeng",
        "Damai Dai",
        "Qinyu Chen",
        "Bingxuan Wang",
        "Zhenda Xie",
        "Kezhao Huang",
        "Xingkai Yu",
        "Zhewen Hao",
        "Yukun Li",
        "Han Zhang",
        "Huishuai Zhang",
        "Dongyan Zhao",
        "Wenfeng Liang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-12T09:54:49+00:00",
      "link": "https://arxiv.org/pdf/2601.07372v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "New axis of sparsity for Large Language Models",
      "llm_evidence_cn": "大语言模型的新型稀疏化维度",
      "llm_evidence": "大语言模型的新型稀疏化维度",
      "llm_tldr_en": "Introduces conditional memory to optimize the trade-off between computation and knowledge lookup in LLMs.",
      "llm_tldr_cn": "引入条件记忆模块，优化大模型在神经计算与知识检索之间的权衡。",
      "llm_tldr": "引入条件记忆模块，优化大模型在神经计算与知识检索之间的权衡。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Transformer architecture for language modeling",
      "matched_requirement_id": "req-2",
      "paper_id": "2601.07372v1",
      "carry_days": 1
    },
    {
      "id": "2602.02405v1",
      "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
      "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.",
      "authors": [
        "Ethan Mendes",
        "Jungsoo Park",
        "Alan Ritter"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02T18:03:43+00:00",
      "link": "https://arxiv.org/pdf/2602.02405v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Improving reasoning capabilities through learnable chain-of-thought data",
      "llm_evidence_cn": "通过可学习的思维链数据提升推理能力",
      "llm_evidence": "通过可学习的思维链数据提升推理能力",
      "llm_tldr_en": "Transforms expert solutions into learnable reasoning signals to enhance LLM problem-solving performance.",
      "llm_tldr_cn": "将专家方案转化为可学习的推理信号，以增强大语言模型处理复杂问题的能力。",
      "llm_tldr": "将专家方案转化为可学习的推理信号，以增强大语言模型处理复杂问题的能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.02405v1",
      "carry_days": 1
    },
    {
      "id": "2601.21164v2",
      "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
      "abstract": "Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.",
      "authors": [
        "Jingyun Wang",
        "Dian Li",
        "Xiaohan Wang",
        "Gang Liu",
        "Jiahong Yan",
        "Guoliang Kang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-29T02:03:33+00:00",
      "link": "https://arxiv.org/pdf/2601.21164v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Unleashing LLM potential for geometric reasoning and problem solving",
      "llm_evidence_cn": "释放LLM在几何推理与问题解决方面的潜力",
      "llm_evidence": "释放LLM在几何推理与问题解决方面的潜力",
      "llm_tldr_en": "Uses concise geometric descriptions to enable LLMs to solve multimodal plane geometry problems via reasoning.",
      "llm_tldr_cn": "利用简洁的几何描述，使LLM能够通过文本推理解决多模态平面几何问题。",
      "llm_tldr": "利用简洁的几何描述，使LLM能够通过文本推理解决多模态平面几何问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.21164v2",
      "carry_days": 1
    },
    {
      "id": "2602.20980v1",
      "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language integration and faster inference. However, existing heuristically predefined supervision signals in latent CoT provide limited guidance for preserving critical visual information in intermediate latent states. To address this limitation, we propose CrystaL (Crystallized Latent Reasoning), a single-stage framework with two paths to process intact and corrupted images, respectively. By explicitly aligning the attention patterns and prediction distributions across the two paths, CrystaL crystallizes latent representations into task-relevant visual semantics, without relying on auxiliary annotations or external modules. Extensive experiments on perception-intensive benchmarks demonstrate that CrystaL consistently outperforms state-of-the-art baselines, achieving substantial gains in fine-grained visual understanding while maintaining robust reasoning capabilities.",
      "authors": [
        "Yang Zhang",
        "Danyang Li",
        "Yuxuan Li",
        "Xin Zhang",
        "Tianyu Xie",
        "Mingming Cheng",
        "Xiang Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-24T15:01:30+00:00",
      "link": "https://arxiv.org/pdf/2602.20980v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Latent Chain-of-Thought reasoning in MLLMs",
      "llm_evidence_cn": "多模态大模型中的隐式思维链推理",
      "llm_evidence": "多模态大模型中的隐式思维链推理",
      "llm_tldr_en": "Proposes CrystaL for spontaneous emergence of visual latents in multimodal reasoning tasks.",
      "llm_tldr_cn": "提出CrystaL框架，在多模态推理任务中实现视觉潜变量的自发涌现。",
      "llm_tldr": "提出CrystaL框架，在多模态推理任务中实现视觉潜变量的自发涌现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.20980v1",
      "carry_days": 1
    },
    {
      "id": "2602.04089v1",
      "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL",
      "abstract": "Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.",
      "authors": [
        "Xiaofeng Lin",
        "Sirou Zhu",
        "Yilei Chen",
        "Mingyu Chen",
        "Hejian Sang",
        "Ioannis Paschalidis",
        "Zhipeng Wang",
        "Aldo Pacchiano",
        "Xuezhou Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-03T23:53:05+00:00",
      "link": "https://arxiv.org/pdf/2602.04089v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "scaling in-context online learning and decision-making",
      "llm_evidence_cn": "扩展上下文在线学习与决策能力",
      "llm_evidence": "扩展上下文在线学习与决策能力",
      "llm_tldr_en": "Introduces ORBIT to enhance LLM decision-making and in-context learning through meta-reinforcement learning.",
      "llm_tldr_cn": "通过元强化学习框架ORBIT提升LLM的在线决策与上下文学习能力。",
      "llm_tldr": "通过元强化学习框架ORBIT提升LLM的在线决策与上下文学习能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.04089v1",
      "carry_days": 1
    },
    {
      "id": "2601.09398v1",
      "title": "Ability Transfer and Recovery via Modularized Parameters Localization",
      "abstract": "Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.",
      "authors": [
        "Songyao Jin",
        "Kun Zhou",
        "Wenqi Li",
        "Peng Wang",
        "Biwei Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-14T11:42:39+00:00",
      "link": "https://arxiv.org/pdf/2601.09398v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "modularized parameters localization for fine-tuning",
      "llm_evidence_cn": "用于微调的模块化参数定位",
      "llm_evidence": "用于微调的模块化参数定位",
      "llm_tldr_en": "Proposes ACT to localize ability-related channels, preventing catastrophic forgetting during LLM fine-tuning.",
      "llm_tldr_cn": "提出ACT方法定位能力相关通道，缓解LLM微调中的灾难性遗忘。",
      "llm_tldr": "提出ACT方法定位能力相关通道，缓解LLM微调中的灾难性遗忘。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.09398v1",
      "carry_days": 1
    },
    {
      "id": "2602.04900v1",
      "title": "Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization",
      "abstract": "As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the benefits of container orchestration, such as scalability and resource efficiency, to complex AI workflows. We implement and evaluate an illustrative, multi-stage use case consisting of automatic speech recognition and summarization. First, we address batch inference by using Kueue to manage jobs that transcribe audio files with Whisper models and Dynamic Accelerator Slicer (DAS) to increase parallel job execution. Second, we address a discrete online inference scenario by feeding the transcripts to a Large Language Model for summarization hosted using llm-d, a novel solution utilizing the recent developments around the Kubernetes Gateway API Inference Extension (GAIE) for optimized routing of inference requests. Our findings illustrate that these complementary components (Kueue, DAS, and GAIE) form a cohesive, high-performance platform, proving Kubernetes' capability to serve as a unified foundation for demanding GenAI workloads: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36%; and GAIE improved Time to First Token by 82\\%.",
      "authors": [
        "Sai Sindhur Malleni",
        "Raúl Sevilla",
        "Aleksei Vasilevskii",
        "José Castillo Lema",
        "André Bauer"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-02-03T15:36:08+00:00",
      "link": "https://arxiv.org/pdf/2602.04900v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "evaluating Kubernetes for LLM inference deployment",
      "llm_evidence_cn": "评估用于LLM推理部署的Kubernetes",
      "llm_evidence": "评估用于LLM推理部署的Kubernetes",
      "llm_tldr_en": "Evaluates Kubernetes-native tools for managing and scaling LLM inference workloads in production.",
      "llm_tldr_cn": "评估Kubernetes原生工具在管理和扩展LLM推理工作负载中的表现。",
      "llm_tldr": "评估Kubernetes原生工具在管理和扩展LLM推理工作负载中的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.04900v1",
      "carry_days": 1
    },
    {
      "id": "2601.17789v1",
      "title": "Neuro-Symbolic Verification on Instruction Following of LLMs",
      "abstract": "A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.",
      "authors": [
        "Yiming Su",
        "Kunzhao Xu",
        "Yanjie Gao",
        "Fan Yang",
        "Cheng Li",
        "Mao Yang",
        "Tianyin Xu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-25T11:03:15+00:00",
      "link": "https://arxiv.org/pdf/2601.17789v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "verifying instruction following of LLMs",
      "llm_evidence_cn": "验证大语言模型的指令遵循能力",
      "llm_evidence": "验证大语言模型的指令遵循能力",
      "llm_tldr_en": "Presents NSVIF, a neuro-symbolic framework to verify if LLM outputs strictly follow provided instructions.",
      "llm_tldr_cn": "提出 NSVIF，一个用于验证 LLM 输出是否严格遵循所提供指令的神经符号框架。",
      "llm_tldr": "提出 NSVIF，一个用于验证 LLM 输出是否严格遵循所提供指令的神经符号框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.17789v1",
      "carry_days": 1
    },
    {
      "id": "2602.02047v1",
      "title": "Dissecting Outlier Dynamics in LLM NVFP4 Pretraining",
      "abstract": "Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with \"post-QK\" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.",
      "authors": [
        "Peijie Dong",
        "Ruibo Fan",
        "Yuechen Tao",
        "Di Mou",
        "Wenhu Hu",
        "Zhenheng Tang",
        "Yinghao Yu",
        "Jiamang Wang",
        "Wenbo Su",
        "Guodong Yang",
        "Liping Zhang",
        "Xiaowen Chu",
        "Baochun Li",
        "Bo Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-02T12:50:27+00:00",
      "link": "https://arxiv.org/pdf/2602.02047v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "outlier dynamics in LLM NVFP4 pretraining",
      "llm_evidence_cn": "LLM NVFP4 预训练中的离群值动态",
      "llm_evidence": "LLM NVFP4 预训练中的离群值动态",
      "llm_tldr_en": "Analyzes outlier dynamics during 4-bit precision pretraining to improve LLM training efficiency and stability.",
      "llm_tldr_cn": "分析 4 位精度预训练期间的离群值动态，以提高 LLM 训练效率和稳定性。",
      "llm_tldr": "分析 4 位精度预训练期间的离群值动态，以提高 LLM 训练效率和稳定性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2602.02047v1",
      "carry_days": 1
    },
    {
      "id": "2601.16038v1",
      "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval",
      "abstract": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.",
      "authors": [
        "Olga Bunkova",
        "Lorenzo Di Fruscia",
        "Sophia Rupprecht",
        "Artur M. Schweidtmann",
        "Marcel J. T. Reinders",
        "Jana M. Weber"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22T15:11:02+00:00",
      "link": "https://arxiv.org/pdf/2601.16038v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "grounding LLMs in knowledge graphs for retrieval",
      "llm_evidence_cn": "将 LLM 植根于知识图谱进行检索",
      "llm_evidence": "将 LLM 植根于知识图谱进行检索",
      "llm_tldr_en": "Evaluates LLM interaction with reaction knowledge graphs for synthesis retrieval via Text2Cypher generation.",
      "llm_tldr_cn": "评估 LLM 与反应知识图谱的交互，通过 Text2Cypher 生成进行合成路径检索。",
      "llm_tldr": "评估 LLM 与反应知识图谱的交互，通过 Text2Cypher 生成进行合成路径检索。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.16038v1",
      "carry_days": 1
    },
    {
      "id": "2602.09701v1",
      "title": "GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation",
      "abstract": "We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.   Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.   We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.",
      "authors": [
        "Sandesh Hegde",
        "Jaison Saji Chacko",
        "Debarshi Banerjee",
        "Uma Mahesh"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-10T11:59:14+00:00",
      "link": "https://arxiv.org/pdf/2602.09701v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "RL-driven reasoning for vision-language models",
      "llm_evidence_cn": "强化学习驱动的视觉语言模型推理",
      "llm_evidence": "强化学习驱动的视觉语言模型推理",
      "llm_tldr_en": "Uses RL to improve reasoning-based image segmentation in vision-language models.",
      "llm_tldr_cn": "利用强化学习提升视觉语言模型中基于推理的图像分割能力。",
      "llm_tldr": "利用强化学习提升视觉语言模型中基于推理的图像分割能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.09701v1",
      "carry_days": 1
    }
  ]
}