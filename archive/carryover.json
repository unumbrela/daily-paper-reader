{
  "generated_at": "2026-02-28T18:59:03.929681+00:00",
  "updated_date": "20260220-20260228",
  "carryover_days": 9,
  "items": [
    {
      "id": "2601.22362v1",
      "title": "Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \\emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.",
      "authors": [
        "Julien Delavande",
        "Regis Pierrard",
        "Sasha Luccioni"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-29T22:16:25+00:00",
      "link": "https://arxiv.org/pdf/2601.22362v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Inference optimization and serving strategies for LLMs",
      "llm_evidence_cn": "大语言模型的推理优化与服务策略",
      "llm_evidence": "大语言模型的推理优化与服务策略",
      "llm_tldr_en": "Studies the impact of quantization and batching on LLM inference energy efficiency and latency.",
      "llm_tldr_cn": "研究了量化、批处理和服务配置对大语言模型推理能效和延迟的影响。",
      "llm_tldr": "研究了量化、批处理和服务配置对大语言模型推理能效和延迟的影响。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.22362v1",
      "carry_days": 1
    },
    {
      "id": "2601.17755v1",
      "title": "ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation",
      "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.",
      "authors": [
        "Jinyoung Park",
        "Sanghyeok Lee",
        "Omar Zia Khan",
        "Hyunwoo J. Kim",
        "Joo-Kyung Kim"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-25T08:58:44+00:00",
      "link": "https://arxiv.org/pdf/2601.17755v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Graph Retrieval Augmented Generation using reinforcement learning",
      "llm_evidence_cn": "使用强化学习的图检索增强生成",
      "llm_evidence": "使用强化学习的图检索增强生成",
      "llm_tldr_en": "Enhances GraphRAG with progress-aware reinforcement learning for better reasoning in knowledge-intensive tasks.",
      "llm_tldr_cn": "通过进度感知强化学习增强GraphRAG，以在知识密集型任务中实现更好的推理。",
      "llm_tldr": "通过进度感知强化学习增强GraphRAG，以在知识密集型任务中实现更好的推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.17755v1",
      "carry_days": 1
    },
    {
      "id": "2602.05728v1",
      "title": "CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering",
      "abstract": "Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.   In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.   Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.",
      "authors": [
        "Hao Yang",
        "Zhiyu Yang",
        "Xupeng Zhang",
        "Wei Wei",
        "Yunjie Zhang",
        "Lin Yang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-05T14:52:06+00:00",
      "link": "https://arxiv.org/pdf/2602.05728v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG for multi-hop question answering and knowledge tasks",
      "llm_evidence_cn": "用于多跳问答和知识任务的检索增强生成",
      "llm_evidence": "用于多跳问答和知识任务的检索增强生成",
      "llm_tldr_en": "Proposes CompactRAG to reduce token overhead and LLM calls in complex multi-hop RAG scenarios.",
      "llm_tldr_cn": "提出CompactRAG框架，旨在减少复杂多跳检索增强生成场景中的Token开销和调用次数。",
      "llm_tldr": "提出CompactRAG框架，旨在减少复杂多跳检索增强生成场景中的Token开销和调用次数。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.05728v1",
      "carry_days": 1
    },
    {
      "id": "2602.02382v1",
      "title": "ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs",
      "abstract": "Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.",
      "authors": [
        "Ziyan Zhang",
        "Chao Wang",
        "Zhuo Chen",
        "Chiyi Li",
        "Kai Song"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-02T17:45:43+00:00",
      "link": "https://arxiv.org/pdf/2602.02382v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Retrieval-augmented LLM reasoning with chain-of-thought",
      "llm_evidence_cn": "结合思维链的检索增强LLM推理",
      "llm_evidence": "结合思维链的检索增强LLM推理",
      "llm_tldr_en": "Combines neighborhood retrieval with chain-of-thought reasoning to solve complex logic queries.",
      "llm_tldr_cn": "结合邻域检索与思维链推理，解决复杂的逻辑查询问题。",
      "llm_tldr": "结合邻域检索与思维链推理，解决复杂的逻辑查询问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.02382v1",
      "carry_days": 1
    },
    {
      "id": "2601.17260v1",
      "title": "The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment",
      "abstract": "Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $β$) yields progressively \"better\" behavior. We instead treat $β$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $β\\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $β$ induces capability losses that persist even after $β$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $β$ landscape rather than reliance on margins or aggregate benchmarks.",
      "authors": [
        "Marco Pollanen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-24T02:19:20+00:00",
      "link": "https://arxiv.org/pdf/2601.17260v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "DPO alignment and reasoning capability",
      "llm_evidence_cn": "DPO 对齐与推理能力",
      "llm_evidence": "DPO 对齐与推理能力",
      "llm_tldr_en": "Analyzes how the beta parameter in Direct Preference Optimization (DPO) affects model alignment and reasoning.",
      "llm_tldr_cn": "分析了直接偏好优化 (DPO) 中的 beta 参数如何影响模型对齐和推理能力。",
      "llm_tldr": "分析了直接偏好优化 (DPO) 中的 beta 参数如何影响模型对齐和推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2601.17260v1",
      "carry_days": 1
    },
    {
      "id": "2602.22593v1",
      "title": "FLYING SERVING: On-the-Fly Parallelism Switching for Large Language Model Serving",
      "abstract": "Production LLM serving must simultaneously deliver high throughput, low latency, and sufficient context capacity under non-stationary traffic and mixed request requirements. Data parallelism (DP) maximizes throughput by running independent replicas, while tensor parallelism (TP) reduces per-request latency and pools memory for long-context inference. However, existing serving stacks typically commit to a static parallelism configuration at deployment; adapting to bursts, priorities, or long-context requests is often disruptive and slow. We present Flying Serving, a vLLM-based system that enables online DP-TP switching without restarting engine workers. Flying Serving makes reconfiguration practical by virtualizing the state that would otherwise force data movement: (i) a zero-copy Model Weights Manager that exposes TP shard views on demand, (ii) a KV Cache Adaptor that preserves request KV state across DP/TP layouts, (iii) an eagerly initialized Communicator Pool to amortize collective setup, and (iv) a deadlock-free scheduler that coordinates safe transitions under execution skew. Across three popular LLMs and realistic serving scenarios, Flying Serving improves performance by up to $4.79\\times$ under high load and $3.47\\times$ under low load while supporting latency- and memory-driven requests.",
      "authors": [
        "Shouwei Gao",
        "Junqi Yin",
        "Feiyi Wang",
        "Wenqian Dong"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-26T03:55:51+00:00",
      "link": "https://arxiv.org/pdf/2602.22593v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "On-the-fly parallelism switching for LLM serving and inference optimization",
      "llm_evidence_cn": "LLM 服务和推理优化的动态并行切换",
      "llm_evidence": "LLM 服务和推理优化的动态并行切换",
      "llm_tldr_en": "Introduces Flying Serving to enable dynamic DP-TP switching for optimized LLM inference throughput and latency.",
      "llm_tldr_cn": "引入 Flying Serving，实现动态 DP-TP 切换，以优化 LLM 推理吞吐量和延迟。",
      "llm_tldr": "引入 Flying Serving，实现动态 DP-TP 切换，以优化 LLM 推理吞吐量和延迟。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.22593v1",
      "carry_days": 1
    },
    {
      "id": "2602.13647v1",
      "title": "PT-RAG: Structure-Fidelity Retrieval-Augmented Generation for Academic Papers",
      "abstract": "Retrieval-augmented generation (RAG) is increasingly applied to question-answering over long academic papers, where accurate evidence allocation under a fixed token budget is critical. Existing approaches typically flatten academic papers into unstructured chunks during preprocessing, which destroys the native hierarchical structure. This loss forces retrieval to operate in a disordered space, thereby producing fragmented contexts, misallocating tokens to non-evidential regions under finite token budgets, and increasing the reasoning burden for downstream language models. To address these issues, we propose PT-RAG, an RAG framework that treats the native hierarchical structure of academic papers as a low-entropy retrieval prior. PT-RAG first inherits the native hierarchy to construct a structure-fidelity PaperTree index, which prevents entropy increase at the source. It then designs a path-guided retrieval mechanism that aligns query semantics to relevant sections and selects high relevance root-to-leaf paths under a fixed token budget, yielding compact, coherent, and low-entropy retrieval contexts. In contrast to existing RAG approaches, PT-RAG avoids entropy increase caused by destructive preprocessing and provides a native low-entropy structural basis for subsequent retrieval. To assess this design, we introduce entropy-based structural diagnostics that quantify retrieval fragmentation and evidence allocation accuracy. On three academic question-answering benchmarks, PT-RAG achieves consistently lower section entropy and evidence alignment cross entropy than strong baselines, indicating reduced context fragmentation and more precise allocation to evidential regions. These structural advantages directly translate into higher answer quality.",
      "authors": [
        "Rui Yu",
        "Tianyi Wang",
        "Ruixia Liu",
        "Yinglong Wang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-14T07:40:09+00:00",
      "link": "https://arxiv.org/pdf/2602.13647v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG framework using hierarchical structure as a retrieval prior",
      "llm_evidence_cn": "利用层级结构作为检索先验的RAG框架",
      "llm_evidence": "利用层级结构作为检索先验的RAG框架",
      "llm_tldr_en": "Introduces PT-RAG to improve retrieval accuracy in academic papers by preserving their native hierarchical structure.",
      "llm_tldr_cn": "提出PT-RAG框架，通过保留学术论文的层级结构来优化检索增强生成的效果。",
      "llm_tldr": "提出PT-RAG框架，通过保留学术论文的层级结构来优化检索增强生成的效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.13647v1",
      "carry_days": 1
    },
    {
      "id": "2602.13628v1",
      "title": "Compact LLM Deployment and World Model Assisted Offloading in Mobile Edge Computing",
      "abstract": "This paper investigates compact large language model (LLM) deployment and world-model-assisted inference offloading in mobile edge computing (MEC) networks. We first propose an edge compact LLM deployment (ECLD) framework that jointly applies structured pruning, low-bit quantization, and knowledge distillation to construct edge-deployable LLM variants, and we evaluate these models using four complementary metrics: accessibility, energy consumption, hallucination rate, and generalization accuracy. Building on the resulting compact models, we formulate an MEC offloading optimization problem that minimizes the long-term average inference latency subject to per-device energy budgets and LLM-specific quality-of-service constraints on effective accuracy and hallucination. To solve this problem under unknown and time-varying network dynamics, we develop a world model-proximal policy optimization (PPO) algorithm, which augments an on-policy PPO algorithm with a learned recurrent world model that provides improved value targets and short imagination rollouts. Extensive experiments on Llama-3.1-8B, Qwen3-8B, and Mistral-12B show that ECLD compresses base models by about 70-80% in storage (i.e., from 15.3 GB to 3.3 GB for Llama-3.1-8B) and reduces per-query energy consumption by up to 50%, while largely preserving accuracy and often lowering hallucination compared with quantization-only or pruning-only baselines. Moreover, they also show that world model-PPO speeds up convergence by about 50%, improves the final reward by 15.8% over vanilla PPO, and reduces average inference latency by 12-30% across different user populations, while satisfying the accuracy and hallucination constraints and approaching the generation quality of always-offloading with much of the efficiency of local execution.",
      "authors": [
        "Ruichen Zhang",
        "Xiaofeng Luo",
        "Jiayi He",
        "Dusit Niyato",
        "Jiawen Kang",
        "Zehui Xiong",
        "Yonghui Li"
      ],
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI"
      ],
      "published": "2026-02-14T06:37:29+00:00",
      "link": "https://arxiv.org/pdf/2602.13628v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "compact LLM deployment and inference offloading",
      "llm_evidence_cn": "紧凑型LLM部署与推理卸载",
      "llm_evidence": "紧凑型LLM部署与推理卸载",
      "llm_tldr_en": "Combines pruning, quantization, and distillation for efficient LLM deployment in mobile edge computing.",
      "llm_tldr_cn": "结合剪枝、量化和蒸馏技术，优化移动边缘计算环境下的LLM部署与推理卸载。",
      "llm_tldr": "结合剪枝、量化和蒸馏技术，优化移动边缘计算环境下的LLM部署与推理卸载。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.13628v1",
      "carry_days": 1
    },
    {
      "id": "2602.04620v2",
      "title": "QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning",
      "abstract": "GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.",
      "authors": [
        "Doyeon Lee",
        "Eunyi Lyou",
        "Hyunsoo Cho",
        "Sookyung Kim",
        "Joonseok Lee",
        "Jaemoo Choi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T14:51:04+00:00",
      "link": "https://arxiv.org/pdf/2602.04620v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Trust-region policy optimization for LLM fine-tuning",
      "llm_evidence_cn": "LLM微调的置信域策略优化",
      "llm_evidence": "LLM微调的置信域策略优化",
      "llm_tldr_en": "Introduces QUATRO, a principled RL-based fine-tuning method for more stable and controlled LLM updates.",
      "llm_tldr_cn": "引入QUATRO，一种基于强化学习的微调方法，使LLM更新更加稳定可控。",
      "llm_tldr": "引入QUATRO，一种基于强化学习的微调方法，使LLM更新更加稳定可控。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.04620v2",
      "carry_days": 1
    },
    {
      "id": "2601.16478v1",
      "title": "DeepEra: A Deep Evidence Reranking Agent for Scientific Retrieval-Augmented Generated Question Answering",
      "abstract": "With the rapid growth of scientific literature, scientific question answering (SciQA) has become increasingly critical for exploring and utilizing scientific knowledge. Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating knowledge from external sources, thereby providing credible evidence for scientific question answering. But existing retrieval and reranking methods remain vulnerable to passages that are semantically similar but logically irrelevant, often reducing factual reliability and amplifying hallucinations.To address this challenge, we propose a Deep Evidence Reranking Agent (DeepEra) that integrates step-by-step reasoning, enabling more precise evaluation of candidate passages beyond surface-level semantics. To support systematic evaluation, we construct SciRAG-SSLI (Scientific RAG - Semantically Similar but Logically Irrelevant), a large-scale dataset comprising about 300K SciQA instances across 10 subjects, constructed from 10M scientific corpus. The dataset combines naturally retrieved contexts with systematically generated distractors to test logical robustness and factual grounding. Comprehensive evaluations confirm that our approach achieves superior retrieval performance compared to leading rerankers. To our knowledge, this work is the first to comprehensively study and empirically validate innegligible SSLI issues in two-stage RAG frameworks.",
      "authors": [
        "Haotian Chen",
        "Qingqing Long",
        "Siyu Pu",
        "Xiao Luo",
        "Wei Ju",
        "Meng Xiao",
        "Yuanchun Zhou",
        "Jianghua Zhao",
        "Xuezhi Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-23T06:19:08+00:00",
      "link": "https://arxiv.org/pdf/2601.16478v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG for scientific question answering and evidence reranking",
      "llm_evidence_cn": "用于科学问答和证据重排序的RAG",
      "llm_evidence": "用于科学问答和证据重排序的RAG",
      "llm_tldr_en": "Proposes a reasoning agent to improve reranking in scientific RAG, reducing hallucinations in knowledge-intensive tasks.",
      "llm_tldr_cn": "提出一种推理代理以改进科学RAG中的重排序，减少知识密集型任务中的幻觉。",
      "llm_tldr": "提出一种推理代理以改进科学RAG中的重排序，减少知识密集型任务中的幻觉。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.16478v1",
      "carry_days": 1
    },
    {
      "id": "2602.02988v1",
      "title": "NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks, but their deployment is often constrained by substantial memory footprints and computational costs. While prior work has achieved significant progress in compressing and accelerating linear layers, nonlinear layers-such as SiLU, RMSNorm, and Softmax-still heavily depend on high-precision floating-point operations. In this paper, we propose a calibration-free, dynamic-programming-optimal, and hardware-friendly framework called Non-uniform Linear Interpolation (NLI). NLI is capable of efficiently approximating a variety of nonlinear functions, enabling seamless integration into LLMs and other deep neural networks with almost no loss in accuracy. NLI ingeniously recasts cutpoint selection as a dynamic-programming problem, achieving the globally minimal interpolation error in O(MxN2) time via Bellman's optimality principle. Based on the NLI algorithm, we also design and implement a plug-and-play universal nonlinear computation unit. Hardware experiments demonstrate that the NLI Engine achieves more than 4x improvement in computational efficiency compared to the state-of-the-art designs.",
      "authors": [
        "Jiangyong Yu",
        "Xiaomeng Han",
        "Xing Hu",
        "Chen Xu",
        "Zhe Jiang",
        "Dawei Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-03T01:47:58+00:00",
      "link": "https://arxiv.org/pdf/2602.02988v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Efficient LLM inference and deployment optimization",
      "llm_evidence_cn": "高效的大模型推理与部署优化",
      "llm_evidence": "高效的大模型推理与部署优化",
      "llm_tldr_en": "A framework to approximate nonlinear operations for faster and more memory-efficient LLM inference.",
      "llm_tldr_cn": "一种通过近似非线性操作来加速大语言模型推理并降低内存占用的硬件友好框架。",
      "llm_tldr": "一种通过近似非线性操作来加速大语言模型推理并降低内存占用的硬件友好框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.02988v1",
      "carry_days": 1
    },
    {
      "id": "2602.14470v1",
      "title": "HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation",
      "abstract": "Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.",
      "authors": [
        "Wen-Sheng Lien",
        "Yu-Kai Chan",
        "Hao-Lung Hsiao",
        "Bo-Kai Ruan",
        "Meng-Fen Chiang",
        "Chien-An Chen",
        "Yi-Ren Yeh",
        "Hong-Han Shuai"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16T05:15:55+00:00",
      "link": "https://arxiv.org/pdf/2602.14470v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "hypergraph-based RAG for knowledge intensive tasks",
      "llm_evidence_cn": "基于超图的检索增强生成用于知识密集型任务",
      "llm_evidence": "基于超图的检索增强生成用于知识密集型任务",
      "llm_tldr_en": "Proposes HyperRAG to capture higher-order relational facts for more efficient retrieval-augmented generation.",
      "llm_tldr_cn": "提出HyperRAG，通过捕获高阶关系事实来实现更高效的检索增强生成。",
      "llm_tldr": "提出HyperRAG，通过捕获高阶关系事实来实现更高效的检索增强生成。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.14470v1",
      "carry_days": 1
    },
    {
      "id": "2601.19225v2",
      "title": "RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering",
      "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications.",
      "authors": [
        "Kaehyun Um",
        "KyuHwan Yeom",
        "Haerim Yang",
        "Minyoung Choi",
        "Hyeongjun Yang",
        "Kyong-Ho Lee"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-27T05:46:32+00:00",
      "link": "https://arxiv.org/pdf/2601.19225v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Retrieval-augmented generation for knowledge-intensive tasks",
      "llm_evidence_cn": "知识密集型任务的检索增强生成",
      "llm_evidence": "知识密集型任务的检索增强生成",
      "llm_tldr_en": "Aligns small LLMs with knowledge graphs using preference optimization for better RAG performance.",
      "llm_tldr_cn": "利用偏好优化将小模型与知识图谱对齐，提升检索增强生成的性能。",
      "llm_tldr": "利用偏好优化将小模型与知识图谱对齐，提升检索增强生成的性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.19225v2",
      "carry_days": 1
    },
    {
      "id": "2602.05152v1",
      "title": "RAG without Forgetting: Continual Query-Infused Key Memory",
      "abstract": "Retrieval-augmented generation (RAG) systems commonly improve robustness via query-time adaptations such as query expansion and iterative retrieval. While effective, these approaches are inherently stateless: adaptations are recomputed for each query and discarded thereafter, precluding cumulative learning and repeatedly incurring inference-time cost. Index-side approaches like key expansion introduce persistence but rely on offline preprocessing or heuristic updates that are weakly aligned with downstream task utility, leading to semantic drift and noise accumulation. We propose Evolving Retrieval Memory (ERM), a training-free framework that transforms transient query-time gains into persistent retrieval improvements. ERM updates the retrieval index through correctness-gated feedback, selectively attributes atomic expansion signals to the document keys they benefit, and progressively evolves keys via stable, norm-bounded updates. We show that query and key expansion are theoretically equivalent under standard similarity functions and prove convergence of ERM's selective updates, amortizing optimal query expansion into a stable index with zero inference-time overhead. Experiments on BEIR and BRIGHT across 13 domains demonstrate consistent gains in retrieval and generation, particularly on reasoning-intensive tasks, at native retrieval speed.",
      "authors": [
        "Yuntong Hu",
        "Sha Li",
        "Naren Ramakrishnan",
        "Liang Zhao"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-05T00:12:45+00:00",
      "link": "https://arxiv.org/pdf/2602.05152v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Retrieval-augmented generation for knowledge tasks",
      "llm_evidence_cn": "知识密集型任务的检索增强生成",
      "llm_evidence": "知识密集型任务的检索增强生成",
      "llm_tldr_en": "Proposes Evolving Retrieval Memory (ERM) to make query-time RAG adaptations persistent and more efficient.",
      "llm_tldr_cn": "提出进化检索内存 (ERM)，使 RAG 的查询时自适应具有持久性且更高效。",
      "llm_tldr": "提出进化检索内存 (ERM)，使 RAG 的查询时自适应具有持久性且更高效。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.05152v1",
      "carry_days": 1
    },
    {
      "id": "2602.02477v1",
      "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability",
      "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.",
      "authors": [
        "Xiao Liang",
        "Zhong-Zhi Li",
        "Zhenghao Lin",
        "Eric Hancheng Jiang",
        "Hengyuan Zhang",
        "Yelong Shen",
        "Kai-Wei Chang",
        "Ying Nian Wu",
        "Yeyun Gong",
        "Weizhu Chen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-02T18:54:54+00:00",
      "link": "https://arxiv.org/pdf/2602.02477v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Divide-and-conquer reasoning for complex problem solving",
      "llm_evidence_cn": "分而治之推理用于复杂问题解决",
      "llm_evidence": "分而治之推理用于复杂问题解决",
      "llm_tldr_en": "Trains LLMs for divide-and-conquer reasoning to improve scalability and performance on hard tasks.",
      "llm_tldr_cn": "通过分而治之推理训练提升大语言模型在处理极具挑战性任务时的可扩展性。",
      "llm_tldr": "通过分而治之推理训练提升大语言模型在处理极具挑战性任务时的可扩展性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.02477v1",
      "carry_days": 1
    },
    {
      "id": "2602.04879v1",
      "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.",
      "authors": [
        "Penghui Qi",
        "Xiangxin Zhou",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Min Lin",
        "Wee Sun Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T18:59:04+00:00",
      "link": "https://arxiv.org/pdf/2602.04879v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "reinforcement learning for fine-tuning LLMs and PPO optimization",
      "llm_evidence_cn": "用于LLM微调的强化学习和PPO优化",
      "llm_evidence": "用于LLM微调的强化学习和PPO优化",
      "llm_tldr_en": "Analyzes and improves the trust region mechanism in PPO for more stable and efficient LLM reinforcement learning.",
      "llm_tldr_cn": "分析并改进了PPO中的置信域机制，以实现更稳定、高效的LLM强化学习。",
      "llm_tldr": "分析并改进了PPO中的置信域机制，以实现更稳定、高效的LLM强化学习。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.04879v1",
      "carry_days": 1
    },
    {
      "id": "2601.21162v1",
      "title": "A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning",
      "abstract": "Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.",
      "authors": [
        "Jiate Liu",
        "Zebin Chen",
        "Shaobo Qiao",
        "Mingchen Ju",
        "Danting Zhang",
        "Bocheng Han",
        "Shuyue Yu",
        "Xin Shu",
        "Jingling Wu",
        "Dong Wen",
        "Xin Cao",
        "Guanfeng Liu",
        "Zhengyi Yang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB"
      ],
      "published": "2026-01-29T01:58:30+00:00",
      "link": "https://arxiv.org/pdf/2601.21162v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Adaptive agentic Graph-RAG for reliable reasoning",
      "llm_evidence_cn": "用于可靠推理的自适应智能体图RAG",
      "llm_evidence": "用于可靠推理的自适应智能体图RAG",
      "llm_tldr_en": "Presents A2RAG, a framework using knowledge graphs and adaptive controllers to improve RAG reliability.",
      "llm_tldr_cn": "提出A2RAG框架，结合知识图谱与自适应控制，提升RAG在复杂任务中的可靠性。",
      "llm_tldr": "提出A2RAG框架，结合知识图谱与自适应控制，提升RAG在复杂任务中的可靠性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.21162v1",
      "carry_days": 1
    },
    {
      "id": "2601.17212v1",
      "title": "DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation",
      "abstract": "Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.",
      "authors": [
        "Saadat Hasan Khan",
        "Spencer Hong",
        "Jingyu Wu",
        "Kevin Lybarger",
        "Youbing Yin",
        "Erin Babinsky",
        "Daben Liu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-23T22:47:16+00:00",
      "link": "https://arxiv.org/pdf/2601.17212v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Diversity-Focused Retrieval-Augmented Generation (DF-RAG)",
      "llm_evidence_cn": "关注多样性的检索增强生成 (DF-RAG)",
      "llm_evidence": "关注多样性的检索增强生成 (DF-RAG)",
      "llm_tldr_en": "Proposes DF-RAG to improve reasoning-intensive QA by incorporating diversity into the retrieval step.",
      "llm_tldr_cn": "提出 DF-RAG 框架，通过在检索步骤中引入多样性来提升复杂推理问答的性能。",
      "llm_tldr": "提出 DF-RAG 框架，通过在检索步骤中引入多样性来提升复杂推理问答的性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.17212v1",
      "carry_days": 1
    },
    {
      "id": "2602.04926v1",
      "title": "Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented Generation",
      "abstract": "Retrieval-augmented generation (RAG) is now standard for knowledge-intensive LLM tasks, but most systems still treat every query as fresh, repeatedly re-retrieving long passages and re-reasoning from scratch, inflating tokens, latency, and cost. We present AutoPrunedRetriever, a graph-style RAG system that persists the minimal reasoning subgraph built for earlier questions and incrementally extends it for later ones. AutoPrunedRetriever stores entities and relations in a compact, ID-indexed codebook and represents questions, facts, and answers as edge sequences, enabling retrieval and prompting over symbolic structure instead of raw text. To keep the graph compact, we apply a two-layer consolidation policy (fast ANN/KNN alias detection plus selective $k$-means once a memory threshold is reached) and prune low-value structure, while prompts retain only overlap representatives and genuinely new evidence. We instantiate two front ends: AutoPrunedRetriever-REBEL, which uses REBEL as a triplet parser, and AutoPrunedRetriever-llm, which swaps in an LLM extractor. On GraphRAG-Benchmark (Medical and Novel), both variants achieve state-of-the-art complex reasoning accuracy, improving over HippoRAG2 by roughly 9--11 points, and remain competitive on contextual summarize and generation. On our harder STEM and TV benchmarks, AutoPrunedRetriever again ranks first, while using up to two orders of magnitude fewer tokens than graph-heavy baselines, making it a practical substrate for long-running sessions, evolving corpora, and multi-agent pipelines.",
      "authors": [
        "Ning Wang",
        "Kuanyan Zhu",
        "Daniel Yuehwoon Yee",
        "Yitang Gao",
        "Shiying Huang",
        "Zirun Xu",
        "Sainyam Galhotra"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-04T08:48:11+00:00",
      "link": "https://arxiv.org/pdf/2602.04926v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Graph-style RAG for knowledge-intensive tasks",
      "llm_evidence_cn": "用于知识密集型任务的图式RAG",
      "llm_evidence": "用于知识密集型任务的图式RAG",
      "llm_tldr_en": "Presents AutoPrunedRetriever, a graph-based RAG system that prunes reasoning graphs for efficiency.",
      "llm_tldr_cn": "提出AutoPrunedRetriever，一种通过剪枝推理图来提高效率的图式RAG系统。",
      "llm_tldr": "提出AutoPrunedRetriever，一种通过剪枝推理图来提高效率的图式RAG系统。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.04926v1",
      "carry_days": 1
    },
    {
      "id": "2601.17532v1",
      "title": "Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection",
      "abstract": "Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \\textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.",
      "authors": [
        "Zhipeng Song",
        "Yizhi Zhou",
        "Xiangyu Kong",
        "Jiulong Jiao",
        "Xinrui Bao",
        "Xu You",
        "Xueqing Shi",
        "Yuhang Zhou",
        "Heng Qi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-24T17:14:10+00:00",
      "link": "https://arxiv.org/pdf/2601.17532v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Information gain pruning for generator-aligned reranking in RAG",
      "llm_evidence_cn": "RAG中生成器对齐重排序的信息增益剪枝",
      "llm_evidence": "RAG中生成器对齐重排序的信息增益剪枝",
      "llm_tldr_en": "Introduces IGP to select the most useful evidence for RAG, improving QA quality under context limits.",
      "llm_tldr_cn": "提出IGP剪枝方法，通过生成器对齐的信号筛选RAG中最有效的证据。",
      "llm_tldr": "提出IGP剪枝方法，通过生成器对齐的信号筛选RAG中最有效的证据。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.17532v1",
      "carry_days": 1
    },
    {
      "id": "2602.16113v1",
      "title": "Evolutionary Context Search for Automated Skill Acquisition",
      "abstract": "Large Language Models cannot reliably acquire new knowledge post-deployment -- even when relevant text resources exist, models fail to transform them into actionable knowledge without retraining. Retrieval-Augmented Generation attempts to bridge this gap by surfacing relevant documents at inference time, yet similarity-based retrieval often fails to identify context that actually improves task performance. We introduce Evolutionary Context Search (ECS), an evolutionary method that searches context combinations using accuracy on a small development set, requiring only inference calls without weight updates. ECS moves beyond semantic similarity to discover non-obvious context pairings that significantly boost performance. Our empirical results show that ECS improves BackendBench by 27\\% and $τ$-bench airline by 7\\%. The evolved contexts are model-agnostic, as those evolved with Gemini-3-Flash transfer effectively to Claude Sonnet and DeepSeek. This suggests that ECS opens a path toward automated context discovery for skill acquisition -- an efficient alternative to manual prompt engineering or costly fine-tuning.",
      "authors": [
        "Qi Sun",
        "Stefan Nielsen",
        "Rio Yokota",
        "Yujin Tang"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "published": "2026-02-18T00:47:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16113v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Evolutionary method for Retrieval-Augmented Generation",
      "llm_evidence_cn": "检索增强生成 (RAG) 的演化搜索方法",
      "llm_evidence": "检索增强生成 (RAG) 的演化搜索方法",
      "llm_tldr_en": "Introduces ECS to improve RAG by searching for optimal context combinations without retraining.",
      "llm_tldr_cn": "引入 ECS 方法，通过搜索最佳上下文组合来优化 RAG 性能而无需重训。",
      "llm_tldr": "引入 ECS 方法，通过搜索最佳上下文组合来优化 RAG 性能而无需重训。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.16113v1",
      "carry_days": 1
    },
    {
      "id": "2601.21109v1",
      "title": "ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference",
      "abstract": "Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.",
      "authors": [
        "Ketan Thakkar",
        "Maitreyi Chatterjee",
        "Ramasubramanian Balasubramanian",
        "Achyuthan Jootoo",
        "Rajendra Ugrani"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-28T22:58:28+00:00",
      "link": "https://arxiv.org/pdf/2601.21109v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Memory-efficient LoRA and accelerated LLM inference",
      "llm_evidence_cn": "内存高效的LoRA与大模型推理加速",
      "llm_evidence": "内存高效的LoRA与大模型推理加速",
      "llm_tldr_en": "Proposes ChunkWise LoRA for adaptive sequence partitioning to improve fine-tuning efficiency and inference speed.",
      "llm_tldr_cn": "提出ChunkWise LoRA，通过自适应序列分区提高微调效率和推理速度。",
      "llm_tldr": "提出ChunkWise LoRA，通过自适应序列分区提高微调效率和推理速度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.21109v1",
      "carry_days": 1
    },
    {
      "id": "2602.13069v1",
      "title": "Memory-Efficient Structured Backpropagation for On-Device LLM Fine-Tuning",
      "abstract": "On-device fine-tuning enables privacy-preserving personalization of large language models, but mobile devices impose severe memory constraints, typically 6--12GB shared across all workloads. Existing approaches force a trade-off between exact gradients with high memory (MeBP) and low memory with noisy estimates (MeZO). We propose Memory-efficient Structured Backpropagation (MeSP), which bridges this gap by manually deriving backward passes that exploit LoRA's low-rank structure. Our key insight is that the intermediate projection $h = xA$ can be recomputed during backward at minimal cost since rank $r \\ll d_{in}$, eliminating the need to store it. MeSP achieves 49\\% average memory reduction compared to MeBP on Qwen2.5 models (0.5B--3B) while computing mathematically identical gradients. Our analysis also reveals that MeZO's gradient estimates show near-zero correlation with true gradients (cosine similarity $\\approx$0.001), explaining its slow convergence. MeSP reduces peak memory from 361MB to 136MB for Qwen2.5-0.5B, enabling fine-tuning scenarios previously infeasible on memory-constrained devices.",
      "authors": [
        "Juneyoung Park",
        "Yuri Hong",
        "Seongwan Kim",
        "Jaeho Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-13T16:24:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13069v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Memory-efficient structured backpropagation for on-device LLM fine-tuning",
      "llm_evidence_cn": "用于端侧LLM微调的内存高效结构化反向传播",
      "llm_evidence": "用于端侧LLM微调的内存高效结构化反向传播",
      "llm_tldr_en": "Proposes MeSP to reduce memory consumption during on-device LLM fine-tuning by exploiting LoRA structure.",
      "llm_tldr_cn": "提出MeSP方法，利用LoRA结构显著降低端侧LLM微调时的显存占用。",
      "llm_tldr": "提出MeSP方法，利用LoRA结构显著降低端侧LLM微调时的显存占用。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.13069v1",
      "carry_days": 1
    },
    {
      "id": "2602.12709v1",
      "title": "ReFilter: Improving Robustness of Retrieval-Augmented Generation via Gated Filter",
      "abstract": "Retrieval-augmented generation (RAG) has become a dominant paradigm for grounding large language models (LLMs) with external evidence in knowledge-intensive question answering. A core design choice is how to fuse retrieved samples into the LLMs, where existing internal fusion approaches broadly fall into query-based fusion, parametric fusion, and latent-based fusion. Despite their effectiveness at modest retrieval scales, these methods often fail to scale gracefully as the number of retrieved candidates k increases: Larger k improves evidence coverage, yet realistic top-k retrieval inevitably contains irrelevant or redundant content and increases the inference cost.   To address these limitations, we propose ReFilter, a novel latent-based fusion framework that performs token-level filtering and fusion. ReFilter consists of three key components: a context encoder for encoding context features, a gated filter for weighting each token, and a token fusion module for integrating the weighted token feature into the LLM's hidden states. Our experiments across four general-domain QA benchmarks show that ReFilter consistently achieves the best average performance under both in-domain adaptation and out-of-domain transfer. ReFilter further generalizes to five biomedical QA benchmarks in zero-shot transfer without domain fine-tuning, reaching 70.01% average accuracy with Qwen2.5-14B-Instruct.",
      "authors": [
        "Yixin Chen",
        "Ying Xiong",
        "Shangyu Wu",
        "Xiangrui Ke",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13T08:25:26+00:00",
      "link": "https://arxiv.org/pdf/2602.12709v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Improving robustness and scaling of Retrieval-Augmented Generation (RAG)",
      "llm_evidence_cn": "提高检索增强生成（RAG）的鲁棒性和扩展性",
      "llm_evidence": "提高检索增强生成（RAG）的鲁棒性和扩展性",
      "llm_tldr_en": "Proposes ReFilter to improve RAG efficiency and robustness by filtering irrelevant retrieved content.",
      "llm_tldr_cn": "提出 ReFilter 框架，通过过滤无关检索内容来提高 RAG 的效率和鲁棒性。",
      "llm_tldr": "提出 ReFilter 框架，通过过滤无关检索内容来提高 RAG 的效率和鲁棒性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.12709v1",
      "carry_days": 1
    },
    {
      "id": "2601.13222v1",
      "title": "Incorporating Q&A Nuggets into Retrieval-Augmented Generation",
      "abstract": "RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.",
      "authors": [
        "Laura Dietz",
        "Bryan Li",
        "Gabrielle Liu",
        "Jia-Huei Ju",
        "Eugene Yang",
        "Dawn Lawrie",
        "William Walden",
        "James Mayfield"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-01-19T16:57:33+00:00",
      "link": "https://arxiv.org/pdf/2601.13222v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Retrieval-augmented Generation with Q&A nuggets and citation provenance",
      "llm_evidence_cn": "结合问答金块与引用溯源的检索增强生成",
      "llm_evidence": "结合问答金块与引用溯源的检索增强生成",
      "llm_tldr_en": "Introduces Crucible, a RAG system using Q&A nuggets to improve citation accuracy and information density.",
      "llm_tldr_cn": "提出Crucible系统，通过问答金块优化RAG的引用准确性与信息密度。",
      "llm_tldr": "提出Crucible系统，通过问答金块优化RAG的引用准确性与信息密度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.13222v1",
      "carry_days": 1
    },
    {
      "id": "2602.07525v1",
      "title": "IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory",
      "abstract": "Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text associations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. However, their misaligned memory organization necessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a framework inspired by human intuition-guided reasoning. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate realistic memory structures. During querying, IGMiRAG distills intuitive strategies via a question parser to control mining depth and memory window, and activates instantaneous memories as anchors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, emulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task complexity (average 6.3k+, minimum 3.0k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness.",
      "authors": [
        "Xingliang Hou",
        "Yuyan Liu",
        "Qi Sun",
        "haoxiu wang",
        "Hao Hu",
        "Shaoyi Du",
        "Zhiqiang Tian"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-07T12:42:31+00:00",
      "link": "https://arxiv.org/pdf/2602.07525v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG with adaptive mining of in-depth memory",
      "llm_evidence_cn": "具有深度记忆自适应挖掘的RAG",
      "llm_evidence": "具有深度记忆自适应挖掘的RAG",
      "llm_tldr_en": "Proposes IGMiRAG using hierarchical hypergraphs to align knowledge and simulate human-like memory retrieval.",
      "llm_tldr_cn": "提出IGMiRAG框架，利用分层超图对齐知识并模拟类人记忆检索。",
      "llm_tldr": "提出IGMiRAG框架，利用分层超图对齐知识并模拟类人记忆检索。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.07525v1",
      "carry_days": 1
    },
    {
      "id": "2602.17856v1",
      "title": "Enhancing Scientific Literature Chatbots with Retrieval-Augmented Generation: A Performance Evaluation of Vector and Graph-Based Systems",
      "abstract": "This paper investigates the enhancement of scientific literature chatbots through retrieval-augmented generation (RAG), with a focus on evaluating vector- and graph-based retrieval systems. The proposed chatbot leverages both structured (graph) and unstructured (vector) databases to access scientific articles and gray literature, enabling efficient triage of sources according to research objectives. To systematically assess performance, we examine two use-case scenarios: retrieval from a single uploaded document and retrieval from a large-scale corpus. Benchmark test sets were generated using a GPT model, with selected outputs annotated for evaluation. The comparative analysis emphasizes retrieval accuracy and response relevance, providing insight into the strengths and limitations of each approach. The findings demonstrate the potential of hybrid RAG systems to improve accessibility to scientific knowledge and to support evidence-based decision making.",
      "authors": [
        "Hamideh Ghanadian",
        "Amin Kamali",
        "Mohammad Hossein Tekieh"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-19T21:42:02+00:00",
      "link": "https://arxiv.org/pdf/2602.17856v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG performance evaluation for knowledge intensive scientific tasks",
      "llm_evidence_cn": "针对知识密集型科学任务的RAG性能评估",
      "llm_evidence": "针对知识密集型科学任务的RAG性能评估",
      "llm_tldr_en": "Evaluates vector and graph-based RAG systems for enhancing scientific literature chatbots.",
      "llm_tldr_cn": "评估了基于向量和图的RAG系统在增强科学文献聊天机器人方面的表现。",
      "llm_tldr": "评估了基于向量和图的RAG系统在增强科学文献聊天机器人方面的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.17856v1",
      "carry_days": 1
    },
    {
      "id": "2602.18750v1",
      "title": "HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD",
      "abstract": "Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\\times$ speedup over baselines while preserving model accuracy.",
      "authors": [
        "He Sun",
        "Li Li",
        "Mingjun Xiao"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR"
      ],
      "published": "2026-02-21T08:19:59+00:00",
      "link": "https://arxiv.org/pdf/2602.18750v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Efficient long-context LLM inference and deployment on edge devices",
      "llm_evidence_cn": "大语言模型在边缘设备上的高效长文本推理与部署",
      "llm_evidence": "大语言模型在边缘设备上的高效长文本推理与部署",
      "llm_tldr_en": "Presents HillInfer, a framework for optimizing long-context LLM inference on memory-constrained edge hardware.",
      "llm_tldr_cn": "提出 HillInfer 框架，优化边缘设备上大语言模型长文本推理的内存与计算效率。",
      "llm_tldr": "提出 HillInfer 框架，优化边缘设备上大语言模型长文本推理的内存与计算效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.18750v1",
      "carry_days": 1
    },
    {
      "id": "2602.05512v2",
      "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
      "abstract": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
      "authors": [
        "Larissa Pusch",
        "Alexandre Courtiol",
        "Tim Conrad"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-05T10:10:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05512v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM-centered architecture for KG-based retrieval and reasoning",
      "llm_evidence_cn": "以大模型为核心的知识图谱检索与推理架构",
      "llm_evidence": "以大模型为核心的知识图谱检索与推理架构",
      "llm_tldr_en": "Integrates LLMs with Knowledge Graphs to reduce hallucinations and improve multi-hop reasoning via RAG.",
      "llm_tldr_cn": "将大模型与知识图谱结合，通过检索增强生成减少幻觉并提升复杂推理能力。",
      "llm_tldr": "将大模型与知识图谱结合，通过检索增强生成减少幻觉并提升复杂推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.05512v2",
      "carry_days": 1
    },
    {
      "id": "2602.00083v1",
      "title": "SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost.",
      "authors": [
        "Yuxin Yang",
        "Gangda Deng",
        "Ömer Faruk Akgül",
        "Nima Chitsazan",
        "Yash Govilkar",
        "Akasha Tigalappanavara",
        "Shi-Xiong Zhang",
        "Sambit Sahu",
        "Viktor Prasanna"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-01-22T20:18:55+00:00",
      "link": "https://arxiv.org/pdf/2602.00083v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "multi-agent framework for scaling retrieval-augmented generation",
      "llm_evidence_cn": "用于扩展检索增强生成的多智能体框架",
      "llm_evidence": "用于扩展检索增强生成的多智能体框架",
      "llm_tldr_en": "SPARC-RAG optimizes inference-time scaling for RAG using a multi-agent coordination and context management.",
      "llm_tldr_cn": "SPARC-RAG通过多智能体协作和上下文管理优化了RAG在推理阶段的扩展效率。",
      "llm_tldr": "SPARC-RAG通过多智能体协作和上下文管理优化了RAG在推理阶段的扩展效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.00083v1",
      "carry_days": 1
    },
    {
      "id": "2602.06072v1",
      "title": "PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference",
      "abstract": "Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.",
      "authors": [
        "Rui Ning",
        "Wei Zhang",
        "Fan Lai"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published": "2026-02-03T01:46:34+00:00",
      "link": "https://arxiv.org/pdf/2602.06072v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Efficient batched LLM inference",
      "llm_evidence_cn": "高效的大模型批处理推理",
      "llm_evidence": "高效的大模型批处理推理",
      "llm_tldr_en": "Introduces PackInfer, a kernel-level framework to optimize GPU utilization for heterogeneous batched LLM inference.",
      "llm_tldr_cn": "引入PackInfer，一个内核级框架，用于优化异构批处理大模型推理中的GPU利用率。",
      "llm_tldr": "引入PackInfer，一个内核级框架，用于优化异构批处理大模型推理中的GPU利用率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.06072v1",
      "carry_days": 1
    },
    {
      "id": "2602.11937v1",
      "title": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration",
      "abstract": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.",
      "authors": [
        "Akhiad Bercovich",
        "Nir Ailon",
        "Vladimir Anisimov",
        "Tomer Asida",
        "Nave Assaf",
        "Mohammad Dabbah",
        "Ido Galil",
        "Amnon Geifman",
        "Yonatan Geifman",
        "Izhak Golan",
        "Roi Koren",
        "Itay Levy",
        "Zach Moshe",
        "Pavlo Molchanov",
        "Najeeb Nabwani",
        "Mostofa Patwari",
        "Omri Puny",
        "Tomer Ronen",
        "Itamar Schen",
        "Elad Segal",
        "Ido Shahaf",
        "Oren Tropp",
        "Ran Zilberstein",
        "Ran El-Yaniv"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12T13:36:19+00:00",
      "link": "https://arxiv.org/pdf/2602.11937v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "inference optimization for reasoning LLMs using MoE pruning and quantization",
      "llm_evidence_cn": "针对推理型LLM的MoE剪枝与量化推理优化",
      "llm_evidence": "针对推理型LLM的MoE剪枝与量化推理优化",
      "llm_tldr_en": "Optimizes reasoning LLM inference via MoE pruning and FP8 quantization to reduce serving costs.",
      "llm_tldr_cn": "通过MoE剪枝和FP8量化优化推理型LLM的推理速度并降低成本。",
      "llm_tldr": "通过MoE剪枝和FP8量化优化推理型LLM的推理速度并降低成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.11937v1",
      "carry_days": 1
    },
    {
      "id": "2602.18734v1",
      "title": "Rethinking Retrieval-Augmented Generation as a Cooperative Decision-Making Problem",
      "abstract": "Retrieval-Augmented Generation (RAG) has demonstrated strong effectiveness in knowledge-intensive tasks by grounding language generation in external evidence. Despite its success, many existing RAG systems are built based on a ranking-centric, asymmetric dependency paradigm, where the generation quality of the generator is highly dependent on reranking results of the reranker. To overcome this limitation, we reformulate RAG as a cooperative multi-agent decision-making problem and propose Cooperative Retrieval-Augmented Generation (CoRAG), a framework in which the reranker and the generator act as peer decision-makers rather than being connected through an asymmetric dependency pipeline. By jointly optimizing their behaviors toward a shared task objective, the reranker and generator are encouraged to cooperate, ensuring that document reranking and generation work in concert to improve the final response. Experimental results demonstrate good generalization and improved generation stability of CoRAG, even when the model is trained on only around 10K PopQA samples. Our model released in https://anonymous.4open.science/r/CoRAG-D63F",
      "authors": [
        "Lichang Song",
        "Ting Long",
        "Yi Chang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-21T06:32:36+00:00",
      "link": "https://arxiv.org/pdf/2602.18734v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Reformulating RAG as a cooperative decision-making problem",
      "llm_evidence_cn": "将RAG重新表述为协作决策问题",
      "llm_evidence": "将RAG重新表述为协作决策问题",
      "llm_tldr_en": "Introduces CoRAG, a framework where reranker and generator act as peer decision-makers for better RAG performance.",
      "llm_tldr_cn": "引入CoRAG框架，使重排序器和生成器作为对等决策者协作，提升RAG性能。",
      "llm_tldr": "引入CoRAG框架，使重排序器和生成器作为对等决策者协作，提升RAG性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.18734v1",
      "carry_days": 1
    },
    {
      "id": "2602.14452v1",
      "title": "WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity",
      "abstract": "Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.",
      "authors": [
        "Lei Chen",
        "Yuan Meng",
        "Xiaoyu Zhan",
        "Zhi Wang",
        "Wenwu Zhu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16T04:18:36+00:00",
      "link": "https://arxiv.org/pdf/2602.14452v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Boosting LLM inference efficiency via activation sparsity",
      "llm_evidence_cn": "通过激活稀疏性提升大模型推理效率",
      "llm_evidence": "通过激活稀疏性提升大模型推理效率",
      "llm_tldr_en": "Proposes WiSparse to improve LLM inference efficiency using weight-aware mixed-granularity activation sparsity.",
      "llm_tldr_cn": "提出WiSparse方法，利用权重感知的混合粒度激活稀疏性来降低大模型的推理成本。",
      "llm_tldr": "提出WiSparse方法，利用权重感知的混合粒度激活稀疏性来降低大模型的推理成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.14452v1",
      "carry_days": 1
    },
    {
      "id": "2602.10210v1",
      "title": "How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge",
      "abstract": "Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.",
      "authors": [
        "Junhong Lin",
        "Bing Zhang",
        "Song Wang",
        "Ziyan Liu",
        "Dan Gutfreund",
        "Julian Shun",
        "Yada Zhu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10T19:04:01+00:00",
      "link": "https://arxiv.org/pdf/2602.10210v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "benchmarking RAG for multi-hop inference over hybrid knowledge",
      "llm_evidence_cn": "基准测试混合知识上的多跳推理RAG",
      "llm_evidence": "基准测试混合知识上的多跳推理RAG",
      "llm_tldr_en": "Introduces HybridRAG-Bench to distinguish genuine retrieval-reasoning from parametric recall in LLMs.",
      "llm_tldr_cn": "引入HybridRAG-Bench，旨在区分大模型中真实的检索推理能力与参数化记忆召回。",
      "llm_tldr": "引入HybridRAG-Bench，旨在区分大模型中真实的检索推理能力与参数化记忆召回。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.10210v1",
      "carry_days": 1
    },
    {
      "id": "2602.13571v1",
      "title": "LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems",
      "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its efficacy depends on accurate document retrieval and ranking. Although existing rerankers demonstrate effectiveness, they frequently necessitate specialized training, impose substantial computational expenses, and fail to fully exploit the semantic capabilities of LLMs, particularly their inherent confidence signals. We propose the LLM-Confidence Reranker (LCR), a training-free, plug-and-play algorithm that enhances reranking in RAG systems by leveraging black-box LLM confidence derived from Maximum Semantic Cluster Proportion (MSCP). LCR employs a two-stage process: confidence assessment via multinomial sampling and clustering, followed by binning and multi-level sorting based on query and document confidence thresholds. This approach prioritizes relevant documents while preserving original rankings for high-confidence queries, ensuring robustness. Evaluated on BEIR and TREC benchmarks with BM25 and Contriever retrievers, LCR--using only 7--9B-parameter pre-trained LLMs--consistently improves NDCG@5 by up to 20.6% across pre-trained LLM and fine-tuned Transformer rerankers, without degradation. Ablation studies validate the hypothesis that LLM confidence positively correlates with document relevance, elucidating LCR's mechanism. LCR offers computational efficiency, parallelism for scalability, and broad compatibility, mitigating hallucinations in applications like medical diagnosis.",
      "authors": [
        "Zhipeng Song",
        "Xiangyu Kong",
        "Xinrui Bao",
        "Yizhi Zhou",
        "Jiulong Jiao",
        "Sitong Liu",
        "Yuhang Zhou",
        "Heng Qi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-14T03:12:05+00:00",
      "link": "https://arxiv.org/pdf/2602.13571v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Enhancing Retrieval-Augmented Generation (RAG) systems",
      "llm_evidence_cn": "增强检索增强生成 (RAG) 系统",
      "llm_evidence": "增强检索增强生成 (RAG) 系统",
      "llm_tldr_en": "Proposes a training-free reranker using LLM confidence signals to improve RAG performance.",
      "llm_tldr_cn": "提出一种无需训练的重排序算法，利用模型置信度信号优化 RAG 系统。",
      "llm_tldr": "提出一种无需训练的重排序算法，利用模型置信度信号优化 RAG 系统。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.13571v1",
      "carry_days": 1
    },
    {
      "id": "2601.20810v1",
      "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs",
      "abstract": "Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph",
      "authors": [
        "Shahd Seddik",
        "Fahd Seddik",
        "Iman Saberi",
        "Fatemeh Fard",
        "Minh Hieu Huynh",
        "Patanamon Thongtanunam"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "published": "2026-01-28T17:58:30+00:00",
      "link": "https://arxiv.org/pdf/2601.20810v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Retrieval-Augmented Generation using Programming Knowledge Graphs",
      "llm_evidence_cn": "使用编程知识图谱的检索增强生成",
      "llm_evidence": "使用编程知识图谱的检索增强生成",
      "llm_tldr_en": "Proposes a Programming Knowledge Graph to improve RAG precision and reduce hallucinations in code generation.",
      "llm_tldr_cn": "提出编程知识图谱（PKG）以提高RAG在代码生成中的检索精度并减少幻觉。",
      "llm_tldr": "提出编程知识图谱（PKG）以提高RAG在代码生成中的检索精度并减少幻觉。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.20810v1",
      "carry_days": 1
    },
    {
      "id": "2602.07356v1",
      "title": "Controllable Value Alignment in Large Language Models through Neuron-Level Editing",
      "abstract": "Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.",
      "authors": [
        "Yonghui Yang",
        "Junwei Li",
        "Jilong Liu",
        "Yicheng He",
        "Fengbin Zhu",
        "Weibiao Huang",
        "Le Wu",
        "Richang Hong",
        "Tat-Seng Chua"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-07T04:35:16+00:00",
      "link": "https://arxiv.org/pdf/2602.07356v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Neuron-level editing for controllable value alignment in LLMs",
      "llm_evidence_cn": "用于大模型可控价值对齐的神经元级编辑",
      "llm_evidence": "用于大模型可控价值对齐的神经元级编辑",
      "llm_tldr_en": "Proposes NeVA, a framework for precise value alignment by editing specific neurons to prevent value leakage.",
      "llm_tldr_cn": "提出NeVA框架，通过神经元级别的编辑实现更精准、可控的大模型价值对齐。",
      "llm_tldr": "提出NeVA框架，通过神经元级别的编辑实现更精准、可控的大模型价值对齐。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.07356v1",
      "carry_days": 1
    },
    {
      "id": "2602.03442v1",
      "title": "A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces",
      "abstract": "Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.",
      "authors": [
        "Mingxuan Du",
        "Benfeng Xu",
        "Chiwei Zhu",
        "Shaohan Wang",
        "Pengyu Wang",
        "Xiaorui Wang",
        "Zhendong Mao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-03T12:07:21+00:00",
      "link": "https://arxiv.org/pdf/2602.03442v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Agentic Retrieval-Augmented Generation framework",
      "llm_evidence_cn": "智能体检索增强生成 (Agentic RAG) 框架",
      "llm_evidence": "智能体检索增强生成 (Agentic RAG) 框架",
      "llm_tldr_en": "Presents A-RAG, a framework allowing models to participate in hierarchical retrieval decisions.",
      "llm_tldr_cn": "提出 A-RAG 框架，允许模型通过分层接口参与检索决策。",
      "llm_tldr": "提出 A-RAG 框架，允许模型通过分层接口参与检索决策。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.03442v1",
      "carry_days": 1
    },
    {
      "id": "2602.04265v1",
      "title": "Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes \"thickening\" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to \"thinning\", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.",
      "authors": [
        "Wenze Lin",
        "Zhen Yang",
        "Xitai Jiang",
        "Pony Ma",
        "Gao Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-04T06:55:58+00:00",
      "link": "https://arxiv.org/pdf/2602.04265v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM reasoning and reward shaping for problem-solving",
      "llm_evidence_cn": "LLM推理与问题解决的奖励塑造",
      "llm_evidence": "LLM推理与问题解决的奖励塑造",
      "llm_tldr_en": "Introduces T2T, a dynamic reward framework to enhance LLM reasoning by balancing exploration and efficiency.",
      "llm_tldr_cn": "引入T2T动态奖励框架，通过平衡探索与效率来增强LLM的推理能力。",
      "llm_tldr": "引入T2T动态奖励框架，通过平衡探索与效率来增强LLM的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.04265v1",
      "carry_days": 1
    },
    {
      "id": "2602.11149v1",
      "title": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning",
      "abstract": "Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.",
      "authors": [
        "Dawid J. Kopiczko",
        "Sagar Vaze",
        "Tijmen Blankevoort",
        "Yuki M. Asano"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11T18:58:54+00:00",
      "link": "https://arxiv.org/pdf/2602.11149v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Supervised fine-tuning on chain-of-thought data",
      "llm_evidence_cn": "思维链数据的监督微调 (SFT)",
      "llm_evidence": "思维链数据的监督微调 (SFT)",
      "llm_tldr_en": "Shows that data repetition in SFT benefits reasoning capabilities more than data scaling.",
      "llm_tldr_cn": "研究发现，在推理模型微调中，数据重复比单纯增加数据量更有效。",
      "llm_tldr": "研究发现，在推理模型微调中，数据重复比单纯增加数据量更有效。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.11149v1",
      "carry_days": 1
    },
    {
      "id": "2601.21698v1",
      "title": "Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics",
      "abstract": "Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.",
      "authors": [
        "Mohamed Elgaar",
        "Hadi Amiri"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-29T13:30:18+00:00",
      "link": "https://arxiv.org/pdf/2601.21698v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Curriculum learning dynamics during LLM pre-training",
      "llm_evidence_cn": "LLM 预训练过程中的课程学习动态",
      "llm_evidence": "LLM 预训练过程中的课程学习动态",
      "llm_tldr_en": "Analyzes how data ordering affects learning phases and performance during large-scale LLM pre-training.",
      "llm_tldr_cn": "分析了在大规模 LLM 预训练过程中，数据排序如何影响学习阶段和性能。",
      "llm_tldr": "分析了在大规模 LLM 预训练过程中，数据排序如何影响学习阶段和性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2601.21698v1",
      "carry_days": 1
    },
    {
      "id": "2602.14696v1",
      "title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)",
      "abstract": "Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.",
      "authors": [
        "Nihal V. Nayak",
        "Paula Rodriguez-Diaz",
        "Neha Hulkund",
        "Sara Beery",
        "David Alvarez-Melis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16T12:33:05+00:00",
      "link": "https://arxiv.org/pdf/2602.14696v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Instruction fine-tuning and data selection algorithms",
      "llm_evidence_cn": "指令微调与数据选择算法",
      "llm_evidence": "指令微调与数据选择算法",
      "llm_tldr_en": "Analyzes data representation and selection algorithms for targeted instruction fine-tuning of LLMs.",
      "llm_tldr_cn": "分析了大语言模型针对性指令微调中的数据表示和选择算法。",
      "llm_tldr": "分析了大语言模型针对性指令微调中的数据表示和选择算法。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.14696v1",
      "carry_days": 1
    },
    {
      "id": "2602.17831v1",
      "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels",
      "abstract": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.",
      "authors": [
        "Simon Henniger",
        "Gabriel Poesia"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-19T20:49:15+00:00",
      "link": "https://arxiv.org/pdf/2602.17831v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "evaluating language model reasoning with puzzles",
      "llm_evidence_cn": "通过谜题评估语言模型的推理能力",
      "llm_evidence": "通过谜题评估语言模型的推理能力",
      "llm_tldr_en": "Presents a framework where LLMs challenge each other with puzzles to evaluate genuine reasoning capabilities.",
      "llm_tldr_cn": "提出了一个框架，让大语言模型通过谜题互相挑战，以评估其真实的推理能力。",
      "llm_tldr": "提出了一个框架，让大语言模型通过谜题互相挑战，以评估其真实的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.17831v1",
      "carry_days": 1
    },
    {
      "id": "2602.00482v1",
      "title": "AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models",
      "abstract": "Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\\times$ in $τ^2$-bench higher training throughput.",
      "authors": [
        "Jiarui Zhang",
        "Yuchen Yang",
        "Ran Yan",
        "Zhiyu Mei",
        "Liyuan Zhang",
        "Daifeng Li",
        "Wei Fu",
        "Jiaxuan Gao",
        "Shusheng Xu",
        "Yi Wu",
        "Binhang Yuan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-31T03:05:34+00:00",
      "link": "https://arxiv.org/pdf/2602.00482v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient RL training and inference optimization",
      "llm_evidence_cn": "高效的强化学习训练与推理优化",
      "llm_evidence": "高效的强化学习训练与推理优化",
      "llm_tldr_en": "Introduces AREAL-DTA to optimize prefix sharing in RL training for LLMs using dynamic tree attention.",
      "llm_tldr_cn": "引入AREAL-DTA，利用动态树注意力优化大模型强化学习训练中的前缀共享。",
      "llm_tldr": "引入AREAL-DTA，利用动态树注意力优化大模型强化学习训练中的前缀共享。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.00482v1",
      "carry_days": 1
    },
    {
      "id": "2602.05695v2",
      "title": "SweetSpot: An Analytical Model for Predicting Energy Efficiency of LLM Inference",
      "abstract": "Large Language Models (LLMs) inference is central to modern AI applications, dominating worldwide datacenter workloads, making it critical to predict its energy footprint. Existing approaches estimate energy consumption as a simple linear function of input and output sequence. However, by analyzing the autoregressive structure of Transformers, which implies a fundamentally non-linear relationship between input and output sequence lengths and energy consumption, we demonstrate the existence of a generation energy minima. Peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs. Consequently, we propose SweetSpot, an analytical model derived from the computational and memory-access complexity of the Transformer architecture, which accurately characterizes the efficiency curve as a function of input and output lengths. To assess accuracy, we measure energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite. We test input and output lengths from 64 to 4096 tokens and achieve a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"sweet spots\" reduce energy usage, up to 33.41x, enabling informed truncation, summarization, and adaptive generation strategies in production systems.",
      "authors": [
        "Hiari Pizzini Cavagna",
        "Andrea Proia",
        "Giacomo Madella",
        "Giovanni B. Esposito",
        "Francesco Antici",
        "Daniele Cesarini",
        "Zeynep Kiziltan",
        "Andrea Bartolini"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.PF"
      ],
      "published": "2026-02-05T14:21:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05695v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "predicting energy efficiency of LLM inference",
      "llm_evidence_cn": "预测大语言模型推理的能源效率",
      "llm_evidence": "预测大语言模型推理的能源效率",
      "llm_tldr_en": "Develops an analytical model to predict and optimize the energy footprint of Transformer-based LLM inference.",
      "llm_tldr_cn": "开发了一个分析模型，用于预测和优化基于Transformer的大语言模型推理的能耗。",
      "llm_tldr": "开发了一个分析模型，用于预测和优化基于Transformer的大语言模型推理的能耗。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.05695v2",
      "carry_days": 1
    },
    {
      "id": "2602.04248v1",
      "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
      "abstract": "Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.",
      "authors": [
        "Hao Lu",
        "Haoyuan Huang",
        "Yulin Zhou",
        "Chen Li",
        "Ningxin Zhu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T06:14:55+00:00",
      "link": "https://arxiv.org/pdf/2602.04248v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Enhancing reasoning capabilities of LLMs via MCTS and memory",
      "llm_evidence_cn": "通过 MCTS 和记忆机制增强大模型的推理能力",
      "llm_evidence": "通过 MCTS 和记忆机制增强大模型的推理能力",
      "llm_tldr_en": "Introduces Empirical-MCTS to transform stateless search into continuous learning for better LLM reasoning.",
      "llm_tldr_cn": "引入 Empirical-MCTS 框架，将无状态搜索转化为持续学习以增强大模型推理。",
      "llm_tldr": "引入 Empirical-MCTS 框架，将无状态搜索转化为持续学习以增强大模型推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.04248v1",
      "carry_days": 1
    },
    {
      "id": "2601.11676v1",
      "title": "HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network",
      "abstract": "The deployment of large language models' (LLMs) inference at the edge can facilitate prompt service responsiveness while protecting user privacy. However, it is critically challenged by the resource constraints of a single edge node. Distributed inference has emerged to aggregate and leverage computational resources across multiple devices. Yet, existing methods typically require strict synchronization, which is often infeasible due to the unreliable network conditions. In this paper, we propose HALO, a novel framework that can boost the distributed LLM inference in lossy edge network. The core idea is to enable a relaxed yet effective synchronization by strategically allocating less critical neuron groups to unstable devices, thus avoiding the excessive waiting time incurred by delayed packets. HALO introduces three key mechanisms: (1) a semantic-aware predictor to assess the significance of neuron groups prior to activation. (2) a parallel execution scheme of neuron group loading during the model inference. (3) a load-balancing scheduler that efficiently orchestrates multiple devices with heterogeneous resources. Experimental results from a Raspberry Pi cluster demonstrate that HALO achieves a 3.41x end-to-end speedup for LLaMA-series LLMs under unreliable network conditions. It maintains performance comparable to optimal conditions and significantly outperforms the state-of-the-art in various scenarios.",
      "authors": [
        "Peirong Zheng",
        "Wenchao Xu",
        "Haozhao Wang",
        "Jinyu Chen",
        "Xuemin Shen"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "published": "2026-01-16T07:37:23+00:00",
      "link": "https://arxiv.org/pdf/2601.11676v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "distributed LLM inference in lossy edge networks",
      "llm_evidence_cn": "有损边缘网络中的分布式LLM推理",
      "llm_evidence": "有损边缘网络中的分布式LLM推理",
      "llm_tldr_en": "Proposes HALO, a framework for efficient distributed LLM inference on resource-constrained edge devices.",
      "llm_tldr_cn": "提出HALO框架，旨在资源受限的边缘设备上实现高效的分布式LLM推理。",
      "llm_tldr": "提出HALO框架，旨在资源受限的边缘设备上实现高效的分布式LLM推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.11676v1",
      "carry_days": 1
    },
    {
      "id": "2602.03652v1",
      "title": "RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.",
      "authors": [
        "Süha Kağan Köse",
        "Mehmet Can Baytekin",
        "Burak Aktaş",
        "Bilge Kaan Görür",
        "Evren Ayberk Munis",
        "Deniz Yılmaz",
        "Muhammed Yusuf Kartal",
        "Çağrı Toraman"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2026-02-03T15:35:11+00:00",
      "link": "https://arxiv.org/pdf/2602.03652v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG for Turkish language and knowledge intensive tasks",
      "llm_evidence_cn": "针对土耳其语和知识密集型任务的RAG",
      "llm_evidence": "针对土耳其语和知识密集型任务的RAG",
      "llm_tldr_en": "Benchmarks RAG strategies for the Turkish language to provide design guidance for morphologically rich languages.",
      "llm_tldr_cn": "对土耳其语的RAG策略进行基准测试，为形态丰富语言的系统设计提供指导。",
      "llm_tldr": "对土耳其语的RAG策略进行基准测试，为形态丰富语言的系统设计提供指导。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.03652v1",
      "carry_days": 1
    },
    {
      "id": "2601.05503v1",
      "title": "Over-Searching in Search-Augmented Large Language Models",
      "abstract": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
      "authors": [
        "Roy Xie",
        "Deepak Gopinath",
        "David Qiu",
        "Dong Lin",
        "Haitian Sun",
        "Saloni Potdar",
        "Bhuwan Dhingra"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-09T03:24:46+00:00",
      "link": "https://arxiv.org/pdf/2601.05503v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Evaluation of over-searching in search-augmented LLMs",
      "llm_evidence_cn": "搜索增强型LLM中过度搜索现象的评估",
      "llm_evidence": "搜索增强型LLM中过度搜索现象的评估",
      "llm_tldr_en": "Analyzes the efficiency and hallucination risks of unnecessary retrieval in search-augmented LLM systems.",
      "llm_tldr_cn": "分析了搜索增强LLM系统中不必要检索带来的效率降低和幻觉风险。",
      "llm_tldr": "分析了搜索增强LLM系统中不必要检索带来的效率降低和幻觉风险。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.05503v1",
      "carry_days": 1
    },
    {
      "id": "2601.12323v2",
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.",
      "authors": [
        "Yin Cai",
        "Zhouhong Gu",
        "Juntao Zhang",
        "Ping Chen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-18T09:10:08+00:00",
      "link": "https://arxiv.org/pdf/2601.12323v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM reasoning abilities in multi-agent social environments",
      "llm_evidence_cn": "多智能体社交环境中的大模型推理能力",
      "llm_evidence": "多智能体社交环境中的大模型推理能力",
      "llm_tldr_en": "Introduces MARO to enhance LLM reasoning through social interaction, negotiation, and competition signals.",
      "llm_tldr_cn": "引入MARO方法，通过社交互动、谈判和竞争信号增强大语言模型的推理能力。",
      "llm_tldr": "引入MARO方法，通过社交互动、谈判和竞争信号增强大语言模型的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.12323v2",
      "carry_days": 1
    },
    {
      "id": "2601.17918v1",
      "title": "Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models",
      "abstract": "Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.",
      "authors": [
        "Dain Kim",
        "Jiwoo Lee",
        "Jaehoon Yun",
        "Yong Hoe Koo",
        "Qingyu Chen",
        "Hyunjae Kim",
        "Jaewoo Kang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "published": "2026-01-25T17:36:53+00:00",
      "link": "https://arxiv.org/pdf/2601.17918v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Direct Preference Optimization (DPO) for medical LLM alignment",
      "llm_evidence_cn": "针对医疗大语言模型对齐的直接偏好优化 (DPO)",
      "llm_evidence": "针对医疗大语言模型对齐的直接偏好优化 (DPO)",
      "llm_tldr_en": "Benchmarks DPO variants for aligning medical vision-language models to improve reliability.",
      "llm_tldr_cn": "评估了多种 DPO 变体在医疗视觉语言模型对齐中的效果，旨在提高其可靠性。",
      "llm_tldr": "评估了多种 DPO 变体在医疗视觉语言模型对齐中的效果，旨在提高其可靠性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2601.17918v1",
      "carry_days": 1
    },
    {
      "id": "2602.05708v1",
      "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
      "abstract": "Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.",
      "authors": [
        "Chuangtao Ma",
        "Zeyu Zhang",
        "Arijit Khan",
        "Sebastian Schelter",
        "Paul Groth"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.CL"
      ],
      "published": "2026-02-05T14:33:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05708v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cost-efficient RAG architecture for entity matching",
      "llm_evidence_cn": "用于实体匹配的高性价比RAG架构",
      "llm_evidence": "用于实体匹配的高性价比RAG架构",
      "llm_tldr_en": "Introduces CE-RAG4EM to reduce RAG costs in large-scale entity matching via blocking-based batching.",
      "llm_tldr_cn": "提出CE-RAG4EM，通过分块批处理降低大规模实体匹配中的RAG计算开销。",
      "llm_tldr": "提出CE-RAG4EM，通过分块批处理降低大规模实体匹配中的RAG计算开销。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.05708v1",
      "carry_days": 1
    },
    {
      "id": "2602.22090v1",
      "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
      "abstract": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20\\% to 40\\%. When applied to GPT-4o API calls, it reduces token usage by approximately 60\\%, further improving cost efficiency. These findings indicate the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications.",
      "authors": [
        "Bo-Wei Chen",
        "Chung-Chi Chen",
        "An-Zi Yen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-25T16:38:03+00:00",
      "link": "https://arxiv.org/pdf/2602.22090v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cost-efficient inference and dynamic model selection for LLMs",
      "llm_evidence_cn": "大语言模型的高性价比推理与动态模型选择",
      "llm_evidence": "大语言模型的高性价比推理与动态模型选择",
      "llm_tldr_en": "Develops a confidence-driven strategy to route tasks between small and large models for efficient inference.",
      "llm_tldr_cn": "开发了一种置信度驱动策略，通过在大小模型间动态切换实现高效推理。",
      "llm_tldr": "开发了一种置信度驱动策略，通过在大小模型间动态切换实现高效推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.22090v1",
      "carry_days": 1
    },
    {
      "id": "2601.13697v1",
      "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
      "abstract": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.",
      "authors": [
        "Zhihang Yuan",
        "Chengyu Yue",
        "Long Huang",
        "Litu Ou",
        "Lei Shi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-20T07:51:32+00:00",
      "link": "https://arxiv.org/pdf/2601.13697v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "data selection for instruction tuning",
      "llm_evidence_cn": "指令微调的数据选择",
      "llm_evidence": "指令微调的数据选择",
      "llm_tldr_en": "Proposes GRADFILTERING, an uncertainty-aware framework to select high-quality data for efficient instruction tuning.",
      "llm_tldr_cn": "提出 GRADFILTERING，一种感知不确定性的框架，用于为高效指令微调选择高质量数据。",
      "llm_tldr": "提出 GRADFILTERING，一种感知不确定性的框架，用于为高效指令微调选择高质量数据。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.13697v1",
      "carry_days": 1
    },
    {
      "id": "2602.05145v1",
      "title": "TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference",
      "abstract": "Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptation directly into high-performance LLM inference systems. TIDE reuses target model hidden states generated during inference as training signals, enabling zero-overhead draft adaptation without reloading the target model, and employs adaptive runtime control to activate speculation and training only when beneficial. TIDE exploits heterogeneous clusters by mapping decoupled inference and training to appropriate GPU classes. Across diverse real-world workloads, TIDE achieves up to 1.15x throughput improvement over static speculative decoding while reducing draft training time by 1.67x compared to approaches that recompute training signals.",
      "authors": [
        "Jiyoung Park",
        "Hankyu Jang",
        "Changseok Song",
        "Wookeun Jung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T00:06:12+00:00",
      "link": "https://arxiv.org/pdf/2602.05145v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "inference optimization and speculative decoding",
      "llm_evidence_cn": "推理优化与投机解码",
      "llm_evidence": "推理优化与投机解码",
      "llm_tldr_en": "Introduces TIDE, a framework for self-improving LLM inference using temporal incremental draft engines.",
      "llm_tldr_cn": "引入 TIDE，一个利用时间增量草稿引擎实现自我改进 LLM 推理的框架。",
      "llm_tldr": "引入 TIDE，一个利用时间增量草稿引擎实现自我改进 LLM 推理的框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.05145v1",
      "carry_days": 1
    },
    {
      "id": "2602.09109v1",
      "title": "Distributed Hybrid Parallelism for Large Language Models: Comparative Study and System Design Guide",
      "abstract": "With the rapid growth of large language models (LLMs), a wide range of methods have been developed to distribute computation and memory across hardware devices for efficient training and inference. While existing surveys provide descriptive overviews of these techniques, systematic analysis of their benefits and trade offs and how such insights can inform principled methodology for designing optimal distributed systems remain limited. This paper offers a comprehensive review of collective operations and distributed parallel strategies, complemented by mathematical formulations to deepen theoretical understanding. We further examine hybrid parallelization designs, emphasizing communication computation overlap across different stages of model deployment, including both training and inference. Recent advances in automated search for optimal hybrid parallelization strategies using cost models are also discussed. Moreover, we present case studies with mainstream architecture categories to reveal empirical insights to guide researchers and practitioners in parallelism strategy selection. Finally, we highlight open challenges and limitations of current LLM training paradigms and outline promising directions for the next generation of large scale model development.",
      "authors": [
        "Hossam Amer",
        "Rezaul Karim",
        "Ali Pourranjbar",
        "Weiwei Zhang",
        "Walid Ahmed",
        "Boxing Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC"
      ],
      "published": "2026-02-09T19:01:13+00:00",
      "link": "https://arxiv.org/pdf/2602.09109v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "distributed parallel strategies for efficient LLM training and inference",
      "llm_evidence_cn": "用于高效LLM训练和推理的分布式并行策略",
      "llm_evidence": "用于高效LLM训练和推理的分布式并行策略",
      "llm_tldr_en": "Provides a comprehensive guide and mathematical framework for hybrid parallelism in LLM systems.",
      "llm_tldr_cn": "为LLM系统中的混合并行化设计提供了全面的综述、数学公式和系统设计指南。",
      "llm_tldr": "为LLM系统中的混合并行化设计提供了全面的综述、数学公式和系统设计指南。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.09109v1",
      "carry_days": 1
    },
    {
      "id": "2601.07475v1",
      "title": "ARCQuant: Boosting NVFP4 Quantization with Augmented Residual Channels for LLMs",
      "abstract": "The emergence of fine-grained numerical formats like NVFP4 presents new opportunities for efficient Large Language Model (LLM) inference. However, it is difficult to adapt existing Post-Training Quantization (PTQ) strategies to these formats: rotation-based methods compromise fine-grained block isolation; smoothing techniques struggle with significant 4-bit quantization errors; and mixed-precision approaches often conflict with hardware constraints on unified-precision computation. To address these challenges, we propose ARCQuant, a framework that boosts NVFP4 performance via Augmented Residual Channels. Distinct from methods that compromise block isolation or hardware uniformity, ARCQuant maintains a strictly unified NVFP4 format by augmenting the activation matrix with quantized residual channels. This design integrates the error compensation process directly into the matrix reduction dimension, enabling the use of standard, highly optimized GEMM kernels with minimal overhead. Theoretical analysis confirms that the worst-case error bound of our dual-stage NVFP4 quantization is comparable to that of standard 8-bit formats such as MXFP8. Extensive experiments on LLaMA and Qwen models demonstrate that ARCQuant achieves state-of-the-art accuracy, comparable to full-precision baselines in perplexity and downstream tasks. Furthermore, deployment on RTX 5090 and RTX PRO 6000 GPUs confirms practical benefits, achieving up to 3x speedup over FP16. Our code is available at https://github.com/actypedef/ARCQuant .",
      "authors": [
        "Haoqian Meng",
        "Yilun Luo",
        "Yafei Zhao",
        "Wenyuan Liu",
        "Peng Zhang",
        "Xindian Ma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-12T12:27:22+00:00",
      "link": "https://arxiv.org/pdf/2601.07475v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Quantization for efficient LLM inference",
      "llm_evidence_cn": "用于高效LLM推理的量化技术",
      "llm_evidence": "用于高效LLM推理的量化技术",
      "llm_tldr_en": "Proposes ARCQuant to boost NVFP4 quantization performance for more efficient LLM inference.",
      "llm_tldr_cn": "提出ARCQuant框架，通过增强残差通道提升NVFP4量化性能，实现高效推理。",
      "llm_tldr": "提出ARCQuant框架，通过增强残差通道提升NVFP4量化性能，实现高效推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.07475v1",
      "carry_days": 1
    },
    {
      "id": "2602.14490v1",
      "title": "Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts",
      "abstract": "Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.",
      "authors": [
        "Buze Zhang",
        "Jinkai Tao",
        "Zilang Zeng",
        "Neil He",
        "Ali Maatouk",
        "Menglin Yang",
        "Rex Ying"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "published": "2026-02-16T06:07:32+00:00",
      "link": "https://arxiv.org/pdf/2602.14490v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "parameter-efficient fine-tuning using mixture of geometric spaces",
      "llm_evidence_cn": "使用混合几何空间的参数高效微调",
      "llm_evidence": "使用混合几何空间的参数高效微调",
      "llm_tldr_en": "Proposes Mixture of Space (MoS) to enhance PEFT by leveraging multiple geometric manifolds for LLM adaptation.",
      "llm_tldr_cn": "提出Mixture of Space (MoS)框架，利用多种几何流形增强LLM的参数高效微调效果。",
      "llm_tldr": "提出Mixture of Space (MoS)框架，利用多种几何流形增强LLM的参数高效微调效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.14490v1",
      "carry_days": 1
    },
    {
      "id": "2602.03075v1",
      "title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution",
      "abstract": "Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.",
      "authors": [
        "Junjie Huang",
        "Jiarui Qin",
        "Di Yin",
        "Weiwen Liu",
        "Yong Yu",
        "Xing Sun",
        "Weinan Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-03T04:04:41+00:00",
      "link": "https://arxiv.org/pdf/2602.03075v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "RL-guided mid-training for LLM evolution",
      "llm_evidence_cn": "强化学习引导的大模型中期训练演进",
      "llm_evidence": "强化学习引导的大模型中期训练演进",
      "llm_tldr_en": "Explores a bidirectional training cycle where RL-tuned models improve the base model during the annealing phase.",
      "llm_tldr_cn": "探索了一种双向训练循环，通过强化学习微调模型在退火阶段改进基础模型。",
      "llm_tldr": "探索了一种双向训练循环，通过强化学习微调模型在退火阶段改进基础模型。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.03075v1",
      "carry_days": 1
    },
    {
      "id": "2602.17520v1",
      "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
      "abstract": "Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local definitions. In this work, we study a systematic failure mode we term semantic override, in which an LLM reverts to its pretrained default interpretation of operators or gate behavior despite explicit redefinition in the prompt. We also identify a related class of errors, assumption injection, where models commit to unstated hardware semantics when critical details are underspecified, rather than requesting clarification. We introduce a compact micro-benchmark of 30 logic and digital-circuit reasoning tasks designed as verifier-style traps, spanning Boolean algebra, operator overloading, redefined gates, and circuit-level semantics. Evaluating three frontier LLMs, we observe persistent noncompliance with local specifications, confident but incompatible assumptions, and dropped constraints even in elementary settings. Our findings highlight a gap between surface-level correctness and specification-faithful reasoning, motivating evaluation protocols that explicitly test local unlearning and semantic compliance in formal domains.",
      "authors": [
        "Yogeswar Reddy Thota",
        "Setareh Rafatirad",
        "Homayoun Houman",
        "Tooraj Nikoubin"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR"
      ],
      "published": "2026-02-19T16:33:46+00:00",
      "link": "https://arxiv.org/pdf/2602.17520v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "measuring semantic override and reasoning failures in LLMs",
      "llm_evidence_cn": "衡量LLM推理中的语义覆盖与失效",
      "llm_evidence": "衡量LLM推理中的语义覆盖与失效",
      "llm_tldr_en": "Investigates why LLMs fail to follow local definitions during reasoning tasks.",
      "llm_tldr_cn": "研究LLM在推理任务中为何无法遵循局部定义而产生语义覆盖错误。",
      "llm_tldr": "研究LLM在推理任务中为何无法遵循局部定义而产生语义覆盖错误。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.17520v1",
      "carry_days": 1
    },
    {
      "id": "2602.15156v1",
      "title": "Panini: Continual Learning in Token Space via Structured Memory",
      "abstract": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.",
      "authors": [
        "Shreyas Rajesh",
        "Pavan Holur",
        "Mehmet Yigit Turali",
        "Chenda Duan",
        "Vwani Roychowdhury"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-16T19:58:03+00:00",
      "link": "https://arxiv.org/pdf/2602.15156v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Retrieval-augmented generation for knowledge intensive tasks and continual learning",
      "llm_evidence_cn": "针对知识密集型任务的检索增强生成与持续学习",
      "llm_evidence": "针对知识密集型任务的检索增强生成与持续学习",
      "llm_tldr_en": "Proposes a non-parametric continual learning framework using structured memory to improve RAG efficiency.",
      "llm_tldr_cn": "提出一种利用结构化存储的非参数持续学习框架，以优化 RAG 的推理效率。",
      "llm_tldr": "提出一种利用结构化存储的非参数持续学习框架，以优化 RAG 的推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.15156v1",
      "carry_days": 1
    },
    {
      "id": "2601.07354v1",
      "title": "Semantic Compression of LLM Instructions via Symbolic Metalanguages",
      "abstract": "We introduce MetaGlyph, a symbolic language for compressing prompts by encoding instructions as mathematical symbols rather than prose. Unlike systems requiring explicit decoding rules, MetaGlyph uses symbols like $\\in$ (membership) and $\\Rightarrow$ (implication) that models already understand from their training data. We test whether these symbols work as ''instruction shortcuts'' that models can interpret without additional teaching.   We evaluate eight models across two dimensions relevant to practitioners: scale (3B-1T parameters) and accessibility (open-source for local deployment vs. proprietary APIs). MetaGlyph achieves 62-81% token reduction across all task types. For API-based deployments, this translates directly to cost savings; for local deployments, it reduces latency and memory pressure.   Results vary by model. Gemini 2.5 Flash achieves 75% semantic equivalence between symbolic and prose instructions on selection tasks, with 49.9% membership operator fidelity. Kimi K2 reaches 98.1% fidelity for implication ($\\Rightarrow$) and achieves perfect (100%) accuracy on selection tasks with symbolic prompts. GPT-5.2 Chat shows the highest membership fidelity observed (91.3%), though with variable parse success across task types. Claude Haiku 4.5 achieves 100% parse success with 26% membership fidelity. Among mid-sized models, Qwen 2.5 7B shows 62% equivalence on extraction tasks. Mid-sized open-source models (7B-12B) show near-zero operator fidelity, suggesting a U-shaped relationship where sufficient scale overcomes instruction-tuning biases.",
      "authors": [
        "Ernst van Gassen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-12T09:26:46+00:00",
      "link": "https://arxiv.org/pdf/2601.07354v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "semantic compression of instructions for efficient LLM deployment",
      "llm_evidence_cn": "LLM指令的语义压缩以实现高效部署",
      "llm_evidence": "LLM指令的语义压缩以实现高效部署",
      "llm_tldr_en": "Introduces MetaGlyph to compress LLM prompts using symbolic metalanguages, reducing token costs by 62-81%.",
      "llm_tldr_cn": "引入MetaGlyph符号元语言压缩LLM提示词，在保持性能的同时降低62-81%的Token成本。",
      "llm_tldr": "引入MetaGlyph符号元语言压缩LLM提示词，在保持性能的同时降低62-81%的Token成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.07354v1",
      "carry_days": 1
    },
    {
      "id": "2601.16444v2",
      "title": "Exploring the Effects of Alignment on Numerical Bias in Large Language Models",
      "abstract": "\"LLM-as-a-judge,\" which utilizes large language models (LLMs) as evaluators, has proven effective in many evaluation tasks. However, evaluator LLMs exhibit numerical bias, a phenomenon where certain evaluation scores are generated disproportionately often, leading reduced evaluation performance. This study investigates the cause of this bias. Given that most evaluator LLMs are aligned through instruction tuning and preference tuning, and that prior research suggests alignment reduces output diversity, we hypothesize that numerical bias arises from alignment. To test this, we compare outputs from pre- and post-alignment LLMs, and observe that alignment indeed increases numerical bias. We also explore mitigation strategies for post-alignment LLMs, including temperature scaling, distribution calibration, and score range adjustment. Among these, score range adjustment is most effective in reducing bias and improving performance, though still heuristic. Our findings highlight the need for further work on optimal score range selection and more robust mitigation strategies.",
      "authors": [
        "Ayako Sato",
        "Hwichan Kim",
        "Zhousi Chen",
        "Masato Mita",
        "Mamoru Komachi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-23T04:45:35+00:00",
      "link": "https://arxiv.org/pdf/2601.16444v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "effects of instruction and preference tuning on LLM bias",
      "llm_evidence_cn": "指令微调和偏好微调对大模型偏差的影响",
      "llm_evidence": "指令微调和偏好微调对大模型偏差的影响",
      "llm_tldr_en": "Investigates how alignment techniques like RLHF increase numerical bias in LLM-as-a-judge scenarios.",
      "llm_tldr_cn": "研究了RLHF等对齐技术如何增加大语言模型作为评测者时的数值偏差。",
      "llm_tldr": "研究了RLHF等对齐技术如何增加大语言模型作为评测者时的数值偏差。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2601.16444v2",
      "carry_days": 1
    },
    {
      "id": "2602.12916v2",
      "title": "Reliable Thinking with Images",
      "abstract": "As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.",
      "authors": [
        "Haobin Li",
        "Yutong Yang",
        "Yijie Lin",
        "Xiang Dai",
        "Mouxing Yang",
        "Xi Peng"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-13T13:22:04+00:00",
      "link": "https://arxiv.org/pdf/2602.12916v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Multimodal extension of Chain-of-Thought (CoT) reasoning",
      "llm_evidence_cn": "多模态思维链（CoT）推理扩展",
      "llm_evidence": "多模态思维链（CoT）推理扩展",
      "llm_tldr_en": "Enhances MLLM reasoning by incorporating visual cues into interleaved Chain-of-Thought processes.",
      "llm_tldr_cn": "通过将视觉线索融入交错的思维链过程，增强多模态大模型的推理能力。",
      "llm_tldr": "通过将视觉线索融入交错的思维链过程，增强多模态大模型的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.12916v2",
      "carry_days": 1
    },
    {
      "id": "2601.22108v1",
      "title": "Value-Based Pre-Training with Downstream Feedback",
      "abstract": "Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.",
      "authors": [
        "Shuqi Ke",
        "Giulia Fanti"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-29T18:38:09+00:00",
      "link": "https://arxiv.org/pdf/2601.22108v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "value-based controlled continued pre-training for foundation models",
      "llm_evidence_cn": "基于价值的基础模型受控持续预训练",
      "llm_evidence": "基于价值的基础模型受控持续预训练",
      "llm_tldr_en": "V-Pretraining uses downstream feedback to steer self-supervised pre-training towards specific capabilities.",
      "llm_tldr_cn": "V-Pretraining利用下游反馈引导自监督预训练，使梯度步骤更符合目标任务需求。",
      "llm_tldr": "V-Pretraining利用下游反馈引导自监督预训练，使梯度步骤更符合目标任务需求。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2601.22108v1",
      "carry_days": 1
    },
    {
      "id": "2601.23155v1",
      "title": "SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training",
      "abstract": "Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost.",
      "authors": [
        "Powei Chang",
        "Jinpeng Zhang",
        "Bowen Chen",
        "Chenyu Wang",
        "Chenlu Guo",
        "Yixing Zhang",
        "Yukang Gao",
        "JianXiang Xiang",
        "Yue Gao",
        "Chaoqun Sun",
        "Yiyi Chen",
        "Dongying Kong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-30T16:40:48+00:00",
      "link": "https://arxiv.org/pdf/2601.23155v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Information-based data selection for efficient instruction tuning",
      "llm_evidence_cn": "基于信息的指令微调高效数据选择",
      "llm_evidence": "基于信息的指令微调高效数据选择",
      "llm_tldr_en": "Introduces SPICE to optimize data selection for instruction tuning by reducing gradient conflicts.",
      "llm_tldr_cn": "引入 SPICE，通过减少梯度冲突来优化指令微调的数据选择。",
      "llm_tldr": "引入 SPICE，通过减少梯度冲突来优化指令微调的数据选择。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.23155v1",
      "carry_days": 1
    },
    {
      "id": "2602.20091v1",
      "title": "How Retrieved Context Shapes Internal Representations in RAG",
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.",
      "authors": [
        "Samuel Yeh",
        "Sharon Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-23T18:02:04+00:00",
      "link": "https://arxiv.org/pdf/2602.20091v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Internal representations in Retrieval-Augmented Generation",
      "llm_evidence_cn": "检索增强生成中的内部表示分析",
      "llm_evidence": "检索增强生成中的内部表示分析",
      "llm_tldr_en": "Analyzes how retrieved context affects the internal latent representations and information integration in RAG.",
      "llm_tldr_cn": "分析了检索到的上下文如何影响RAG中的内部潜变量表示和信息整合。",
      "llm_tldr": "分析了检索到的上下文如何影响RAG中的内部潜变量表示和信息整合。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.20091v1",
      "carry_days": 1
    },
    {
      "id": "2601.12241v1",
      "title": "Power Aware Dynamic Reallocation For Inference",
      "abstract": "Disaggregation has emerged as a powerful strategy for optimizing large language model (LLM) inference by separating compute-intensive prefill and memory-bound decode phases across specialized GPUs. This separation improves utilization and throughput under fixed hardware capacity. However, as model and cluster scales grow, power, rather than compute, has become the dominant limiter of overall performance and cost efficiency. In this paper, we propose RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to sustain goodput within strict power caps. RAPID utilizes static and dynamic power reallocation in addition to GPU reallocation to improve performance under fixed power bounds. RAPID improves overall performance and application consistency beyond what is achievable in current disaggregation solutions, resulting in up to a 2x improvement in SLO attainment at peak load when compared to a static assignment without an increase in complexity or cost.",
      "authors": [
        "Yiwei Jiang",
        "Sangeeta Chowdhary",
        "Nathaniel Morris",
        "Rutwik Jain",
        "Srilatha Manne",
        "Sam Bayliss"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-01-18T03:30:38+00:00",
      "link": "https://arxiv.org/pdf/2601.12241v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Power-aware disaggregated inference framework for LLM deployment",
      "llm_evidence_cn": "用于 LLM 部署的功耗感知解耦推理框架",
      "llm_evidence": "用于 LLM 部署的功耗感知解耦推理框架",
      "llm_tldr_en": "Presents RAPID, a framework that manages GPU power budgets to optimize LLM inference performance.",
      "llm_tldr_cn": "提出 RAPID 框架，通过管理 GPU 功耗预算来优化 LLM 推理性能。",
      "llm_tldr": "提出 RAPID 框架，通过管理 GPU 功耗预算来优化 LLM 推理性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.12241v1",
      "carry_days": 1
    },
    {
      "id": "2601.11652v1",
      "title": "WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching",
      "abstract": "As Large Language Models (LLMs) become increasingly accessible to end users, an ever-growing number of inference requests are initiated from edge devices and computed on centralized GPU clusters. However, the resulting exponential growth in computation workload is placing significant strain on data centers, while edge devices remain largely underutilized, leading to imbalanced workloads and resource inefficiency across the network. Integrating edge devices into the LLM inference process via speculative decoding helps balance the workload between the edge and the cloud, while maintaining lossless prediction accuracy. In this paper, we identify and formalize two critical bottlenecks that limit the efficiency and scalability of distributed speculative LLM serving: Wasted Drafting Time and Verification Interference. To address these challenges, we propose WISP, an efficient and SLO-aware distributed LLM inference system that consists of an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components collaboratively enhance drafting efficiency and optimize verification request scheduling on the server. Extensive numerical results show that WISP improves system capacity by up to 2.1x and 4.1x, and increases system goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED, respectively.",
      "authors": [
        "Xiangchen Li",
        "Jiakun Fan",
        "Qingyuan Wang",
        "Dimitrios Spatharakis",
        "Saeid Ghafouri",
        "Hans Vandierendonck",
        "Deepu John",
        "Bo Ji",
        "Ali R. Butt",
        "Dimitrios S. Nikolopoulos"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "published": "2026-01-15T16:46:01+00:00",
      "link": "https://arxiv.org/pdf/2601.11652v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Distributed speculative LLM serving and inference optimization",
      "llm_evidence_cn": "分布式投机性大模型服务与推理优化",
      "llm_evidence": "分布式投机性大模型服务与推理优化",
      "llm_tldr_en": "Introduces WISP to optimize LLM inference at the edge using dynamic drafting and SLO-aware batching.",
      "llm_tldr_cn": "提出WISP框架，通过动态草稿和感知SLO的分批处理优化边缘设备上的大模型推理效率。",
      "llm_tldr": "提出WISP框架，通过动态草稿和感知SLO的分批处理优化边缘设备上的大模型推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.11652v1",
      "carry_days": 1
    },
    {
      "id": "2601.20147v1",
      "title": "Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization",
      "abstract": "Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.",
      "authors": [
        "Saima Afrin",
        "Zaiyu Cheng",
        "Tushar Sharma",
        "Alexander Serebrenik",
        "Massimiliano Di Penta",
        "Antonio Mastropaolo"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-01-28T00:45:28+00:00",
      "link": "https://arxiv.org/pdf/2601.20147v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Instruction-tuned Language Models and fine-tuning impact",
      "llm_evidence_cn": "指令微调语言模型及其微调影响",
      "llm_evidence": "指令微调语言模型及其微调影响",
      "llm_tldr_en": "Explores data-centric optimization and system prompts for instruction-tuned code language models.",
      "llm_tldr_cn": "探讨了指令微调代码语言模型的数据中心优化和系统提示影响。",
      "llm_tldr": "探讨了指令微调代码语言模型的数据中心优化和系统提示影响。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.20147v1",
      "carry_days": 1
    },
    {
      "id": "2602.22261v1",
      "title": "Sustainable LLM Inference using Context-Aware Model Switching",
      "abstract": "Large language models have become central to many AI applications, but their growing energy consumption raises serious sustainability concerns. A key limitation in current AI deployments is the reliance on a one-size-fits-all inference strategy where most systems route every request to the same large model, regardless of task complexity, leading to substantial and unnecessary energy waste. To address this issue, we propose a context-aware model switching approach that dynamically selects an appropriate language model based on query complexity. The proposed system uses a Context-Aware Model Switching for Energy-Efficient LLM Inference that combines caching for repeated queries, rulebased complexity scoring for fast and explainable decisions, machine learning classification to capture semantic intent, and a user-adaptive component that learns from interaction patterns over time. The proposed architecture was evaluated using real conversation workloads and three open-source language models (Gemma3 1B, Gemma3 4B and Qwen3 4B) with different computational costs, measuring energy consumption (via NVML GPU power telemetry), response latency, routing accuracy, and output quality (BERTScore F1) to reflect real-world usage conditions. Experimental results show that the model switching approach can reduce energy consumption by up to 67.5% compared to always using the largest model while maintaining a response quality of 93.6%. In addition, the response time for simple queries also improved significantly by approximately 68%. These results show that model switching inference offers a practical and scalable path toward more energy-efficient and sustainable AI systems, demonstrating that significant efficiency gains can be achieved without major sacrifices in response quality.",
      "authors": [
        "Yuvarani",
        "Akashdeep Singh",
        "Zahra Fathanah",
        "Salsabila Harlen",
        "Syeikha Syafura Al-Zahra binti Zahari",
        "Hema Subramaniam"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-25T03:42:12+00:00",
      "link": "https://arxiv.org/pdf/2602.22261v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "efficient LLM inference and model switching",
      "llm_evidence_cn": "高效 LLM 推理与模型切换",
      "llm_evidence": "高效 LLM 推理与模型切换",
      "llm_tldr_en": "Introduces context-aware model switching to optimize energy efficiency and performance during LLM inference.",
      "llm_tldr_cn": "引入上下文感知模型切换，以优化 LLM 推理过程中的能源效率和性能。",
      "llm_tldr": "引入上下文感知模型切换，以优化 LLM 推理过程中的能源效率和性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.22261v1",
      "carry_days": 1
    },
    {
      "id": "2601.13995v1",
      "title": "From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning",
      "abstract": "Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \\textbf{+5.84\\%} using only \\textbf{5\\%} of the data, while our aligned sampling strategy further boosts average performance by \\textbf{+4.24\\%}.",
      "authors": [
        "Zihan Niu",
        "Wenping Hu",
        "Junmin Chen",
        "Xiyue Wang",
        "Tong Xu",
        "Ruiming Tang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-20T14:06:51+00:00",
      "link": "https://arxiv.org/pdf/2601.13995v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "data selection for LLM instruction tuning",
      "llm_evidence_cn": "LLM指令微调的数据选择",
      "llm_evidence": "LLM指令微调的数据选择",
      "llm_tldr_en": "Proposes TAGS framework for hierarchical, knowledge-aligned data selection in instruction tuning.",
      "llm_tldr_cn": "提出TAGS框架，通过构建知识树实现LLM指令微调中更精准、可控的数据选择。",
      "llm_tldr": "提出TAGS框架，通过构建知识树实现LLM指令微调中更精准、可控的数据选择。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.13995v1",
      "carry_days": 1
    },
    {
      "id": "2602.19509v1",
      "title": "Pyramid MoA: A Probabilistic Framework for Cost-Optimized Anytime Inference",
      "abstract": "Large Language Models (LLMs) face a persistent trade-off between inference cost and reasoning capability. While \"Oracle\" models (e.g., Llama-3-70B) achieve state-of-the-art accuracy, they are prohibitively expensive for high-volume deployment. Smaller models (e.g., 8B parameters) are cost-effective but struggle with complex tasks. In this work, we propose \"Pyramid MoA\", a hierarchical Mixture-of-Agents architecture that uses a lightweight Router to dynamically escalate queries only when necessary. By leveraging semantic agreement and confidence calibration among an ensemble of small models, our Router identifies \"hard\" problems with high precision. On the GSM8K benchmark, our system achieves 93.0% accuracy, effectively matching the Oracle baseline (98.0%) while reducing compute costs by 61%. We demonstrate that the system introduces negligible latency overhead (+0.82s) and allows for a tunable trade-off between performance and budget.",
      "authors": [
        "Arindam Khaled"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-23T04:47:47+00:00",
      "link": "https://arxiv.org/pdf/2602.19509v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cost-optimized inference and Mixture-of-Agents",
      "llm_evidence_cn": "成本优化的推理与智能体混合架构",
      "llm_evidence": "成本优化的推理与智能体混合架构",
      "llm_tldr_en": "Proposes Pyramid MoA, a hierarchical framework to optimize the trade-off between inference cost and reasoning.",
      "llm_tldr_cn": "提出Pyramid MoA分层框架，优化推理成本与推理能力之间的权衡。",
      "llm_tldr": "提出Pyramid MoA分层框架，优化推理成本与推理能力之间的权衡。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.19509v1",
      "carry_days": 1
    },
    {
      "id": "2602.06493v1",
      "title": "Adaptive Uncertainty-Aware Tree Search for Robust Reasoning",
      "abstract": "Inference-time reasoning scaling has significantly advanced the capabilities of Large Language Models (LLMs) in complex problem-solving. A prevalent approach involves external search guided by Process Reward Models (PRMs). However, a fundamental limitation of this framework is the epistemic uncertainty of PRMs when evaluating reasoning paths that deviate from their training distribution. In this work, we conduct a systematic analysis of this challenge. We first provide empirical evidence that PRMs exhibit high uncertainty and unreliable scoring on out-of-distribution (OOD) samples. We then establish a theoretical framework proving that while standard search incurs linear regret accumulation, an uncertainty-aware strategy can achieve sublinear regret. Motivated by these findings, we propose Uncertainty-Aware Tree Search (UATS), a unified method that estimates uncertainty via Monte Carlo Dropout and dynamically allocates compute budget using a reinforcement learning-based controller. Extensive experiments demonstrate that our approach effectively mitigates the impact of OOD errors.",
      "authors": [
        "Zeen Song",
        "Zihao Ma",
        "Wenwen Qiang",
        "Changwen Zheng",
        "Gang Hua"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-06T08:40:05+00:00",
      "link": "https://arxiv.org/pdf/2602.06493v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Inference-time reasoning scaling and search for LLMs",
      "llm_evidence_cn": "大模型推理时推理扩展与搜索",
      "llm_evidence": "大模型推理时推理扩展与搜索",
      "llm_tldr_en": "Develops an uncertainty-aware tree search strategy to improve the robustness of LLM reasoning in complex problems.",
      "llm_tldr_cn": "开发了一种不确定性感知树搜索策略，以提高大模型在复杂问题中的推理鲁棒性。",
      "llm_tldr": "开发了一种不确定性感知树搜索策略，以提高大模型在复杂问题中的推理鲁棒性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.06493v1",
      "carry_days": 1
    },
    {
      "id": "2602.21144v1",
      "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism",
      "abstract": "Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path.   This paper presents a communication-efficient TP design for selective SSM inference that addresses three practical engineering challenges: enabling TTFT improvements via an SSM state cache across prefill and decode, partitioning the mixer's packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing TP aggregation overhead with quantized AllReduce. We evaluate on three representative SSM-based LLMs spanning pure-SSM and hybrid architectures - Mamba, Falcon-Mamba, and Zamba - on NVIDIA A6000 and A100 clusters. Our experiments show substantial throughput gains from tensor-parallel SSM inference, improving batch-request throughput by ~1.6-2.1x on 2 GPUs and ~2.6-4.0x on 4 GPUs for Mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead.",
      "authors": [
        "Anurag Dutt",
        "Nimit Shah",
        "Hazem Masarani",
        "Anshul Gandhi"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published": "2026-02-24T17:47:54+00:00",
      "link": "https://arxiv.org/pdf/2602.21144v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Tensor parallelism for scaling SSM-based language models",
      "llm_evidence_cn": "用于扩展SSM语言模型的张量并行技术",
      "llm_evidence": "用于扩展SSM语言模型的张量并行技术",
      "llm_tldr_en": "Optimizes multi-GPU inference for State-Space Models, a compelling alternative backbone to Transformers.",
      "llm_tldr_cn": "优化了状态空间模型（SSM）的多GPU推理性能，提升长文本处理效率。",
      "llm_tldr": "优化了状态空间模型（SSM）的多GPU推理性能，提升长文本处理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.21144v1",
      "carry_days": 1
    },
    {
      "id": "2602.05512v1",
      "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
      "abstract": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
      "authors": [
        "Larissa Pusch",
        "Alexandre Courtiol",
        "Tim Conrad"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-05T10:10:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05512v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM-centered architecture for Knowledge-Graph RAG",
      "llm_evidence_cn": "以LLM为中心的知识图谱检索增强生成架构",
      "llm_evidence": "以LLM为中心的知识图谱检索增强生成架构",
      "llm_tldr_en": "Introduces a framework combining LLMs with Knowledge Graphs to improve RAG accuracy and multi-hop reasoning.",
      "llm_tldr_cn": "引入了一个结合LLM与知识图谱的框架，以提高RAG的准确性和多跳推理能力。",
      "llm_tldr": "引入了一个结合LLM与知识图谱的框架，以提高RAG的准确性和多跳推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.05512v1",
      "carry_days": 1
    },
    {
      "id": "2602.20338v1",
      "title": "Emergent Manifold Separability during Reasoning in Large Language Models",
      "abstract": "Chain-of-Thought (CoT) prompting significantly improves reasoning in Large Language Models, yet the temporal dynamics of the underlying representation geometry remain poorly understood. We investigate these dynamics by applying Manifold Capacity Theory (MCT) to a compositional Boolean logic task, allowing us to quantify the linear separability of latent representations without the confounding factors of probe training. Our analysis reveals that reasoning manifests as a transient geometric pulse, where concept manifolds are untangled into linearly separable subspaces immediately prior to computation and rapidly compressed thereafter. This behavior diverges from standard linear probe accuracy, which remains high long after computation, suggesting a fundamental distinction between information that is merely retrievable and information that is geometrically prepared for processing. We interpret this phenomenon as \\emph{Dynamic Manifold Management}, a mechanism where the model dynamically modulates representational capacity to optimize the bandwidth of the residual stream throughout the reasoning chain.",
      "authors": [
        "Alexandre Polo",
        "Chanwoo Chun",
        "SueYeon Chung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-23T20:36:17+00:00",
      "link": "https://arxiv.org/pdf/2602.20338v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Geometric analysis of Chain-of-Thought (CoT) reasoning dynamics",
      "llm_evidence_cn": "思维链（CoT）推理动态的几何分析",
      "llm_evidence": "思维链（CoT）推理动态的几何分析",
      "llm_tldr_en": "Investigates the latent representation geometry of LLMs during Chain-of-Thought reasoning using Manifold Capacity Theory.",
      "llm_tldr_cn": "利用流形容量理论研究了LLM在思维链推理过程中的潜层表示几何动态。",
      "llm_tldr": "利用流形容量理论研究了LLM在思维链推理过程中的潜层表示几何动态。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.20338v1",
      "carry_days": 1
    },
    {
      "id": "2602.03073v1",
      "title": "TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT",
      "abstract": "Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) are the two dominant paradigms for enhancing Large Language Model (LLM) performance on downstream tasks. While RL generally preserves broader model capabilities (retention) better than SFT, it comes with significant costs: complex reward engineering, instability, and expensive on-policy sampling. In contrast, SFT is efficient but brittle, often suffering from catastrophic forgetting due to $\\textbf{Supervision Mismatch}$: the divergence between the model's evolving policy and static training labels. We address this trade-off with $\\textbf{Trajectory-Mixed Supervision (TMS)}$, a reward-free framework that approximates the on-policy benefits of RL by creating a dynamic curriculum from the model's own historical checkpoints. TMS minimizes $\\textit{Policy-Label Divergence (PLD)}$, preventing the mode collapse that drives forgetting in standard SFT. Experiments across reasoning (MATH, GSM8K) and instruction-following benchmarks demonstrate that TMS effectively shifts the accuracy--retention Pareto frontier. While RL remains the gold standard for retention, TMS significantly outperforms standard and iterative SFT, bridging the gap to RL without requiring reward models or verifiers. Mechanistic analysis confirms that PLD drift accurately predicts forgetting and that TMS successfully mitigates this drift.",
      "authors": [
        "Rana Muhammad Shahroz Khan",
        "Zijie Liu",
        "Zhen Tan",
        "Charles Fleming",
        "Tianlong Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03T04:01:26+00:00",
      "link": "https://arxiv.org/pdf/2602.03073v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "reward-free on-policy supervised fine-tuning",
      "llm_evidence_cn": "无奖励的在策略监督微调",
      "llm_evidence": "无奖励的在策略监督微调",
      "llm_tldr_en": "Introduces a dynamic curriculum framework to bridge the gap between SFT and RL for LLM enhancement.",
      "llm_tldr_cn": "引入了一个动态课程框架，以弥合监督微调与强化学习在大语言模型增强方面的差距。",
      "llm_tldr": "引入了一个动态课程框架，以弥合监督微调与强化学习在大语言模型增强方面的差距。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.03073v1",
      "carry_days": 1
    },
    {
      "id": "2602.04900v3",
      "title": "Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization",
      "abstract": "As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the benefits of container orchestration, such as scalability and resource efficiency, to complex AI workflows. We implement and evaluate an illustrative, multi-stage use case consisting of automatic speech recognition and summarization. First, we address batch inference by using Kueue to manage jobs that transcribe audio files with Whisper models and Dynamic Accelerator Slicer (DAS) to increase parallel job execution. Second, we address a discrete online inference scenario by feeding the transcripts to a Large Language Model for summarization hosted using llm-d, a novel solution utilizing the recent developments around the Kubernetes Gateway API Inference Extension (GAIE) for optimized routing of inference requests. Our findings illustrate that these complementary components (Kueue, DAS, and GAIE) form a cohesive, high-performance platform, proving Kubernetes' capability to serve as a unified foundation for demanding GenAI workloads: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36\\%; and GAIE working in conjunction with llm-d improved tail Time to First Token latency by up to 90% even under high loads.",
      "authors": [
        "Sai Sindhur Malleni",
        "Raúl Sevilla",
        "Aleksei Vasilevskii",
        "José Castillo Lema",
        "André Bauer"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-02-03T15:36:08+00:00",
      "link": "https://arxiv.org/pdf/2602.04900v3",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Kubernetes performance for LLM inference and summarization",
      "llm_evidence_cn": "大模型推理与摘要的Kubernetes性能评估",
      "llm_evidence": "大模型推理与摘要的Kubernetes性能评估",
      "llm_tldr_en": "Evaluates Kubernetes-native projects for managing complex GenAI and LLM inference workloads.",
      "llm_tldr_cn": "评估了用于管理复杂生成式AI和大模型推理工作负载的Kubernetes原生项目。",
      "llm_tldr": "评估了用于管理复杂生成式AI和大模型推理工作负载的Kubernetes原生项目。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.04900v3",
      "carry_days": 1
    },
    {
      "id": "2602.07616v1",
      "title": "SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models",
      "abstract": "Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.",
      "authors": [
        "Juntong Wu",
        "Jialiang Cheng",
        "Fuyu Lv",
        "Ou Dan",
        "Li Yuan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-07T16:51:16+00:00",
      "link": "https://arxiv.org/pdf/2602.07616v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient batch decoding for Mixture-of-Experts LLMs",
      "llm_evidence_cn": "混合专家模型LLM的高效批量解码",
      "llm_evidence": "混合专家模型LLM的高效批量解码",
      "llm_tldr_en": "Presents SERE to optimize inference efficiency and expert sparsity in MoE-based large language models.",
      "llm_tldr_cn": "提出了SERE方法，旨在优化基于MoE的大语言模型的推理效率和专家稀疏性。",
      "llm_tldr": "提出了SERE方法，旨在优化基于MoE的大语言模型的推理效率和专家稀疏性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.07616v1",
      "carry_days": 1
    },
    {
      "id": "2601.10254v1",
      "title": "NoReGeo: Non-Reasoning Geometry Benchmark",
      "abstract": "We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.",
      "authors": [
        "Irina Abdullaeva",
        "Anton Vasiliuk",
        "Elizaveta Goncharova",
        "Temurbek Rahmatullaev",
        "Zagorulko Ivan",
        "Maxim Kurkin",
        "Andrey Kuznetsov"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-15T10:22:55+00:00",
      "link": "https://arxiv.org/pdf/2601.10254v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Benchmark to evaluate geometric understanding and reasoning in LLMs",
      "llm_evidence_cn": "评估大模型几何理解与推理能力的基准测试",
      "llm_evidence": "评估大模型几何理解与推理能力的基准测试",
      "llm_tldr_en": "Presents NoReGeo, a benchmark for testing the intrinsic spatial and geometric reasoning capabilities of LLMs.",
      "llm_tldr_cn": "推出NoReGeo基准，用于评估大模型在不依赖代数计算情况下的内在几何推理能力。",
      "llm_tldr": "推出NoReGeo基准，用于评估大模型在不依赖代数计算情况下的内在几何推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.10254v1",
      "carry_days": 1
    },
    {
      "id": "2602.02313v2",
      "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient",
      "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.",
      "authors": [
        "Changming Li",
        "Kaixing Zhang",
        "Haoyun Xu",
        "Yingdong Shi",
        "Zheng Zhang",
        "Kaitao Song",
        "Kan Ren"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-02T16:43:09+00:00",
      "link": "https://arxiv.org/pdf/2602.02313v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Interpreting and controlling LLM reasoning mechanisms",
      "llm_evidence_cn": "解释并控制大模型的推理机制",
      "llm_evidence": "解释并控制大模型的推理机制",
      "llm_tldr_en": "Identifies internal components contributing to sequential reasoning behaviors in large language models.",
      "llm_tldr_cn": "识别了大模型中对序列推理行为有贡献的内部组件。",
      "llm_tldr": "识别了大模型中对序列推理行为有贡献的内部组件。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.02313v2",
      "carry_days": 1
    },
    {
      "id": "2602.08948v1",
      "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute",
      "abstract": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.",
      "authors": [
        "Chen Jin",
        "Ryutaro Tanno",
        "Tom Diethe",
        "Philip Teare"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-09T17:44:41+00:00",
      "link": "https://arxiv.org/pdf/2602.08948v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "confidence-guided self-refinement for reasoning traces",
      "llm_evidence_cn": "针对推理链的置信度引导自优化",
      "llm_evidence": "针对推理链的置信度引导自优化",
      "llm_tldr_en": "Introduces CoRefine to improve LLM reasoning efficiency through adaptive test-time compute.",
      "llm_tldr_cn": "引入CoRefine，通过自适应测试时计算提高LLM推理效率和准确性。",
      "llm_tldr": "引入CoRefine，通过自适应测试时计算提高LLM推理效率和准确性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.08948v1",
      "carry_days": 1
    },
    {
      "id": "2602.07086v1",
      "title": "Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation",
      "abstract": "Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.",
      "authors": [
        "Michael Marketsmüller",
        "Simon Martin",
        "Tim Schlippe"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-06T08:37:06+00:00",
      "link": "https://arxiv.org/pdf/2602.07086v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "evaluating RAG variants for knowledge-intensive code generation",
      "llm_evidence_cn": "评估知识密集型代码生成中的RAG变体",
      "llm_evidence": "评估知识密集型代码生成中的RAG变体",
      "llm_tldr_en": "Compares standard RAG, Self-RAG, and CoRAG for translating natural language into SQL and API calls.",
      "llm_tldr_cn": "对比了标准RAG、Self-RAG和CoRAG在自然语言转SQL及API调用任务中的表现。",
      "llm_tldr": "对比了标准RAG、Self-RAG和CoRAG在自然语言转SQL及API调用任务中的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.07086v1",
      "carry_days": 1
    },
    {
      "id": "2602.12222v1",
      "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training",
      "abstract": "Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \\textbf{\\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \\textbf{\\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT",
      "authors": [
        "Miaosen Zhang",
        "Yishan Liu",
        "Shuxia Lin",
        "Xu Yang",
        "Qi Dai",
        "Chong Luo",
        "Weihao Jiang",
        "Peng Hou",
        "Anxiang Zeng",
        "Xin Geng",
        "Baining Guo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-12T17:59:58+00:00",
      "link": "https://arxiv.org/pdf/2602.12222v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "On-policy supervised fine-tuning (SFT) techniques",
      "llm_evidence_cn": "在线策略监督微调 (SFT) 技术",
      "llm_evidence": "在线策略监督微调 (SFT) 技术",
      "llm_tldr_en": "Proposes Distribution Discriminant Theory to bridge the gap between SFT and RL for better alignment.",
      "llm_tldr_cn": "提出分布判别理论，通过在线策略微调缩小 SFT 与强化学习之间的差距。",
      "llm_tldr": "提出分布判别理论，通过在线策略微调缩小 SFT 与强化学习之间的差距。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.12222v1",
      "carry_days": 1
    },
    {
      "id": "2602.12196v1",
      "title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education",
      "abstract": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.",
      "authors": [
        "Mohamed Huti",
        "Alasdair Mackintosh",
        "Amy Waldock",
        "Dominic Andrews",
        "Maxime Lelièvre",
        "Moritz Boos",
        "Tobias Murray",
        "Paul Atherton",
        "Robin A. A. Ince",
        "Oliver G. B. Garrod"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-12T17:29:03+00:00",
      "link": "https://arxiv.org/pdf/2602.12196v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Evaluating Multimodal LLMs on visual reasoning problems",
      "llm_evidence_cn": "评估多模态大模型在视觉推理问题上的表现",
      "llm_evidence": "评估多模态大模型在视觉推理问题上的表现",
      "llm_tldr_en": "Introduces a benchmark to evaluate the spatial and relational reasoning capabilities of Multimodal LLMs.",
      "llm_tldr_cn": "发布VRB基准测试，用于评估多模态大语言模型在真实教育场景中的视觉推理能力。",
      "llm_tldr": "发布VRB基准测试，用于评估多模态大语言模型在真实教育场景中的视觉推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.12196v1",
      "carry_days": 1
    },
    {
      "id": "2601.13288v1",
      "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
      "abstract": "Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.",
      "authors": [
        "Gonzalo Ariel Meyoyan",
        "Luciano Del Corro"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-19T18:40:29+00:00",
      "link": "https://arxiv.org/pdf/2601.13288v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "efficient single-pass classification and inference optimization",
      "llm_evidence_cn": "高效单次分类与推理优化",
      "llm_evidence": "高效单次分类与推理优化",
      "llm_tldr_en": "Reuses LLM hidden states for classification tasks to reduce latency and memory footprint during inference.",
      "llm_tldr_cn": "复用LLM隐藏状态进行分类任务，以减少推理过程中的延迟和显存占用。",
      "llm_tldr": "复用LLM隐藏状态进行分类任务，以减少推理过程中的延迟和显存占用。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.13288v1",
      "carry_days": 1
    },
    {
      "id": "2602.04816v2",
      "title": "Horizon-LM: A RAM-Centric Architecture for LLM Training",
      "abstract": "The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2$\\times$ higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.",
      "authors": [
        "Zhengqing Yuan",
        "Lichao Sun",
        "Yanfang Ye"
      ],
      "primary_category": "cs.OS",
      "categories": [
        "cs.OS",
        "cs.CL",
        "cs.DC"
      ],
      "published": "2026-02-04T18:04:46+00:00",
      "link": "https://arxiv.org/pdf/2602.04816v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Architecture for LLM instruction tuning and alignment",
      "llm_evidence_cn": "用于大语言模型指令微调与对齐的架构",
      "llm_evidence": "用于大语言模型指令微调与对齐的架构",
      "llm_tldr_en": "Presents a RAM-centric architecture to lower memory barriers for LLM post-training workloads.",
      "llm_tldr_cn": "提出一种以内存为中心的架构，降低了大模型指令微调与对齐的硬件门槛。",
      "llm_tldr": "提出一种以内存为中心的架构，降低了大模型指令微调与对齐的硬件门槛。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.04816v2",
      "carry_days": 1
    },
    {
      "id": "2602.11137v1",
      "title": "Weight Decay Improves Language Model Plasticity",
      "abstract": "The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.",
      "authors": [
        "Tessa Han",
        "Sebastian Bordt",
        "Hanlin Zhang",
        "Sham Kakade"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-11T18:49:26+00:00",
      "link": "https://arxiv.org/pdf/2602.11137v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM pre-training and downstream fine-tuning plasticity",
      "llm_evidence_cn": "大模型预训练与下游微调的可塑性",
      "llm_evidence": "大模型预训练与下游微调的可塑性",
      "llm_tldr_en": "Demonstrates that weight decay during pre-training significantly improves the plasticity of LLMs for fine-tuning.",
      "llm_tldr_cn": "研究表明预训练期间的权重衰减能显著提高大语言模型在微调时的可塑性。",
      "llm_tldr": "研究表明预训练期间的权重衰减能显著提高大语言模型在微调时的可塑性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.11137v1",
      "carry_days": 1
    },
    {
      "id": "2602.17691v1",
      "title": "Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering",
      "abstract": "Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.   On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.   Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.",
      "authors": [
        "Craig Atkinson"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-06T06:24:37+00:00",
      "link": "https://arxiv.org/pdf/2602.17691v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Inference optimization and hallucination control in quantized LLMs",
      "llm_evidence_cn": "量化大语言模型的推理优化与幻觉控制",
      "llm_evidence": "量化大语言模型的推理优化与幻觉控制",
      "llm_tldr_en": "Presents HELIX, a framework to improve the truthfulness and stability of quantized LLM inference.",
      "llm_tldr_cn": "提出HELIX几何框架，用于提升量化大语言模型推理过程中的真实性和稳定性。",
      "llm_tldr": "提出HELIX几何框架，用于提升量化大语言模型推理过程中的真实性和稳定性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.17691v1",
      "carry_days": 1
    },
    {
      "id": "2602.08329v1",
      "title": "Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference",
      "abstract": "A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.",
      "authors": [
        "Yifei Gao",
        "Lei Wang",
        "Rong-Cheng Tu",
        "Qixin Zhang",
        "Jun Cheng",
        "Dacheng Tao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "published": "2026-02-09T07:05:23+00:00",
      "link": "https://arxiv.org/pdf/2602.08329v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Inference optimization via KV cache selection",
      "llm_evidence_cn": "通过KV缓存选择进行推理优化",
      "llm_evidence": "通过KV缓存选择进行推理优化",
      "llm_tldr_en": "Proposes Pre-hoc Sparsity to optimize LLM inference by selecting KV entries before attention scoring.",
      "llm_tldr_cn": "提出Pre-hoc Sparsity，通过在注意力计算前选择KV条目来优化长文本推理效率。",
      "llm_tldr": "提出Pre-hoc Sparsity，通过在注意力计算前选择KV条目来优化长文本推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.08329v1",
      "carry_days": 1
    },
    {
      "id": "2602.01990v1",
      "title": "SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning",
      "abstract": "Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.",
      "authors": [
        "Zhen-Hao Xie",
        "Jun-Tao Tang",
        "Yu-Cheng Shi",
        "Han-Jia Ye",
        "De-Chuan Zhan",
        "Da-Wei Zhou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02T11:47:06+00:00",
      "link": "https://arxiv.org/pdf/2602.01990v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Continual instruction tuning for multimodal LLMs",
      "llm_evidence_cn": "多模态LLM的持续指令微调",
      "llm_evidence": "多模态LLM的持续指令微调",
      "llm_tldr_en": "Addresses expert drift in multimodal continual instruction tuning using stabilized mixture-of-experts.",
      "llm_tldr_cn": "利用稳定混合专家模型解决多模态持续指令微调中的专家漂移问题。",
      "llm_tldr": "利用稳定混合专家模型解决多模态持续指令微调中的专家漂移问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.01990v1",
      "carry_days": 1
    },
    {
      "id": "2601.17261v2",
      "title": "AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning",
      "abstract": "Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.",
      "authors": [
        "Wei Lin",
        "Yining Jiang",
        "Qingyu Song",
        "Qiao Xiang",
        "Hong Xu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-24T02:28:15+00:00",
      "link": "https://arxiv.org/pdf/2601.17261v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "parameter-efficient tuning and zeroth-order optimization",
      "llm_evidence_cn": "参数高效微调与零阶优化",
      "llm_evidence": "参数高效微调与零阶优化",
      "llm_tldr_en": "Proposes AGZO, an activation-guided zeroth-order optimization method for memory-efficient LLM fine-tuning.",
      "llm_tldr_cn": "提出 AGZO，一种激活引导的零阶优化方法，用于内存高效的 LLM 微调。",
      "llm_tldr": "提出 AGZO，一种激活引导的零阶优化方法，用于内存高效的 LLM 微调。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.17261v2",
      "carry_days": 1
    },
    {
      "id": "2602.20973v1",
      "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving",
      "abstract": "To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.",
      "authors": [
        "Yuliang Ji",
        "Fuchen Shen",
        "Jian Wu",
        "Qiujie Xie",
        "Yue Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-24T14:53:34+00:00",
      "link": "https://arxiv.org/pdf/2602.20973v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "evaluating mathematical and first-order logic reasoning in LLMs",
      "llm_evidence_cn": "评估大语言模型在数学和一阶逻辑推理中的能力",
      "llm_evidence": "评估大语言模型在数学和一阶逻辑推理中的能力",
      "llm_tldr_en": "Introduces PC-FOL dataset to evaluate LLMs on non-linear reasoning like proof by cases and contradiction.",
      "llm_tldr_cn": "引入PC-FOL数据集，评估大语言模型在分类证明和反证法等非线性推理中的表现。",
      "llm_tldr": "引入PC-FOL数据集，评估大语言模型在分类证明和反证法等非线性推理中的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.20973v1",
      "carry_days": 1
    },
    {
      "id": "2601.10823v2",
      "title": "Mugi: Value Level Parallelism For Efficient LLMs",
      "abstract": "Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\\times$ and $668\\times$ for nonlinear softmax operations, and $2.07\\times$ and $3.11\\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\\times$ and embodied carbon by $1.48\\times$.",
      "authors": [
        "Daniel Price",
        "Prabhu Vellaisamy",
        "John Shen",
        "Di Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "published": "2026-01-15T19:48:21+00:00",
      "link": "https://arxiv.org/pdf/2601.10823v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Value level parallelism for efficient LLM inference",
      "llm_evidence_cn": "用于高效大语言模型推理的数值级并行",
      "llm_evidence": "用于高效大语言模型推理的数值级并行",
      "llm_tldr_en": "Explores value level parallelism to optimize GEMM and quantization for efficient LLM execution.",
      "llm_tldr_cn": "探索数值级并行技术，优化矩阵乘法与量化以提升大模型推理效率。",
      "llm_tldr": "探索数值级并行技术，优化矩阵乘法与量化以提升大模型推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.10823v2",
      "carry_days": 1
    },
    {
      "id": "2602.08060v2",
      "title": "Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices",
      "abstract": "LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\\times$ speedup for translation tasks, closely matching analytic expectations.",
      "authors": [
        "Alejandro Ruiz y Mesa",
        "Guilherme Korol",
        "Moritz Riesterer",
        "João Paulo Cardoso de Lima",
        "Jeronimo Castrillon"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-08T17:09:51+00:00",
      "link": "https://arxiv.org/pdf/2602.08060v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Accelerated LLM inference and speculative decoding on edge devices",
      "llm_evidence_cn": "边缘设备上的加速LLM推理和投机解码",
      "llm_evidence": "边缘设备上的加速LLM推理和投机解码",
      "llm_tldr_en": "Proposes a compiler-assisted speculative sampling framework to optimize LLM inference latency on heterogeneous edge hardware.",
      "llm_tldr_cn": "提出一种编译器辅助的投机采样框架，旨在优化异构边缘硬件上的LLM推理延迟。",
      "llm_tldr": "提出一种编译器辅助的投机采样框架，旨在优化异构边缘硬件上的LLM推理延迟。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.08060v2",
      "carry_days": 1
    },
    {
      "id": "2601.23006v1",
      "title": "InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning",
      "abstract": "Supervised fine-tuning (SFT) is fundamental to adapting large language models, yet training on complete datasets incurs prohibitive costs with diminishing returns. Existing data selection methods suffer from severe domain specificity: techniques optimized for general instruction-following fail on reasoning tasks, and vice versa. We observe that measuring entropy differences between base models and minimally instruction-tuned calibrated models reveals a pattern -- samples with the lowest differential entropy consistently yield optimal performance across domains, yet this principle manifests domain-adaptively: reasoning tasks favor entropy increase (cognitive expansion), while general tasks favor entropy decrease (cognitive compression). We introduce InstructDiff, a unified framework that operationalizes differential entropy as a domain-adaptive selection criterion through warmup calibration, bi-directional NLL filtering, and entropy-based ranking. Extensive experiments show that InstructDiff achieves 17\\% relative improvement over full data training on mathematical reasoning and 52\\% for general instruction-following, outperforming prior baselines while using only 10\\% of the data.",
      "authors": [
        "Junyou Su",
        "He Zhu",
        "Xiao Luo",
        "Liyu Zhang",
        "Hong-Yu Zhou",
        "Yun Chen",
        "Peng Li",
        "Yang Liu",
        "Guanhua Chen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-30T14:15:44+00:00",
      "link": "https://arxiv.org/pdf/2601.23006v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient LLM Fine-Tuning and instruction-following data selection",
      "llm_evidence_cn": "高效LLM微调与指令遵循数据选择",
      "llm_evidence": "高效LLM微调与指令遵循数据选择",
      "llm_tldr_en": "Proposes InstructDiff for domain-adaptive data selection to improve efficiency in LLM supervised fine-tuning.",
      "llm_tldr_cn": "提出InstructDiff，通过领域自适应数据选择提高大语言模型有监督微调的效率。",
      "llm_tldr": "提出InstructDiff，通过领域自适应数据选择提高大语言模型有监督微调的效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.23006v1",
      "carry_days": 1
    },
    {
      "id": "2602.22538v1",
      "title": "RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format",
      "abstract": "Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.",
      "authors": [
        "Zhehao Huang",
        "Yuhang Liu",
        "Baijiong Lin",
        "Yixin Lou",
        "Zhengbao He",
        "Hanling Tian",
        "Tao Li",
        "Xiaolin Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-26T02:26:45+00:00",
      "link": "https://arxiv.org/pdf/2602.22538v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "merging instruction-tuned models with large reasoning models",
      "llm_evidence_cn": "将指令微调模型与大型推理模型合并",
      "llm_evidence": "将指令微调模型与大型推理模型合并",
      "llm_tldr_en": "Proposes RAIN-Merging to enhance instruction following in reasoning models without losing thinking formats.",
      "llm_tldr_cn": "提出RAIN-Merging方法，在保留推理格式的同时增强大型推理模型的指令遵循能力。",
      "llm_tldr": "提出RAIN-Merging方法，在保留推理格式的同时增强大型推理模型的指令遵循能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.22538v1",
      "carry_days": 1
    },
    {
      "id": "2602.14516v1",
      "title": "Efficient Multi-round LLM Inference over Disaggregated Serving",
      "abstract": "With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases.   In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines.",
      "authors": [
        "Wenhao He",
        "Youhe Jiang",
        "Penghao Zhao",
        "Quanqing Xu",
        "Eiko Yoneki",
        "Bin Cui",
        "Fangcheng Fu"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-16T07:07:30+00:00",
      "link": "https://arxiv.org/pdf/2602.14516v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient multi-round LLM inference over disaggregated serving",
      "llm_evidence_cn": "解耦服务架构下的高效多轮LLM推理",
      "llm_evidence": "解耦服务架构下的高效多轮LLM推理",
      "llm_tldr_en": "Presents AMPD, a framework to optimize prefill-decode coordination for multi-round LLM inference workflows.",
      "llm_tldr_cn": "提出AMPD框架，旨在优化多轮LLM推理工作流中的预填充与解码协作。",
      "llm_tldr": "提出AMPD框架，旨在优化多轮LLM推理工作流中的预填充与解码协作。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.14516v1",
      "carry_days": 1
    },
    {
      "id": "2601.17112v1",
      "title": "Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.",
      "authors": [
        "A. El Ichi",
        "K. Jbilou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-23T18:37:17+00:00",
      "link": "https://arxiv.org/pdf/2601.17112v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "tensor compression for efficient LLM deployment and memory reduction",
      "llm_evidence_cn": "用于LLM高效部署和减少内存占用的张量压缩",
      "llm_evidence": "用于LLM高效部署和减少内存占用的张量压缩",
      "llm_tldr_en": "Proposes a low-rank tensor approximation framework to compress LLM weights for efficient inference.",
      "llm_tldr_cn": "提出一种低秩张量近似框架，通过压缩LLM权重实现高效推理。",
      "llm_tldr": "提出一种低秩张量近似框架，通过压缩LLM权重实现高效推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.17112v1",
      "carry_days": 1
    },
    {
      "id": "2602.00161v1",
      "title": "Block removal for large language models through constrained binary optimization",
      "abstract": "Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.",
      "authors": [
        "David Jansen",
        "Roman Rausch",
        "David Montero",
        "Roman Orus"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "quant-ph"
      ],
      "published": "2026-01-29T19:46:39+00:00",
      "link": "https://arxiv.org/pdf/2602.00161v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Compressing LLMs by removing transformer blocks",
      "llm_evidence_cn": "通过移除Transformer层压缩大模型",
      "llm_evidence": "通过移除Transformer层压缩大模型",
      "llm_tldr_en": "Formulates LLM block removal as a binary optimization problem to improve inference efficiency.",
      "llm_tldr_cn": "将大模型层级剪枝建模为二元优化问题，在保持性能的同时实现模型压缩与推理加速。",
      "llm_tldr": "将大模型层级剪枝建模为二元优化问题，在保持性能的同时实现模型压缩与推理加速。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.00161v1",
      "carry_days": 1
    },
    {
      "id": "2602.01237v1",
      "title": "Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models",
      "abstract": "Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.",
      "authors": [
        "Katrina Brown",
        "Aneesh Muppidi",
        "Rana Shahout"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-01T13:58:23+00:00",
      "link": "https://arxiv.org/pdf/2602.01237v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient inference-time reasoning and predictive scheduling for LLMs",
      "llm_evidence_cn": "LLM的高效推理时间推理与预测调度",
      "llm_evidence": "LLM的高效推理时间推理与预测调度",
      "llm_tldr_en": "Proposes Predictive Scheduling to optimize token distribution and reasoning efficiency during LLM inference.",
      "llm_tldr_cn": "提出预测调度框架，通过动态分配Token预算优化LLM推理效率。",
      "llm_tldr": "提出预测调度框架，通过动态分配Token预算优化LLM推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.01237v1",
      "carry_days": 1
    },
    {
      "id": "2602.11688v1",
      "title": "GORGO: Maximizing KV-Cache Reuse While Minimizing Network Latency in Cross-Region LLM Load Balancing",
      "abstract": "Distributing LLM inference across geographical regions can improve Time-to-First-Token (TTFT) by regionalizing service deployments. While existing multi-region load balancers save prefill computation by prioritizing Key--Value (KV) Cache hit rate, they ignore cluster networking latency, a critical factor in routing decisions. We introduce GORGO, a method for minimizing TTFT by optimizing a total serving cost as a function of available compute, network latency, and prefix caching. Using extensive profiling on custom infrastructure, we analyze component-level latency bottlenecks and benchmark GORGO against three baselines: (1) naive least-load routing, which ignores prefix-cache overlap; (2) prefix-similarity routing, which selectively pushes requests to the replica with the highest cached-prefix overlap; and (3) a centralized HTTP proxy that runs the GORGO policy while tracking requests across all nodes. We demonstrate that GORGO reduces P99 TTFT through network-aware routing and improves average TTFT by preventing pathological cross-region forwarding. Additionally, we find that GORGO-proxy overcomes synchronization overhead in previous methods and is 2.5x faster on median TTFT, demonstrating the success of a centralized router.",
      "authors": [
        "Alessio Ricci Toniolo",
        "Abinaya Dinesh",
        "Rome Thorstenson"
      ],
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI",
        "cs.DC"
      ],
      "published": "2026-02-12T08:09:14+00:00",
      "link": "https://arxiv.org/pdf/2602.11688v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Inference optimization and KV-cache reuse in LLM deployment",
      "llm_evidence_cn": "LLM部署中的推理优化与KV缓存复用",
      "llm_evidence": "LLM部署中的推理优化与KV缓存复用",
      "llm_tldr_en": "Optimizes cross-region LLM inference by maximizing KV-cache reuse and minimizing network latency.",
      "llm_tldr_cn": "通过最大化KV缓存复用和最小化网络延迟，优化跨区域LLM推理性能。",
      "llm_tldr": "通过最大化KV缓存复用和最小化网络延迟，优化跨区域LLM推理性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.11688v1",
      "carry_days": 1
    },
    {
      "id": "2602.06127v1",
      "title": "Compressing LLMs with MoP: Mixture of Pruners",
      "abstract": "The high computational demands of Large Language Models (LLMs) motivate methods that reduce parameter count and accelerate inference. In response, model pruning emerges as an effective strategy, yet current methods typically focus on a single dimension-depth or width. We introduce MoP (Mixture of Pruners), an iterative framework that unifies these dimensions. At each iteration, MoP generates two branches-pruning in depth versus pruning in width-and selects a candidate to advance the path. On LLaMA-2 and LLaMA-3, MoP advances the frontier of structured pruning, exceeding the accuracy of competing methods across a broad set of compression regimes. It also consistently outperforms depth-only and width-only pruning. Furthermore, MoP translates structural pruning into real speedup, reducing end-to-end latency by 39% at 40% compression. Finally, extending MoP to the vision-language model LLaVA-1.5, we notably improve computational efficiency and demonstrate that text-only recovery fine-tuning can restore performance even on visual tasks.",
      "authors": [
        "Bruno Lopes Yamamoto",
        "Lucas Lauton de Alcantara",
        "Victor Zacarias",
        "Leandro Giusti Mugnaini",
        "Keith Ando Ogawa",
        "Lucas Pellicer",
        "Rosimeire Pereira Costa",
        "Edson Bollis",
        "Anna Helena Reali Costa",
        "Artur Jordao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T19:01:06+00:00",
      "link": "https://arxiv.org/pdf/2602.06127v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Model pruning for LLM inference acceleration",
      "llm_evidence_cn": "用于大语言模型推理加速的模型剪枝",
      "llm_evidence": "用于大语言模型推理加速的模型剪枝",
      "llm_tldr_en": "Introduces MoP to compress LLMs by unifying depth and width pruning for faster inference.",
      "llm_tldr_cn": "提出 MoP 框架，通过统一深度和宽度剪枝来压缩大模型并加速推理。",
      "llm_tldr": "提出 MoP 框架，通过统一深度和宽度剪枝来压缩大模型并加速推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.06127v1",
      "carry_days": 1
    },
    {
      "id": "2602.13942v1",
      "title": "A Theoretical Framework for LLM Fine-tuning Using Early Stopping for Non-random Initialization",
      "abstract": "In the era of large language models (LLMs), fine-tuning pretrained models has become ubiquitous. Yet the theoretical underpinning remains an open question. A central question is why only a few epochs of fine-tuning are typically sufficient to achieve strong performance on many different tasks. In this work, we approach this question by developing a statistical framework, combining rigorous early stopping theory with the attention-based Neural Tangent Kernel (NTK) for LLMs, offering new theoretical insights on fine-tuning practices. Specifically, we formally extend classical NTK theory [Jacot et al., 2018] to non-random (i.e., pretrained) initializations and provide a convergence guarantee for attention-based fine-tuning. One key insight provided by the theory is that the convergence rate with respect to sample size is closely linked to the eigenvalue decay rate of the empirical kernel matrix induced by the NTK. We also demonstrate how the framework can be used to explain task vectors for multiple tasks in LLMs. Finally, experiments with modern language models on real-world datasets provide empirical evidence supporting our theoretical insights.",
      "authors": [
        "Zexuan Sun",
        "Garvesh Raskutti"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-15T00:43:21+00:00",
      "link": "https://arxiv.org/pdf/2602.13942v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "theoretical framework for LLM fine-tuning and convergence",
      "llm_evidence_cn": "LLM微调与收敛的理论框架",
      "llm_evidence": "LLM微调与收敛的理论框架",
      "llm_tldr_en": "Provides a statistical framework to explain why few-epoch fine-tuning is effective for LLMs.",
      "llm_tldr_cn": "提供统计框架，从理论上解释为何LLM仅需少量轮次微调即可奏效。",
      "llm_tldr": "提供统计框架，从理论上解释为何LLM仅需少量轮次微调即可奏效。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.13942v1",
      "carry_days": 1
    },
    {
      "id": "2601.06431v2",
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "abstract": "Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.",
      "authors": [
        "Qingyu Ren",
        "Qianyu He",
        "Jingwen Chang",
        "Jie Zeng",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Han Xia",
        "Zeye Sun",
        "Fei Yu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-10T05:11:38+00:00",
      "link": "https://arxiv.org/pdf/2601.06431v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Reinforcement learning for instruction following with logical structures",
      "llm_evidence_cn": "针对具有逻辑结构的指令遵循的强化学习",
      "llm_evidence": "针对具有逻辑结构的指令遵循的强化学习",
      "llm_tldr_en": "Develops LSRIF to improve LLM instruction following by explicitly modeling logical dependencies in training.",
      "llm_tldr_cn": "开发 LSRIF 框架，通过在训练中显式建模逻辑依赖来提升大模型的指令遵循能力。",
      "llm_tldr": "开发 LSRIF 框架，通过在训练中显式建模逻辑依赖来提升大模型的指令遵循能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.06431v2",
      "carry_days": 1
    },
    {
      "id": "2602.08005v1",
      "title": "DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity",
      "abstract": "The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.",
      "authors": [
        "Jitai Hao",
        "Qiang Huang",
        "Yaowei Wang",
        "Min Zhang",
        "Jun Yu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-08T15:14:36+00:00",
      "link": "https://arxiv.org/pdf/2602.08005v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "KV cache compression for efficient LLM inference",
      "llm_evidence_cn": "用于高效LLM推理的KV缓存压缩",
      "llm_evidence": "用于高效LLM推理的KV缓存压缩",
      "llm_tldr_en": "Proposes DeltaKV to compress KV cache using residuals, reducing memory for long-context LLM deployment.",
      "llm_tldr_cn": "提出DeltaKV框架，通过残差压缩KV缓存，显著降低长文本LLM部署的显存占用。",
      "llm_tldr": "提出DeltaKV框架，通过残差压缩KV缓存，显著降低长文本LLM部署的显存占用。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.08005v1",
      "carry_days": 1
    },
    {
      "id": "2601.06562v1",
      "title": "Mosaic: Unlocking Long-Context Inference for Diffusion LLMs via Global Memory Planning and Dynamic Peak Taming",
      "abstract": "Diffusion-based large language models (dLLMs) have emerged as a promising paradigm, utilizing simultaneous denoising to enable global planning and iterative refinement. While these capabilities are particularly advantageous for long-context generation, deploying such models faces a prohibitive memory capacity barrier stemming from severe system inefficiencies. We identify that existing inference systems are ill-suited for this paradigm: unlike autoregressive models constrained by the cumulative KV-cache, dLLMs are bottlenecked by transient activations recomputed at every step. Furthermore, general-purpose memory reuse mechanisms lack the global visibility to adapt to dLLMs' dynamic memory peaks, which toggle between logits and FFNs. To address these mismatches, we propose Mosaic, a memory-efficient inference system that shifts from local, static management to a global, dynamic paradigm. Mosaic integrates a mask-only logits kernel to eliminate redundancy, a lazy chunking optimizer driven by an online heuristic search to adaptively mitigate dynamic peaks, and a global memory manager to resolve fragmentation via virtual addressing. Extensive evaluations demonstrate that Mosaic achieves an average 2.71$\\times$ reduction in the memory peak-to-average ratio and increases the maximum inference sequence length supportable on identical hardware by 15.89-32.98$\\times$. This scalability is achieved without compromising accuracy and speed, and in fact reducing latency by 4.12%-23.26%.",
      "authors": [
        "Liang Zheng",
        "Bowen Shi",
        "Yitao Hu",
        "Jiawei Zhang",
        "Ruofan Li",
        "Sheng Chen",
        "Wenxin Li",
        "Keqiu Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-10T13:17:08+00:00",
      "link": "https://arxiv.org/pdf/2601.06562v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Long-context inference optimization for diffusion LLMs",
      "llm_evidence_cn": "扩散大模型的长上下文推理优化",
      "llm_evidence": "扩散大模型的长上下文推理优化",
      "llm_tldr_en": "Proposes Mosaic to unlock long-context inference for diffusion LLMs via memory planning.",
      "llm_tldr_cn": "提出Mosaic框架，通过内存规划解锁扩散大模型的长上下文推理。",
      "llm_tldr": "提出Mosaic框架，通过内存规划解锁扩散大模型的长上下文推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.06562v1",
      "carry_days": 1
    },
    {
      "id": "2602.10729v1",
      "title": "BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization",
      "abstract": "The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-cost models and complex queries to higher-cost models (i.e., heterogeneous query routing); and (ii) a systems perspective that utilizes heterogeneous GPU resources as cost-effective alternatives to homogeneous high-end GPUs (i.e., heterogeneous model deployment). However, algorithm-system co-design for cost-efficient LLM serving necessitates sophisticated management: (i) Determining optimal query routing strategies under latency and quality requirements, (ii) configuring model deployment across heterogeneous GPUs with appropriate resource allocation and parallelism strategies, and (iii) co-optimizing routing and deployment decisions to maximize overall system performance. To address these challenges, we present BOute, a quality-aware scheduling system that jointly exploits heterogeneous model and GPU capabilities for cost-efficient LLM serving. BOute employs a multi-objective Bayesian optimization (MOBO) framework to co-optimize the routing strategy and model deployment, thereby maximizing the cost-efficiency of the serving system while guaranteeing response quality. Evaluation results demonstrate that BOute outperforms state-of-the-art LLM serving systems by up to 157% and 59% on average under identical cost budgets and quality requirements, or reducing serving costs by 15%-61% (38% on average) while maintaining the same performance targets, validating its effectiveness in achieving cost-efficient LLM serving.",
      "authors": [
        "Youhe Jiang",
        "Fangcheng Fu",
        "Eiko Yoneki"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-11T10:44:12+00:00",
      "link": "https://arxiv.org/pdf/2602.10729v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Cost-efficient LLM serving and inference optimization",
      "llm_evidence_cn": "高性价比的大模型服务与推理优化",
      "llm_evidence": "高性价比的大模型服务与推理优化",
      "llm_tldr_en": "Optimizes LLM deployment costs using Bayesian optimization for heterogeneous query routing and GPU resources.",
      "llm_tldr_cn": "利用贝叶斯优化进行异构查询路由和GPU资源管理，优化大模型部署成本。",
      "llm_tldr": "利用贝叶斯优化进行异构查询路由和GPU资源管理，优化大模型部署成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.10729v1",
      "carry_days": 1
    },
    {
      "id": "2601.21623v1",
      "title": "LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models",
      "abstract": "Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.",
      "authors": [
        "Stanislav Budzinskiy",
        "Marian Gloser",
        "Tolunay Yilmaz",
        "Ying Hong Tham",
        "Yuanyi Lin",
        "Wenyi Fang",
        "Fan Wu",
        "Philipp Petersen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-01-29T12:26:00+00:00",
      "link": "https://arxiv.org/pdf/2601.21623v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Mixed-precision inference optimization",
      "llm_evidence_cn": "混合精度推理优化",
      "llm_evidence": "混合精度推理优化",
      "llm_tldr_en": "Presents LAMP, an adaptive mixed-precision strategy to optimize transformer inference efficiency.",
      "llm_tldr_cn": "提出LAMP，一种自适应混合精度策略，旨在优化Transformer的推理效率。",
      "llm_tldr": "提出LAMP，一种自适应混合精度策略，旨在优化Transformer的推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.21623v1",
      "carry_days": 1
    },
    {
      "id": "2601.22705v1",
      "title": "CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control",
      "abstract": "Batch inference for agentic workloads stresses the GPU key-value (KV) cache in a sustained and cumulative manner, often causing severe throughput degradation well before memory capacity is exhausted. We identify this phenomenon as middle-phase thrashing, a previously under-characterized pathology in which cache efficiency collapses as long-lived agents accumulate state over time.   We argue that mitigating this pathology requires moving beyond reactive, request-level cache management to proactive, agent-level admission control. Drawing inspiration from congestion control in distributed systems, we view the KV cache as a shared resource whose efficient utilization depends on feedback-driven regulation. Based on this insight, we present CONCUR, a lightweight control layer that regulates agent admission to bound aggregate cache pressure while preserving execution continuity. CONCUR adapts a cache-aware control algorithm to dynamically adjust the number of active agents using runtime cache signals.   Across large models and real-world agent workloads, CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.",
      "authors": [
        "Qiaoling Chen",
        "Zhisheng Ye",
        "Tian Tang",
        "Peng Sun",
        "Boyu Tian",
        "Guoteng Wang",
        "Shenggui Li",
        "Yonggang Wen",
        "Zhenhua Han",
        "Tianwei Zhang"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-01-30T08:27:20+00:00",
      "link": "https://arxiv.org/pdf/2601.22705v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "high-throughput agentic batch inference optimization",
      "llm_evidence_cn": "高吞吐量智能体批处理推理优化",
      "llm_evidence": "高吞吐量智能体批处理推理优化",
      "llm_tldr_en": "Introduces congestion control for KV cache to improve throughput in agentic LLM batch inference workloads.",
      "llm_tldr_cn": "为KV缓存引入拥塞控制，以提高智能体大语言模型批处理推理工作负载的吞吐量。",
      "llm_tldr": "为KV缓存引入拥塞控制，以提高智能体大语言模型批处理推理工作负载的吞吐量。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.22705v1",
      "carry_days": 1
    },
    {
      "id": "2602.21140v1",
      "title": "ReviveMoE: Fast Recovery for Hardware Failures in Large-Scale MoE LLM Inference Deployments",
      "abstract": "As LLM deployments scale over more hardware, the probability of a single failure in a system increases significantly, and cloud operators must consider robust countermeasures to handle these inevitable failures. A common recovery approach is to simply restart the LLM serving instance; however, this is costly in model-as-a-service (MaaS) inference settings, where reloading model weights and recompiling computation graphs can introduce significant delays to incoming requests. We propose ReviveMoE, a method for rapid failure recovery in large-scale LLM deployments without restarting the serving instance. ReviveMoE is designed to support both the traditional LLM architecture, which collocates MoE and attention on the same hardware, and the disaggregated architectures, which separate MoE from attention. Integrated into Huawei Cloud's MaaS, ReviveMoE is built on top of Huawei's xDeepServe serving platform and the XCCL communications library.",
      "authors": [
        "Haley Li",
        "Xinglu Wang",
        "Cong Feng",
        "Chunxu Zuo",
        "Yanan Wang",
        "Hei Lo",
        "Yufei Cui",
        "Bingji Wang",
        "Duo Cui",
        "Shuming Jing",
        "Yizhou Shan",
        "Ying Xiong",
        "Jiannan Wang",
        "Yong Zhang",
        "Zhenan Fan"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-24T17:39:41+00:00",
      "link": "https://arxiv.org/pdf/2602.21140v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "fast recovery for large-scale MoE LLM inference deployments",
      "llm_evidence_cn": "大规模MoE LLM推理部署的快速故障恢复",
      "llm_evidence": "大规模MoE LLM推理部署的快速故障恢复",
      "llm_tldr_en": "Presents ReviveMoE for rapid failure recovery in large-scale LLM deployments without restarts.",
      "llm_tldr_cn": "提出ReviveMoE方法，实现大规模LLM部署中无需重启的快速故障恢复。",
      "llm_tldr": "提出ReviveMoE方法，实现大规模LLM部署中无需重启的快速故障恢复。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.21140v1",
      "carry_days": 1
    },
    {
      "id": "2602.00276v1",
      "title": "Localizing and Correcting Errors for LLM-based Planners",
      "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.",
      "authors": [
        "Aditya Kumar",
        "William W. Cohen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-30T19:56:15+00:00",
      "link": "https://arxiv.org/pdf/2602.00276v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM reasoning capabilities and failure on symbolic classical planning tasks",
      "llm_evidence_cn": "大语言模型的推理能力及其在符号经典规划任务中的失败",
      "llm_evidence": "大语言模型的推理能力及其在符号经典规划任务中的失败",
      "llm_tldr_en": "Proposes Localized In-Context Learning to correct constraint violations in LLM-based planning and reasoning.",
      "llm_tldr_cn": "提出局部上下文学习方法，用于纠正大模型在规划推理任务中的约束违反问题。",
      "llm_tldr": "提出局部上下文学习方法，用于纠正大模型在规划推理任务中的约束违反问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.00276v1",
      "carry_days": 1
    },
    {
      "id": "2602.01587v1",
      "title": "Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment",
      "abstract": "Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.",
      "authors": [
        "Zehua Cheng",
        "Jianwei Yang",
        "Wei Dai",
        "Jiahao Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-02T03:26:45+00:00",
      "link": "https://arxiv.org/pdf/2602.01587v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM alignment tuning for certifiable robustness against jailbreaks",
      "llm_evidence_cn": "针对越狱攻击的可证明鲁棒性LLM对齐微调",
      "llm_evidence": "针对越狱攻击的可证明鲁棒性LLM对齐微调",
      "llm_tldr_en": "Introduces Noise-Augmented Alignment Tuning to enhance LLM safety and robustness against adaptive attacks.",
      "llm_tldr_cn": "引入噪声增强对齐微调，提升LLM抵御自适应攻击的安全性和鲁棒性。",
      "llm_tldr": "引入噪声增强对齐微调，提升LLM抵御自适应攻击的安全性和鲁棒性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.01587v1",
      "carry_days": 1
    },
    {
      "id": "2602.01137v1",
      "title": "Self-Generative Adversarial Fine-Tuning for Large Language Models",
      "abstract": "Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.",
      "authors": [
        "Shiguang Wu",
        "Yaqing Wang",
        "Quanming Yao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-01T10:20:27+00:00",
      "link": "https://arxiv.org/pdf/2602.01137v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "LLM alignment techniques and self-generative adversarial fine-tuning",
      "llm_evidence_cn": "大语言模型对齐技术与自我生成对抗微调",
      "llm_evidence": "大语言模型对齐技术与自我生成对抗微调",
      "llm_tldr_en": "Proposes SGALM, a framework for LLM alignment using a generative adversarial game without external reward models.",
      "llm_tldr_cn": "提出 SGALM 框架，通过生成对抗博弈实现大模型对齐，无需外部奖励模型。",
      "llm_tldr": "提出 SGALM 框架，通过生成对抗博弈实现大模型对齐，无需外部奖励模型。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.01137v1",
      "carry_days": 1
    },
    {
      "id": "2601.21803v1",
      "title": "RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes",
      "abstract": "Retrieval-Augmented Generation (RAG) systems combine dense retrievers and language models to ground LLM outputs in retrieved documents. However, the opacity of how these components interact creates challenges for deployment in high-stakes domains. We present RAG-E, an end-to-end explainability framework that quantifies retriever-generator alignment through mathematically grounded attribution methods. Our approach adapts Integrated Gradients for retriever analysis, introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value approximation, for generator attribution, and introduces the Weighted Attribution-Relevance Gap (WARG) metric to measure how well a generator's document usage aligns with a retriever's ranking. Empirical analysis on TREC CAsT and FoodSafeSum reveals critical misalignments: for 47.4% to 66.7% of queries, generators ignore the retriever's top-ranked documents, while 48.1% to 65.9% rely on documents ranked as less relevant. These failure modes demonstrate that RAG output quality depends not solely on individual component performance but on their interplay, which can be audited via RAG-E.",
      "authors": [
        "Korbinian Randl",
        "Guido Rocchietti",
        "Aron Henriksson",
        "Ziawasch Abedjan",
        "Tony Lindgren",
        "John Pavlopoulos"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-29T14:47:00+00:00",
      "link": "https://arxiv.org/pdf/2601.21803v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "quantifying retriever-generator alignment in RAG",
      "llm_evidence_cn": "量化RAG中的检索器-生成器对齐",
      "llm_evidence": "量化RAG中的检索器-生成器对齐",
      "llm_tldr_en": "Introduces RAG-E to explain and measure how generators use retrieved documents in RAG systems.",
      "llm_tldr_cn": "引入RAG-E框架，通过数学归因方法量化和解释RAG系统中检索器与生成器的对齐情况。",
      "llm_tldr": "引入RAG-E框架，通过数学归因方法量化和解释RAG系统中检索器与生成器的对齐情况。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.21803v1",
      "carry_days": 1
    },
    {
      "id": "2602.05695v1",
      "title": "Determining Energy Efficiency Sweet Spots in Production LLM Inference",
      "abstract": "Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"Sweet Spots\" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.",
      "authors": [
        "Hiari Pizzini Cavagna",
        "Andrea Proia",
        "Giacomo Madella",
        "Giovanni B. Esposito",
        "Francesco Antici",
        "Daniele Cesarini",
        "Zeynep Kiziltan",
        "Andrea Bartolini"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.PF"
      ],
      "published": "2026-02-05T14:21:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05695v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Energy efficiency and inference optimization for Transformers",
      "llm_evidence_cn": "Transformer推理的能效与推理优化",
      "llm_evidence": "Transformer推理的能效与推理优化",
      "llm_tldr_en": "Proposes an analytical model to optimize energy efficiency in production LLM inference based on sequence lengths.",
      "llm_tldr_cn": "提出一种分析模型，根据序列长度优化生产环境中LLM推理的能效。",
      "llm_tldr": "提出一种分析模型，根据序列长度优化生产环境中LLM推理的能效。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.05695v1",
      "carry_days": 1
    },
    {
      "id": "2601.17768v2",
      "title": "LLM-42: Enabling Determinism in LLM Inference with Verified Speculation",
      "abstract": "In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.   Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.",
      "authors": [
        "Raja Gond",
        "Aditya K Kamath",
        "Ramachandran Ramjee",
        "Ashish Panwar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-01-25T09:58:57+00:00",
      "link": "https://arxiv.org/pdf/2601.17768v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Deterministic LLM inference with verified speculation",
      "llm_evidence_cn": "通过验证投机实现确定性LLM推理",
      "llm_evidence": "通过验证投机实现确定性LLM推理",
      "llm_tldr_en": "Presents LLM-42, a scheduling approach to enable deterministic outputs in LLM inference without sacrificing throughput.",
      "llm_tldr_cn": "提出LLM-42调度方法，在不损失吞吐量的情况下实现LLM推理的确定性输出。",
      "llm_tldr": "提出LLM-42调度方法，在不损失吞吐量的情况下实现LLM推理的确定性输出。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.17768v2",
      "carry_days": 1
    },
    {
      "id": "2601.21909v1",
      "title": "From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning",
      "abstract": "Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\\% and 4.63\\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.",
      "authors": [
        "Shaojie Wang",
        "Liang Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-29T16:00:48+00:00",
      "link": "https://arxiv.org/pdf/2601.21909v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cognitively aligned post-training for LLM reasoning",
      "llm_evidence_cn": "大语言模型推理的认知对齐后训练",
      "llm_evidence": "大语言模型推理的认知对齐后训练",
      "llm_tldr_en": "Aligns LLM post-training with human cognition to improve generalizable and reliable reasoning capabilities.",
      "llm_tldr_cn": "将LLM后训练与人类认知对齐，以提高可泛化且可靠的推理能力。",
      "llm_tldr": "将LLM后训练与人类认知对齐，以提高可泛化且可靠的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.21909v1",
      "carry_days": 1
    },
    {
      "id": "2602.05472v1",
      "title": "ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation",
      "abstract": "The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \\textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \\textbf{costly} to scale, \\textbf{brittle} across domains, and \\textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \\textbf{ALIVE} (\\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \\emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.",
      "authors": [
        "Yiwen Duan",
        "Jing Ye",
        "Xinpei Zhao"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T09:20:23+00:00",
      "link": "https://arxiv.org/pdf/2602.05472v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Adversarial learning and verbal evaluation for LLM alignment",
      "llm_evidence_cn": "用于LLM对齐的对抗学习与言语评估",
      "llm_evidence": "用于LLM对齐的对抗学习与言语评估",
      "llm_tldr_en": "Introduces ALIVE, an alignment framework using verbal feedback instead of scalar rewards for reasoning.",
      "llm_tldr_cn": "提出ALIVE框架，利用言语反馈代替标量奖励来实现LLM的推理对齐。",
      "llm_tldr": "提出ALIVE框架，利用言语反馈代替标量奖励来实现LLM的推理对齐。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.05472v1",
      "carry_days": 1
    },
    {
      "id": "2601.08146v2",
      "title": "Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning",
      "abstract": "Adapting LLMs to low-resource languages is difficult: labeled data is scarce, full-model fine-tuning is unstable, and continued cross-lingual tuning can cause catastrophic forgetting. We propose Circuit-Targeted Supervised Fine-Tuning (CT-SFT): a counterfactual-free adaptation of CD-T (Contextual Decomposition Transformer) that uses a label-balanced mean baseline and task-directional relevance scoring to identify a sparse set of task-relevant attention heads in a proxy-language checkpoint, then transfer learns to a target language by updating only those heads (plus LayerNorm) via head-level gradient masking. Across NusaX-Senti and XNLI, CT-SFT improves cross-lingual accuracy over continued full fine-tuning while updating only a small subset of model parameters. We find an editing-preserving trade-off: harder transfers favor editing circuit heads, while easier transfers often favor near-zero (i.e., low-relevance heads) updates, preserving the source mechanism. CT-SFT also substantially reduces catastrophic forgetting, preserving proxy/source-language competence during transfer.",
      "authors": [
        "Khumaisa Nur'aini",
        "Ayu Purwarianti",
        "Alham Fikri Aji",
        "Derry Wijaya"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-13T02:20:53+00:00",
      "link": "https://arxiv.org/pdf/2601.08146v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Data-efficient adaptation and circuit-targeted supervised fine-tuning",
      "llm_evidence_cn": "高效数据适配与针对性监督微调",
      "llm_evidence": "高效数据适配与针对性监督微调",
      "llm_tldr_en": "Presents CT-SFT for efficient cross-lingual adaptation of LLMs by targeting specific attention heads.",
      "llm_tldr_cn": "提出CT-SFT方法，通过针对特定注意力头实现LLM的高效跨语言适配。",
      "llm_tldr": "提出CT-SFT方法，通过针对特定注意力头实现LLM的高效跨语言适配。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.08146v2",
      "carry_days": 1
    },
    {
      "id": "2602.17937v1",
      "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification",
      "abstract": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.",
      "authors": [
        "Xiaotang Du",
        "Giwon Hong",
        "Wai-Chung Kwan",
        "Rohit Saxena",
        "Ivan Titov",
        "Pasquale Minervini",
        "Emily Allaway"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.PL"
      ],
      "published": "2026-02-20T01:56:27+00:00",
      "link": "https://arxiv.org/pdf/2602.17937v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Instruction optimization for Chain-of-Thought reasoning",
      "llm_evidence_cn": "针对思维链推理的指令优化",
      "llm_evidence": "针对思维链推理的指令优化",
      "llm_tldr_en": "Analyzes instruction optimization techniques like CoT and DSPy for tabular fact verification in LLMs.",
      "llm_tldr_cn": "系统分析了CoT等指令优化技术在提升大模型表格事实验证推理性能方面的作用。",
      "llm_tldr": "系统分析了CoT等指令优化技术在提升大模型表格事实验证推理性能方面的作用。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.17937v1",
      "carry_days": 1
    },
    {
      "id": "2602.07215v1",
      "title": "Multi-Agentic AI for Fairness-Aware and Accelerated Multi-modal Large Model Inference in Real-world Mobile Edge Networks",
      "abstract": "Generative AI (GenAI) has transformed applications in natural language processing and content creation, yet centralized inference remains hindered by high latency, limited customizability, and privacy concerns. Deploying large models (LMs) in mobile edge networks emerges as a promising solution. However, it also poses new challenges, including heterogeneous multi-modal LMs with diverse resource demands and inference speeds, varied prompt/output modalities that complicate orchestration, and resource-limited infrastructure ill-suited for concurrent LM execution. In response, we propose a Multi-Agentic AI framework for latency- and fairness-aware multi-modal LM inference in mobile edge networks. Our solution includes a long-term planning agent, a short-term prompt scheduling agent, and multiple on-node LM deployment agents, all powered by foundation language models. These agents cooperatively optimize prompt routing and LM deployment through natural language reasoning over runtime telemetry and historical experience. To evaluate its performance, we further develop a city-wide testbed that supports network monitoring, containerized LM deployment, intra-server resource management, and inter-server communications. Experiments demonstrate that our solution reduces average latency by over 80% and improves fairness (Normalized Jain index) to 0.90 compared to other baselines. Moreover, our solution adapts quickly without fine-tuning, offering a generalizable solution for optimizing GenAI services in edge environments.",
      "authors": [
        "Haiyuan Li",
        "Hari Madhukumar",
        "Shuangyi Yan",
        "Yulei Wu",
        "Dimitra Simeonidou"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-02-06T21:52:49+00:00",
      "link": "https://arxiv.org/pdf/2602.07215v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Multi-agentic AI for accelerated multi-modal large model inference",
      "llm_evidence_cn": "用于加速多模态大模型推理的多智能体AI",
      "llm_evidence": "用于加速多模态大模型推理的多智能体AI",
      "llm_tldr_en": "Proposes a multi-agent framework to optimize latency and fairness for LLM inference in edge networks.",
      "llm_tldr_cn": "提出多智能体框架，优化移动边缘网络中大模型推理的延迟与公平性。",
      "llm_tldr": "提出多智能体框架，优化移动边缘网络中大模型推理的延迟与公平性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.07215v1",
      "carry_days": 1
    },
    {
      "id": "2601.17551v1",
      "title": "GreenServ: Energy-Efficient Context-Aware Dynamic Routing for Multi-Model LLM Inference",
      "abstract": "Large language models (LLMs) demonstrate remarkable capabilities, but their broad deployment is limited by significant computational resource demands, particularly energy consumption during inference. Static, one-model-fits-all inference strategies are often inefficient, as they do not exploit the diverse range of available models or adapt to varying query requirements.   This paper presents GreenServ, a dynamic, context-aware routing framework that optimizes the trade-off between inference accuracy and energy efficiency. GreenServ extracts lightweight contextual features from each query, including task type, semantic cluster, and text complexity, and routes queries to the most suitable model from a heterogeneous pool, based on observed accuracy and energy usage. We employ a multi-armed bandit approach to learn adaptive routing policies online. This approach operates under partial feedback, eliminates the need for extensive offline calibration, and streamlines the integration of new models into the inference pipeline.   We evaluated GreenServ across five benchmark tasks and a pool of 16 contemporary open-access LLMs. Experimental results show that GreenServ consistently outperforms static (single-model) and random baselines. In particular, compared to random routing, GreenServ achieved a 22% increase in accuracy while reducing cumulative energy consumption by 31%. Finally, we evaluated GreenServ with RouterBench, achieving an average accuracy of 71.7% with a peak accuracy of 75.7%. All artifacts are open-source and available as an anonymous repository for review purposes here: https://anonymous.4open.science/r/llm-inference-router-EBEA/README.md",
      "authors": [
        "Thomas Ziller",
        "Shashikant Ilager",
        "Alessandro Tundo",
        "Ezio Bartocci",
        "Leonardo Mariani",
        "Ivona Brandic"
      ],
      "primary_category": "cs.PF",
      "categories": [
        "cs.PF",
        "cs.LG"
      ],
      "published": "2026-01-24T18:42:16+00:00",
      "link": "https://arxiv.org/pdf/2601.17551v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "efficient deployment and inference optimization",
      "llm_evidence_cn": "高效部署与推理优化",
      "llm_evidence": "高效部署与推理优化",
      "llm_tldr_en": "Presents GreenServ, a dynamic routing framework to optimize the accuracy-energy trade-off in multi-model inference.",
      "llm_tldr_cn": "提出 GreenServ，一种动态路由框架，用于优化多模型推理中的准确性与能耗权衡。",
      "llm_tldr": "提出 GreenServ，一种动态路由框架，用于优化多模型推理中的准确性与能耗权衡。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.17551v1",
      "carry_days": 1
    },
    {
      "id": "2601.20417v2",
      "title": "SpeechMapper: Speech-to-text Embedding Projector for LLMs",
      "abstract": "Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.",
      "authors": [
        "Biswesh Mohapatra",
        "Marcely Zanon Boito",
        "Ioan Calapodescu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-28T09:22:58+00:00",
      "link": "https://arxiv.org/pdf/2601.20417v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "cost-efficient speech-to-LLM instruction tuning",
      "llm_evidence_cn": "高效的语音到LLM指令微调",
      "llm_evidence": "高效的语音到LLM指令微调",
      "llm_tldr_en": "Presents SpeechMapper, a modular approach for attaching speech embeddings to LLMs via instruction tuning.",
      "llm_tldr_cn": "提出SpeechMapper，一种通过指令微调将语音嵌入高效连接到LLM的低成本方法。",
      "llm_tldr": "提出SpeechMapper，一种通过指令微调将语音嵌入高效连接到LLM的低成本方法。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.20417v2",
      "carry_days": 1
    },
    {
      "id": "2601.07898v1",
      "title": "Large Language Models and Algorithm Execution: Application to an Arithmetic Function",
      "abstract": "Large Language Models (LLMs) have recently developed new advanced functionalities. Their effectiveness relies on statistical learning and generalization capabilities. However, they face limitations in internalizing the data they process and struggle, for instance, to autonomously execute algorithms. In this paper, we investigate the possibility of extending these models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. We introduce a training model called LLM-DAL (Large Language Model - Decompositional Algorithmic Learning), through which we demonstrate that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.",
      "authors": [
        "Farah Ben Slama",
        "Frédéric Armetta"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-12T12:27:59+00:00",
      "link": "https://arxiv.org/pdf/2601.07898v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Improving reasoning and algorithm execution in LLMs",
      "llm_evidence_cn": "提升大模型的推理与算法执行能力",
      "llm_evidence": "提升大模型的推理与算法执行能力",
      "llm_tldr_en": "Introduces LLM-DAL to enhance algorithmic reasoning and generalization through decompositional learning.",
      "llm_tldr_cn": "提出LLM-DAL模型，通过分解式学习显著提升大模型的算法推理与泛化能力。",
      "llm_tldr": "提出LLM-DAL模型，通过分解式学习显著提升大模型的算法推理与泛化能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.07898v1",
      "carry_days": 1
    },
    {
      "id": "2602.05235v1",
      "title": "FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.",
      "authors": [
        "Zhilin Liang",
        "Yuxiang Wang",
        "Zimu Zhou",
        "Hainan Zhang",
        "Boyi Liu",
        "Yongxin Tong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T02:52:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05235v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Federated Retrieval-Augmented Generation (FedRAG) using parametric adapters",
      "llm_evidence_cn": "使用参数化适配器的联邦检索增强生成 (FedRAG)",
      "llm_evidence": "使用参数化适配器的联邦检索增强生成 (FedRAG)",
      "llm_tldr_en": "Proposes FedMosaic for privacy-preserving RAG by encoding documents into adapters instead of sharing raw text.",
      "llm_tldr_cn": "提出 FedMosaic，通过将文档编码为适配器而非共享原始文本来实现隐私保护的 RAG。",
      "llm_tldr": "提出 FedMosaic，通过将文档编码为适配器而非共享原始文本来实现隐私保护的 RAG。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.05235v1",
      "carry_days": 1
    },
    {
      "id": "2602.03103v1",
      "title": "Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision",
      "abstract": "Instruction tuning is now the default way to train and adapt large language models, but many instruction--input--output pairs are only weakly specified: for a given input, the same output can remain plausible under several alternative instructions. This raises a simple question: \\emph{does the instruction uniquely determine the target output?}   We propose the \\textbf{Task--Specificity Score (TSS)} to quantify how much an instruction matters for predicting its output, by contrasting the true instruction against plausible alternatives for the same input. We further introduce \\textbf{TSS++}, which uses hard alternatives and a small quality term to mitigate easy-negative effects. Across three instruction datasets (\\textsc{Alpaca}, \\textsc{Dolly-15k}, \\textsc{NI-20}) and three open LLMs (Gemma, Llama, Qwen), we show that selecting task-specific examples improves downstream performance under tight token budgets and complements quality-based filters such as perplexity and IFD.",
      "authors": [
        "Pritam Kadasi",
        "Abhishek Upperwal",
        "Mayank Singh"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-03T04:57:47+00:00",
      "link": "https://arxiv.org/pdf/2602.03103v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Instruction tuning analysis and task-specificity in instruction-input-output pairs",
      "llm_evidence_cn": "指令微调分析以及指令-输入-输出对的任务特异性研究",
      "llm_evidence": "指令微调分析以及指令-输入-输出对的任务特异性研究",
      "llm_tldr_en": "Introduces TSS to quantify how much instructions matter for predicting outputs in instruction tuning datasets.",
      "llm_tldr_cn": "引入TSS指标来量化指令在指令微调数据集中对预测输出的重要性。",
      "llm_tldr": "引入TSS指标来量化指令在指令微调数据集中对预测输出的重要性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.03103v1",
      "carry_days": 1
    },
    {
      "id": "2602.19240v1",
      "title": "Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks.",
      "authors": [
        "Sen Zhao",
        "Lincheng Zhou",
        "Yue Chen",
        "Ding Zou"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-22T15:44:53+00:00",
      "link": "https://arxiv.org/pdf/2602.19240v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Retrieval-Augmented Generation for graph-based reasoning and knowledge tasks",
      "llm_evidence_cn": "用于图推理和知识任务的检索增强生成",
      "llm_evidence": "用于图推理和知识任务的检索增强生成",
      "llm_tldr_en": "Enhances RAG for textual graphs by incorporating higher-dimensional structures to improve closed-loop reasoning.",
      "llm_tldr_cn": "通过引入高维结构增强文本图的RAG，以改进闭环推理能力。",
      "llm_tldr": "通过引入高维结构增强文本图的RAG，以改进闭环推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.19240v1",
      "carry_days": 1
    },
    {
      "id": "2602.22642v1",
      "title": "Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning",
      "abstract": "Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.",
      "authors": [
        "Qin-Wen Luo",
        "Sheng Ren",
        "Xiang Chen",
        "Rui Liu",
        "Jun Fang",
        "Naiqiang Tan",
        "Sheng-Jun Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-26T05:47:30+00:00",
      "link": "https://arxiv.org/pdf/2602.22642v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "efficient LLM reasoning and Chain-of-Thought compression",
      "llm_evidence_cn": "高效的大模型推理与思维链压缩",
      "llm_evidence": "高效的大模型推理与思维链压缩",
      "llm_tldr_en": "Proposes entropy regularization to compress Chain-of-Thought reasoning without sacrificing model capability.",
      "llm_tldr_cn": "提出熵正则化方法，在不牺牲模型能力的前提下压缩思维链推理过程。",
      "llm_tldr": "提出熵正则化方法，在不牺牲模型能力的前提下压缩思维链推理过程。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.22642v1",
      "carry_days": 1
    },
    {
      "id": "2602.01075v2",
      "title": "ConvexBench: Can LLMs Recognize Convex Functions?",
      "abstract": "Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \\cb, a scalable and mechanically verifiable benchmark for testing \\textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \\textit{parsing failure} and \\textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).",
      "authors": [
        "Yepeng Liu",
        "Yu Huang",
        "Yu-Xiang Wang",
        "Yingbin Liang",
        "Yuheng Bu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-01T07:41:17+00:00",
      "link": "https://arxiv.org/pdf/2602.01075v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Evaluating LLM reasoning and problem solving in mathematics",
      "llm_evidence_cn": "评估大语言模型在数学领域的推理与问题解决能力",
      "llm_evidence": "评估大语言模型在数学领域的推理与问题解决能力",
      "llm_tldr_en": "Introduces ConvexBench to test LLMs' ability to reason about convex functions and symbolic math.",
      "llm_tldr_cn": "引入ConvexBench基准测试，评估大模型在凸函数识别和符号数学推理方面的能力。",
      "llm_tldr": "引入ConvexBench基准测试，评估大模型在凸函数识别和符号数学推理方面的能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.01075v2",
      "carry_days": 1
    },
    {
      "id": "2601.18999v1",
      "title": "Randomization Boosts KV Caching, Learning Balances Query Load: A Joint Perspective",
      "abstract": "KV caching is a fundamental technique for accelerating Large Language Model (LLM) inference by reusing key-value (KV) pairs from previous queries, but its effectiveness under limited memory is highly sensitive to the eviction policy. The default Least Recently Used (LRU) eviction algorithm struggles with dynamic online query arrivals, especially in multi-LLM serving scenarios, where balancing query load across workers and maximizing cache hit rate of each worker are inherently conflicting objectives. We give the first unified mathematical model that captures the core trade-offs between KV cache eviction and query routing. Our analysis reveals the theoretical limitations of existing methods and leads to principled algorithms that integrate provably competitive randomized KV cache eviction with learning-based methods to adaptively route queries with evolving patterns, thus balancing query load and cache hit rate. Our theoretical results are validated by extensive experiments across 4 benchmarks and 3 prefix-sharing settings, demonstrating improvements of up to 6.92$\\times$ in cache hit rate, 11.96$\\times$ reduction in latency, 14.06$\\times$ reduction in time-to-first-token (TTFT), and 77.4% increase in throughput over the state-of-the-art methods. Our code is available at https://github.com/fzwark/KVRouting.",
      "authors": [
        "Fangzhou Wu",
        "Sandeep Silwal",
        "Qiuyi",
        "Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-26T22:20:59+00:00",
      "link": "https://arxiv.org/pdf/2601.18999v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "KV caching and inference optimization for LLMs",
      "llm_evidence_cn": "LLM推理中的KV缓存与推理优化",
      "llm_evidence": "LLM推理中的KV缓存与推理优化",
      "llm_tldr_en": "Provides a mathematical model to balance KV cache eviction and query routing for efficient LLM serving.",
      "llm_tldr_cn": "提供数学模型平衡KV缓存置换与查询路由，以实现高效的LLM服务。",
      "llm_tldr": "提供数学模型平衡KV缓存置换与查询路由，以实现高效的LLM服务。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.18999v1",
      "carry_days": 1
    },
    {
      "id": "2601.15434v2",
      "title": "ManuRAG: Multi-modal Retrieval Augmented Generation for Manufacturing Question Answering (Early Version)",
      "abstract": "The evolution of digital manufacturing requires intelligent Question Answering (QA) systems that can seamlessly integrate and analyze complex multi-modal data, such as text, images, formulas, and tables. Conventional Retrieval Augmented Generation (RAG) methods often fall short in handling this complexity, resulting in subpar performance. We introduce ManuRAG, an innovative multi-modal RAG framework designed for manufacturing QA, incorporating specialized techniques to improve answer accuracy, reliability, and interpretability. To benchmark performance, we evaluate ManuRAG on three datasets comprising a total of 1,515 QA pairs, corresponding to mathematical, multiple-choice, and review-based questions in manufacturing principles and practices. Experimental results show that ManuRAG consistently outperforms existing methods across all evaluated datasets. Furthermore, ManuRAG's adaptable design makes it applicable to other domains, including law, healthcare, and finance, positioning it as a versatile tool for domain-specific QA.",
      "authors": [
        "Yunqing Li",
        "Zihan Dong",
        "Farhad Ameri",
        "Jianbang Zhang"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE"
      ],
      "published": "2026-01-21T19:59:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15434v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Multi-modal Retrieval Augmented Generation framework",
      "llm_evidence_cn": "多模态检索增强生成框架",
      "llm_evidence": "多模态检索增强生成框架",
      "llm_tldr_en": "Proposes ManuRAG, a multi-modal RAG framework for complex question answering in manufacturing.",
      "llm_tldr_cn": "提出ManuRAG框架，利用多模态检索增强技术解决制造业中的复杂问答问题。",
      "llm_tldr": "提出ManuRAG框架，利用多模态检索增强技术解决制造业中的复杂问答问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.15434v2",
      "carry_days": 1
    },
    {
      "id": "2602.13680v1",
      "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
      "abstract": "Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that integrates Sliding Window Attention (SWA) with non-linear Test-Time Training (TTT) memory networks. \\textsc{AllMem} enables models to effectively scale to ultra-long contexts while mitigating catastrophic forgetting. This approach not only overcomes the representation constraints typical of linear memory models but also significantly reduces the computational and memory footprint during long-sequence inference. Furthermore, we implement a Memory-Efficient Fine-Tuning strategy to replace standard attention layers in pre-trained models with memory-augmented sliding window layers. This framework facilitates the efficient transformation of any off-the-shelf pre-trained LLM into an \\textsc{AllMem}-based architecture. Empirical evaluations confirm that our 4k window model achieves near-lossless performance on 37k LongBench with a marginal 0.83 drop compared to full attention. Furthermore, on InfiniteBench at a 128k context, our 8k window variant outperforms full attention, which validates the effectiveness of our parameterized memory in mitigating noise and maintaining robust long-range modeling without the prohibitive costs of global attention.",
      "authors": [
        "Ziming Wang",
        "Xiang Wang",
        "Kailong Peng",
        "Lang Qin",
        "Juan Gabriel Kostelec",
        "Christos Sourmpis",
        "Axel Laborieux",
        "Qinghai Guo"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-14T09:04:28+00:00",
      "link": "https://arxiv.org/pdf/2602.13680v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient long-context modeling and memory-efficient inference optimization",
      "llm_evidence_cn": "高效长文本建模与内存高效的推理优化",
      "llm_evidence": "高效长文本建模与内存高效的推理优化",
      "llm_tldr_en": "Introduces AllMem to reduce computational and memory overhead during long-sequence LLM inference.",
      "llm_tldr_cn": "推出AllMem架构，通过混合注意力与内存网络降低LLM长文本推理的计算与内存开销。",
      "llm_tldr": "推出AllMem架构，通过混合注意力与内存网络降低LLM长文本推理的计算与内存开销。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.13680v1",
      "carry_days": 1
    },
    {
      "id": "2602.14302v1",
      "title": "Floe: Federated Specialization for Real-Time LLM-SLM Inference",
      "abstract": "Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.",
      "authors": [
        "Chunlin Tian",
        "Kahou Tam",
        "Yebo Wu",
        "Shuaihang Zhong",
        "Li Li",
        "Nicholas D. Lane",
        "Chengzhong Xu"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published": "2026-02-15T20:28:38+00:00",
      "link": "https://arxiv.org/pdf/2602.14302v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient deployment and inference optimization via federated LLM-SLM hybrid",
      "llm_evidence_cn": "通过联邦LLM-SLM混合架构实现高效部署与推理优化",
      "llm_evidence": "通过联邦LLM-SLM混合架构实现高效部署与推理优化",
      "llm_tldr_en": "Proposes Floe, a federated framework combining cloud LLMs and edge SLMs for low-latency inference.",
      "llm_tldr_cn": "提出Floe框架，结合云端LLM与边缘SLM实现低延迟、隐私保护的推理。",
      "llm_tldr": "提出Floe框架，结合云端LLM与边缘SLM实现低延迟、隐私保护的推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.14302v1",
      "carry_days": 1
    },
    {
      "id": "2602.17809v1",
      "title": "Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning",
      "abstract": "Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.",
      "authors": [
        "Ibne Farabi Shihab",
        "Sanjeda Akter",
        "Anuj Sharma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-19T20:17:54+00:00",
      "link": "https://arxiv.org/pdf/2602.17809v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Bayesian PEFT framework for parameter-efficient fine-tuning",
      "llm_evidence_cn": "用于参数高效微调的贝叶斯PEFT框架",
      "llm_evidence": "用于参数高效微调的贝叶斯PEFT框架",
      "llm_tldr_en": "Proposes Stiefel-Bayes Adapters for reliable and calibrated parameter-efficient fine-tuning of LLMs.",
      "llm_tldr_cn": "提出Stiefel-Bayes适配器，为大语言模型的参数高效微调提供可靠的校准预测。",
      "llm_tldr": "提出Stiefel-Bayes适配器，为大语言模型的参数高效微调提供可靠的校准预测。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.17809v1",
      "carry_days": 1
    },
    {
      "id": "2601.19924v1",
      "title": "OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \\textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.",
      "authors": [
        "Yitian Chen",
        "Cheng Cheng",
        "Yinan Sun",
        "Zi Ling",
        "Dongdong Ge"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-09T09:22:33+00:00",
      "link": "https://arxiv.org/pdf/2601.19924v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Benchmarking LLM reasoning capabilities in optimization modeling",
      "llm_evidence_cn": "评估 LLM 在优化建模中的推理能力基准",
      "llm_evidence": "评估 LLM 在优化建模中的推理能力基准",
      "llm_tldr_en": "Introduces OPT-ENGINE to evaluate and scale the difficulty of optimization reasoning tasks for LLMs.",
      "llm_tldr_cn": "引入 OPT-ENGINE，用于评估和扩展 LLM 优化推理任务的难度。",
      "llm_tldr": "引入 OPT-ENGINE，用于评估和扩展 LLM 优化推理任务的难度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.19924v1",
      "carry_days": 1
    },
    {
      "id": "2602.11609v1",
      "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
      "abstract": "We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.   To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.   Code, data, and package are available at https://github.com/maitrix-org/scPilot",
      "authors": [
        "Yiming Gao",
        "Zhen Wang",
        "Jefferson Chen",
        "Mark Antkowiak",
        "Mengzhou Hu",
        "JungHo Kong",
        "Dexter Pratt",
        "Jieyuan Liu",
        "Enze Ma",
        "Zhiting Hu",
        "Eric P. Xing"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "q-bio.GN"
      ],
      "published": "2026-02-12T06:04:11+00:00",
      "link": "https://arxiv.org/pdf/2602.11609v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "reasoning and problem solving capabilities",
      "llm_evidence_cn": "推理与问题解决能力",
      "llm_evidence": "推理与问题解决能力",
      "llm_tldr_en": "Introduces scPilot, a framework evaluating LLM reasoning capabilities in the context of single-cell analysis.",
      "llm_tldr_cn": "引入 scPilot，一个在单细胞分析背景下评估 LLM 推理能力的框架。",
      "llm_tldr": "引入 scPilot，一个在单细胞分析背景下评估 LLM 推理能力的框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.11609v1",
      "carry_days": 1
    },
    {
      "id": "2601.22132v1",
      "title": "Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference",
      "abstract": "Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.",
      "authors": [
        "Ziming Dong",
        "Hardik Sharma",
        "Evan O'Toole",
        "Jaya Prakash Champati",
        "Kui Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-29T18:52:54+00:00",
      "link": "https://arxiv.org/pdf/2601.22132v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Cost-efficient LLM inference via shepherding",
      "llm_evidence_cn": "通过引导实现低成本的大语言模型推理",
      "llm_evidence": "通过引导实现低成本的大语言模型推理",
      "llm_tldr_en": "Introduces LLM Shepherding to reduce inference costs by using LLM hints to guide smaller models.",
      "llm_tldr_cn": "提出 LLM Shepherding 框架，利用大模型提示引导小模型以降低推理成本。",
      "llm_tldr": "提出 LLM Shepherding 框架，利用大模型提示引导小模型以降低推理成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.22132v1",
      "carry_days": 1
    },
    {
      "id": "2601.11464v1",
      "title": "MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models",
      "abstract": "As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.",
      "authors": [
        "Xiaoran Fan",
        "Zhichao Sun",
        "Tao Ji",
        "Lixing Shen",
        "Tao Gui"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-01-16T17:45:34+00:00",
      "link": "https://arxiv.org/pdf/2601.11464v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "KV cache compression and inference acceleration for VLMs",
      "llm_evidence_cn": "KV缓存压缩与视觉语言模型推理加速",
      "llm_evidence": "KV缓存压缩与视觉语言模型推理加速",
      "llm_tldr_en": "Adapts VLMs to Multi-Head Latent Attention to reduce memory and speed up inference without full retraining.",
      "llm_tldr_cn": "将视觉语言模型适配至MLA架构，在无需重新训练的情况下减少内存占用并加速推理。",
      "llm_tldr": "将视觉语言模型适配至MLA架构，在无需重新训练的情况下减少内存占用并加速推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.11464v1",
      "carry_days": 1
    },
    {
      "id": "2602.05393v1",
      "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better",
      "abstract": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \\textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\\times$ speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\\times$ fewer parameters than the target model.",
      "authors": [
        "Ji Zhao",
        "Yufei Gu",
        "Shitong Shao",
        "Xun Zhou",
        "Liang Xiang",
        "Zeke Xie"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-05T07:19:34+00:00",
      "link": "https://arxiv.org/pdf/2602.05393v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Accelerating LLM pre-training using existing models",
      "llm_evidence_cn": "利用现有模型加速大模型预训练",
      "llm_evidence": "利用现有模型加速大模型预训练",
      "llm_tldr_en": "Proposes LET paradigm to accelerate larger LLM training by leveraging knowledge from smaller pretrained models.",
      "llm_tldr_cn": "提出LET范式，利用现有小模型的知识加速大模型的预训练过程。",
      "llm_tldr": "提出LET范式，利用现有小模型的知识加速大模型的预训练过程。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.05393v1",
      "carry_days": 1
    },
    {
      "id": "2601.13244v1",
      "title": "Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks",
      "abstract": "Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.",
      "authors": [
        "Prateek Munjal",
        "Clement Christophe",
        "Ronnie Rajan",
        "Praveenkumar Kanithi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-19T17:26:49+00:00",
      "link": "https://arxiv.org/pdf/2601.13244v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Evaluation of instruction-tuned models versus base models on reasoning tasks",
      "llm_evidence_cn": "在推理任务上评估指令微调模型与基座模型",
      "llm_evidence": "在推理任务上评估指令微调模型与基座模型",
      "llm_tldr_en": "Analyzes the limitations of instruction tuning, finding that base models sometimes outperform tuned ones in zero-shot CoT.",
      "llm_tldr_cn": "分析了指令微调的局限性，发现在零样本CoT中基座模型有时优于微调模型。",
      "llm_tldr": "分析了指令微调的局限性，发现在零样本CoT中基座模型有时优于微调模型。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.13244v1",
      "carry_days": 1
    },
    {
      "id": "2601.14546v1",
      "title": "Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation",
      "abstract": "The quality of answers generated by large language models (LLMs) in retrieval-augmented generation (RAG) is largely influenced by the contextual information contained in the retrieved documents. A key challenge for improving RAG is to predict both the utility of retrieved documents -- quantified as the performance gain from using context over generation without context -- and the quality of the final answers in terms of correctness and relevance. In this paper, we define two prediction tasks within RAG. The first is retrieval performance prediction (RPP), which estimates the utility of retrieved documents. The second is generation performance prediction (GPP), which estimates the final answer quality. We hypothesise that in RAG, the topical relevance of retrieved documents correlates with their utility, suggesting that query performance prediction (QPP) approaches can be adapted for RPP and GPP. Beyond these retriever-centric signals, we argue that reader-centric features, such as the LLM's perplexity of the retrieved context conditioned on the input query, can further enhance prediction accuracy for both RPP and GPP. Finally, we propose that features reflecting query-agnostic document quality and readability can also provide useful signals to the predictions. We train linear regression models with the above categories of predictors for both RPP and GPP. Experiments on the Natural Questions (NQ) dataset show that combining predictors from multiple feature categories yields the most accurate estimates of RAG performance.",
      "authors": [
        "Fangzheng Tian",
        "Debasis Ganguly",
        "Craig Macdonald"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-01-20T23:59:54+00:00",
      "link": "https://arxiv.org/pdf/2601.14546v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Predicting utility and quality in Retrieval-Augmented Generation",
      "llm_evidence_cn": "预测检索增强生成中的效用与质量",
      "llm_evidence": "预测检索增强生成中的效用与质量",
      "llm_tldr_en": "Analyzes and predicts retrieval utility and answer quality to improve RAG systems for knowledge tasks.",
      "llm_tldr_cn": "分析并预测检索效用与答案质量，以改进知识密集型任务中的RAG系统。",
      "llm_tldr": "分析并预测检索效用与答案质量，以改进知识密集型任务中的RAG系统。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.14546v1",
      "carry_days": 1
    },
    {
      "id": "2602.07213v1",
      "title": "Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used",
      "abstract": "Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.",
      "authors": [
        "Srijan Shakya",
        "Anamaria-Roberta Hartl",
        "Sepp Hochreiter",
        "Korbinian Pöppel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-06T21:48:26+00:00",
      "link": "https://arxiv.org/pdf/2602.07213v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Adaptive retrieval and Chain-of-Thought reasoning in LLMs",
      "llm_evidence_cn": "大模型中的自适应检索与思维链推理",
      "llm_evidence": "大模型中的自适应检索与思维链推理",
      "llm_tldr_en": "Investigates when LLMs should query external knowledge during the reasoning process.",
      "llm_tldr_cn": "研究了大模型在推理过程中何时主动调用外部知识库的自适应策略。",
      "llm_tldr": "研究了大模型在推理过程中何时主动调用外部知识库的自适应策略。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.07213v1",
      "carry_days": 1
    },
    {
      "id": "2602.14536v1",
      "title": "Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets",
      "abstract": "Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.",
      "authors": [
        "Yuchen Yang",
        "Wenze Lin",
        "Enhao Huang",
        "Zhixuan Chu",
        "Hongbin Zhou",
        "Lan Tao",
        "Yiming Li",
        "Zhan Qin",
        "Kui Ren"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-16T07:49:33+00:00",
      "link": "https://arxiv.org/pdf/2602.14536v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "LLM fine-tuning datasets and token-level noise filtering",
      "llm_evidence_cn": "大语言模型微调数据集与标记级噪声过滤",
      "llm_evidence": "大语言模型微调数据集与标记级噪声过滤",
      "llm_tldr_en": "Introduces XTF, a framework to improve LLM fine-tuning by filtering token-level noise from datasets.",
      "llm_tldr_cn": "提出 XTF 框架，通过过滤数据集中的标记级噪声来优化大模型的微调效果。",
      "llm_tldr": "提出 XTF 框架，通过过滤数据集中的标记级噪声来优化大模型的微调效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.14536v1",
      "carry_days": 1
    },
    {
      "id": "2601.14603v1",
      "title": "Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum",
      "abstract": "Large Language Models (LLMs) achieve competitive performance across diverse natural language processing (NLP) tasks, yet pretraining is computationally demanding, making optimizer efficiency an important practical consideration. Muon accelerates LLM pretraining via orthogonal momentum updates that serve as a matrix analogue of the element-wise sign operator. Motivated by the recent perspective that Adam is a variance-adaptive sign update algorithm, we propose two variants of Muon, Muon-NSR and Muon-VS, which apply variance-adaptive normalization to momentum before orthogonalization. Muon-NSR applies noise-to-signal ratio (NSR) modulation, while Muon-VS performs variance-based scaling without introducing additional hyperparameters. Experiments on GPT-2 and LLaMA pretraining demonstrate that our proposed methods accelerate convergence and consistently achieve lower validation loss than both competitive, well-tuned AdamW and Muon baselines. For example, on the LLaMA-1.2B model, Muon-NSR and Muon-VS reduce the iterations required to reach the target validation loss by $1.36\\times$ relative to the well-tuned Muon following the recent benchmark.",
      "authors": [
        "Jingru Li",
        "Yibo Fan",
        "Huan Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-21T02:41:56+00:00",
      "link": "https://arxiv.org/pdf/2601.14603v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Accelerating LLM pretraining with optimizers",
      "llm_evidence_cn": "利用优化器加速大模型预训练",
      "llm_evidence": "利用优化器加速大模型预训练",
      "llm_tldr_en": "Proposes variance-adaptive variants of the Muon optimizer to improve the efficiency of LLM pretraining.",
      "llm_tldr_cn": "提出Muon优化器的方差自适应变体，以提高大模型预训练的效率。",
      "llm_tldr": "提出Muon优化器的方差自适应变体，以提高大模型预训练的效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2601.14603v1",
      "carry_days": 1
    },
    {
      "id": "2602.01734v1",
      "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration",
      "abstract": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.",
      "authors": [
        "Lianhai Ren",
        "Yucheng Ding",
        "Xiao Liu",
        "Qianxiao Li",
        "Peng Cheng",
        "Yeyun Gong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-02T07:18:45+00:00",
      "link": "https://arxiv.org/pdf/2602.01734v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Optimizer for stable LLM pretraining",
      "llm_evidence_cn": "用于稳定大语言模型预训练的优化器",
      "llm_evidence": "用于稳定大语言模型预训练的优化器",
      "llm_tldr_en": "Introduces MSign optimizer to prevent gradient explosions and instability during LLM pretraining.",
      "llm_tldr_cn": "提出 MSign 优化器，通过稳定秩恢复解决大模型预训练中的梯度爆炸问题。",
      "llm_tldr": "提出 MSign 优化器，通过稳定秩恢复解决大模型预训练中的梯度爆炸问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.01734v1",
      "carry_days": 1
    },
    {
      "id": "2602.02987v1",
      "title": "Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control",
      "abstract": "Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \\emph{prefill} phase that processes user input, followed by a memory-bound \\emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.",
      "authors": [
        "Ruihan Lin",
        "Zezhen Ding",
        "Zean Han",
        "Jiheng Zhang"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "math.OC"
      ],
      "published": "2026-02-03T01:47:37+00:00",
      "link": "https://arxiv.org/pdf/2602.02987v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Inference optimization and prefill-decode contention",
      "llm_evidence_cn": "推理优化与预填充-解码竞争",
      "llm_evidence": "推理优化与预填充-解码竞争",
      "llm_tldr_en": "Develops a control framework to manage prefill-decode contention in large-scale LLM inference.",
      "llm_tldr_cn": "开发了一个控制框架，用于管理大规模大模型推理中的预填充-解码竞争。",
      "llm_tldr": "开发了一个控制框架，用于管理大规模大模型推理中的预填充-解码竞争。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.02987v1",
      "carry_days": 1
    },
    {
      "id": "2602.01410v1",
      "title": "SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training",
      "abstract": "Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.",
      "authors": [
        "Yunjie Pan",
        "Yongyi Yang",
        "Hanmei Yang",
        "Scott Mahlke"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "published": "2026-02-01T19:34:27+00:00",
      "link": "https://arxiv.org/pdf/2602.01410v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "adaptive mixed precision for LLM pre-training",
      "llm_evidence_cn": "用于LLM预训练的自适应混合精度",
      "llm_evidence": "用于LLM预训练的自适应混合精度",
      "llm_tldr_en": "Introduces SNIP, a framework for stable and efficient subbyte precision LLM training.",
      "llm_tldr_cn": "引入SNIP框架，支持在LLM预训练中使用稳定的子字节混合精度训练。",
      "llm_tldr": "引入SNIP框架，支持在LLM预训练中使用稳定的子字节混合精度训练。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2602.01410v1",
      "carry_days": 1
    },
    {
      "id": "2602.17697v1",
      "title": "Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters",
      "abstract": "Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.",
      "authors": [
        "Nada Zine",
        "Clément Quinton",
        "Romain Rouvoy"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.SE"
      ],
      "published": "2026-02-06T16:18:22+00:00",
      "link": "https://arxiv.org/pdf/2602.17697v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "tuning inference hyperparameters for efficiency",
      "llm_evidence_cn": "调整推理超参数以提高效率",
      "llm_evidence": "调整推理超参数以提高效率",
      "llm_tldr_en": "Applies variability modeling to optimize LLM inference server configurations for energy and performance.",
      "llm_tldr_cn": "利用可变性建模技术系统地分析和优化LLM推理服务器的超参数配置。",
      "llm_tldr": "利用可变性建模技术系统地分析和优化LLM推理服务器的超参数配置。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.17697v1",
      "carry_days": 1
    },
    {
      "id": "2601.07892v1",
      "title": "Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification",
      "abstract": "The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .",
      "authors": [
        "Hong Huang",
        "Decheng Wu",
        "Qiangqiang Hu",
        "Guanghua Yu",
        "Jinhai Yang",
        "Jianchen Zhu",
        "Xue Liu",
        "Dapeng Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-12T08:49:34+00:00",
      "link": "https://arxiv.org/pdf/2601.07892v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Hardware-efficient quantization for LLM deployment",
      "llm_evidence_cn": "针对大模型部署的硬件高效量化",
      "llm_evidence": "针对大模型部署的硬件高效量化",
      "llm_tldr_en": "Proposes a 1.25-bit ternary quantization method to enable efficient LLM deployment on edge devices.",
      "llm_tldr_cn": "提出一种 1.25-bit 三值量化方法，以实现大语言模型在边缘设备上的高效部署。",
      "llm_tldr": "提出一种 1.25-bit 三值量化方法，以实现大语言模型在边缘设备上的高效部署。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.07892v1",
      "carry_days": 1
    },
    {
      "id": "2601.15710v1",
      "title": "FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design",
      "abstract": "We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.",
      "authors": [
        "Jiahao Zhang",
        "Zifan He",
        "Nicholas Fraser",
        "Michaela Blott",
        "Yizhou Sun",
        "Jason Cong"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22T07:31:51+00:00",
      "link": "https://arxiv.org/pdf/2601.15710v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "flexible hybrid LLM accelerator for inference optimization",
      "llm_evidence_cn": "用于推理优化的灵活混合LLM加速器",
      "llm_evidence": "用于推理优化的灵活混合LLM加速器",
      "llm_tldr_en": "Develops FlexLLM, an HLS library for rapid design of domain-specific, efficient LLM inference accelerators.",
      "llm_tldr_cn": "开发FlexLLM库，支持快速设计针对LLM推理优化的领域特定硬件加速器。",
      "llm_tldr": "开发FlexLLM库，支持快速设计针对LLM推理优化的领域特定硬件加速器。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.15710v1",
      "carry_days": 1
    },
    {
      "id": "2601.22297v1",
      "title": "Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning",
      "abstract": "The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.",
      "authors": [
        "Chenxi Liu",
        "Yanshuo Chen",
        "Ruibo Chen",
        "Tianyi Xiong",
        "Tong Zheng",
        "Heng Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-29T20:21:44+00:00",
      "link": "https://arxiv.org/pdf/2601.22297v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Reinforcement learning for reasoning and multi-agent alignment",
      "llm_evidence_cn": "用于推理和多智能体对齐的强化学习",
      "llm_evidence": "用于推理和多智能体对齐的强化学习",
      "llm_tldr_en": "Proposes Self-Debate RL to improve LLM reasoning and collaborative problem-solving through multi-agent debate.",
      "llm_tldr_cn": "提出自博弈强化学习框架，通过多智能体辩论提升大模型的推理与协作能力。",
      "llm_tldr": "提出自博弈强化学习框架，通过多智能体辩论提升大模型的推理与协作能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2601.22297v1",
      "carry_days": 1
    },
    {
      "id": "2601.18195v1",
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.",
      "authors": [
        "Linhan Cao",
        "Wei Sun",
        "Weixia Zhang",
        "Xiangyang Zhu",
        "Kaiwei Zhang",
        "Jun Jia",
        "Dandan Zhu",
        "Guangtao Zhai",
        "Xiongkuo Min"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-26T06:27:03+00:00",
      "link": "https://arxiv.org/pdf/2601.18195v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "training-free Retrieval-Augmented Generation (RAG) framework",
      "llm_evidence_cn": "无需训练的检索增强生成 (RAG) 框架",
      "llm_evidence": "无需训练的检索增强生成 (RAG) 框架",
      "llm_tldr_en": "Introduces QualiRAG, a training-free RAG framework for visual quality assessment using multimodal models.",
      "llm_tldr_cn": "引入 QualiRAG，一种利用多模态模型进行视觉质量评估的无需训练的 RAG 框架。",
      "llm_tldr": "引入 QualiRAG，一种利用多模态模型进行视觉质量评估的无需训练的 RAG 框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.18195v1",
      "carry_days": 1
    },
    {
      "id": "2601.16462v1",
      "title": "Graph-Anchored Knowledge Indexing for Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a dominant paradigm for mitigating hallucinations in Large Language Models (LLMs) by incorporating external knowledge. Nevertheless, effectively integrating and interpreting key evidence scattered across noisy documents remains a critical challenge for existing RAG systems. In this paper, we propose GraphAnchor, a novel Graph-Anchored Knowledge Indexing approach that reconceptualizes graph structures from static knowledge representations into active, evolving knowledge indices. GraphAnchor incrementally updates a graph during iterative retrieval to anchor salient entities and relations, yielding a structured index that guides the LLM in evaluating knowledge sufficiency and formulating subsequent subqueries. The final answer is generated by jointly leveraging all retrieved documents and the final evolved graph. Experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of GraphAnchor, and reveal that GraphAnchor modulates the LLM's attention to more effectively associate key information distributed in retrieved documents. All code and data are available at https://github.com/NEUIR/GraphAnchor.",
      "authors": [
        "Zhenghao Liu",
        "Mingyan Wu",
        "Xinze Li",
        "Yukun Yan",
        "Shuo Wang",
        "Cheng Yang",
        "Minghe Yu",
        "Zheni Zeng",
        "Maosong Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-23T05:41:05+00:00",
      "link": "https://arxiv.org/pdf/2601.16462v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "graph-anchored indexing for RAG hallucination mitigation",
      "llm_evidence_cn": "用于减轻RAG幻觉的图锚定索引",
      "llm_evidence": "用于减轻RAG幻觉的图锚定索引",
      "llm_tldr_en": "Proposes GraphAnchor to improve knowledge integration and sufficiency evaluation in RAG systems.",
      "llm_tldr_cn": "提出GraphAnchor方法，将图结构转化为动态索引以增强RAG系统的知识整合能力。",
      "llm_tldr": "提出GraphAnchor方法，将图结构转化为动态索引以增强RAG系统的知识整合能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.16462v1",
      "carry_days": 1
    },
    {
      "id": "2601.18116v1",
      "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning",
      "abstract": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.   We present \\textbf{FABLE}, a \\textbf{F}orest-based \\textbf{A}daptive \\textbf{B}i-path \\textbf{L}LM-\\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.   Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.",
      "authors": [
        "Lin Sun",
        "Linglin Zhang",
        "Jingang Huang",
        "Change Jia",
        "Zhengwei Cheng",
        "Xiangzheng Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-26T04:00:56+00:00",
      "link": "https://arxiv.org/pdf/2601.18116v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "RAG framework for multi-document reasoning",
      "llm_evidence_cn": "用于多文档推理的RAG框架",
      "llm_evidence": "用于多文档推理的RAG框架",
      "llm_tldr_en": "Introduces FABLE, a forest-based adaptive retrieval framework to improve multi-document synthesis in RAG.",
      "llm_tldr_cn": "引入FABLE，一种基于森林的自适应检索框架，旨在改进RAG中的多文档综合推理。",
      "llm_tldr": "引入FABLE，一种基于森林的自适应检索框架，旨在改进RAG中的多文档综合推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2601.18116v1",
      "carry_days": 1
    },
    {
      "id": "2602.03359v1",
      "title": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
      "abstract": "Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at https://github.com/ningding-o/MeKi.",
      "authors": [
        "Ning Ding",
        "Fangcheng Liu",
        "Kyungrae Kim",
        "Linji Hao",
        "Kyeng-Hun Lee",
        "Hyeonmok Ko",
        "Yehui Tang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-03T10:32:04+00:00",
      "link": "https://arxiv.org/pdf/2602.03359v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Efficient LLM scaling for edge device deployment and inference optimization",
      "llm_evidence_cn": "针对边缘设备部署和推理优化的LLM高效扩展",
      "llm_evidence": "针对边缘设备部署和推理优化的LLM高效扩展",
      "llm_tldr_en": "Proposes MeKi to scale LLM capacity via storage rather than FLOPs for efficient edge device deployment.",
      "llm_tldr_cn": "提出MeKi系统，通过存储空间而非计算量扩展LLM容量，实现高效的边缘设备部署。",
      "llm_tldr": "提出MeKi系统，通过存储空间而非计算量扩展LLM容量，实现高效的边缘设备部署。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.03359v1",
      "carry_days": 1
    },
    {
      "id": "2602.04620v1",
      "title": "QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning",
      "abstract": "GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.",
      "authors": [
        "Doyeon Lee",
        "Eunyi Lyou",
        "Hyunsoo Cho",
        "Sookyung Kim",
        "Joonseok Lee",
        "Jaemoo Choi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T14:51:04+00:00",
      "link": "https://arxiv.org/pdf/2602.04620v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM fine-tuning and alignment using trust region policy optimization",
      "llm_evidence_cn": "使用置信域策略优化进行大模型微调与对齐",
      "llm_evidence": "使用置信域策略优化进行大模型微调与对齐",
      "llm_tldr_en": "Introduces QUATRO, a principled RL fine-tuning algorithm for more stable and interpretable LLM alignment.",
      "llm_tldr_cn": "提出 QUATRO 算法，通过置信域约束实现更稳定、可解释的大模型强化学习微调。",
      "llm_tldr": "提出 QUATRO 算法，通过置信域约束实现更稳定、可解释的大模型强化学习微调。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.04620v1",
      "carry_days": 1
    },
    {
      "id": "2602.22812v1",
      "title": "Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching",
      "abstract": "Since local LLM inference on resource-constrained edge devices imposes a severe performance bottleneck, this paper proposes distributed prompt caching to enhance inference performance by cooperatively sharing intermediate processing states across multiple low-end edge devices. To fully utilize prompt similarity, our distributed caching mechanism also supports partial matching. As this approach introduces communication overhead associated with state sharing over a wireless network, we introduce a Bloom-filter-based data structure, referred to as a catalog, to determine whether a remote server possesses the desired internal states, thereby suppressing unnecessary communication. Experiments using the Gemma-3 270M model and the MMLU dataset on the Raspberry Pi Zero 2W platform demonstrate that the proposed approach reduces TTFT (Time to First Token) and TTLT (Time to Last Token) by 93.12% and 50.07% on average, respectively.",
      "authors": [
        "Hiroki Matsutani",
        "Naoki Matsuda",
        "Naoto Sugiura"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published": "2026-02-26T09:53:17+00:00",
      "link": "https://arxiv.org/pdf/2602.22812v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Efficient deployment and inference optimization via distributed prompt caching",
      "llm_evidence_cn": "通过分布式提示词缓存实现高效部署与推理优化",
      "llm_evidence": "通过分布式提示词缓存实现高效部署与推理优化",
      "llm_tldr_en": "Accelerates local LLM inference on edge devices using distributed prompt caching and Bloom filters.",
      "llm_tldr_cn": "利用分布式提示词缓存和布隆过滤器加速边缘设备上的本地LLM推理。",
      "llm_tldr": "利用分布式提示词缓存和布隆过滤器加速边缘设备上的本地LLM推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.22812v1",
      "carry_days": 1
    },
    {
      "id": "2602.06932v1",
      "title": "When RL Meets Adaptive Speculative Training: A Unified Training-Serving System",
      "abstract": "Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before deployment; (2) delayed utility feedback, since the true end-to-end decoding speedup is only known after training and cannot be inferred reliably from acceptance rate alone due to model-architecture and system-level overheads; and (3) domain-drift degradation, as the target model is repurposed to new domains and the speculator becomes stale and less effective.   To address these issues, we present Aurora, a unified training-serving system that closes the loop by continuously learning a speculator directly from live inference traces. Aurora reframes online speculator learning as an asynchronous reinforcement-learning problem: accepted tokens provide positive feedback, while rejected speculator proposals provide implicit negative feedback that we exploit to improve sample efficiency. Our design integrates an SGLang-based inference server with an asynchronous training server, enabling hot-swapped speculator updates without service interruption. Crucially, Aurora supports day-0 deployment: a speculator can be served immediately and rapidly adapted to live traffic, improving system performance while providing immediate utility feedback. Across experiments, Aurora achieves a 1.5x day-0 speedup on recently released frontier models (e.g., MiniMax M2.1 229B and Qwen3-Coder-Next 80B). Aurora also adapts effectively to distribution shifts in user traffic, delivering an additional 1.25x speedup over a well-trained but static speculator on widely used models (e.g., Qwen3 and Llama3).",
      "authors": [
        "Junxiong Wang",
        "Fengxiang Bie",
        "Jisen Li",
        "Zhongzhu Zhou",
        "Zelei Shao",
        "Yubo Wang",
        "Yinghui Liu",
        "Qingyang Wu",
        "Avner May",
        "Sri Yanamandra",
        "Yineng Zhang",
        "Ce Zhang",
        "Tri Dao",
        "Percy Liang",
        "Ben Athiwaratkun",
        "Shuaiwen Leon Song",
        "Chenfeng Xu",
        "Xiaoxia Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-06T18:28:54+00:00",
      "link": "https://arxiv.org/pdf/2602.06932v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Speculative decoding for LLM serving acceleration",
      "llm_evidence_cn": "用于大模型服务加速的投机采样解码",
      "llm_evidence": "用于大模型服务加速的投机采样解码",
      "llm_tldr_en": "Integrates RL with adaptive speculative training to reduce deployment lag and improve LLM decoding speed.",
      "llm_tldr_cn": "将强化学习与自适应投机训练结合，解决了大模型推理加速中的部署延迟和领域偏移问题。",
      "llm_tldr": "将强化学习与自适应投机训练结合，解决了大模型推理加速中的部署延迟和领域偏移问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.06932v1",
      "carry_days": 1
    },
    {
      "id": "2601.18350v3",
      "title": "When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs",
      "abstract": "Large language models can exhibit surprising adapter interference when combining domain adaptation and instruction alignment in safety-critical settings. We study a two-stage LoRA pipeline for medical LLMs, where domain-oriented pre-training (PT) and supervised fine-tuning (SFT) are trained separately and later merged through weighted adapter merging. We observe that introducing PT signal can systematically alter model behavior and produce reasoning-style outputs, even when evaluation templates explicitly attempt to suppress such behavior. This interference leads to a divergence between surface metrics and reasoning or alignment behavior: BLEU/ROUGE scores drop significantly, while multiple-choice accuracy improves. We further show that small pipeline mistakes can easily misattribute SFT-only behavior to merged models, and provide a lightweight merge-verification routine to ensure correctness and reproducibility. Our findings highlight an interaction between knowledge injection and instruction alignment in adapter-based fine-tuning, with important implications for safety-critical model deployment.",
      "authors": [
        "Junyi Zou"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-26T10:54:06+00:00",
      "link": "https://arxiv.org/pdf/2601.18350v3",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Adapter merging for domain pretraining and instruction alignment",
      "llm_evidence_cn": "领域预训练与指令对齐的适配器合并",
      "llm_evidence": "领域预训练与指令对齐的适配器合并",
      "llm_tldr_en": "Investigates how merging domain-specific LoRA adapters with instruction-tuned ones affects LLM behavior.",
      "llm_tldr_cn": "研究了在医疗LLM中合并领域预训练与指令微调LoRA适配器时的干扰问题。",
      "llm_tldr": "研究了在医疗LLM中合并领域预训练与指令微调LoRA适配器时的干扰问题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.18350v3",
      "carry_days": 1
    },
    {
      "id": "2602.11513v1",
      "title": "Differentially Private and Communication Efficient Large Language Model Split Inference via Stochastic Quantization and Soft Prompt",
      "abstract": "Large Language Models (LLMs) have achieved remarkable performance and received significant research interest. The enormous computational demands, however, hinder the local deployment on devices with limited resources. The current prevalent LLM inference paradigms require users to send queries to the service providers for processing, which raises critical privacy concerns. Existing approaches propose to allow the users to obfuscate the token embeddings before transmission and utilize local models for denoising. Nonetheless, transmitting the token embeddings and deploying local models may result in excessive communication and computation overhead, preventing practical implementation. In this work, we propose \\textbf{DEL}, a framework for \\textbf{D}ifferentially private and communication \\textbf{E}fficient \\textbf{L}LM split inference. More specifically, an embedding projection module and a differentially private stochastic quantization mechanism are proposed to reduce the communication overhead in a privacy-preserving manner. To eliminate the need for local models, we adapt soft prompt at the server side to compensate for the utility degradation caused by privacy. To the best of our knowledge, this is the first work that utilizes soft prompt to improve the trade-off between privacy and utility in LLM inference, and extensive experiments on text generation and natural language understanding benchmarks demonstrate the effectiveness of the proposed method.",
      "authors": [
        "Yujie Gu",
        "Richeng Jin",
        "Xiaoyu Ji",
        "Yier Jin",
        "Wenyuan Xu"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published": "2026-02-12T03:13:16+00:00",
      "link": "https://arxiv.org/pdf/2602.11513v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "efficient and private LLM split inference",
      "llm_evidence_cn": "高效且私密的LLM拆分推理",
      "llm_evidence": "高效且私密的LLM拆分推理",
      "llm_tldr_en": "Proposes a private and communication-efficient split inference framework for LLMs.",
      "llm_tldr_cn": "提出一种针对LLM的隐私保护且通信高效的拆分推理框架。",
      "llm_tldr": "提出一种针对LLM的隐私保护且通信高效的拆分推理框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.11513v1",
      "carry_days": 1
    },
    {
      "id": "2602.08387v1",
      "title": "Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research",
      "abstract": "Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.",
      "authors": [
        "Max Lübbering",
        "Timm Ruland",
        "Richard Rutmann",
        "Felix Stollenwerk",
        "David Fitzek",
        "Michael Fromm",
        "Alexander Weber",
        "Rafet Sifa",
        "Nicolas Flores-Herr",
        "Joachim Köhler",
        "Mehdi Ali"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published": "2026-02-09T08:39:41+00:00",
      "link": "https://arxiv.org/pdf/2602.08387v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Framework for large-scale LLM training and research",
      "llm_evidence_cn": "大规模大语言模型训练与研究框架",
      "llm_evidence": "大规模大语言模型训练与研究框架",
      "llm_tldr_en": "Introduces Modalities, a PyTorch-native framework for efficient LLM pretraining and ablations.",
      "llm_tldr_cn": "推出了 Modalities，一个用于高效大模型预训练和消融实验的框架。",
      "llm_tldr": "推出了 Modalities，一个用于高效大模型预训练和消融实验的框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Large Language Models research papers and surveys",
      "matched_requirement_id": "req-1",
      "paper_id": "2602.08387v1",
      "carry_days": 1
    },
    {
      "id": "2601.22735v1",
      "title": "MM-THEBench: Do Reasoning MLLMs Think Reasonably?",
      "abstract": "Recent advances in multimodal large language models (MLLMs) mark a shift from non-thinking models to post-trained reasoning models capable of solving complex problems through thinking. However, whether such thinking mitigates hallucinations in multimodal perception and reasoning remains unclear. Self-reflective reasoning enhances robustness but introduces additional hallucinations, and subtle perceptual errors still result in incorrect or coincidentally correct answers. Existing benchmarks primarily focus on models before the emergence of reasoning MLLMs, neglecting the internal thinking process and failing to measure the hallucinations that occur during thinking. To address these challenges, we introduce MM-THEBench, a comprehensive benchmark for assessing hallucinations of intermediate CoTs in reasoning MLLMs. MM-THEBench features a fine-grained taxonomy grounded in cognitive dimensions, diverse data with verified reasoning annotations, and a multi-level automated evaluation framework. Extensive experiments on mainstream reasoning MLLMs reveal insights into how thinking affects hallucination and reasoning capability in various multimodal tasks.",
      "authors": [
        "Zhidian Huang",
        "Zijun Yao",
        "Ji Qi",
        "Shangqing Tu",
        "Junxian Ma",
        "Jinxin Liu",
        "Weichuan Liu",
        "Xiaoyin Che",
        "Lei Hou",
        "Juanzi Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-30T09:17:50+00:00",
      "link": "https://arxiv.org/pdf/2601.22735v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Reasoning process and hallucinations in multimodal LLMs",
      "llm_evidence_cn": "多模态大模型中的推理过程与幻觉问题",
      "llm_evidence": "多模态大模型中的推理过程与幻觉问题",
      "llm_tldr_en": "Benchmarks internal thinking processes and hallucinations in reasoning-capable MLLMs.",
      "llm_tldr_cn": "提出了评估推理型多模态大模型内部思考过程和幻觉的新基准。",
      "llm_tldr": "提出了评估推理型多模态大模型内部思考过程和幻觉的新基准。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2601.22735v1",
      "carry_days": 1
    },
    {
      "id": "2601.07464v1",
      "title": "IFDNS: An Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning",
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across a wide range of reasoning tasks, including logical and mathematical problem-solving. While prompt-based methods like Chain-of-Thought (CoT) can enhance LLM reasoning abilities to some extent, they often suffer from a lack of faithfulness, where the derived conclusions may not align with the generated reasoning chain. To address this issue, researchers have explored neuro-symbolic approaches to bolster LLM logical reasoning capabilities. However, existing neuro-symbolic methods still face challenges with information loss during the process. To overcome these limitations, we introduce Iterative Feedback-Driven Neuro-Symbolic (IFDNS), a novel prompt-based method that employs a multi-round feedback mechanism to address LLM limitations in handling complex logical relationships. IFDNS utilizes iterative feedback during the logic extraction phase to accurately extract causal relationship statements and translate them into propositional and logical implication expressions, effectively mitigating information loss issues. Furthermore, IFDNS is orthogonal to existing prompt methods, allowing for seamless integration with various prompting approaches. Empirical evaluations across six datasets demonstrate the effectiveness of IFDNS in significantly improving the performance of CoT and Chain-of-Thought with Self-Consistency (CoT-SC). Specifically, IFDNS achieves a +9.40% accuracy boost for CoT on the LogiQA dataset and a +11.70% improvement for CoT-SC on the PrOntoQA dataset.",
      "authors": [
        "Xiaoheng Wang",
        "Tongxuan Liu",
        "Zi Gong",
        "Xianzhe Dong",
        "Yuting Zeng",
        "Minhan Hu",
        "Weizhe Huang",
        "Jing Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-12T12:20:19+00:00",
      "link": "https://arxiv.org/pdf/2601.07464v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Neuro-symbolic method for faithful logical reasoning in LLMs",
      "llm_evidence_cn": "用于LLM忠实逻辑推理的神经符号方法",
      "llm_evidence": "用于LLM忠实逻辑推理的神经符号方法",
      "llm_tldr_en": "Proposes IFDNS to improve the faithfulness of LLM reasoning chains compared to standard CoT prompting.",
      "llm_tldr_cn": "提出IFDNS方法，通过神经符号反馈增强LLM逻辑推理的忠实度。",
      "llm_tldr": "提出IFDNS方法，通过神经符号反馈增强LLM逻辑推理的忠实度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2601.07464v1",
      "carry_days": 1
    },
    {
      "id": "2602.10965v1",
      "title": "MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs",
      "abstract": "Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable LLMs. Although MoEs offer strong efficiency and capacity scaling, naively adapting dense-model editors is both computationally costly and prone to routing distribution shifts that undermine stability and consistency. To address these challenges, we introduce MoEEdit, the first routing-stable framework for parameter-modifying knowledge editing in MoE LLMs. Our method reparameterizes expert updates via per-expert null-space projections that keep router inputs invariant and thereby suppress routing shifts. The resulting block-structured optimization is solved efficiently with a block coordinate descent (BCD) solver. Experiments show that MoEEdit attains state-of-the-art efficacy and generalization while preserving high specificity and routing stability, with superior compute and memory efficiency. These results establish a robust foundation for scalable, precise knowledge editing in sparse LLMs and underscore the importance of routing-stable interventions.",
      "authors": [
        "Yupu Gu",
        "Rongzhe Wei",
        "Andy Zhu",
        "Pan Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11T15:56:30+00:00",
      "link": "https://arxiv.org/pdf/2602.10965v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Efficient knowledge editing and parameter-modifying for MoE LLMs",
      "llm_evidence_cn": "针对MoE架构LLM的高效知识编辑与参数修改",
      "llm_evidence": "针对MoE架构LLM的高效知识编辑与参数修改",
      "llm_tldr_en": "Introduces MoEEdit for stable and efficient parameter-modifying knowledge editing in MoE-based LLMs.",
      "llm_tldr_cn": "为MoE架构的LLM引入了MoEEdit，实现稳定且高效的参数修改知识编辑。",
      "llm_tldr": "为MoE架构的LLM引入了MoEEdit，实现稳定且高效的参数修改知识编辑。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.10965v1",
      "carry_days": 1
    },
    {
      "id": "2602.02855v1",
      "title": "When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models",
      "abstract": "Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.",
      "authors": [
        "Gibbs Nwemadji",
        "Bruno Loureiro",
        "Jean Barbier"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "math.ST"
      ],
      "published": "2026-02-02T22:02:52+00:00",
      "link": "https://arxiv.org/pdf/2602.02855v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Dynamical analysis of LoRA fine-tuning and its relationship with pre-training",
      "llm_evidence_cn": "LoRA微调的动力学分析及其与预训练的关系",
      "llm_evidence": "LoRA微调的动力学分析及其与预训练的关系",
      "llm_tldr_en": "Analyzes how excessive pre-training can computationally slow down LoRA fine-tuning optimization.",
      "llm_tldr_cn": "分析了过度预训练如何从计算上减慢LoRA微调的优化速度。",
      "llm_tldr": "分析了过度预训练如何从计算上减慢LoRA微调的优化速度。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.02855v1",
      "carry_days": 1
    },
    {
      "id": "2602.20770v1",
      "title": "Pipeline for Verifying LLM-Generated Mathematical Solutions",
      "abstract": "With the growing popularity of Large Reasoning Models and their results in solving mathematical problems, it becomes crucial to measure their capabilities. We introduce a pipeline for both automatic and interactive verification as a more accurate alternative to only checking the answer which is currently the most popular approach for benchmarks. The pipeline can also be used as a generator of correct solutions both in formal and informal languages. 3 AI agents, which can be chosen for the benchmark accordingly, are included in the structure. The key idea is the use of prompts to obtain the solution in the specific form which allows for easier verification using proof assistants and possible use of small models ($\\le 8B$). Experiments on several datasets suggest low probability of False Positives. The open-source implementation with instructions on setting up a server is available at https://github.com/LogicEnj/lean4_verification_pipeline.",
      "authors": [
        "Varvara Sazonova",
        "Dmitri Shmelkin",
        "Stanislav Kikot",
        "Vasily Motolygin"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-24T11:01:25+00:00",
      "link": "https://arxiv.org/pdf/2602.20770v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Verifying LLM-generated mathematical solutions and reasoning",
      "llm_evidence_cn": "验证LLM生成的数学解法与推理",
      "llm_evidence": "验证LLM生成的数学解法与推理",
      "llm_tldr_en": "Introduces a pipeline for verifying mathematical reasoning in large models using AI agents and proof assistants.",
      "llm_tldr_cn": "介绍了一种利用AI智能体和证明助手验证大模型数学推理能力的流水线。",
      "llm_tldr": "介绍了一种利用AI智能体和证明助手验证大模型数学推理能力的流水线。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.20770v1",
      "carry_days": 1
    },
    {
      "id": "2601.09981v2",
      "title": "DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models",
      "abstract": "Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to mitigate overthinking and the associated attention dispersion. Extensive experiments conducted on 3B and 7B variants of Qwen2.5-VL, as well as on both SAM2 and SAM3, demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation accuracy.",
      "authors": [
        "Yulin He",
        "Wei Chen",
        "Zhikang Jian",
        "Tianhang Guo",
        "Wenjuan Zhou",
        "Minglong Li",
        "Shaowu Yang",
        "Wenjing Yang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-15T01:48:45+00:00",
      "link": "https://arxiv.org/pdf/2601.09981v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Improving reasoning efficiency and accuracy in MLLMs",
      "llm_evidence_cn": "提高多模态大模型的推理效率与准确性",
      "llm_evidence": "提高多模态大模型的推理效率与准确性",
      "llm_tldr_en": "A two-stage rollout framework to decompose reasoning and segmentation for better MLLM performance.",
      "llm_tldr_cn": "一种通过分解推理与分割任务来提升多模态大模型推理效率和准确性的框架。",
      "llm_tldr": "一种通过分解推理与分割任务来提升多模态大模型推理效率和准确性的框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.09981v2",
      "carry_days": 1
    },
    {
      "id": "2601.13892v1",
      "title": "Multi-Objective Hierarchical Optimization with Large Language Models",
      "abstract": "Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.",
      "authors": [
        "Andrej Schwanke",
        "Lyubomir Ivanov",
        "David Salinas",
        "Frank Hutter",
        "Arber Zela"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-20T12:10:13+00:00",
      "link": "https://arxiv.org/pdf/2601.13892v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "LLMs for multi-objective optimization and reasoning",
      "llm_evidence_cn": "利用LLM进行多目标优化与推理",
      "llm_evidence": "利用LLM进行多目标优化与推理",
      "llm_tldr_en": "Uses LLMs as surrogate models and samplers in a hierarchical search strategy for complex optimization tasks.",
      "llm_tldr_cn": "在分层搜索策略中利用LLM作为代理模型和采样器，处理复杂的优化任务。",
      "llm_tldr": "在分层搜索策略中利用LLM作为代理模型和采样器，处理复杂的优化任务。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.13892v1",
      "carry_days": 1
    },
    {
      "id": "2601.18554v1",
      "title": "Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities",
      "abstract": "Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.",
      "authors": [
        "Alberto Purpura",
        "Li Wang",
        "Sahil Badyal",
        "Eugenio Beaufrand",
        "Adam Faulkner"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-26T15:02:15+00:00",
      "link": "https://arxiv.org/pdf/2601.18554v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "benchmark for evaluating instruction compliance in LLMs",
      "llm_evidence_cn": "评估LLM指令遵循能力的基准测试",
      "llm_evidence": "评估LLM指令遵循能力的基准测试",
      "llm_tldr_en": "Introduces MOSAIC to evaluate how well LLMs follow complex, multi-constraint instructions.",
      "llm_tldr_cn": "引入MOSAIC框架，用于细粒度评估LLM对复杂指令的遵循能力。",
      "llm_tldr": "引入MOSAIC框架，用于细粒度评估LLM对复杂指令的遵循能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.18554v1",
      "carry_days": 1
    },
    {
      "id": "2602.04634v1",
      "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
      "abstract": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
      "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Ruize Zhang",
        "Chunyang Zhu",
        "Shi Yu",
        "Weilin Liu",
        "Quanlu Zhang",
        "Wenbo Ding",
        "Chao Yu",
        "Yu Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2026-02-04T15:05:12+00:00",
      "link": "https://arxiv.org/pdf/2602.04634v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Multi-agent reinforcement learning for scaling LLM information seeking",
      "llm_evidence_cn": "用于扩展LLM信息寻求的多智能体强化学习",
      "llm_evidence": "用于扩展LLM信息寻求的多智能体强化学习",
      "llm_tldr_en": "Explores width scaling via multi-agent reinforcement learning to improve broad information seeking in LLM systems.",
      "llm_tldr_cn": "通过多智能体强化学习探索宽度扩展，以提高LLM系统在广泛信息寻求中的表现。",
      "llm_tldr": "通过多智能体强化学习探索宽度扩展，以提高LLM系统在广泛信息寻求中的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2602.04634v1",
      "carry_days": 1
    },
    {
      "id": "2601.09285v1",
      "title": "Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction",
      "abstract": "Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.",
      "authors": [
        "Mianzhi Pan",
        "JianFei Li",
        "Peishuo Liu",
        "Botian Wang",
        "Yawen Ouyang",
        "Yiming Rong",
        "Hao Zhou",
        "Jianbing Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-01-14T08:45:07+00:00",
      "link": "https://arxiv.org/pdf/2601.09285v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Spatial-aware continual pre-training for LLMs",
      "llm_evidence_cn": "大语言模型的空间感知持续预训练",
      "llm_evidence": "大语言模型的空间感知持续预训练",
      "llm_tldr_en": "Adapts LLMs for MOF structure prediction using spatial-aware continual pre-training and modular assembly.",
      "llm_tldr_cn": "通过空间感知持续预训练，将大模型应用于金属有机框架（MOF）的结构预测。",
      "llm_tldr": "通过空间感知持续预训练，将大模型应用于金属有机框架（MOF）的结构预测。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2601.09285v1",
      "carry_days": 1
    },
    {
      "id": "2602.19058v1",
      "title": "Do LLMs and VLMs Share Neurons for Inference? Evidence and Mechanisms of Cross-Modal Transfer",
      "abstract": "Large vision-language models (LVLMs) have rapidly advanced across various domains, yet they still lag behind strong text-only large language models (LLMs) on tasks that require multi-step inference and compositional decision-making. Motivated by their shared transformer architectures, we investigate whether the two model families rely on common internal computation for such inference. At the neuron level, we uncover a surprisingly large overlap: more than half of the top-activated units during multi-step inference are shared between representative LLMs and LVLMs, revealing a modality-invariant inference subspace.   Through causal probing via activation amplification, we further show that these shared neurons encode consistent and interpretable concept-level effects, demonstrating their functional contribution to inference. Building on this insight, we propose Shared Neuron Low-Rank Fusion (SNRF), a parameter-efficient framework that transfers mature inference circuitry from LLMs to LVLMs. SNRF profiles cross-model activations to identify shared neurons, computes a low-rank approximation of inter-model weight differences, and injects these updates selectively within the shared-neuron subspace. This mechanism strengthens multimodal inference performance with minimal parameter changes and requires no large-scale multimodal fine-tuning.   Across diverse mathematics and perception benchmarks, SNRF consistently enhances LVLM inference performance while preserving perceptual capabilities. Our results demonstrate that shared neurons form an interpretable bridge between LLMs and LVLMs, enabling low-cost transfer of inference ability into multimodal models. Our code is available at [https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons](https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons).",
      "authors": [
        "Chenhang Cui",
        "An Zhang",
        "Yuxin Chen",
        "Gelei Deng",
        "Jingnan Zheng",
        "Zhenkai Liang",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-22T06:04:05+00:00",
      "link": "https://arxiv.org/pdf/2602.19058v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Shared neurons for inference in LLMs and VLMs",
      "llm_evidence_cn": "LLM与VLM在推理中的共享神经元",
      "llm_evidence": "LLM与VLM在推理中的共享神经元",
      "llm_tldr_en": "Discovers shared internal computation mechanisms for multi-step inference across different model modalities.",
      "llm_tldr_cn": "发现了跨不同模态模型进行多步推理时共享的内部计算机制。",
      "llm_tldr": "发现了跨不同模态模型进行多步推理时共享的内部计算机制。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.19058v1",
      "carry_days": 1
    },
    {
      "id": "2601.22623v1",
      "title": "SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly",
      "abstract": "Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.",
      "authors": [
        "Wei Zhu",
        "Zhiwen Tang",
        "Kun Yue"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-01-30T06:26:34+00:00",
      "link": "https://arxiv.org/pdf/2601.22623v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Multi-agent planning for complex problem-solving tasks",
      "llm_evidence_cn": "用于复杂问题解决任务的多智能体规划",
      "llm_evidence": "用于复杂问题解决任务的多智能体规划",
      "llm_tldr_en": "Proposes SYMPHONY, a multi-agent framework to enhance exploration and planning in LLM-based agents.",
      "llm_tldr_cn": "提出 SYMPHONY，一个旨在增强基于 LLM 的智能体探索和规划能力的多智能体框架。",
      "llm_tldr": "提出 SYMPHONY，一个旨在增强基于 LLM 的智能体探索和规划能力的多智能体框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.22623v1",
      "carry_days": 1
    },
    {
      "id": "2602.11731v1",
      "title": "Thinking with Drafting: Optical Decompression via Logical Reconstruction",
      "abstract": "Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.",
      "authors": [
        "Jingxuan Wei",
        "Honghao He",
        "Caijun Jia",
        "Siyuan Li",
        "Zheng Sun",
        "Yuhang Xu",
        "Yuanyuan Lin",
        "Linzhuang Sun",
        "Yuchen Wu",
        "Bihui Yu",
        "Xiangxiang Zhang",
        "Cheng Tan"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12T08:54:02+00:00",
      "link": "https://arxiv.org/pdf/2602.11731v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Reasoning over visual inputs via logical reconstruction in MLLMs",
      "llm_evidence_cn": "通过多模态大模型中的逻辑重构进行视觉输入推理",
      "llm_evidence": "通过多模态大模型中的逻辑重构进行视觉输入推理",
      "llm_tldr_en": "Proposes Thinking with Drafting to improve complex reasoning in multimodal LLMs using a Domain-Specific Language.",
      "llm_tldr_cn": "提出 Thinking with Drafting，利用领域特定语言提升多模态 LLM 的复杂推理能力。",
      "llm_tldr": "提出 Thinking with Drafting，利用领域特定语言提升多模态 LLM 的复杂推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.11731v1",
      "carry_days": 1
    },
    {
      "id": "2602.17680v1",
      "title": "BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs",
      "abstract": "Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.",
      "authors": [
        "Yujia Wang",
        "Jihong Guan",
        "Wengen Li",
        "Shuigeng Zhou",
        "Xuhong Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T13:24:49+00:00",
      "link": "https://arxiv.org/pdf/2602.17680v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Domain-adaptive continual pretraining and biological reasoning with LLMs",
      "llm_evidence_cn": "领域自适应持续预训练与LLM生物推理",
      "llm_evidence": "领域自适应持续预训练与LLM生物推理",
      "llm_tldr_en": "Proposes BioBridge, a framework for continual pretraining to enhance LLMs' biological reasoning and protein understanding.",
      "llm_tldr_cn": "提出BioBridge框架，通过持续预训练增强LLM的生物推理和蛋白质理解能力。",
      "llm_tldr": "提出BioBridge框架，通过持续预训练增强LLM的生物推理和蛋白质理解能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.17680v1",
      "carry_days": 1
    },
    {
      "id": "2602.04634v2",
      "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
      "abstract": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
      "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Ruize Zhang",
        "Chunyang Zhu",
        "Shi Yu",
        "Weilin Liu",
        "Quanlu Zhang",
        "Wenbo Ding",
        "Chao Yu",
        "Yu Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2026-02-04T15:05:12+00:00",
      "link": "https://arxiv.org/pdf/2602.04634v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Multi-agent reinforcement learning for LLM reasoning",
      "llm_evidence_cn": "用于大语言模型推理的多智能体强化学习",
      "llm_evidence": "用于大语言模型推理的多智能体强化学习",
      "llm_tldr_en": "Proposes WideSeek-R1, a multi-agent RL framework to scale LLM information seeking capabilities.",
      "llm_tldr_cn": "提出WideSeek-R1，一种通过多智能体强化学习扩展大模型信息检索能力的任务框架。",
      "llm_tldr": "提出WideSeek-R1，一种通过多智能体强化学习扩展大模型信息检索能力的任务框架。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Large Language Models research papers and surveys",
      "matched_requirement_id": "req-1",
      "paper_id": "2602.04634v2",
      "carry_days": 1
    },
    {
      "id": "2601.10131v2",
      "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
      "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \\textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
      "authors": [
        "Yizhan Li",
        "Florence Cloutier",
        "Sifan Wu",
        "Ali Parviz",
        "Boris Knyazev",
        "Yan Zhang",
        "Glen Berseth",
        "Bang Liu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-01-15T07:18:05+00:00",
      "link": "https://arxiv.org/pdf/2601.10131v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Retrieval-augmented framework for LLM generation",
      "llm_evidence_cn": "大模型生成的检索增强框架",
      "llm_evidence": "大模型生成的检索增强框架",
      "llm_tldr_en": "Introduces a retrieval-augmented, multi-agent framework for constrained molecular generation using LLMs.",
      "llm_tldr_cn": "引入了一种检索增强的多智能体框架，用于大模型约束下的分子生成。",
      "llm_tldr": "引入了一种检索增强的多智能体框架，用于大模型约束下的分子生成。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.10131v2",
      "carry_days": 1
    },
    {
      "id": "2602.01956v1",
      "title": "Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation",
      "abstract": "Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.",
      "authors": [
        "Seonghyeon Park",
        "Jewon Yeom",
        "Jaewon Sok",
        "Jeongjae Park",
        "Heejun Kim",
        "Taesup Kim"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02T11:03:37+00:00",
      "link": "https://arxiv.org/pdf/2602.01956v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Efficient uncertainty estimation for LLM deployment",
      "llm_evidence_cn": "大模型部署中的高效不确定性估计",
      "llm_evidence": "大模型部署中的高效不确定性估计",
      "llm_tldr_en": "Uses knowledge distillation and draft models to efficiently estimate epistemic uncertainty in LLMs.",
      "llm_tldr_cn": "利用知识蒸馏和草稿模型高效估计大模型的不确定性，以减少幻觉并支持安全部署。",
      "llm_tldr": "利用知识蒸馏和草稿模型高效估计大模型的不确定性，以减少幻觉并支持安全部署。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.01956v1",
      "carry_days": 1
    },
    {
      "id": "2602.19938v1",
      "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
      "abstract": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.   We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.   We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference.",
      "authors": [
        "Zijie Liu",
        "Jie Peng",
        "Jinhao Duan",
        "Zirui Liu",
        "Kaixiong Zhou",
        "Mingfu Liang",
        "Luke Simon",
        "Xi Liu",
        "Zhaozhuo Xu",
        "Tianlong Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-23T15:11:16+00:00",
      "link": "https://arxiv.org/pdf/2602.19938v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "inference-time behavior and load balancing for SMoE LLMs",
      "llm_evidence_cn": "SMoE大语言模型的推理时行为与负载均衡",
      "llm_evidence": "SMoE大语言模型的推理时行为与负载均衡",
      "llm_tldr_en": "Proposes a strategy to balance expert utilization in Sparse Mixture-of-Experts LLMs during inference.",
      "llm_tldr_cn": "提出了一种在推理过程中平衡稀疏混合专家大语言模型专家利用率的策略。",
      "llm_tldr": "提出了一种在推理过程中平衡稀疏混合专家大语言模型专家利用率的策略。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.19938v1",
      "carry_days": 1
    },
    {
      "id": "2602.10138v1",
      "title": "Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement",
      "abstract": "Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragmented and lacks systematic organization. This survey provides a comprehensive roadmap of this nascent frontier by structuring the domain's core components. We begin by analyzing the fundamental challenges of fusing visual and linguistic information in charts. We then categorize downstream tasks and datasets, introducing a novel taxonomy of canonical and non-canonical benchmarks to highlight the field's expanding scope. Subsequently, we present a comprehensive evolution of methodologies, tracing the progression from classic deep learning techniques to state-of-the-art MLLM paradigms that leverage sophisticated fusion strategies. By critically examining the limitations of current models, particularly their perceptual and reasoning deficits, we identify promising future directions, including advanced alignment techniques and reinforcement learning for cognitive enhancement. This survey aims to equip researchers and practitioners with a structured understanding of how MLLMs are transforming chart information fusion and to catalyze progress toward more robust and reliable systems.",
      "authors": [
        "Zhihang Yi",
        "Jian Zhao",
        "Jiancheng Lv",
        "Tao Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-08T12:59:50+00:00",
      "link": "https://arxiv.org/pdf/2602.10138v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Survey and taxonomy of Multimodal Large Language Models (MLLMs)",
      "llm_evidence_cn": "多模态大语言模型（MLLM）的综述与分类",
      "llm_evidence": "多模态大语言模型（MLLM）的综述与分类",
      "llm_tldr_en": "Provides a comprehensive survey and taxonomy of Multimodal LLMs specifically for chart understanding and fusion tasks.",
      "llm_tldr_cn": "针对图表理解和融合任务，提供了多模态大语言模型的全面综述和分类。",
      "llm_tldr": "针对图表理解和融合任务，提供了多模态大语言模型的全面综述和分类。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "State-of-the-art large language models survey and taxonomy",
      "matched_requirement_id": "req-7",
      "paper_id": "2602.10138v1",
      "carry_days": 1
    },
    {
      "id": "2602.02465v1",
      "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
      "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
      "authors": [
        "Jana Zeller",
        "Thaddäus Wiedemer",
        "Fanfei Li",
        "Thomas Klein",
        "Prasanna Mayilvahanan",
        "Matthias Bethge",
        "Felix Wichmann",
        "Ryan Cotterell",
        "Wieland Brendel"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-02T18:49:06+00:00",
      "link": "https://arxiv.org/pdf/2602.02465v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Evaluating reasoning and mental imagery in frontier LLMs",
      "llm_evidence_cn": "评估前沿LLM的推理与心理表象能力",
      "llm_evidence": "评估前沿LLM的推理与心理表象能力",
      "llm_tldr_en": "Probes the limits of multi-step reasoning in multimodal LLMs using a new suite called MentisOculi.",
      "llm_tldr_cn": "通过MentisOculi测试集评估多模态大模型在复杂推理中的心理表象能力。",
      "llm_tldr": "通过MentisOculi测试集评估多模态大模型在复杂推理中的心理表象能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.02465v1",
      "carry_days": 1
    },
    {
      "id": "2601.13384v1",
      "title": "From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning",
      "abstract": "The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.",
      "authors": [
        "Jiajun Zhang",
        "Zeyu Cui",
        "Jiaxi Yang",
        "Lei Zhang",
        "Yuheng Jing",
        "Zeyao Ma",
        "Tianyi Bai",
        "Zilei Wang",
        "Qiang Liu",
        "Liang Wang",
        "Binyuan Hui",
        "Junyang Lin"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "published": "2026-01-19T20:33:53+00:00",
      "link": "https://arxiv.org/pdf/2601.13384v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Search-and-replace instruction tuning for code infilling",
      "llm_evidence_cn": "用于代码填充的搜索与替换指令微调",
      "llm_evidence": "用于代码填充的搜索与替换指令微调",
      "llm_tldr_en": "Develops SRI, a framework that uses instruction tuning to improve context-aware code editing in LLMs.",
      "llm_tldr_cn": "开发SRI框架，通过指令微调将代码填充扩展为具备上下文感知能力的编辑任务。",
      "llm_tldr": "开发SRI框架，通过指令微调将代码填充扩展为具备上下文感知能力的编辑任务。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.13384v1",
      "carry_days": 1
    },
    {
      "id": "2602.16100v1",
      "title": "LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum",
      "abstract": "With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).",
      "authors": [
        "Zijie Su",
        "Muhammed Tawfiqul Islam",
        "Mohammad Goudarzi",
        "Adel N. Toosi"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-18T00:09:09+00:00",
      "link": "https://arxiv.org/pdf/2602.16100v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "efficiently serving LLM inference under limited resources",
      "llm_evidence_cn": "在有限资源下高效进行LLM推理服务",
      "llm_evidence": "在有限资源下高效进行LLM推理服务",
      "llm_tldr_en": "Explores serverless paradigms and online reconfiguration for efficient LLM deployment and inference.",
      "llm_tldr_cn": "探索无服务器架构和在线重配置技术，以优化LLM在云边协同环境下的部署与推理效率。",
      "llm_tldr": "探索无服务器架构和在线重配置技术，以优化LLM在云边协同环境下的部署与推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.16100v1",
      "carry_days": 1
    },
    {
      "id": "2601.21522v1",
      "title": "More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)",
      "abstract": "The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.",
      "authors": [
        "Sagi Meir",
        "Tommer D. Keidar",
        "Noam Levi",
        "Shlomi Reuveni",
        "Barak Hirshberg"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-01-29T10:37:32+00:00",
      "link": "https://arxiv.org/pdf/2601.21522v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Improving LLM inference efficiency at a fixed budget",
      "llm_evidence_cn": "在固定预算下提高大模型推理效率",
      "llm_evidence": "在固定预算下提高大模型推理效率",
      "llm_tldr_en": "Proposes the ReD method to increase coverage and efficiency of LLM inference under budget constraints.",
      "llm_tldr_cn": "提出了ReD方法，以在预算限制下提高大模型推理的覆盖率和效率。",
      "llm_tldr": "提出了ReD方法，以在预算限制下提高大模型推理的覆盖率和效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.21522v1",
      "carry_days": 1
    },
    {
      "id": "2601.18699v1",
      "title": "Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
      "abstract": "Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.",
      "authors": [
        "Olaf Yunus Laitinen Imanov"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-26T17:15:10+00:00",
      "link": "https://arxiv.org/pdf/2601.18699v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Mechanistic analysis of forgetting during LLM fine-tuning",
      "llm_evidence_cn": "大语言模型微调过程中遗忘机制的分析",
      "llm_evidence": "大语言模型微调过程中遗忘机制的分析",
      "llm_tldr_en": "Analyzes catastrophic forgetting in Transformer-based LLMs during sequential fine-tuning across various scales.",
      "llm_tldr_cn": "对Transformer架构大模型在序列微调过程中的灾难性遗忘进行了深入的机理分析。",
      "llm_tldr": "对Transformer架构大模型在序列微调过程中的灾难性遗忘进行了深入的机理分析。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.18699v1",
      "carry_days": 1
    },
    {
      "id": "2602.04254v1",
      "title": "Scaling Agentic Verifier for Competitive Coding",
      "abstract": "Large language models (LLMs) have demonstrated strong coding capabilities but still struggle to solve competitive programming problems correctly in a single attempt. Execution-based re-ranking offers a promising test-time scaling strategy, yet existing methods are constrained by either difficult test case generation or inefficient random input sampling. To address this limitation, we propose Agentic Verifier, an execution-based agent that actively reasons about program behaviors and searches for highly discriminative test inputs that expose behavioral discrepancies among candidate solutions. Through multi-turn interaction with code execution environments, the verifier iteratively refines the candidate input generator and produces targeted counterexamples rather than blindly sampling inputs. We train the verifier to acquire this discriminative input generation capability via a scalable pipeline combining large-scale data synthesis, rejection fine-tuning, and agentic reinforcement learning. Extensive experiments across five competitive programming benchmarks demonstrate consistent improvements over strong execution-based baselines, achieving up to +10-15% absolute gains in Best@K accuracy. Further analysis reveals clear test-time scaling behavior and highlights the verifier's broader potential beyond reranking.",
      "authors": [
        "Zeyao Ma",
        "Jing Zhang",
        "Xiaokang Zhang",
        "Jiaxi Yang",
        "Zongmeng Zhang",
        "Jiajun Zhang",
        "Yuheng Jing",
        "Lei Zhang",
        "Hao Zheng",
        "Wenting Zhao",
        "Junyang Lin",
        "Binyuan Hui"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T06:30:40+00:00",
      "link": "https://arxiv.org/pdf/2602.04254v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Agentic verifier for reasoning and problem solving in coding tasks",
      "llm_evidence_cn": "用于编程任务推理和问题解决的智能体验证器",
      "llm_evidence": "用于编程任务推理和问题解决的智能体验证器",
      "llm_tldr_en": "Develops an agentic verifier that uses execution feedback to improve LLM performance in competitive programming.",
      "llm_tldr_cn": "开发了一种智能体验证器，利用执行反馈提升 LLM 在竞赛编程中的表现。",
      "llm_tldr": "开发了一种智能体验证器，利用执行反馈提升 LLM 在竞赛编程中的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.04254v1",
      "carry_days": 1
    },
    {
      "id": "2601.18924v1",
      "title": "RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures",
      "abstract": "Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.",
      "authors": [
        "Andrew Jaffe",
        "Noah Reicin",
        "Jinho D. Choi"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-26T19:52:42+00:00",
      "link": "https://arxiv.org/pdf/2601.18924v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Evaluating instruction following in complex prompt structures",
      "llm_evidence_cn": "评估复杂提示结构中的指令遵循能力",
      "llm_evidence": "评估复杂提示结构中的指令遵循能力",
      "llm_tldr_en": "Introduces RIFT, a benchmark to evaluate how prompt topology affects LLM instruction following performance.",
      "llm_tldr_cn": "引入 RIFT 基准测试，用于评估提示词拓扑结构对大模型指令遵循性能的影响。",
      "llm_tldr": "引入 RIFT 基准测试，用于评估提示词拓扑结构对大模型指令遵循性能的影响。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.18924v1",
      "carry_days": 1
    },
    {
      "id": "2602.01541v1",
      "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.",
      "authors": [
        "Boyi Li",
        "Yifan Shen",
        "Yuanzhe Liu",
        "Yifan Xu",
        "Jiateng Liu",
        "Xinzhuo Li",
        "Zhengyuan Li",
        "Jingyuan Zhu",
        "Yunhan Zhong",
        "Fangzhou Lan",
        "Jianguo Cao",
        "James M. Rehg",
        "Heng Ji",
        "Ismini Lourentzou",
        "Xu Cao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-02T02:19:50+00:00",
      "link": "https://arxiv.org/pdf/2602.01541v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "scaling Chain-of-Thought reasoning in multimodal models",
      "llm_evidence_cn": "在多模态模型中扩展思维链推理",
      "llm_evidence": "在多模态模型中扩展思维链推理",
      "llm_tldr_en": "Proposes Cognitive Supersensing to enhance MLLM reasoning through latent visual imagery and CoT scaling.",
      "llm_tldr_cn": "提出认知超感官范式，通过潜视觉想象和思维链扩展增强多模态大模型的推理能力。",
      "llm_tldr": "提出认知超感官范式，通过潜视觉想象和思维链扩展增强多模态大模型的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.01541v1",
      "carry_days": 1
    },
    {
      "id": "2602.03773v1",
      "title": "Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL",
      "abstract": "Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.",
      "authors": [
        "Ian Wu",
        "Yuxiao Qu",
        "Amrith Setlur",
        "Aviral Kumar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03T17:34:04+00:00",
      "link": "https://arxiv.org/pdf/2602.03773v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "constructing reasoning chains for continual improvement",
      "llm_evidence_cn": "构建推理链以实现持续改进",
      "llm_evidence": "构建推理链以实现持续改进",
      "llm_tldr_en": "Proposes an iterative decoding algorithm that uses reasoning chains to help LLMs solve difficult problems.",
      "llm_tldr_cn": "提出了一种迭代解码算法，利用推理链帮助大语言模型解决难题。",
      "llm_tldr": "提出了一种迭代解码算法，利用推理链帮助大语言模型解决难题。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.03773v1",
      "carry_days": 1
    },
    {
      "id": "2601.07372v1",
      "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.",
      "authors": [
        "Xin Cheng",
        "Wangding Zeng",
        "Damai Dai",
        "Qinyu Chen",
        "Bingxuan Wang",
        "Zhenda Xie",
        "Kezhao Huang",
        "Xingkai Yu",
        "Zhewen Hao",
        "Yukun Li",
        "Han Zhang",
        "Huishuai Zhang",
        "Dongyan Zhao",
        "Wenfeng Liang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-12T09:54:49+00:00",
      "link": "https://arxiv.org/pdf/2601.07372v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Scalable lookup and sparsity for Large Language Models",
      "llm_evidence_cn": "大语言模型的可扩展查找与稀疏性",
      "llm_evidence": "大语言模型的可扩展查找与稀疏性",
      "llm_tldr_en": "Introduces Engram to optimize the trade-off between neural computation and static memory in LLMs.",
      "llm_tldr_cn": "引入 Engram 模块，通过平衡神经计算与静态内存来优化大语言模型的扩展能力。",
      "llm_tldr": "引入 Engram 模块，通过平衡神经计算与静态内存来优化大语言模型的扩展能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Large Language Models research papers and surveys",
      "matched_requirement_id": "req-1",
      "paper_id": "2601.07372v1",
      "carry_days": 1
    },
    {
      "id": "2602.02405v1",
      "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
      "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.",
      "authors": [
        "Ethan Mendes",
        "Jungsoo Park",
        "Alan Ritter"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02T18:03:43+00:00",
      "link": "https://arxiv.org/pdf/2602.02405v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Improving reasoning capabilities by turning expert solutions into training data",
      "llm_evidence_cn": "通过将专家方案转化为训练数据来提升推理能力",
      "llm_evidence": "通过将专家方案转化为训练数据来提升推理能力",
      "llm_tldr_en": "Explores methods to convert human expert solutions into learnable data to boost LLM reasoning performance.",
      "llm_tldr_cn": "研究如何将人类专家方案转化为可学习的信号，以增强大模型在困难问题上的推理能力。",
      "llm_tldr": "研究如何将人类专家方案转化为可学习的信号，以增强大模型在困难问题上的推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.02405v1",
      "carry_days": 1
    },
    {
      "id": "2601.21164v2",
      "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
      "abstract": "Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.",
      "authors": [
        "Jingyun Wang",
        "Dian Li",
        "Xiaohan Wang",
        "Gang Liu",
        "Jiahong Yan",
        "Guoliang Kang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-29T02:03:33+00:00",
      "link": "https://arxiv.org/pdf/2601.21164v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM for plane geometry problem solving and reasoning",
      "llm_evidence_cn": "用于平面几何问题解决和推理的LLM",
      "llm_evidence": "用于平面几何问题解决和推理的LLM",
      "llm_tldr_en": "Explores using LLMs for geometric reasoning by converting visual diagrams into concise textual descriptions.",
      "llm_tldr_cn": "通过将视觉图形转换为简洁的文本描述，探索利用LLM进行几何推理的方法。",
      "llm_tldr": "通过将视觉图形转换为简洁的文本描述，探索利用LLM进行几何推理的方法。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.21164v2",
      "carry_days": 1
    },
    {
      "id": "2602.20980v1",
      "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language integration and faster inference. However, existing heuristically predefined supervision signals in latent CoT provide limited guidance for preserving critical visual information in intermediate latent states. To address this limitation, we propose CrystaL (Crystallized Latent Reasoning), a single-stage framework with two paths to process intact and corrupted images, respectively. By explicitly aligning the attention patterns and prediction distributions across the two paths, CrystaL crystallizes latent representations into task-relevant visual semantics, without relying on auxiliary annotations or external modules. Extensive experiments on perception-intensive benchmarks demonstrate that CrystaL consistently outperforms state-of-the-art baselines, achieving substantial gains in fine-grained visual understanding while maintaining robust reasoning capabilities.",
      "authors": [
        "Yang Zhang",
        "Danyang Li",
        "Yuxuan Li",
        "Xin Zhang",
        "Tianyu Xie",
        "Mingming Cheng",
        "Xiang Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-24T15:01:30+00:00",
      "link": "https://arxiv.org/pdf/2602.20980v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Latent Chain-of-Thought reasoning in MLLMs",
      "llm_evidence_cn": "多模态大模型中的隐式思维链推理",
      "llm_evidence": "多模态大模型中的隐式思维链推理",
      "llm_tldr_en": "Introduces CrystaL to enhance latent Chain-of-Thought reasoning in multimodal LLMs via visual alignment.",
      "llm_tldr_cn": "提出CrystaL框架，通过对齐视觉特征来增强多模态大模型中的隐式思维链推理能力。",
      "llm_tldr": "提出CrystaL框架，通过对齐视觉特征来增强多模态大模型中的隐式思维链推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.20980v1",
      "carry_days": 1
    },
    {
      "id": "2602.04089v1",
      "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL",
      "abstract": "Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.",
      "authors": [
        "Xiaofeng Lin",
        "Sirou Zhu",
        "Yilei Chen",
        "Mingyu Chen",
        "Hejian Sang",
        "Ioannis Paschalidis",
        "Zhipeng Wang",
        "Aldo Pacchiano",
        "Xuezhou Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-03T23:53:05+00:00",
      "link": "https://arxiv.org/pdf/2602.04089v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "scaling in-context online learning and decision-making",
      "llm_evidence_cn": "扩展上下文在线学习与决策能力",
      "llm_evidence": "扩展上下文在线学习与决策能力",
      "llm_tldr_en": "Introduces ORBIT to enhance LLM decision-making and in-context learning through meta-reinforcement learning.",
      "llm_tldr_cn": "通过元强化学习框架ORBIT提升LLM的在线决策与上下文学习能力。",
      "llm_tldr": "通过元强化学习框架ORBIT提升LLM的在线决策与上下文学习能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.04089v1",
      "carry_days": 1
    },
    {
      "id": "2601.09398v1",
      "title": "Ability Transfer and Recovery via Modularized Parameters Localization",
      "abstract": "Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.",
      "authors": [
        "Songyao Jin",
        "Kun Zhou",
        "Wenqi Li",
        "Peng Wang",
        "Biwei Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-14T11:42:39+00:00",
      "link": "https://arxiv.org/pdf/2601.09398v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "modularized parameters localization for fine-tuning",
      "llm_evidence_cn": "用于微调的模块化参数定位",
      "llm_evidence": "用于微调的模块化参数定位",
      "llm_tldr_en": "Proposes ACT to localize ability-related channels, preventing catastrophic forgetting during LLM fine-tuning.",
      "llm_tldr_cn": "提出ACT方法定位能力相关通道，缓解LLM微调中的灾难性遗忘。",
      "llm_tldr": "提出ACT方法定位能力相关通道，缓解LLM微调中的灾难性遗忘。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.09398v1",
      "carry_days": 1
    },
    {
      "id": "2602.04900v1",
      "title": "Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization",
      "abstract": "As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the benefits of container orchestration, such as scalability and resource efficiency, to complex AI workflows. We implement and evaluate an illustrative, multi-stage use case consisting of automatic speech recognition and summarization. First, we address batch inference by using Kueue to manage jobs that transcribe audio files with Whisper models and Dynamic Accelerator Slicer (DAS) to increase parallel job execution. Second, we address a discrete online inference scenario by feeding the transcripts to a Large Language Model for summarization hosted using llm-d, a novel solution utilizing the recent developments around the Kubernetes Gateway API Inference Extension (GAIE) for optimized routing of inference requests. Our findings illustrate that these complementary components (Kueue, DAS, and GAIE) form a cohesive, high-performance platform, proving Kubernetes' capability to serve as a unified foundation for demanding GenAI workloads: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36%; and GAIE improved Time to First Token by 82\\%.",
      "authors": [
        "Sai Sindhur Malleni",
        "Raúl Sevilla",
        "Aleksei Vasilevskii",
        "José Castillo Lema",
        "André Bauer"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-02-03T15:36:08+00:00",
      "link": "https://arxiv.org/pdf/2602.04900v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "evaluating Kubernetes for LLM inference deployment",
      "llm_evidence_cn": "评估用于LLM推理部署的Kubernetes",
      "llm_evidence": "评估用于LLM推理部署的Kubernetes",
      "llm_tldr_en": "Evaluates Kubernetes-native tools for managing and scaling LLM inference workloads in production.",
      "llm_tldr_cn": "评估Kubernetes原生工具在管理和扩展LLM推理工作负载中的表现。",
      "llm_tldr": "评估Kubernetes原生工具在管理和扩展LLM推理工作负载中的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.04900v1",
      "carry_days": 1
    },
    {
      "id": "2601.17789v1",
      "title": "Neuro-Symbolic Verification on Instruction Following of LLMs",
      "abstract": "A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.",
      "authors": [
        "Yiming Su",
        "Kunzhao Xu",
        "Yanjie Gao",
        "Fan Yang",
        "Cheng Li",
        "Mao Yang",
        "Tianyin Xu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-25T11:03:15+00:00",
      "link": "https://arxiv.org/pdf/2601.17789v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Verifying instruction following in LLM outputs and agentic workflows",
      "llm_evidence_cn": "验证LLM输出和代理工作流中的指令遵循情况",
      "llm_evidence": "验证LLM输出和代理工作流中的指令遵循情况",
      "llm_tldr_en": "Presents a neuro-symbolic framework to verify if LLM outputs strictly adhere to provided instructions and constraints.",
      "llm_tldr_cn": "提出一个神经符号框架，用于验证LLM输出是否严格遵守提供的指令和约束。",
      "llm_tldr": "提出一个神经符号框架，用于验证LLM输出是否严格遵守提供的指令和约束。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.17789v1",
      "carry_days": 1
    },
    {
      "id": "2602.02047v1",
      "title": "Dissecting Outlier Dynamics in LLM NVFP4 Pretraining",
      "abstract": "Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with \"post-QK\" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.",
      "authors": [
        "Peijie Dong",
        "Ruibo Fan",
        "Yuechen Tao",
        "Di Mou",
        "Wenhu Hu",
        "Zhenheng Tang",
        "Yinghao Yu",
        "Jiamang Wang",
        "Wenbo Su",
        "Guodong Yang",
        "Liping Zhang",
        "Xiaowen Chu",
        "Baochun Li",
        "Bo Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-02T12:50:27+00:00",
      "link": "https://arxiv.org/pdf/2602.02047v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "outlier dynamics in LLM pretraining with 4-bit arithmetic",
      "llm_evidence_cn": "LLM 4位量化预训练中的离群值动态",
      "llm_evidence": "LLM 4位量化预训练中的离群值动态",
      "llm_tldr_en": "Analyzes architectural outliers during 4-bit LLM pretraining to improve training efficiency and stability.",
      "llm_tldr_cn": "分析LLM在4位量化预训练中的架构离群值，以提高训练效率和稳定性。",
      "llm_tldr": "分析LLM在4位量化预训练中的架构离群值，以提高训练效率和稳定性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Self-supervised pre-training techniques for LLMs",
      "matched_requirement_id": "req-3",
      "paper_id": "2602.02047v1",
      "carry_days": 1
    },
    {
      "id": "2601.16038v1",
      "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval",
      "abstract": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.",
      "authors": [
        "Olga Bunkova",
        "Lorenzo Di Fruscia",
        "Sophia Rupprecht",
        "Artur M. Schweidtmann",
        "Marcel J. T. Reinders",
        "Jana M. Weber"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22T15:11:02+00:00",
      "link": "https://arxiv.org/pdf/2601.16038v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "LLM grounding in knowledge graphs for retrieval",
      "llm_evidence_cn": "大模型结合知识图谱进行检索增强",
      "llm_evidence": "大模型结合知识图谱进行检索增强",
      "llm_tldr_en": "Uses Text2Cypher to ground LLMs in reaction knowledge graphs for more accurate chemical synthesis retrieval.",
      "llm_tldr_cn": "利用Text2Cypher将大模型与化学反应知识图谱结合，实现更准确的合成路径检索。",
      "llm_tldr": "利用Text2Cypher将大模型与化学反应知识图谱结合，实现更准确的合成路径检索。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.16038v1",
      "carry_days": 1
    },
    {
      "id": "2601.15094v1",
      "title": "Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks",
      "abstract": "Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.",
      "authors": [
        "Md Zahidul Haque",
        "Saima Afrin",
        "Antonio Mastropaolo"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-01-21T15:33:16+00:00",
      "link": "https://arxiv.org/pdf/2601.15094v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Parameter-Efficient Fine-Tuning (PEFT) and QLoRA for LLMs",
      "llm_evidence_cn": "大语言模型的参数高效微调 (PEFT) 和 QLoRA 技术",
      "llm_evidence": "大语言模型的参数高效微调 (PEFT) 和 QLoRA 技术",
      "llm_tldr_en": "Explores QLoRA for multi-task fine-tuning in code-related LLM applications.",
      "llm_tldr_cn": "探索了 QLoRA 在代码相关大模型多任务微调中的应用效果。",
      "llm_tldr": "探索了 QLoRA 在代码相关大模型多任务微调中的应用效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.15094v1",
      "carry_days": 1
    },
    {
      "id": "2602.03578v1",
      "title": "Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs",
      "abstract": "Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.",
      "authors": [
        "Su Dong",
        "Qinggang Zhang",
        "Yilin Xiao",
        "Shengyuan Chen",
        "Chuang Zhou",
        "Xiao Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-03T14:26:28+00:00",
      "link": "https://arxiv.org/pdf/2602.03578v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Integrating Retrieval-Augmented Generation with knowledge graphs",
      "llm_evidence_cn": "将检索增强生成与知识图谱相结合",
      "llm_evidence": "将检索增强生成与知识图谱相结合",
      "llm_tldr_en": "Introduces an adaptive framework to efficiently integrate GraphRAG with vanilla RAG for better reasoning.",
      "llm_tldr_cn": "提出一种自适应框架，将图检索增强生成与传统RAG结合以提升推理效率。",
      "llm_tldr": "提出一种自适应框架，将图检索增强生成与传统RAG结合以提升推理效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.03578v1",
      "carry_days": 1
    },
    {
      "id": "2602.07739v1",
      "title": "HypRAG: Hyperbolic Dense Retrieval for Retrieval Augmented Generation",
      "abstract": "Embedding geometry plays a fundamental role in retrieval quality, yet dense retrievers for retrieval-augmented generation (RAG) remain largely confined to Euclidean space. However, natural language exhibits hierarchical structure from broad topics to specific entities that Euclidean embeddings fail to preserve, causing semantically distant documents to appear spuriously similar and increasing hallucination risk. To address these limitations, we introduce hyperbolic dense retrieval, developing two model variants in the Lorentz model of hyperbolic space: HyTE-FH, a fully hyperbolic transformer, and HyTE-H, a hybrid architecture projecting pre-trained Euclidean embeddings into hyperbolic space. To prevent representational collapse during sequence aggregation, we introduce the Outward Einstein Midpoint, a geometry-aware pooling operator that provably preserves hierarchical structure. On MTEB, HyTE-FH outperforms equivalent Euclidean baselines, while on RAGBench, HyTE-H achieves up to 29% gains over Euclidean baselines in context relevance and answer relevance using substantially smaller models than current state-of-the-art retrievers. Our analysis also reveals that hyperbolic representations encode document specificity through norm-based separation, with over 20% radial increase from general to specific concepts, a property absent in Euclidean embeddings, underscoring the critical role of geometric inductive bias in faithful RAG systems.",
      "authors": [
        "Hiren Madhu",
        "Ngoc Bui",
        "Ali Maatouk",
        "Leandros Tassiulas",
        "Smita Krishnaswamy",
        "Menglin Yang",
        "Sukanta Ganguly",
        "Kiran Srinivasan",
        "Rex Ying"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-08T00:18:05+00:00",
      "link": "https://arxiv.org/pdf/2602.07739v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Hyperbolic dense retrieval for RAG frameworks",
      "llm_evidence_cn": "用于RAG框架的双曲空间稠密检索",
      "llm_evidence": "用于RAG框架的双曲空间稠密检索",
      "llm_tldr_en": "Introduces HypRAG, using hyperbolic embeddings to improve retrieval quality and reduce hallucinations in RAG.",
      "llm_tldr_cn": "提出HypRAG，利用双曲空间嵌入提升RAG的检索质量并降低幻觉风险。",
      "llm_tldr": "提出HypRAG，利用双曲空间嵌入提升RAG的检索质量并降低幻觉风险。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.07739v1",
      "carry_days": 1
    },
    {
      "id": "2602.08545v1",
      "title": "DA-RAG: Dynamic Attributed Community Search for Retrieval-Augmented Generation",
      "abstract": "Owing to their unprecedented comprehension capabilities, large language models (LLMs) have become indispensable components of modern web search engines. From a technical perspective, this integration represents retrieval-augmented generation (RAG), which enhances LLMs by grounding them in external knowledge bases. A prevalent technical approach in this context is graph-based RAG (G-RAG). However, current G-RAG methodologies frequently underutilize graph topology, predominantly focusing on low-order structures or pre-computed static communities. This limitation affects their effectiveness in addressing dynamic and complex queries. Thus, we propose DA-RAG, which leverages attributed community search (ACS) to extract relevant subgraphs based on the queried question dynamically. DA-RAG captures high-order graph structures, allowing for the retrieval of self-complementary knowledge. Furthermore, DA-RAG is equipped with a chunk-layer oriented graph index, which facilitates efficient multi-granularity retrieval while significantly reducing both computational and economic costs. We evaluate DA-RAG on multiple datasets, demonstrating that it outperforms existing RAG methods by up to 40% in head-to-head comparisons across four metrics while reducing index construction time and token overhead by up to 37% and 41%, respectively.",
      "authors": [
        "Xingyuan Zeng",
        "Zuohan Wu",
        "Yue Wang",
        "Chen Zhang",
        "Quanming Yao",
        "Libin Zheng",
        "Jian Yin"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-09T11:45:13+00:00",
      "link": "https://arxiv.org/pdf/2602.08545v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Graph-based Retrieval-Augmented Generation (G-RAG) and attributed community search",
      "llm_evidence_cn": "基于图的检索增强生成（G-RAG）与属性社区搜索",
      "llm_evidence": "基于图的检索增强生成（G-RAG）与属性社区搜索",
      "llm_tldr_en": "Introduces DA-RAG to improve retrieval-augmented generation by leveraging dynamic attributed community search in graphs.",
      "llm_tldr_cn": "引入DA-RAG，通过利用图中的动态属性社区搜索来改进检索增强生成。",
      "llm_tldr": "引入DA-RAG，通过利用图中的动态属性社区搜索来改进检索增强生成。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.08545v1",
      "carry_days": 1
    },
    {
      "id": "2601.19827v2",
      "title": "When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering",
      "abstract": "Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.",
      "authors": [
        "Mahdi Astaraki",
        "Mohammad Arshi Saloot",
        "Ali Shiraee Kasmaee",
        "Hamidreza Mahyar",
        "Soheila Samiee"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2026-01-27T17:35:05+00:00",
      "link": "https://arxiv.org/pdf/2601.19827v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Iterative retrieval-reasoning loops in RAG",
      "llm_evidence_cn": "RAG中的迭代检索推理循环",
      "llm_evidence": "RAG中的迭代检索推理循环",
      "llm_tldr_en": "Studies when iterative RAG outperforms static gold-context RAG in scientific multi-hop question answering.",
      "llm_tldr_cn": "研究了在科学多跳问答中，迭代式RAG何时优于静态金标准上下文RAG。",
      "llm_tldr": "研究了在科学多跳问答中，迭代式RAG何时优于静态金标准上下文RAG。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.19827v2",
      "carry_days": 1
    },
    {
      "id": "2602.01198v1",
      "title": "A State-Transition Framework for Efficient LLM Reasoning",
      "abstract": "While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.",
      "authors": [
        "Liang Zhang",
        "Yu Zhao",
        "Longyue Wang",
        "Tianqi Shi",
        "Weihua Luo",
        "Kaifu Zhang",
        "Jinsong Su"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-01T12:40:40+00:00",
      "link": "https://arxiv.org/pdf/2602.01198v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Efficient LLM reasoning and Chain-of-Thought sequences",
      "llm_evidence_cn": "高效的大模型推理与思维链序列",
      "llm_evidence": "高效的大模型推理与思维链序列",
      "llm_tldr_en": "Models the reasoning process as a state-transition process to reduce computational costs of long CoT.",
      "llm_tldr_cn": "将推理建模为状态转移过程，旨在降低长思维链生成的计算与内存成本。",
      "llm_tldr": "将推理建模为状态转移过程，旨在降低长思维链生成的计算与内存成本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "paper_id": "2602.01198v1",
      "carry_days": 1
    },
    {
      "id": "2602.00328v1",
      "title": "Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference",
      "abstract": "Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.",
      "authors": [
        "Nikhil Gopal",
        "Kostis Kaffes"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-30T21:29:04+00:00",
      "link": "https://arxiv.org/pdf/2602.00328v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "GPU cache management for efficient LLM inference optimization",
      "llm_evidence_cn": "用于高效LLM推理优化的GPU缓存管理",
      "llm_evidence": "用于高效LLM推理优化的GPU缓存管理",
      "llm_tldr_en": "Presents Harvest, a P2P GPU caching framework to mitigate memory constraints during LLM inference.",
      "llm_tldr_cn": "提出Harvest框架，利用点对点GPU互联缓存KV张量，优化LLM推理的内存瓶颈。",
      "llm_tldr": "提出Harvest框架，利用点对点GPU互联缓存KV张量，优化LLM推理的内存瓶颈。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.00328v1",
      "carry_days": 1
    },
    {
      "id": "2602.03019v1",
      "title": "FedKRSO: Communication and Memory Efficient Federated Fine-Tuning of Large Language Models",
      "abstract": "Fine-tuning is essential to adapt general-purpose large language models (LLMs) to domain-specific tasks. As a privacy-preserving framework to leverage decentralized data for collaborative model training, Federated Learning (FL) is gaining popularity in LLM fine-tuning, but remains challenging due to the high cost of transmitting full model parameters and computing full gradients on resource-constrained clients. While Parameter-Efficient Fine-Tuning (PEFT) methods are widely used in FL to reduce communication and memory costs, they often sacrifice model performance compared to FFT. This paper proposes FedKRSO (Federated $K$-Seed Random Subspace Optimization), a novel method that enables communication and memory efficient FFT of LLMs in federated settings. In FedKRSO, clients update the model within a shared set of random low-dimension subspaces generated by the server to save memory usage. Furthermore, instead of transmitting full model parameters in each FL round, clients send only the model update accumulators along the subspaces to the server, enabling efficient global model aggregation and dissemination. By using these strategies, FedKRSO can substantially reduce communication and memory overhead while overcoming the performance limitations of PEFT, closely approximating the performance of federated FFT. The convergence properties of FedKRSO are analyzed rigorously under general FL settings. Extensive experiments on the GLUE benchmark across diverse FL scenarios demonstrate that FedKRSO achieves both superior performance and low communication and memory overhead, paving the way towards on federated LLM fine-tuning at the resource-constrained edge.",
      "authors": [
        "Guohao Yang",
        "Tongle Wu",
        "Yuanxiong Guo",
        "Ying Sun",
        "Yanmin Gong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-03T02:39:33+00:00",
      "link": "https://arxiv.org/pdf/2602.03019v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "communication and memory efficient federated fine-tuning of LLMs",
      "llm_evidence_cn": "通信与内存高效的LLM联邦微调",
      "llm_evidence": "通信与内存高效的LLM联邦微调",
      "llm_tldr_en": "Proposes FedKRSO for efficient federated fine-tuning of LLMs using random subspace optimization.",
      "llm_tldr_cn": "提出FedKRSO方法，通过随机子空间优化实现高效的LLM联邦全参数微调。",
      "llm_tldr": "提出FedKRSO方法，通过随机子空间优化实现高效的LLM联邦全参数微调。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.03019v1",
      "carry_days": 1
    },
    {
      "id": "2601.22801v1",
      "title": "Clipping-Free Policy Optimization for Large Language Models",
      "abstract": "Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.",
      "authors": [
        "Ömer Veysel Çağatan",
        "Barış Akgün",
        "Gözde Gül Şahin",
        "Xuandong Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-30T10:32:37+00:00",
      "link": "https://arxiv.org/pdf/2601.22801v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Clipping-free policy optimization for LLM alignment and reasoning",
      "llm_evidence_cn": "用于LLM对齐与推理的无裁剪策略优化",
      "llm_evidence": "用于LLM对齐与推理的无裁剪策略优化",
      "llm_tldr_en": "Introduces CFPO, a stable reinforcement learning objective for LLM post-training and alignment.",
      "llm_tldr_cn": "提出CFPO算法，通过凸二次惩罚替代裁剪机制，提升LLM对齐与推理的稳定性。",
      "llm_tldr": "提出CFPO算法，通过凸二次惩罚替代裁剪机制，提升LLM对齐与推理的稳定性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "paper_id": "2601.22801v1",
      "carry_days": 1
    },
    {
      "id": "2602.09616v1",
      "title": "With Argus Eyes: Assessing Retrieval Gaps via Uncertainty Scoring to Detect and Remedy Retrieval Blind Spots",
      "abstract": "Reliable retrieval-augmented generation (RAG) systems depend fundamentally on the retriever's ability to find relevant information. We show that neural retrievers used in RAG systems have blind spots, which we define as the failure to retrieve entities that are relevant to the query, but have low similarity to the query embedding. We investigate the training-induced biases that cause such blind spot entities to be mapped to inaccessible parts of the embedding space, resulting in low retrievability. Using a large-scale dataset constructed from Wikidata relations and first paragraphs of Wikipedia, and our proposed Retrieval Probability Score (RPS), we show that blind spot risk in standard retrievers (e.g., CONTRIEVER, REASONIR) can be predicted pre-index from entity embedding geometry, avoiding expensive retrieval evaluations. To address these blind spots, we introduce ARGUS, a pipeline that enables the retrievability of high-risk (low-RPS) entities through targeted document augmentation from a knowledge base (KB), first paragraphs of Wikipedia, in our case. Extensive experiments on BRIGHT, IMPLIRET, and RAR-B show that ARGUS achieves consistent improvements across all evaluated retrievers (averaging +3.4 nDCG@5 and +4.5 nDCG@10 absolute points), with substantially larger gains in challenging subsets. These results establish that preemptively remedying blind spots is critical for building robust and trustworthy RAG systems (Code and Data).",
      "authors": [
        "Zeinab Sadat Taghavi",
        "Ali Modarressi",
        "Hinrich Schutze",
        "Andreas Marfurt"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-10T10:04:55+00:00",
      "link": "https://arxiv.org/pdf/2602.09616v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Assessing retrieval gaps and blind spots in RAG systems",
      "llm_evidence_cn": "评估RAG系统中的检索差距与盲点",
      "llm_evidence": "评估RAG系统中的检索差距与盲点",
      "llm_tldr_en": "Investigates retrieval blind spots in RAG and proposes a score to detect and remedy these gaps.",
      "llm_tldr_cn": "研究RAG系统中的检索盲点，并提出RPS评分以检测和修复检索失效。",
      "llm_tldr": "研究RAG系统中的检索盲点，并提出RPS评分以检测和修复检索失效。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2602.09616v1",
      "carry_days": 1
    },
    {
      "id": "2601.11342v1",
      "title": "Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models",
      "abstract": "Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.",
      "authors": [
        "Chuanyue Yu",
        "Jiahui Wang",
        "Yuhan Li",
        "Heng Chang",
        "Ge Lan",
        "Qingyun Sun",
        "Jia Li",
        "Jianxin Li",
        "Ziwei Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-16T14:45:46+00:00",
      "link": "https://arxiv.org/pdf/2601.11342v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Retrieval-Augmented Generation for Diffusion Language Models",
      "llm_evidence_cn": "扩散语言模型的检索增强生成",
      "llm_evidence": "扩散语言模型的检索增强生成",
      "llm_tldr_en": "Explores RAG for Diffusion Language Models, identifying semantic drift issues and potential for contextual dependency.",
      "llm_tldr_cn": "探索扩散语言模型的RAG技术，识别了语义漂移问题及其对上下文依赖的潜力。",
      "llm_tldr": "探索扩散语言模型的RAG技术，识别了语义漂移问题及其对上下文依赖的潜力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.11342v1",
      "carry_days": 1
    },
    {
      "id": "2602.13073v1",
      "title": "LCSB: Layer-Cyclic Selective Backpropagation for Memory-Efficient On-Device LLM Fine-Tuning",
      "abstract": "Memory-efficient backpropagation (MeBP) has enabled first-order fine-tuning of large language models (LLMs) on mobile devices with less than 1GB memory. However, MeBP requires backward computation through all transformer layers at every step, where weight decompression alone accounts for 32--42% of backward time. We propose Layer-Cyclic Selective Backpropagation (LCSB), which computes gradients for only a subset of layers per step. Our key insight is that residual connections guarantee gradient flow through identity paths, while AdamW momentum provides implicit updates for non-selected layers. We interpret LCSB as Block Coordinate Descent on the LoRA parameter space, providing theoretical justification for convergence. LCSB achieves up to 1.40$\\times$ speedup with less than 2\\% quality degradation across five models and three tasks. Surprisingly, in 4-bit quantized settings, LCSB exhibits superior stability: a 3B model that completely diverges under full backpropagation converges smoothly with LCSB, suggesting an implicit regularization effect from selective gradient computation.",
      "authors": [
        "Juneyoung Park",
        "Eunbeen Yoon",
        "Seongwan Kim. Jaeho Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-13T16:32:53+00:00",
      "link": "https://arxiv.org/pdf/2602.13073v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Memory-efficient on-device LLM fine-tuning via selective backpropagation",
      "llm_evidence_cn": "通过选择性反向传播实现内存高效的设备端 LLM 微调",
      "llm_evidence": "通过选择性反向传播实现内存高效的设备端 LLM 微调",
      "llm_tldr_en": "Proposes LCSB to enable efficient LLM fine-tuning on mobile devices by reducing backward computation time.",
      "llm_tldr_cn": "提出 LCSB，通过减少反向计算时间，实现在移动设备上高效微调 LLM。",
      "llm_tldr": "提出 LCSB，通过减少反向计算时间，实现在移动设备上高效微调 LLM。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2602.13073v1",
      "carry_days": 1
    },
    {
      "id": "2602.07721v2",
      "title": "ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs",
      "abstract": "KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\\times$ and 44$\\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.",
      "authors": [
        "Yanlin Qi",
        "Xinhang Chen",
        "Huiqiang Jiang",
        "Qitong Wang",
        "Botao Peng",
        "Themis Palpanas"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.DB"
      ],
      "published": "2026-02-07T22:26:45+00:00",
      "link": "https://arxiv.org/pdf/2602.07721v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Fast KV-Cache retrieval for long-context LLM inference optimization",
      "llm_evidence_cn": "长上下文LLM推理优化的快速KV缓存检索",
      "llm_evidence": "长上下文LLM推理优化的快速KV缓存检索",
      "llm_tldr_en": "Presents ParisKV for efficient, drift-robust KV-cache retrieval in long-context LLM inference.",
      "llm_tldr_cn": "提出ParisKV，用于长上下文LLM推理中高效且鲁棒的KV缓存检索。",
      "llm_tldr": "提出ParisKV，用于长上下文LLM推理中高效且鲁棒的KV缓存检索。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.07721v2",
      "carry_days": 1
    },
    {
      "id": "2602.16093v1",
      "title": "Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities",
      "abstract": "Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \\methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.",
      "authors": [
        "Shankar Padmanabhan",
        "Mustafa Omer Gul",
        "Tanya Goyal"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-17T23:49:47+00:00",
      "link": "https://arxiv.org/pdf/2602.16093v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Continual knowledge adaptation and instruction-following",
      "llm_evidence_cn": "持续知识适配与指令遵循",
      "llm_evidence": "持续知识适配与指令遵循",
      "llm_tldr_en": "Introduces DiSC to update LLM knowledge while retaining post-training capabilities like instruction-following.",
      "llm_tldr_cn": "提出DiSC方法，在更新大模型知识的同时保留指令遵循和推理等后训练能力。",
      "llm_tldr": "提出DiSC方法，在更新大模型知识的同时保留指令遵循和推理等后训练能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "paper_id": "2602.16093v1",
      "carry_days": 1
    },
    {
      "id": "2602.12630v1",
      "title": "TensorCommitments: A Lightweight Verifiable Inference for Language Models",
      "abstract": "Most large language models (LLMs) run on external clouds: users send a prompt, pay for inference, and must trust that the remote GPU executes the LLM without any adversarial tampering. We critically ask how to achieve verifiable LLM inference, where a prover (the service) must convince a verifier (the client) that an inference was run correctly without rerunning the LLM. Existing cryptographic works are too slow at the LLM scale, while non-cryptographic ones require a strong verifier GPU. We propose TensorCommitments (TCs), a tensor-native proof-of-inference scheme. TC binds the LLM inference to a commitment, an irreversible tag that breaks under tampering, organized in our multivariate Terkle Trees. For LLaMA2, TC adds only 0.97% prover and 0.12% verifier time over inference while improving robustness to tailored LLM attacks by up to 48% over the best prior work requiring a verifier GPU.",
      "authors": [
        "Oguzhan Baser",
        "Elahe Sadeghi",
        "Eric Wang",
        "David Ribeiro Alves",
        "Sam Kazemian",
        "Hong Kang",
        "Sandeep P. Chinchali",
        "Sriram Vishwanath"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published": "2026-02-13T05:23:31+00:00",
      "link": "https://arxiv.org/pdf/2602.12630v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Verifiable LLM inference for cloud deployment",
      "llm_evidence_cn": "云端部署的可验证大模型推理",
      "llm_evidence": "云端部署的可验证大模型推理",
      "llm_tldr_en": "Proposes TensorCommitments for lightweight, verifiable LLM inference to ensure execution integrity on remote GPUs.",
      "llm_tldr_cn": "提出TensorCommitments，通过轻量级验证确保远程GPU上大模型推理执行的完整性。",
      "llm_tldr": "提出TensorCommitments，通过轻量级验证确保远程GPU上大模型推理执行的完整性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.12630v1",
      "carry_days": 1
    },
    {
      "id": "2601.07055v1",
      "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
      "abstract": "As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.",
      "authors": [
        "Zhenrui Yue",
        "Kartikeya Upasani",
        "Xianjun Yang",
        "Suyu Ge",
        "Shaoliang Nie",
        "Yuning Mao",
        "Zhe Liu",
        "Dong Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-11T20:27:55+00:00",
      "link": "https://arxiv.org/pdf/2601.07055v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "self-evolving LLMs to improve reasoning capabilities",
      "llm_evidence_cn": "大语言模型自我进化以提升推理能力",
      "llm_evidence": "大语言模型自我进化以提升推理能力",
      "llm_tldr_en": "Dr. Zero enables search agents to self-evolve reasoning and tool-use skills without external training data.",
      "llm_tldr_cn": "Dr. Zero框架使搜索智能体能够在无训练数据的情况下自我进化推理和工具使用能力。",
      "llm_tldr": "Dr. Zero框架使搜索智能体能够在无训练数据的情况下自我进化推理和工具使用能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.07055v1",
      "carry_days": 1
    },
    {
      "id": "2601.20676v1",
      "title": "Efficient Multimodal Planning Agent for Visual Question-Answering",
      "abstract": "Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.",
      "authors": [
        "Zhuo Chen",
        "Xinyu Geng",
        "Xinyu Wang",
        "Yong Jiang",
        "Zhen Zhang",
        "Pengjun Xie",
        "Kewei Tu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-28T14:58:59+00:00",
      "link": "https://arxiv.org/pdf/2601.20676v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Multimodal Retrieval-Augmented Generation (mRAG) for VQA",
      "llm_evidence_cn": "用于视觉问答的多模态检索增强生成 (mRAG)",
      "llm_evidence": "用于视觉问答的多模态检索增强生成 (mRAG)",
      "llm_tldr_en": "Proposes a multimodal planning agent to optimize the efficiency and effectiveness of mRAG pipelines for VQA tasks.",
      "llm_tldr_cn": "提出一种多模态规划智能体，通过动态分解检索增强生成流水线来优化视觉问答的效率与性能。",
      "llm_tldr": "提出一种多模态规划智能体，通过动态分解检索增强生成流水线来优化视觉问答的效率与性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.20676v1",
      "carry_days": 1
    },
    {
      "id": "2602.00816v1",
      "title": "Hessian Spectral Analysis at Foundation Model Scale",
      "abstract": "Accurate Hessian spectra of foundation models have remained out of reach, leading most prior work to rely on small models or strong structural approximations. We show that faithful spectral analysis of the true Hessian is tractable at frontier scale. Using shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism, we perform stochastic Lanczos quadrature on open-source language models with up to 100B parameters, producing the first large-scale spectral density estimates beyond the sub-10B regime. We characterize the numerical behavior of this pipeline, including finite-difference bias, floating-point noise amplification, and their effect on Krylov stability in fp32 and bf16, and derive practical operating regimes that are validated empirically. We further provide end-to-end runtime and memory scaling laws, showing that full-operator spectral probing incurs only a modest constant-factor overhead over first-order training. Crucially, direct access to the Hessian reveals that widely used block-diagonal curvature approximations can fail catastrophically, exhibiting order-one relative error and poor directional alignment even in mid-scale LLMs. Together, our results demonstrate that foundation-model Hessian spectra are both computable and qualitatively misrepresented by prevailing approximations, opening the door to principled curvature-based analysis at scale.",
      "authors": [
        "Diego Granziol",
        "Khurshid Juarev"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-01-31T16:57:06+00:00",
      "link": "https://arxiv.org/pdf/2602.00816v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Hessian spectral analysis of 100B parameter open-source language models",
      "llm_evidence_cn": "对100B参数开源语言模型的Hessian谱分析",
      "llm_evidence": "对100B参数开源语言模型的Hessian谱分析",
      "llm_tldr_en": "Provides the first large-scale Hessian spectral density estimates for frontier-scale foundation models.",
      "llm_tldr_cn": "首次对百亿级参数规模的基座模型进行了Hessian谱密度估计研究。",
      "llm_tldr": "首次对百亿级参数规模的基座模型进行了Hessian谱密度估计研究。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Large Language Models research papers and surveys",
      "matched_requirement_id": "req-1",
      "paper_id": "2602.00816v1",
      "carry_days": 1
    },
    {
      "id": "2601.08105v1",
      "title": "Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning",
      "abstract": "Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction.   In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.",
      "authors": [
        "Fabian Spaeh",
        "Tianyi Chen",
        "Chen-Hao Chiang",
        "Bin Shen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-13T00:56:38+00:00",
      "link": "https://arxiv.org/pdf/2601.08105v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "query suggestion for agentic RAG systems",
      "llm_evidence_cn": "智能体RAG系统的查询建议",
      "llm_evidence": "智能体RAG系统的查询建议",
      "llm_tldr_en": "Studies query suggestion to improve user interaction and reduce hallucinations in RAG frameworks.",
      "llm_tldr_cn": "研究RAG系统中的查询建议技术，旨在通过引导用户提问来减少幻觉并改善交互。",
      "llm_tldr": "研究RAG系统中的查询建议技术，旨在通过引导用户提问来减少幻觉并改善交互。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "paper_id": "2601.08105v1",
      "carry_days": 1
    },
    {
      "id": "2602.03134v1",
      "title": "SwiftVLM: Efficient Vision-Language Model Inference via Cross-Layer Token Bypass",
      "abstract": "Visual token pruning is a promising approach for reducing the computational cost of vision-language models (VLMs), and existing methods often rely on early pruning decisions to improve efficiency. While effective on coarse-grained reasoning tasks, they suffer from significant performance degradation on tasks requiring fine-grained visual details. Through layer-wise analysis, we reveal substantial discrepancies in visual token importance across layers, showing that tokens deemed unimportant at shallow layers can later become highly relevant for text-conditioned reasoning. To avoid irreversible critical information loss caused by premature pruning, we introduce a new pruning paradigm, termed bypass, which preserves unselected visual tokens and forwards them to subsequent pruning stages for re-evaluation. Building on this paradigm, we propose SwiftVLM, a simple and training-free method that performs pruning at model-specific layers with strong visual token selection capability, while enabling independent pruning decisions across layers. Experiments across multiple VLMs and benchmarks demonstrate that SwiftVLM consistently outperforms existing pruning strategies, achieving superior accuracy-efficiency trade-offs and more faithful visual token selection behavior.",
      "authors": [
        "Chen Qian",
        "Xinran Yu",
        "Danyang Li",
        "Guoxuan Chi",
        "Zheng Yang",
        "Qiang Ma",
        "Xin Miao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-03T05:42:51+00:00",
      "link": "https://arxiv.org/pdf/2602.03134v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Efficient inference via token bypass in vision-language models",
      "llm_evidence_cn": "通过视觉语言模型中的令牌旁路实现高效推理",
      "llm_evidence": "通过视觉语言模型中的令牌旁路实现高效推理",
      "llm_tldr_en": "Introduces a cross-layer token bypass mechanism to improve the efficiency and accuracy of VLM inference.",
      "llm_tldr_cn": "引入跨层令牌旁路机制，提高视觉语言模型推理的效率和准确性。",
      "llm_tldr": "引入跨层令牌旁路机制，提高视觉语言模型推理的效率和准确性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2602.03134v1",
      "carry_days": 1
    },
    {
      "id": "2602.02780v2",
      "title": "A Study of Adaptive Modeling Towards Robust Generalization",
      "abstract": "Large language models (LLMs) increasingly support reasoning over biomolecular structures, but most existing approaches remain modality-specific and rely on either sequence-style encodings or fixed-length connector tokens for structural inputs. These designs can under-expose explicit geometric cues and impose rigid fusion bottlenecks, leading to over-compression and poor token allocation as structural complexity grows. We present a unified all-atom framework that grounds language reasoning in geometric information while adaptively scaling structural tokens. The method first constructs variable-size structural patches on molecular graphs using an instruction-conditioned gating policy, enabling complexity-aware allocation of query tokens. It then refines the resulting patch tokens via cross-attention with modality embeddings and injects geometry-informed tokens into the language model to improve structure grounding and reduce structural hallucinations. Across diverse all-atom benchmarks, the proposed approach yields consistent gains in heterogeneous structure-grounded reasoning. An anonymized implementation is provided in the supplementary material.",
      "authors": [
        "Zihao Jing",
        "Qiuhao Zeng",
        "Ruiyi Fang",
        "Yan Yi Li",
        "Yan Sun",
        "Boyu Wang",
        "Pingzhao Hu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-02T20:35:44+00:00",
      "link": "https://arxiv.org/pdf/2602.02780v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Reasoning over complex structural data",
      "llm_evidence_cn": "对复杂结构化数据的推理",
      "llm_evidence": "对复杂结构化数据的推理",
      "llm_tldr_en": "Develops an all-atom framework for LLMs to reason over biomolecular structures using adaptive token allocation.",
      "llm_tldr_cn": "开发了一个全原子框架，使大模型能够通过自适应Token分配对生物分子结构进行推理。",
      "llm_tldr": "开发了一个全原子框架，使大模型能够通过自适应Token分配对生物分子结构进行推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.02780v2",
      "carry_days": 1
    },
    {
      "id": "2601.10343v2",
      "title": "OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding",
      "abstract": "Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.",
      "authors": [
        "Deming Ding",
        "Shichun Liu",
        "Enhui Yang",
        "Jiahang Lin",
        "Ziying Chen",
        "Shihan Dou",
        "Honglin Guo",
        "Weiyu Cheng",
        "Pengyu Zhao",
        "Chengjun Xiao",
        "Qunhong Zeng",
        "Qi Zhang",
        "Xuanjing Huang",
        "Qidi Xu",
        "Tao Gui"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-15T12:36:08+00:00",
      "link": "https://arxiv.org/pdf/2601.10343v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "benchmarking instruction following in agentic coding tasks",
      "llm_evidence_cn": "基准测试智能编程任务中的指令遵循能力",
      "llm_evidence": "基准测试智能编程任务中的指令遵循能力",
      "llm_tldr_en": "OctoBench evaluates how well LLM agents follow complex, scaffold-specified instructions in repository coding.",
      "llm_tldr_cn": "OctoBench基准测试评估了LLM智能体在仓库级编程中遵循复杂脚手架指令的能力。",
      "llm_tldr": "OctoBench基准测试评估了LLM智能体在仓库级编程中遵循复杂脚手架指令的能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "paper_id": "2601.10343v2",
      "carry_days": 1
    },
    {
      "id": "2601.06944v1",
      "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
      "abstract": "While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.",
      "authors": [
        "Yuhang Su",
        "Mei Wang",
        "Yaoyao Zhong",
        "Guozhang Li",
        "Shixing Li",
        "Yihan Feng",
        "Hua Huang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-11T15:08:05+00:00",
      "link": "https://arxiv.org/pdf/2601.06944v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Evaluating MLLMs on complex structural and metacognitive reasoning",
      "llm_evidence_cn": "评估多模态大模型在复杂结构和元认知推理上的表现",
      "llm_evidence": "评估多模态大模型在复杂结构和元认知推理上的表现",
      "llm_tldr_en": "A benchmark for diagnosing errors in hand-drawn diagrams using multimodal large language models.",
      "llm_tldr_cn": "一个利用多模态大语言模型对手绘图表进行错误诊断和评分的基准测试。",
      "llm_tldr": "一个利用多模态大语言模型对手绘图表进行错误诊断和评分的基准测试。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.06944v1",
      "carry_days": 1
    },
    {
      "id": "2601.19311v1",
      "title": "Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems",
      "abstract": "As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising responsiveness and output quality in a multi-agent, real-world environments. We conduct a comparative analysis across language models of varying scales to quantify trade-offs between efficiency and performance. Results show that smaller open-weights models can lower energy usage while preserving task quality. Building on these findings, we propose practical guidelines for sustainable artificial intelligence design, including optimal batch size configuration and computation resource allocation. These insights offer actionable strategies for developing scalable, environmentally responsible artificial intelligence systems.",
      "authors": [
        "Anh Khoa Ngo Ho",
        "Martin Chauvin",
        "Simon Gosset",
        "Philippe Cordier",
        "Boris Gamazaychikov"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-27T07:49:55+00:00",
      "link": "https://arxiv.org/pdf/2601.19311v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Efficiency and performance trade-offs in small-scale LLMs",
      "llm_evidence_cn": "小规模大模型的效率与性能权衡",
      "llm_evidence": "小规模大模型的效率与性能权衡",
      "llm_tldr_en": "Investigates the role of small-scale LLMs in reducing energy consumption for sustainable agentic AI systems.",
      "llm_tldr_cn": "研究了小规模语言模型在保持性能的同时降低能耗的作用，旨在构建可持续的智能体系统。",
      "llm_tldr": "研究了小规模语言模型在保持性能的同时降低能耗的作用，旨在构建可持续的智能体系统。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.19311v1",
      "carry_days": 1
    },
    {
      "id": "2602.16702v1",
      "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
      "abstract": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \\emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.",
      "authors": [
        "Mingjia Shi",
        "Yinhan He",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-18T18:49:56+00:00",
      "link": "https://arxiv.org/pdf/2602.16702v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Reasoning and problem solving in vision-language models via multi-route thinking",
      "llm_evidence_cn": "通过多路径思考提升视觉语言模型的推理与问题解决能力",
      "llm_evidence": "通过多路径思考提升视觉语言模型的推理与问题解决能力",
      "llm_tldr_en": "Proposes a saliency-aware multi-route thinking approach to improve reasoning in vision-language models.",
      "llm_tldr_cn": "提出显著性感知多路径思考方法，优化视觉语言模型的推理性能。",
      "llm_tldr": "提出显著性感知多路径思考方法，优化视觉语言模型的推理性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2602.16702v1",
      "carry_days": 1
    },
    {
      "id": "2602.03496v1",
      "title": "Lookahead Path Likelihood Optimization for Diffusion LLMs",
      "abstract": "Diffusion Large Language Models (dLLMs) support arbitrary-order generation, yet their inference performance critically depends on the unmasking order. Existing strategies rely on heuristics that greedily optimize local confidence, offering limited guidance for identifying unmasking paths that are globally consistent and accurate. To bridge this gap, we introduce path log-likelihood (Path LL), a trajectory-conditioned objective that strongly correlates with downstream accuracy and enables principled selection of unmasking paths. To optimize Path LL at inference time, we propose POKE, an efficient value estimator that predicts the expected future Path LL of a partial decoding trajectory. We then integrate this lookahead signal into POKE-SMC, a Sequential Monte Carlo-based search framework for dynamically identifying optimal unmasking paths. Extensive experiments across 6 reasoning tasks show that POKE-SMC consistently improves accuracy, achieving 2%--3% average gains over strong decoding-time scaling baselines at comparable inference overhead on LLaDA models and advancing the accuracy--compute Pareto frontier.",
      "authors": [
        "Xuejie Liu",
        "Yap Vit Chun",
        "Yitao Liang",
        "Anji Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03T13:12:41+00:00",
      "link": "https://arxiv.org/pdf/2602.03496v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Inference performance and path optimization for Diffusion LLMs",
      "llm_evidence_cn": "扩散LLM的推理性能与路径优化",
      "llm_evidence": "扩散LLM的推理性能与路径优化",
      "llm_tldr_en": "Introduces Path LL and POKE to optimize unmasking paths for better inference in Diffusion LLMs.",
      "llm_tldr_cn": "引入Path LL和POKE，通过优化去掩码路径提升扩散LLM的推理效果。",
      "llm_tldr": "引入Path LL和POKE，通过优化去掩码路径提升扩散LLM的推理效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Large Language Models research papers and surveys",
      "matched_requirement_id": "req-1",
      "paper_id": "2602.03496v1",
      "carry_days": 1
    },
    {
      "id": "2601.05746v1",
      "title": "DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation",
      "abstract": "Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.",
      "authors": [
        "Zhenghao Li",
        "Zhi Zheng",
        "Wei Chen",
        "Jielun Zhao",
        "Yong Chen",
        "Tong Xu",
        "Enhong Chen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-09T12:01:33+00:00",
      "link": "https://arxiv.org/pdf/2601.05746v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Enhancing reasoning and collaboration in LLM-based multi-agent systems",
      "llm_evidence_cn": "增强基于LLM的多智能体系统的推理与协作能力",
      "llm_evidence": "增强基于LLM的多智能体系统的推理与协作能力",
      "llm_tldr_en": "Introduces DynaDebate to improve multi-agent reasoning by breaking homogeneity in debate paths.",
      "llm_tldr_cn": "提出DynaDebate，通过打破辩论路径的同质化来提升多智能体推理能力。",
      "llm_tldr": "提出DynaDebate，通过打破辩论路径的同质化来提升多智能体推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "paper_id": "2601.05746v1",
      "carry_days": 1
    }
  ]
}