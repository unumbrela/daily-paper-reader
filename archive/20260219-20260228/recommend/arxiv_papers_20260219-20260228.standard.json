{
  "mode": "standard",
  "generated_at": "2026-02-28T04:08:40.072702+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 1,
    "deep_divecandidates": 230,
    "deep_cap": 6,
    "deep_selected": 6,
    "quick_candidates": 246,
    "quick_skim_target": 11,
    "quick_selected": 11
  },
  "deep_dive": [
    {
      "id": "2601.06799v1",
      "title": "CIRAG: Construction-Integration Retrieval and Adaptive Generation for Multi-hop Question Answering",
      "abstract": "Triple-based Iterative Retrieval-Augmented Generation (iRAG) mitigates document-level noise for multi-hop question answering. However, existing methods still face limitations: (i) greedy single-path expansion, which propagates early errors and fails to capture parallel evidence from different reasoning branches, and (ii) granularity-demand mismatch, where a single evidence representation struggles to balance noise control with contextual sufficiency. In this paper, we propose the Construction-Integration Retrieval and Adaptive Generation model, CIRAG. It introduces an Iterative Construction-Integration module that constructs candidate triples and history-conditionally integrates them to distill core triples and generate the next-hop query. This module mitigates the greedy trap by preserving multiple plausible evidence chains. Besides, we propose an Adaptive Cascaded Multi-Granularity Generation module that progressively expands contextual evidence based on the problem requirements, from triples to supporting sentences and full passages. Moreover, we introduce Trajectory Distillation, which distills the teacher model's integration policy into a lightweight student, enabling efficient and reliable long-horizon reasoning. Extensive experiments demonstrate that CIRAG achieves superior performance compared to existing iRAG methods.",
      "authors": [
        "Zili Wei",
        "Xiaocui Yang",
        "Yilin Wang",
        "Zihan Wang",
        "Weidong Bao",
        "Shi Feng",
        "Daling Wang",
        "Yifei Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-11T07:56:02+00:00",
      "link": "https://arxiv.org/pdf/2601.06799v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Iterative Retrieval-Augmented Generation for multi-hop question answering",
      "llm_evidence_cn": "用于多跳问答的迭代检索增强生成技术",
      "llm_evidence": "用于多跳问答的迭代检索增强生成技术",
      "llm_tldr_en": "Proposes CIRAG, a model that improves multi-hop RAG through iterative triple construction and adaptive generation.",
      "llm_tldr_cn": "提出了 CIRAG 模型，通过迭代三元组构建和自适应生成改进了多跳检索增强生成。",
      "llm_tldr": "提出了 CIRAG 模型，通过迭代三元组构建和自适应生成改进了多跳检索增强生成。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11"
    },
    {
      "id": "2601.06842v1",
      "title": "Seeing through the Conflict: Transparent Knowledge Conflict Handling in Retrieval-Augmented Generation",
      "abstract": "Large language models (LLMs) equipped with retrieval--the Retrieval-Augmented Generation (RAG) paradigm--should combine their parametric knowledge with external evidence, yet in practice they often hallucinate, over-trust noisy snippets, or ignore vital context. We introduce TCR (Transparent Conflict Resolution), a plug-and-play framework that makes this decision process observable and controllable. TCR (i) disentangles semantic match and factual consistency via dual contrastive encoders, (ii) estimates self-answerability to gauge confidence in internal memory, and (iii) feeds the three scalar signals to the generator through a lightweight soft-prompt with SNR-based weighting. Across seven benchmarks TCR improves conflict detection (+5-18 F1), raises knowledge-gap recovery by +21.4 pp and cuts misleading-context overrides by -29.3 pp, while adding only 0.3% parameters. The signals align with human judgements and expose temporal decision patterns.",
      "authors": [
        "Hua Ye",
        "Siyuan Chen",
        "Ziqi Zhong",
        "Canran Xiao",
        "Haoliang Zhang",
        "Yuhan Wu",
        "Fei Shen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-11T10:08:49+00:00",
      "link": "https://arxiv.org/pdf/2601.06842v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "handling knowledge conflict in RAG",
      "llm_evidence_cn": "处理检索增强生成中的知识冲突",
      "llm_evidence": "处理检索增强生成中的知识冲突",
      "llm_tldr_en": "Introduces TCR to resolve conflicts between LLM parametric knowledge and retrieved external evidence.",
      "llm_tldr_cn": "提出TCR框架，通过透明的冲突解决机制平衡大模型内部知识与外部检索信息。",
      "llm_tldr": "提出TCR框架，通过透明的冲突解决机制平衡大模型内部知识与外部检索信息。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11"
    },
    {
      "id": "2601.07199v1",
      "title": "Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization",
      "abstract": "Large language models exhibit impressive reasoning capabilities yet frequently generate plausible but incorrect solutions, a phenomenon commonly termed hallucination. This paper investigates the effect of training objective composition on reasoning reliability through Direct Preference Optimization. Two complementary training signals are examined: forward chain-of-thought generation, which trains the model to produce correct reasoning traces, and backward verification, which trains the model to verify and acknowledge errors in candidate solutions. Experiments on GSM8K reveal a fundamental trade-off between these objectives. Forward-only DPO training achieves the highest accuracy improvement, increasing from 83.1% to 86.6% (+3.5 percentage points), while backward-only training yields minimal accuracy gains but substantially reduces the false positive rate from 13.4% to 4.3%. Notably, both training variants reduce acknowledgement rate compared to the baseline, suggesting that preference optimization increases model confidence in its outputs. These findings indicate that forward and backward reasoning objectives provide distinct and complementary learning signals: forward training improves problem-solving capability, while backward training improves verification calibration. The complete training and evaluation pipeline, implemented efficiently through Low-Rank Adaptation, is released to facilitate further research.",
      "authors": [
        "Murtaza Nikzad",
        "Raghuram Ramanujan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-12T04:46:27+00:00",
      "link": "https://arxiv.org/pdf/2601.07199v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "comparing reasoning objectives in Direct Preference Optimization",
      "llm_evidence_cn": "比较直接偏好优化中的推理目标",
      "llm_evidence": "比较直接偏好优化中的推理目标",
      "llm_tldr_en": "Analyzes forward reasoning versus backward verification in DPO to improve LLM reasoning reliability.",
      "llm_tldr_cn": "分析了DPO中前向推理与后向验证的权衡，以提高大语言模型的推理可靠性。",
      "llm_tldr": "分析了DPO中前向推理与后向验证的权衡，以提高大语言模型的推理可靠性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9"
    },
    {
      "id": "2601.08620v1",
      "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
      "abstract": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.",
      "authors": [
        "António Loison",
        "Quentin Macé",
        "Antoine Edy",
        "Victor Xing",
        "Tom Balough",
        "Gabriel Moreira",
        "Bo Liu",
        "Manuel Faysse",
        "Céline Hudelot",
        "Gautier Viaud"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-01-13T15:00:33+00:00",
      "link": "https://arxiv.org/pdf/2601.08620v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Comprehensive evaluation of Retrieval Augmented Generation (RAG) in complex scenarios",
      "llm_evidence_cn": "复杂场景下检索增强生成（RAG）的全面评估",
      "llm_evidence": "复杂场景下检索增强生成（RAG）的全面评估",
      "llm_tldr_en": "Introduces ViDoRe v3, a multimodal RAG benchmark for evaluating retrieval and generation in visually rich documents.",
      "llm_tldr_cn": "引入了 ViDoRe v3，这是一个用于评估视觉丰富文档中检索与生成的模型多模态 RAG 基准。",
      "llm_tldr": "引入了 ViDoRe v3，这是一个用于评估视觉丰富文档中检索与生成的模型多模态 RAG 基准。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5"
    },
    {
      "id": "2601.08670v1",
      "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
      "abstract": "Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.",
      "authors": [
        "Giulio Corallo",
        "Paolo Papotti"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-13T15:46:59+00:00",
      "link": "https://arxiv.org/pdf/2601.08670v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Parallel decoding framework for Retrieval Augmented Generation",
      "llm_evidence_cn": "检索增强生成的并行解码框架",
      "llm_evidence": "检索增强生成的并行解码框架",
      "llm_tldr_en": "Introduces Pced, a training-free decoding rule that enables efficient multi-document reasoning in RAG systems.",
      "llm_tldr_cn": "引入Pced，一种无需训练的解码规则，可实现RAG系统中高效的多文档推理。",
      "llm_tldr": "引入Pced，一种无需训练的解码规则，可实现RAG系统中高效的多文档推理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5"
    },
    {
      "id": "2601.08747v2",
      "title": "To Retrieve or To Think? An Agentic Approach for Context Evolution",
      "abstract": "Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks. However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting. It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption. Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.",
      "authors": [
        "Rubing Chen",
        "Jian Wang",
        "Wenjie Li",
        "Xiao-Yong Wei",
        "Qing Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-13T17:25:57+00:00",
      "link": "https://arxiv.org/pdf/2601.08747v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "agentic approach for retrieval-augmented generation",
      "llm_evidence_cn": "检索增强生成的智能体方法",
      "llm_evidence": "检索增强生成的智能体方法",
      "llm_tldr_en": "Proposes ACE, a framework that dynamically decides when to retrieve or reason to improve RAG efficiency.",
      "llm_tldr_cn": "提出ACE框架，动态决定检索或推理时机以提升RAG效率。",
      "llm_tldr": "提出ACE框架，动态决定检索或推理时机以提升RAG效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5"
    }
  ],
  "quick_skim": [
    {
      "id": "2601.09253v1",
      "title": "RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning",
      "abstract": "While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.",
      "authors": [
        "Zehua Liu",
        "Shuqi Liu",
        "Tao Zhong",
        "Mingxuan Yuan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-14T07:41:03+00:00",
      "link": "https://arxiv.org/pdf/2601.09253v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "LLM alignment and reward-informed fine-tuning",
      "llm_evidence_cn": "大语言模型对齐与奖励信息微调",
      "llm_evidence": "大语言模型对齐与奖励信息微调",
      "llm_tldr_en": "Proposes RIFT to improve LLM alignment by utilizing both positive and negative samples with scalar rewards.",
      "llm_tldr_cn": "提出RIFT框架，通过标量奖励利用正负样本改进大语言模型对齐效率。",
      "llm_tldr": "提出RIFT框架，通过标量奖励利用正负样本改进大语言模型对齐效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.05633v1",
      "title": "GIFT: Games as Informal Training for Generalizable LLMs",
      "abstract": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.",
      "authors": [
        "Nuoyan Lyu",
        "Bingbing Xu",
        "Weihao Meng",
        "Yige Yuan",
        "Yang Zhang",
        "Zhiyong Huang",
        "Tat-Seng Chua",
        "Huawei Shen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-09T08:42:44+00:00",
      "link": "https://arxiv.org/pdf/2601.05633v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "informal training for generalizable LLMs using games",
      "llm_evidence_cn": "利用游戏对通用LLM进行非正式训练",
      "llm_evidence": "利用游戏对通用LLM进行非正式训练",
      "llm_tldr_en": "Uses games as a training environment to improve LLM strategic creativity and social reasoning.",
      "llm_tldr_cn": "将游戏作为训练环境，以提升LLM的策略创造力和社交推理能力。",
      "llm_tldr": "将游戏作为训练环境，以提升LLM的策略创造力和社交推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Advances in LLM pre-training and instruction tuning",
      "matched_requirement_id": "req-8",
      "quick_tier": "7"
    },
    {
      "id": "2602.01574v1",
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.",
      "authors": [
        "Haobo Wang",
        "Weiqi Luo",
        "Xiaojun Jia",
        "Xiaochun Cao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-02T03:10:41+00:00",
      "link": "https://arxiv.org/pdf/2602.01574v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Alignment of vision-language models and adversarial robustness",
      "llm_evidence_cn": "视觉语言模型的对齐与对抗鲁棒性",
      "llm_evidence": "视觉语言模型的对齐与对抗鲁棒性",
      "llm_tldr_en": "Proposes a hierarchical alignment framework to improve the transferable targeted attack robustness of VLMs.",
      "llm_tldr_cn": "提出一种层次化对齐框架，旨在提升视觉语言模型在迁移定向攻击中的鲁棒性。",
      "llm_tldr": "提出一种层次化对齐框架，旨在提升视觉语言模型在迁移定向攻击中的鲁棒性。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "LLM alignment techniques including RLHF and DPO",
      "matched_requirement_id": "req-9",
      "quick_tier": "6"
    },
    {
      "id": "2601.09527v1",
      "title": "Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs",
      "abstract": "SMEs increasingly seek alternatives to cloud LLM APIs, which raise data privacy concerns. Dedicated cloud GPU instances offer improved privacy but with limited guarantees and ongoing costs, while professional on-premise hardware (A100, H100) remains prohibitively expensive. We present a systematic evaluation of NVIDIA's Blackwell consumer GPUs (RTX 5060 Ti, 5070 Ti, 5090) for production LLM inference, benchmarking four open-weight models (Qwen3-8B, Gemma3-12B, Gemma3-27B, GPT-OSS-20B) across 79 configurations spanning quantization formats (BF16, W4A16, NVFP4, MXFP4), context lengths (8k-64k), and three workloads: RAG, multi-LoRA agentic serving, and high-concurrency APIs. The RTX 5090 delivers 3.5-4.6x higher throughput than the 5060 Ti with 21x lower latency for RAG, but budget GPUs achieve the highest throughput-per-dollar for API workloads with sub-second latency. NVFP4 quantization provides 1.6x throughput over BF16 with 41% energy reduction and only 2-4% quality loss. Self-hosted inference costs $0.001-0.04 per million tokens (electricity only), which is 40-200x cheaper than budget-tier cloud APIs, with hardware breaking even in under four months at moderate volume (30M tokens/day). Our results show that consumer GPUs can reliably replace cloud inference for most SME workloads, except latency-critical long-context RAG, where high-end GPUs remain essential. We provide deployment guidance and release all benchmark data for reproducible SME-scale deployments.",
      "authors": [
        "Jonathan Knoop",
        "Hendrik Holtmann"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "published": "2026-01-14T14:49:07+00:00",
      "link": "https://arxiv.org/pdf/2601.09527v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "LLM inference benchmarking on consumer GPUs",
      "llm_evidence_cn": "消费级 GPU 上的大模型推理基准测试",
      "llm_evidence": "消费级 GPU 上的大模型推理基准测试",
      "llm_tldr_en": "Evaluates NVIDIA Blackwell consumer GPUs for cost-effective local LLM deployment and inference.",
      "llm_tldr_cn": "评估英伟达 Blackwell 消费级 GPU 在高性价比本地大模型部署与推理中的表现。",
      "llm_tldr": "评估英伟达 Blackwell 消费级 GPU 在高性价比本地大模型部署与推理中的表现。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.06599v1",
      "title": "How Context Shapes Truth: Geometric Transformations of Statement-level Truth Representations in LLMs",
      "abstract": "Large Language Models (LLMs) often encode whether a statement is true as a vector in their residual stream activations. These vectors, also known as truth vectors, have been studied in prior work, however how they change when context is introduced remains unexplored. We study this question by measuring (1) the directional change ($θ$) between the truth vectors with and without context and (2) the relative magnitude of the truth vectors upon adding context. Across four LLMs and four datasets, we find that (1) truth vectors are roughly orthogonal in early layers, converge in middle layers, and may stabilize or continue increasing in later layers; (2) adding context generally increases the truth vector magnitude, i.e., the separation between true and false representations in the activation space is amplified; (3) larger models distinguish relevant from irrelevant context mainly through directional change ($θ$), while smaller models show this distinction through magnitude differences. We also find that context conflicting with parametric knowledge produces larger geometric changes than parametrically aligned context. To the best of our knowledge, this is the first work that provides a geometric characterization of how context transforms the truth vector in the activation space of LLMs.",
      "authors": [
        "Shivam Adarsh",
        "Maria Maistro",
        "Christina Lioma"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-10T15:43:26+00:00",
      "link": "https://arxiv.org/pdf/2601.06599v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Analysis of truth representations in LLM activations",
      "llm_evidence_cn": "LLM激活中真值表示的分析",
      "llm_evidence": "LLM激活中真值表示的分析",
      "llm_tldr_en": "Studies how context influences the geometric representation of truth within LLM internal activations.",
      "llm_tldr_cn": "研究了上下文如何影响LLM内部激活中真值向量的几何表示。",
      "llm_tldr": "研究了上下文如何影响LLM内部激活中真值向量的几何表示。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Large Language Models research papers and surveys",
      "matched_requirement_id": "req-1",
      "quick_tier": "7"
    },
    {
      "id": "2601.09865v1",
      "title": "Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment",
      "abstract": "Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.",
      "authors": [
        "Jacob Sander",
        "Brian Jalaian",
        "Venkat R. Dasari"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-14T20:50:30+00:00",
      "link": "https://arxiv.org/pdf/2601.09865v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "quantization and distillation for LLM deployment",
      "llm_evidence_cn": "LLM部署的量化与蒸馏",
      "llm_evidence": "LLM部署的量化与蒸馏",
      "llm_tldr_en": "Combines quantization, LoRA, and distillation to optimize LLMs for resource-constrained deployment.",
      "llm_tldr_cn": "结合量化、LoRA和蒸馏技术，优化LLM以实现在资源受限环境下的部署。",
      "llm_tldr": "结合量化、LoRA和蒸馏技术，优化LLM以实现在资源受限环境下的部署。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.07645v1",
      "title": "PlaM: Training-Free Plateau-Guided Model Merging for Better Visual Grounding in MLLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) rely on strong linguistic reasoning inherited from their base language models. However, multimodal instruction fine-tuning paradoxically degrades this text's reasoning capability, undermining multimodal performance. To address this issue, we propose a training-free framework to mitigate this degradation. Through layer-wise vision token masking, we reveal a common three-stage pattern in multimodal large language models: early-modal separation, mid-modal alignment, and late-modal degradation. By analyzing the behavior of MLLMs at different stages, we propose a plateau-guided model merging method that selectively injects base language model parameters into MLLMs. Experimental results based on five MLLMs on nine benchmarks demonstrate the effectiveness of our method. Attention-based analysis further reveals that merging shifts attention from diffuse, scattered patterns to focused localization on task-relevant visual regions. Our repository is on https://github.com/wzj1718/PlaM.",
      "authors": [
        "Zijing Wang",
        "Yongkang Liu",
        "Mingyang Wang",
        "Ercong Nie",
        "Deyuan Chen",
        "Zhengjie Zhao",
        "Shi Feng",
        "Daling Wang",
        "Xiaocui Yang",
        "Yifei Zhang",
        "Hinrich Schütze"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-12T15:27:51+00:00",
      "link": "https://arxiv.org/pdf/2601.07645v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Multimodal instruction fine-tuning and model merging",
      "llm_evidence_cn": "多模态指令微调与模型合并",
      "llm_evidence": "多模态指令微调与模型合并",
      "llm_tldr_en": "Proposes a training-free model merging method to preserve reasoning during multimodal fine-tuning.",
      "llm_tldr_cn": "提出了一种无需训练的模型合并方法，以在多模态微调期间保留推理能力。",
      "llm_tldr": "提出了一种无需训练的模型合并方法，以在多模态微调期间保留推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "quick_tier": "7"
    },
    {
      "id": "2601.10064v1",
      "title": "Long-Chain Reasoning Distillation via Adaptive Prefix Alignment",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.",
      "authors": [
        "Zhenghao Liu",
        "Zhuoyang Wu",
        "Xinze Li",
        "Yukun Yan",
        "Shuo Wang",
        "Zulong Chen",
        "Yu Gu",
        "Ge Yu",
        "Maosong Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-15T04:40:45+00:00",
      "link": "https://arxiv.org/pdf/2601.10064v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "distilling long-chain reasoning trajectories",
      "llm_evidence_cn": "蒸馏长链条推理轨迹",
      "llm_evidence": "蒸馏长链条推理轨迹",
      "llm_tldr_en": "Introduces P-ALIGN to enhance small model reasoning by adaptively aligning with teacher model reasoning chains.",
      "llm_tldr_cn": "提出P-ALIGN框架，通过自适应前缀对齐将教师模型的长链条推理能力蒸馏至小模型。",
      "llm_tldr": "提出P-ALIGN框架，通过自适应前缀对齐将教师模型的长链条推理能力蒸馏至小模型。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.08113v1",
      "title": "Coordinated Cooling and Compute Management for AI Datacenters",
      "abstract": "The AI datacenters are currently being deployed on a large scale to support the training and deployment of power-intensive large-language models (LLMs). Extensive amount of computation and cooling required in datacenters increase concerns about the energy use and carbon emissions of AI datacenters. Although current state-of-the-art has examined the energy efficiency of LLM inference, most prior research focused on optimizing compute-side scheduling without considering thermal objectives or constraints. Since GPU-intensive inference generates substantial heat that can degrade datacenter performance, ignoring thermal effects can increase total energy consumption and reduce the efficiency of LLM serving. To fill this gap, we profile the characteristics of GPU servers under varying cooling and AI jobs, and develop a joint cooling and computing modeling approach for AI datacenters. Built upon such workload and thermal dynamics models, a novel hierarchical control framework is proposed to co-optimize computing and thermal management by identifying the optimal GPU parallelism, frequency (DVFS), and cooling control knobs. Using real Azure inference traces and detailed GPU profiling, our model balances serving latency and thermal constraints in AI datacenters while significantly improving AI datacenters' energy efficiency.",
      "authors": [
        "Nardos Belay Abera",
        "Yize Chen"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.DC"
      ],
      "published": "2026-01-13T01:07:02+00:00",
      "link": "https://arxiv.org/pdf/2601.08113v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Cooling and compute management for LLM serving",
      "llm_evidence_cn": "大模型推理服务的冷却与计算协同管理",
      "llm_evidence": "大模型推理服务的冷却与计算协同管理",
      "llm_tldr_en": "Optimizes LLM inference efficiency by coordinating GPU compute scheduling with datacenter thermal constraints.",
      "llm_tldr_cn": "通过协同优化GPU计算调度与数据中心热能管理，提升大模型推理服务的能源效率。",
      "llm_tldr": "通过协同优化GPU计算调度与数据中心热能管理，提升大模型推理服务的能源效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "quick_tier": "7"
    },
    {
      "id": "2601.10825v1",
      "title": "Reasoning Models Generate Societies of Thought",
      "abstract": "Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.",
      "authors": [
        "Junsol Kim",
        "Shiyang Lai",
        "Nino Scherrer",
        "Blaise Agüera y Arcas",
        "James Evans"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "published": "2026-01-15T19:52:33+00:00",
      "link": "https://arxiv.org/pdf/2601.10825v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "mechanisms of chain of thought and reasoning in LLMs",
      "llm_evidence_cn": "LLM中思维链与推理的机制",
      "llm_evidence": "LLM中思维链与推理的机制",
      "llm_tldr_en": "Explores how reasoning models like DeepSeek-R1 emerge through internal multi-agent-like interactions in CoT traces.",
      "llm_tldr_cn": "探索DeepSeek-R1等推理模型如何通过思维链轨迹中的内部多智能体交互产生推理能力。",
      "llm_tldr": "探索DeepSeek-R1等推理模型如何通过思维链轨迹中的内部多智能体交互产生推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.11024v1",
      "title": "PruneRAG: Confidence-Guided Query Decomposition Trees for Efficient Retrieval-Augmented Generation",
      "abstract": "Retrieval-augmented generation (RAG) has become a powerful framework for enhancing large language models in knowledge-intensive and reasoning tasks. However, as reasoning chains deepen or search trees expand, RAG systems often face two persistent failures: evidence forgetting, where retrieved knowledge is not effectively used, and inefficiency, caused by uncontrolled query expansions and redundant retrieval. These issues reveal a critical gap between retrieval and evidence utilization in current RAG architectures. We propose PruneRAG, a confidence-guided query decomposition framework that builds a structured query decomposition tree to perform stable and efficient reasoning. PruneRAG introduces three key mechanisms: adaptive node expansion that regulates tree width and depth, confidence-guided decisions that accept reliable answers and prune uncertain branches, and fine-grained retrieval that extracts entity-level anchors to improve retrieval precision. Together, these components preserve salient evidence throughout multi-hop reasoning while significantly reducing retrieval overhead. To better analyze evidence misuse, we define the Evidence Forgetting Rate as a metric to quantify cases where golden evidence is retrieved but not correctly used. Extensive experiments across various multi-hop QA benchmarks show that PruneRAG achieves superior accuracy and efficiency over state-of-the-art baselines.",
      "authors": [
        "Shuguang Jiao",
        "Xinyu Xiao",
        "Yunfan Wei",
        "Shuhan Qi",
        "Chengkai Huang",
        "Quan Z. Michael Sheng",
        "Lina Yao"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-01-16T06:38:17+00:00",
      "link": "https://arxiv.org/pdf/2601.11024v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "query decomposition for efficient Retrieval-Augmented Generation",
      "llm_evidence_cn": "用于高效检索增强生成的查询分解",
      "llm_evidence": "用于高效检索增强生成的查询分解",
      "llm_tldr_en": "Proposes PruneRAG, a confidence-guided framework to improve reasoning and efficiency in RAG systems.",
      "llm_tldr_cn": "提出PruneRAG框架，通过置信度引导的查询分解提升RAG系统的推理能力和效率。",
      "llm_tldr": "提出PruneRAG框架，通过置信度引导的查询分解提升RAG系统的推理能力和效率。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "quick_tier": "8plus"
    }
  ]
}