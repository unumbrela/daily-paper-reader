{
  "mode": "standard",
  "generated_at": "2026-02-28T18:59:03.928283+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 1,
    "deep_divecandidates": 242,
    "deep_cap": 6,
    "deep_selected": 6,
    "quick_candidates": 254,
    "quick_skim_target": 11,
    "quick_selected": 11
  },
  "deep_dive": [
    {
      "id": "2601.05616v1",
      "title": "Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks",
      "abstract": "In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.",
      "authors": [
        "ShaoZhen Liu",
        "Xinting Huang",
        "Houwen Peng",
        "Xin Chen",
        "Xinyang Song",
        "Qi Li",
        "Zhenan Sun"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-09T08:19:11+00:00",
      "link": "https://arxiv.org/pdf/2601.05616v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Self-generated long chain-of-thought (CoT) data for reasoning",
      "llm_evidence_cn": "用于推理的自生成长思维链 (CoT) 数据",
      "llm_evidence": "用于推理的自生成长思维链 (CoT) 数据",
      "llm_tldr_en": "Enhances mathematical reasoning via a two-stage SFT framework using self-evolved CoT data.",
      "llm_tldr_cn": "通过两阶段微调框架和自进化思维链数据提升模型的数学推理能力。",
      "llm_tldr": "通过两阶段微调框架和自进化思维链数据提升模型的数学推理能力。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6"
    },
    {
      "id": "2601.06002v2",
      "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
      "abstract": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
      "authors": [
        "Qiguang Chen",
        "Yantao Du",
        "Ziniu Li",
        "Jinhao Liu",
        "Songyao Duan",
        "Jiarui Guo",
        "Minghao Liu",
        "Jiaheng Liu",
        "Tong Yang",
        "Ge Zhang",
        "Libo Qin",
        "Wanxiang Che",
        "Wenhao Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-09T18:39:01+00:00",
      "link": "https://arxiv.org/pdf/2601.06002v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Mapping the topology and structure of Long Chain-of-Thought reasoning",
      "llm_evidence_cn": "映射长链条思维导图的拓扑结构与推理路径",
      "llm_evidence": "映射长链条思维导图的拓扑结构与推理路径",
      "llm_tldr_en": "Analyzes the structural properties of Long CoT trajectories to understand how LLMs learn complex reasoning.",
      "llm_tldr_cn": "通过分析长思维链轨迹的分子状结构，揭示了大模型学习复杂推理的内在机制。",
      "llm_tldr": "通过分析长思维链轨迹的分子状结构，揭示了大模型学习复杂推理的内在机制。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6"
    },
    {
      "id": "2601.08584v1",
      "title": "Ministral 3",
      "abstract": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",
      "authors": [
        "Alexander H. Liu",
        "Kartik Khandelwal",
        "Sandeep Subramanian",
        "Victor Jouault",
        "Abhinav Rastogi",
        "Adrien Sadé",
        "Alan Jeffares",
        "Albert Jiang",
        "Alexandre Cahill",
        "Alexandre Gavaudan",
        "Alexandre Sablayrolles",
        "Amélie Héliou",
        "Amos You",
        "Andy Ehrenberg",
        "Andy Lo",
        "Anton Eliseev",
        "Antonia Calvi",
        "Avinash Sooriyarachchi",
        "Baptiste Bout",
        "Baptiste Rozière",
        "Baudouin De Monicault",
        "Clémence Lanfranchi",
        "Corentin Barreau",
        "Cyprien Courtot",
        "Daniele Grattarola",
        "Darius Dabert",
        "Diego de las Casas",
        "Elliot Chane-Sane",
        "Faruk Ahmed",
        "Gabrielle Berrada",
        "Gaëtan Ecrepont",
        "Gauthier Guinet",
        "Georgii Novikov",
        "Guillaume Kunsch",
        "Guillaume Lample",
        "Guillaume Martin",
        "Gunshi Gupta",
        "Jan Ludziejewski",
        "Jason Rute",
        "Joachim Studnia",
        "Jonas Amar",
        "Joséphine Delas",
        "Josselin Somerville Roberts",
        "Karmesh Yadav",
        "Khyathi Chandu",
        "Kush Jain",
        "Laurence Aitchison",
        "Laurent Fainsin",
        "Léonard Blier",
        "Lingxiao Zhao",
        "Louis Martin",
        "Lucile Saulnier",
        "Luyu Gao",
        "Maarten Buyl",
        "Margaret Jennings",
        "Marie Pellat",
        "Mark Prins",
        "Mathieu Poirée",
        "Mathilde Guillaumin",
        "Matthieu Dinot",
        "Matthieu Futeral",
        "Maxime Darrin",
        "Maximilian Augustin",
        "Mia Chiquier",
        "Michel Schimpf",
        "Nathan Grinsztajn",
        "Neha Gupta",
        "Nikhil Raghuraman",
        "Olivier Bousquet",
        "Olivier Duchenne",
        "Patricia Wang",
        "Patrick von Platen",
        "Paul Jacob",
        "Paul Wambergue",
        "Paula Kurylowicz",
        "Pavankumar Reddy Muddireddy",
        "Philomène Chagniot",
        "Pierre Stock",
        "Pravesh Agrawal",
        "Quentin Torroba",
        "Romain Sauvestre",
        "Roman Soletskyi",
        "Rupert Menneer",
        "Sagar Vaze",
        "Samuel Barry",
        "Sanchit Gandhi",
        "Siddhant Waghjale",
        "Siddharth Gandhi",
        "Soham Ghosh",
        "Srijan Mishra",
        "Sumukh Aithal",
        "Szymon Antoniak",
        "Teven Le Scao",
        "Théo Cachet",
        "Theo Simon Sorg",
        "Thibaut Lavril",
        "Thiziri Nait Saada",
        "Thomas Chabal",
        "Thomas Foubert",
        "Thomas Robert",
        "Thomas Wang",
        "Tim Lawson",
        "Tom Bewley",
        "Tom Bewley",
        "Tom Edwards",
        "Umar Jamil",
        "Umberto Tomasini",
        "Valeriia Nemychnikova",
        "Van Phung",
        "Vincent Maladière",
        "Virgile Richard",
        "Wassim Bouaziz",
        "Wen-Ding Li",
        "William Marshall",
        "Xinghui Li",
        "Xinyu Yang",
        "Yassine El Ouahidi",
        "Yihan Wang",
        "Yunhao Tang",
        "Zaccharie Ramzi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-13T14:06:03+00:00",
      "link": "https://arxiv.org/pdf/2601.08584v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "parameter-efficient dense language models and instruction finetuned variants",
      "llm_evidence_cn": "参数高效稠密语言模型及指令微调变体",
      "llm_evidence": "参数高效稠密语言模型及指令微调变体",
      "llm_tldr_en": "Introduces Ministral 3, a family of efficient LLMs with instruction-tuned and reasoning-specialized variants.",
      "llm_tldr_cn": "推出 Ministral 3 系列高效大模型，包含指令微调和推理增强版本。",
      "llm_tldr": "推出 Ministral 3 系列高效大模型，包含指令微调和推理增强版本。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4"
    },
    {
      "id": "2601.17668v1",
      "title": "Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction",
      "abstract": "Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.",
      "authors": [
        "Jang-Hyun Kim",
        "Dongyoon Han",
        "Sangdoo Yun"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-25T03:07:54+00:00",
      "link": "https://arxiv.org/pdf/2601.17668v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "efficient and accurate LLM inference with KV cache eviction",
      "llm_evidence_cn": "通过KV缓存逐出实现高效准确的LLM推理",
      "llm_evidence": "通过KV缓存逐出实现高效准确的LLM推理",
      "llm_tldr_en": "Develops a gated KV cache eviction method to reduce memory overhead during LLM inference.",
      "llm_tldr_cn": "开发了一种门控KV缓存逐出方法，以减少LLM推理过程中的内存开销。",
      "llm_tldr": "开发了一种门控KV缓存逐出方法，以减少LLM推理过程中的内存开销。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.17668v1"
    },
    {
      "id": "2601.19139v2",
      "title": "Native LLM and MLLM Inference at Scale on Apple Silicon",
      "abstract": "The growing adoption of Apple Silicon for machine learning development has created demand for efficient inference solutions that leverage its unique unified memory architecture. However, existing tools either lack native optimization (PyTorch MPS) or focus solely on text models, leaving multimodal workloads underserved. We present vllm-mlx, a framework for efficient LLM and MLLM inference on Apple Silicon built natively on MLX. For text models, we achieve 21\\% to 87\\% higher throughput than llama-cpp across models ranging from Qwen3-0.6B to Nemotron-30B, while providing continuous batching that scales to 4.3x aggregate throughput at 16 concurrent requests. For multimodal models, we introduce content-based prefix caching that eliminates redundant vision encoding by identifying identical images through content hashing, regardless of input format. Our evaluation on Apple M4 Max demonstrates throughput of up to 525 tokens per second on text models and 28x speedup on repeated image queries, reducing multimodal latency from 21.7 seconds to under 1 second. Video analysis with up to 64 frames achieves 24.7x cache speedup. We release our implementation as open source to support efficient inference on consumer Apple hardware.",
      "authors": [
        "Wayner Barrios"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.ET"
      ],
      "published": "2026-01-27T03:11:02+00:00",
      "link": "https://arxiv.org/pdf/2601.19139v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Efficient LLM and MLLM inference optimization on Apple Silicon",
      "llm_evidence_cn": "Apple Silicon上的高效LLM与MLLM推理优化",
      "llm_evidence": "Apple Silicon上的高效LLM与MLLM推理优化",
      "llm_tldr_en": "vllm-mlx optimizes LLM inference throughput and batching natively for Apple Silicon's unified memory.",
      "llm_tldr_cn": "vllm-mlx针对Apple Silicon的统一内存架构优化了LLM推理吞吐量和批处理。",
      "llm_tldr": "vllm-mlx针对Apple Silicon的统一内存架构优化了LLM推理吞吐量和批处理。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Efficient deployment and inference optimization of LLMs",
      "matched_requirement_id": "req-12",
      "paper_id": "2601.19139v2"
    },
    {
      "id": "2602.12278v1",
      "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
      "abstract": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.",
      "authors": [
        "David Jiahao Fu",
        "Lam Thanh Do",
        "Jiayu Li",
        "Kevin Chen-Chuan Chang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-12T18:59:35+00:00",
      "link": "https://arxiv.org/pdf/2602.12278v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Long document retrieval model for RAG tasks",
      "llm_evidence_cn": "用于RAG任务的长文档检索模型",
      "llm_evidence": "用于RAG任务的长文档检索模型",
      "llm_tldr_en": "Introduces AttentionRetriever, leveraging attention mechanisms to improve retrieval for long-context RAG.",
      "llm_tldr_cn": "引入AttentionRetriever，利用注意力机制改进长文本RAG的检索效果。",
      "llm_tldr": "引入AttentionRetriever，利用注意力机制改进长文本RAG的检索效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "paper_id": "2602.12278v1"
    }
  ],
  "quick_skim": [
    {
      "id": "2601.09402v1",
      "title": "Structured Knowledge Representation through Contextual Pages for Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating external knowledge. Recently, some works have incorporated iterative knowledge accumulation processes into RAG models to progressively accumulate and refine query-related knowledge, thereby constructing more comprehensive knowledge representations. However, these iterative processes often lack a coherent organizational structure, which limits the construction of more comprehensive and cohesive knowledge representations. To address this, we propose PAGER, a page-driven autonomous knowledge representation framework for RAG. PAGER first prompts an LLM to construct a structured cognitive outline for a given question, which consists of multiple slots representing a distinct knowledge aspect. Then, PAGER iteratively retrieves and refines relevant documents to populate each slot, ultimately constructing a coherent page that serves as contextual input for guiding answer generation. Experiments on multiple knowledge-intensive benchmarks and backbone models show that PAGER consistently outperforms all RAG baselines. Further analyses demonstrate that PAGER constructs higher-quality and information-dense knowledge representations, better mitigates knowledge conflicts, and enables LLMs to leverage external knowledge more effectively. All code is available at https://github.com/OpenBMB/PAGER.",
      "authors": [
        "Xinze Li",
        "Zhenghao Liu",
        "Haidong Xin",
        "Yukun Yan",
        "Shuo Wang",
        "Zheni Zeng",
        "Sen Mei",
        "Ge Yu",
        "Maosong Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-14T11:44:31+00:00",
      "link": "https://arxiv.org/pdf/2601.09402v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Structured knowledge representation for Retrieval-Augmented Generation",
      "llm_evidence_cn": "检索增强生成的结构化知识表示",
      "llm_evidence": "检索增强生成的结构化知识表示",
      "llm_tldr_en": "Presents PAGER, a framework that uses structured cognitive outlines to improve knowledge representation in RAG.",
      "llm_tldr_cn": "提出 PAGER 框架，利用结构化认知大纲改进 RAG 中的知识表示。",
      "llm_tldr": "提出 PAGER 框架，利用结构化认知大纲改进 RAG 中的知识表示。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.12256v1",
      "title": "Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration",
      "abstract": "Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.",
      "authors": [
        "Jinyoung Park",
        "Minseong Bae",
        "Jeehye Na",
        "Hyunwoo J. Kim"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-18T04:38:19+00:00",
      "link": "https://arxiv.org/pdf/2601.12256v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Instruction-following and multimodal LLM integration",
      "llm_evidence_cn": "指令遵循与多模态LLM集成",
      "llm_evidence": "指令遵循与多模态LLM集成",
      "llm_tldr_en": "Proposes CoLLaMo to improve molecular language models through multimodal collaboration and instruction tuning.",
      "llm_tldr_cn": "提出CoLLaMo，通过多模态协作和指令微调改进分子语言模型。",
      "llm_tldr": "提出CoLLaMo，通过多模态协作和指令微调改进分子语言模型。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Instruction fine-tuning and parameter-efficient tuning",
      "matched_requirement_id": "req-4",
      "quick_tier": "7"
    },
    {
      "id": "2601.11255v1",
      "title": "Reasoning in Trees: Improving Retrieval-Augmented Generation for Multi-Hop Question Answering",
      "abstract": "Retrieval-Augmented Generation (RAG) has demonstrated significant effectiveness in enhancing large language models (LLMs) for complex multi-hop question answering (QA). For multi-hop QA tasks, current iterative approaches predominantly rely on LLMs to self-guide and plan multi-step exploration paths during retrieval, leading to substantial challenges in maintaining reasoning coherence across steps from inaccurate query decomposition and error propagation. To address these issues, we introduce Reasoning Tree Guided RAG (RT-RAG), a novel hierarchical framework for complex multi-hop QA. RT-RAG systematically decomposes multi-hop questions into explicit reasoning trees, minimizing inaccurate decomposition through structured entity analysis and consensus-based tree selection that clearly separates core queries, known entities, and unknown entities. Subsequently, a bottom-up traversal strategy employs iterative query rewriting and refinement to collect high-quality evidence, thereby mitigating error propagation. Comprehensive experiments show that RT-RAG substantially outperforms state-of-the-art methods by 7.0% F1 and 6.0% EM, demonstrating the effectiveness of RT-RAG in complex multi-hop QA.",
      "authors": [
        "Yuling Shi",
        "Maolin Sun",
        "Zijun Liu",
        "Mo Yang",
        "Yixiong Fang",
        "Tianran Sun",
        "Xiaodong Gu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-01-16T13:02:25+00:00",
      "link": "https://arxiv.org/pdf/2601.11255v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Improving RAG for multi-hop question answering via reasoning trees",
      "llm_evidence_cn": "通过推理树改进多跳问答中的检索增强生成",
      "llm_evidence": "通过推理树改进多跳问答中的检索增强生成",
      "llm_tldr_en": "Proposes RT-RAG to decompose complex questions into reasoning trees for better multi-hop QA.",
      "llm_tldr_cn": "提出 RT-RAG 框架，将复杂问题分解为推理树以提升多跳问答效果。",
      "llm_tldr": "提出 RT-RAG 框架，将复杂问题分解为推理树以提升多跳问答效果。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-augmented generation for knowledge intensive tasks",
      "matched_requirement_id": "req-11",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.14127v1",
      "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
      "abstract": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.",
      "authors": [
        "Renmiao Chen",
        "Yida Lu",
        "Shiyao Cui",
        "Xuan Ouyang",
        "Victor Shea-Jay Huang",
        "Shumin Zhang",
        "Chengwei Pan",
        "Han Qiu",
        "Minlie Huang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "published": "2026-01-20T16:24:18+00:00",
      "link": "https://arxiv.org/pdf/2601.14127v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Safety risks in Multimodal LLMs' multi-image reasoning",
      "llm_evidence_cn": "多模态大模型多图推理中的安全风险",
      "llm_evidence": "多模态大模型多图推理中的安全风险",
      "llm_tldr_en": "Introduces a benchmark to evaluate safety risks associated with advanced reasoning in multimodal LLMs.",
      "llm_tldr_cn": "引入基准测试，评估多模态大模型在高级推理过程中存在的安全风险。",
      "llm_tldr": "引入基准测试，评估多模态大模型在高级推理过程中存在的安全风险。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "quick_tier": "7"
    },
    {
      "id": "2601.11340v1",
      "title": "Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models",
      "abstract": "Chain-of-Thought reasoning has significantly enhanced the problem-solving capabilities of Large Language Models. Unfortunately, current models generate reasoning steps sequentially without foresight, often becoming trapped in suboptimal reasoning paths with redundant steps. In contrast, we introduce Neural Chain-of-Thought Search (NCoTS), a framework that reformulates reasoning as a dynamic search for the optimal thinking strategy. By quantitatively characterizing the solution space, we reveal the existence of sparse superior reasoning paths that are simultaneously more accurate and concise than standard outputs. Our method actively navigates towards these paths by evaluating candidate reasoning operators using a dual-factor heuristic that optimizes for both correctness and computational cost. Consequently, NCoTS achieves a Pareto improvement across diverse reasoning benchmarks, boosting accuracy by over 3.5% while reducing generation length by over 22%. Our code and data are available at https://github.com/MilkThink-Lab/Neural-CoT-Search.",
      "authors": [
        "Guoming Ling",
        "Zhongzhan Huang",
        "Yupei Lin",
        "Junxin Li",
        "Shanshan Zhong",
        "Hefeng Wu",
        "Liang Lin"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-16T14:38:18+00:00",
      "link": "https://arxiv.org/pdf/2601.11340v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Searching optimal reasoning paths to enhance CoT in LLMs",
      "llm_evidence_cn": "搜索最优推理路径以增强大模型的思维链",
      "llm_evidence": "搜索最优推理路径以增强大模型的思维链",
      "llm_tldr_en": "Introduces NCoTS to reformulate reasoning as a dynamic search for more accurate and concise CoT paths.",
      "llm_tldr_cn": "提出NCoTS框架，将推理重构为动态搜索，以寻找更准确、简洁的最优思维链路径。",
      "llm_tldr": "提出NCoTS框架，将推理重构为动态搜索，以寻找更准确、简洁的最优思维链路径。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.14598v2",
      "title": "HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation",
      "abstract": "Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \\textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \\textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.   On HumanEval-Decompile for \\texttt{x86\\_64}, \\textsc{HELIOS} raises average object file compilability from 45.0\\% to 85.2\\% for Gemini~2.0 and from 71.4\\% to 89.6\\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \\textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \\textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.",
      "authors": [
        "Yonatan Gizachew Achamyeleh",
        "Harsh Thomare",
        "Mohammad Abdullah Al Faruque"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-01-21T02:37:33+00:00",
      "link": "https://arxiv.org/pdf/2601.14598v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "LLM-based structured reasoning for code decompilation",
      "llm_evidence_cn": "基于大模型的代码反编译结构化推理",
      "llm_evidence": "基于大模型的代码反编译结构化推理",
      "llm_tldr_en": "Reframes decompilation as a structured reasoning task for LLMs using hierarchical graph abstraction.",
      "llm_tldr_cn": "利用层次化图抽象将反编译重构为大模型的结构化推理任务。",
      "llm_tldr": "利用层次化图抽象将反编译重构为大模型的结构化推理任务。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "quick_tier": "7"
    },
    {
      "id": "2601.11443v1",
      "title": "Predict the Retrieval! Test time adaptation for Retrieval Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language models' question-answering capabilities through the integration of external knowledge. However, when adapting RAG systems to specialized domains, challenges arise from distribution shifts, resulting in suboptimal generalization performance. In this work, we propose TTARAG, a test-time adaptation method that dynamically updates the language model's parameters during inference to improve RAG system performance in specialized domains. Our method introduces a simple yet effective approach where the model learns to predict retrieved content, enabling automatic parameter adjustment to the target domain. Through extensive experiments across six specialized domains, we demonstrate that TTARAG achieves substantial performance improvements over baseline RAG systems. Code available at https://github.com/sunxin000/TTARAG.",
      "authors": [
        "Xin Sun",
        "Zhongqi Chen",
        "Qiang Liu",
        "Shu Wu",
        "Bowen Song",
        "Weiqiang Wang",
        "Zilei Wang",
        "Liang Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-16T17:07:01+00:00",
      "link": "https://arxiv.org/pdf/2601.11443v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "test time adaptation for retrieval augmented generation",
      "llm_evidence_cn": "检索增强生成的测试时自适应",
      "llm_evidence": "检索增强生成的测试时自适应",
      "llm_tldr_en": "Proposes TTARAG to dynamically update model parameters during inference for better domain-specific RAG.",
      "llm_tldr_cn": "提出TTARAG方法，在推理时动态更新模型参数以提升特定领域的RAG性能。",
      "llm_tldr": "提出TTARAG方法，在推理时动态更新模型参数以提升特定领域的RAG性能。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.15333v2",
      "title": "Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference",
      "abstract": "Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.",
      "authors": [
        "Xuanning Hu",
        "Anchen Li",
        "Qianli Xing",
        "Jinglong Ji",
        "Hao Tuo",
        "Bo Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "published": "2026-01-20T08:10:48+00:00",
      "link": "https://arxiv.org/pdf/2601.15333v2",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "LLM representation and reasoning capabilities for design",
      "llm_evidence_cn": "大模型在设计中的表示与推理能力",
      "llm_evidence": "大模型在设计中的表示与推理能力",
      "llm_tldr_en": "Empowers LLMs for drug design by reinterpreting generation as latent space exploration and inference.",
      "llm_tldr_cn": "通过将生成过程重新解释为潜空间探索与推理，赋能大模型进行药物设计。",
      "llm_tldr": "通过将生成过程重新解释为潜空间探索与推理，赋能大模型进行药物设计。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Reasoning and problem solving capabilities of large language models",
      "matched_requirement_id": "req-10",
      "quick_tier": "7"
    },
    {
      "id": "2601.11863v1",
      "title": "Utilizing Metadata for Better Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.",
      "authors": [
        "Raquib Bin Yousuf",
        "Shengzhe Xu",
        "Mandar Sharma",
        "Andrew Neeser",
        "Chris Latimer",
        "Naren Ramakrishnan"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "published": "2026-01-17T01:11:03+00:00",
      "link": "https://arxiv.org/pdf/2601.11863v1",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Retrieval-Augmented Generation systems and metadata-aware retrieval strategies",
      "llm_evidence_cn": "检索增强生成系统与元数据感知检索策略",
      "llm_evidence": "检索增强生成系统与元数据感知检索策略",
      "llm_tldr_en": "Systematically studies how metadata integration improves retrieval accuracy in RAG systems for structured corpora.",
      "llm_tldr_cn": "系统研究了在 RAG 系统中利用元数据提升结构化语料检索准确性的方法。",
      "llm_tldr": "系统研究了在 RAG 系统中利用元数据提升结构化语料检索准确性的方法。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.12538v1",
      "title": "Agentic Reasoning for Large Language Models",
      "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
      "authors": [
        "Tianxin Wei",
        "Ting-Wei Li",
        "Zhining Liu",
        "Xuying Ning",
        "Ze Yang",
        "Jiaru Zou",
        "Zhichen Zeng",
        "Ruizhong Qiu",
        "Xiao Lin",
        "Dongqi Fu",
        "Zihao Li",
        "Mengting Ai",
        "Duo Zhou",
        "Wenxuan Bao",
        "Yunzhe Li",
        "Gaotang Li",
        "Cheng Qian",
        "Yu Wang",
        "Xiangru Tang",
        "Yin Xiao",
        "Liri Fang",
        "Hui Liu",
        "Xianfeng Tang",
        "Yuji Zhang",
        "Chi Wang",
        "Jiaxuan You",
        "Heng Ji",
        "Hanghang Tong",
        "Jingrui He"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-18T18:58:23+00:00",
      "link": "https://arxiv.org/pdf/2601.12538v1",
      "tags": [
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Survey on agentic reasoning and planning in LLMs",
      "llm_evidence_cn": "关于大语言模型智能体推理与规划的综述",
      "llm_evidence": "关于大语言模型智能体推理与规划的综述",
      "llm_tldr_en": "A comprehensive survey organizing LLM agentic reasoning into foundational, self-evolving, and multi-agent dimensions.",
      "llm_tldr_cn": "该综述从基础推理、自我演化和多智能体协作三个维度系统梳理了大模型智能体推理技术。",
      "llm_tldr": "该综述从基础推理、自我演化和多智能体协作三个维度系统梳理了大模型智能体推理技术。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Chain of Thought reasoning in large language models",
      "matched_requirement_id": "req-6",
      "quick_tier": "8plus"
    },
    {
      "id": "2601.12658v2",
      "title": "Augmenting Question Answering with A Hybrid RAG Approach",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.",
      "authors": [
        "Tianyi Yang",
        "Nashrah Haque",
        "Vaishnave Jonnalagadda",
        "Yuya Jeremy Ong",
        "Zhehui Chen",
        "Yanzhao Wu",
        "Lei Yu",
        "Divyesh Jadav",
        "Wenqi Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-19T02:08:47+00:00",
      "link": "https://arxiv.org/pdf/2601.12658v2",
      "tags": [
        "keyword:SR-LLM",
        "query:SR-LLM"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Hybrid RAG architecture for question answering",
      "llm_evidence_cn": "用于问答的混合RAG架构",
      "llm_evidence": "用于问答的混合RAG架构",
      "llm_tldr_en": "Introduces SSRAG, combining vector and graph retrieval to improve accuracy and informativeness in QA tasks.",
      "llm_tldr_cn": "引入SSRAG架构，结合向量和图检索以提高问答任务的准确性和信息量。",
      "llm_tldr": "引入SSRAG架构，结合向量和图检索以提高问答任务的准确性和信息量。",
      "llm_tags": [
        "query:sr-llm"
      ],
      "matched_query_tag": "query:sr-llm",
      "matched_query_text": "Retrieval-Augmented Generation techniques and frameworks",
      "matched_requirement_id": "req-5",
      "quick_tier": "8plus"
    }
  ]
}