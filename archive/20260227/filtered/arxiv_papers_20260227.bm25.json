{
  "top_k": 30,
  "generated_at": "2026-02-27T20:18:38.002486+00:00",
  "papers": [
    {
      "id": "2601.14693v1",
      "title": "Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning",
      "abstract": "Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.",
      "authors": [
        "Jianwen Sun",
        "Xinrui Li",
        "Fuqing Li",
        "Xiaoxuan Shen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-21T06:08:37+00:00",
      "link": "https://arxiv.org/pdf/2601.14693v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL",
        "query:SR-RL"
      ]
    },
    {
      "id": "2602.08270v1",
      "title": "A few-shot and physically restorable symbolic regression turbulence model based on normalized general effective-viscosity hypothesis",
      "abstract": "Turbulence is a complex, irregular flow phenomenon ubiquitous in natural processes and engineering applications. The Reynolds-averaged Navier-Stokes (RANS) method, owing to its low computational cost, has become the primary approach for rapid simulation of engineering turbulence problems. However, the inaccuracy of classical turbulence models constitutes the main drawback of the RANS framework. With the rapid development of data-driven approaches, many data-driven turbulence models have been proposed, yet they still suffer from issues of generalizability and accuracy. In this work, we propose a few-shot, physically restorable, symbolic regression turbulence model based on the normalized general effective-viscosity hypothesis. Few-shot indicates that our model is trained on limited flow configurations spanning only a narrow subset of turbulent flow physics, yet can still outperform the baseline model in substantially different turbulent flows. Physically restorable means our model can nearly revert to the baseline model in regimes satisfying specific physical conditions, using only the symbolic regression training results. The normalized general effective-viscosity hypothesis was proposed in our previous study. Specifically, we first formalize the concept of few-shot data-driven turbulence models. Second, we train our symbolic regression turbulence models using only direct numerical simulation (DNS) data for three-dimensional periodic hill flow slices. Third, we evaluate our models on periodic hill flows, zero pressure gradient flat plate flow, NACA0012 airfoil flows, and NASA Rotor 37 transonic axial compressor flows. One of our symbolic regression turbulence models consistently outperforms the baseline model, and we further demonstrate that this model can nearly revert to baseline behavior in certain flow regimes.",
      "authors": [
        "Ziqi Ji",
        "Penghao Duan",
        "Gang Du"
      ],
      "primary_category": "physics.flu-dyn",
      "categories": [
        "physics.flu-dyn"
      ],
      "published": "2026-02-09T05:08:04+00:00",
      "link": "https://arxiv.org/pdf/2602.08270v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL",
        "query:SR"
      ]
    },
    {
      "id": "2602.08885v3",
      "title": "Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression",
      "abstract": "Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.",
      "authors": [
        "Paul Saegert",
        "Ullrich Köthe"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC"
      ],
      "published": "2026-02-09T16:47:00+00:00",
      "link": "https://arxiv.org/pdf/2602.08885v3",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.15169v1",
      "title": "Learning the S-matrix from data: Rediscovering gravity from gauge theory via symbolic regression",
      "abstract": "We demonstrate that modern machine-learning methods can autonomously reconstruct several flagship analytic structures in scattering amplitudes directly from numerical on-shell data. In particular, we show that the Kawai--Lewellen--Tye (KLT) relations can be rediscovered using symbolic regression applied to colour-ordered Yang--Mills amplitudes with Mandelstam invariants as input features. Using standard feature-selection techniques, specifically column-pivoted QR factorisation, we simultaneously recover the Kleiss--Kuijf and Bern--Carrasco--Johansson (BCJ) relations, identifying a minimal basis of partial amplitudes without any group-theoretic input. We obtain the tree-level KLT relations with high numerical accuracy up to five external legs, using only minimal theoretical priors, and we comment on the obstacles to generalising the method to higher multiplicity. Our results establish symbolic regression as a practical tool for exploring the analytic structure of the scattering-amplitude landscape, and suggests a general data-driven strategy for uncovering hidden relations in general theories. For comparison, we benchmark this general approach with a recently introduced neural-network based method.",
      "authors": [
        "Nathan Moynihan"
      ],
      "primary_category": "hep-th",
      "categories": [
        "hep-th",
        "cs.LG",
        "hep-ph"
      ],
      "published": "2026-02-16T20:15:50+00:00",
      "link": "https://arxiv.org/pdf/2602.15169v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.20637v1",
      "title": "An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems",
      "abstract": "Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.",
      "authors": [
        "Panayiotis Ioannou",
        "Pietro Liò",
        "Pietro Cicuta"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.comp-ph"
      ],
      "published": "2026-01-28T14:23:59+00:00",
      "link": "https://arxiv.org/pdf/2601.20637v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.13021v2",
      "title": "Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery",
      "abstract": "Symbolic Regression (SR) aims to discover interpretable equations from observational data, with the potential to reveal underlying principles behind natural phenomena. However, existing approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Annealing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a guarantee against pseudo-equations. Experimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining robustness to varying prior quality, noisy data, and data scarcity.",
      "authors": [
        "Jing Xiao",
        "Xinhai Chen",
        "Jiaming Peng",
        "Qinglin Wang",
        "Menghan Jia",
        "Zhiquan Lai",
        "Guangping Yu",
        "Dongsheng Li",
        "Tiejun Li",
        "Jie Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13T15:26:21+00:00",
      "link": "https://arxiv.org/pdf/2602.13021v2",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.03506v1",
      "title": "Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models",
      "abstract": "Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits for SR. Using PATCHES, we isolate 28 circuits, providing the first circuit-level characterisation of an SR transformer. We validate these findings through a robust causal evaluation framework based on key notions such as faithfulness, completeness, and minimality. Our analysis shows that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. In contrast, we demonstrate that direct logit attribution and probing classifiers primarily capture correlational features rather than causal ones, limiting their utility for circuit discovery. Overall, these results establish SR as a high-potential application domain for mechanistic interpretability and propose a principled methodology for circuit discovery.",
      "authors": [
        "Arco van Breda",
        "Erman Acar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-03T13:27:10+00:00",
      "link": "https://arxiv.org/pdf/2602.03506v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.00031v1",
      "title": "Optimal Control-Based Falsification of Learnt Dynamics via Neural ODEs and Symbolic Regression",
      "abstract": "We present a falsification framework that integrates learned surrogate dynamics with optimal control to efficiently generate counterexamples for cyber-physical systems specified in signal temporal logic (STL). The unknown system dynamics are identified using neural ODEs, while known a-priori structure is embedded directly into the model, reducing data requirements. The learned neural ODE is converted into an analytical form via symbolic regression, enabling fast and interpretable trajectory optimization. Falsification is cast as minimizing STL robustness over input trajectories; negative robustness yields candidate counterexamples, which are validated on the original system. Spurious traces are iteratively used to refine the surrogate, while true counterexamples are returned as final results. Experiments on ARCH-COMP 2024 benchmarks show that this method requires orders of magnitude fewer experiments of the system under test than optimization-based approaches that do not model system dynamics.",
      "authors": [
        "Lasse Kötz",
        "Jonas Sjöberg",
        "Knut Åkesson"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-01-18T13:23:35+00:00",
      "link": "https://arxiv.org/pdf/2602.00031v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.07727v1",
      "title": "Learning the relations between neutron star and nuclear matter properties with symbolic regression",
      "abstract": "The equation of state (EOS) of dense matter in neutron stars (NSs) remains uncertain, particularly at supra-nuclear densities where complex nuclear interactions and the potential presence of exotic matter, like hyperons, come into play. The complex relationships existing between nuclear matter and neutron star properties are investigated. The focus is on their nonlinearities and interdependencies. In our analysis, we apply a machine learning algorithm known as symbolic regression, paired with principal component analysis, to datasets generated from Bayesian inference over relativistic mean-field models. A systematic Principal Component Analysis has allowed to break down the percentage contribution of each element or feature in the relationships obtained. This study examines two main models (datasets): the NL model, which includes nucleonic degrees of freedom; and the NL-hyp model, which includes hyperons in addition to nucleons. Our analysis confirms a robust correlation between the tidal deformability of a 1.4 \\(M_\\odot\\) neutron star and $β$-equilibrium pressure at twice the nuclear saturation density. This correlation remains once hyperons are included. The contribution of the different nuclear matter properties at saturation to the radius and tidal deformability was calculated. It was shown that the isovector properties have the largest impact, with a contribution of about 90\\%. We also studied the relationship between the proton fraction at different densities and various symmetry energy parameters defined at saturation density. For the hyperon data set, we took into account the effects of the negatively charged hyperon $Ξ$ in order to recover the relationships. Our study reveals the individual impact of various symmetry energy parameters on proton fractions at different densities.",
      "authors": [
        "N. K. Patra",
        "Tuhin Malik",
        "Kai Zhou",
        "Constança Providência"
      ],
      "primary_category": "nucl-th",
      "categories": [
        "nucl-th",
        "astro-ph.SR",
        "gr-qc",
        "hep-ph",
        "nucl-ex"
      ],
      "published": "2026-01-12T16:58:40+00:00",
      "link": "https://arxiv.org/pdf/2601.07727v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.17082v1",
      "title": "Order of Magnitude Analysis and Data-Based Physics-Informed Symbolic Regression for Turbulent Pipe Flow",
      "abstract": "Friction losses in rough pipes are often predicted using semi-empirical correlations, such as the Colebrook-White equation (Colebrook,1939), which do not fully replicate Nikuradse's rough-pipe experiments (1950). This study derives scaling relations for the viscous and turbulent contributions to the streamwise pressure drop through an order-of-magnitude analysis of the Reynolds-averaged Navier-Stokes equations and the kinetic-energy transport equations. These relations impose constraints on the local sensitivity of the pressure drop to factors such as mean velocity, roughness, viscosity, and density through exponent envelopes and serve as a physical prior for symbolic regression. By combining Nikuradse's rough-pipe and smooth-pipe data of Zagarola and Smits (1998), we aim to derive compact correlations for the friction factor that fit experimental data while adhering to the derived constraints. A modified genetic programming engine (GPTIPS2) optimizes model structure and evaluates it based on fitness, complexity, and constraint violation. This method yields interpretable expressions that accurately reproduce friction factors across various roughness levels and Reynolds numbers, validated up to $Re \\sim 10^7$.",
      "authors": [
        "Yunus Emre Ünal",
        "Özgür Ertunç",
        "Ismail Ari",
        "Ivan Otić"
      ],
      "primary_category": "physics.flu-dyn",
      "categories": [
        "physics.flu-dyn",
        "physics.comp-ph"
      ],
      "published": "2026-02-19T05:02:58+00:00",
      "link": "https://arxiv.org/pdf/2602.17082v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.01510v1",
      "title": "Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization",
      "abstract": "Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.",
      "authors": [
        "Hengzhe Zhang",
        "Qi Chen",
        "Bing Xue",
        "Wolfgang Banzhaf",
        "Mengjie Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "published": "2026-02-02T00:46:16+00:00",
      "link": "https://arxiv.org/pdf/2602.01510v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.22328v1",
      "title": "Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery",
      "abstract": "Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.",
      "authors": [
        "Luca Muscarnera",
        "Silas Ruhrberg Estévez",
        "Samuel Holt",
        "Evgeny Saveliev",
        "Mihaela van der Schaar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-29T21:15:52+00:00",
      "link": "https://arxiv.org/pdf/2601.22328v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.21789v1",
      "title": "ECSEL: Explainable Classification via Signomial Equation Learning",
      "abstract": "We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, our method recovers a larger fraction of target equations than competing state-of-the-art approaches while requiring substantially less computation. Leveraging this efficiency, ECSEL achieves classification accuracy competitive with established machine learning models without sacrificing interpretability. Further, we show that ECSEL satisfies some desirable properties regarding global feature behavior, decision-boundary analysis, and local feature attributions. Experiments on benchmark datasets and two real-world case studies i.e., e-commerce and fraud detection, demonstrate that the learned equations expose dataset biases, support counterfactual reasoning, and yield actionable insights.",
      "authors": [
        "Adia Lumadjeng",
        "Ilker Birbil",
        "Erman Acar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-01-29T14:35:43+00:00",
      "link": "https://arxiv.org/pdf/2601.21789v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.02311v1",
      "title": "Introns and Templates Matter: Rethinking Linkage in GP-GOMEA",
      "abstract": "GP-GOMEA is among the state-of-the-art for symbolic regression, especially when it comes to finding small and potentially interpretable solutions. A key mechanism employed in any GOMEA variant is the exploitation of linkage, the dependencies between variables, to ensure efficient evolution. In GP-GOMEA, mutual information between node positions in GP trees has so far been used to learn linkage. For this, a fixed expression template is used. This however leads to introns for expressions smaller than the full template. As introns have no impact on fitness, their occurrences are not directly linked to selection. Consequently, introns can adversely affect the extent to which mutual information captures dependencies between tree nodes. To overcome this, we propose two new measures for linkage learning, one that explicitly considers introns in mutual information estimates, and one that revisits linkage learning in GP-GOMEA from a grey-box perspective, yielding a measure that needs not to be learned from the population but is derived directly from the template. Across five standard symbolic regression problems, GP-GOMEA achieves substantial improvements using both measures. We also find that the newly learned linkage structure closely reflects the template linkage structure, and that explicitly using the template structure yields the best performance overall.",
      "authors": [
        "Johannes Koch",
        "Tanja Alderliesten",
        "Peter A. N. Bosman"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-02T16:42:30+00:00",
      "link": "https://arxiv.org/pdf/2602.02311v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.19477v1",
      "title": "ROIDS: Robust Outlier-Aware Informed Down-Sampling",
      "abstract": "Informed down-sampling (IDS) is known to improve performance in symbolic regression when combined with various selection strategies, especially tournament selection. However, recent work found that IDS's gains are not consistent across all problems. Our analysis reveals that IDS performance is worse for problems containing outliers. IDS systematically favors including outliers in subsets which pushes GP towards finding solutions that overfit to outliers. To address this, we introduce ROIDS (Robust Outlier-Aware Informed Down-Sampling), which excludes potential outliers from the sampling process of IDS. With ROIDS it is possible to keep the advantages of IDS without overfitting to outliers and to compete on a wide range of benchmark problems. This is also reflected in our experiments in which ROIDS shows the desired behavior on all studied benchmark problems. ROIDS consistently outperforms IDS on synthetic problems with added outliers as well as on a wide range of complex real-world problems, surpassing IDS on over 80% of the real-world benchmark problems. Moreover, compared to all studied baseline approaches, ROIDS achieves the best average rank across all tested benchmark problems. This robust behavior makes ROIDS a reliable down-sampling method for selection in symbolic regression, especially when outliers may be included in the data set.",
      "authors": [
        "Alina Geiger",
        "Martin Briesch",
        "Dominik Sobania",
        "Franz Rothlauf"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-27T11:07:47+00:00",
      "link": "https://arxiv.org/pdf/2601.19477v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.16852v1",
      "title": "Charting the Landscape of Oxygen Ion Conductors: A 60-Year Dataset with Interpretable Regression Models",
      "abstract": "Oxygen ion conductors are indispensable materials for such as solid oxide fuel cells, sensors, and membranes. Despite extensive research across diverse structural families, systematic data enabling comparative analysis remain scarce. Here, we present a curated dataset of oxygen ion conductors compiled from $84$ experimental reports spanning $60$ years, covering $483$ materials. Each record includes activation energy ($E_a$) and prefactor ($A$) derived from Arrhenius plots, alongside detailed metadata on structure, composition, measurement method, and data source. When the original papers derive these using an erroneous Arrhenius equation $σ_T=A\\exp{\\left(-\\frac{E_a}{RT}\\right)}$, where ($σ_T$ is the oxygen ion conductivity at temperature $T$ and $R$ is the gas constant), we replotted these using the correct one, $σ_{T}T=A\\exp{\\left(-\\frac{E_a}{RT}\\right)}$. To illustrate how the database can be used, we constructed interpretable regression models for predicting oxygen ionic conductivity. Two symbolic regression models for E_a and A suggest that oxygen ion transport is primarily governed by local coordination environment and the electrostatic interactions, respectively. This dataset establishes a reliable foundation for data-driven discovery and predictive modeling of next-generation oxygen ion conductors.",
      "authors": [
        "Seong-Hoon Jang",
        "Shin Kiyohara",
        "Hitoshi Takamura",
        "Yu Kumagai"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-01-23T16:00:58+00:00",
      "link": "https://arxiv.org/pdf/2601.16852v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.12259v1",
      "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
      "abstract": "Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.",
      "authors": [
        "Jianke Yang",
        "Ohm Venkatachalam",
        "Mohammad Kianezhad",
        "Sharvaree Vadgama",
        "Rose Yu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-12T18:49:27+00:00",
      "link": "https://arxiv.org/pdf/2602.12259v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.08733v1",
      "title": "Foundation Inference Models for Ordinary Differential Equations",
      "abstract": "Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.",
      "authors": [
        "Maximilian Mauel",
        "Johannes R. Hübers",
        "David Berghaus",
        "Patrick Seifner",
        "Ramses J. Sanchez"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09T14:39:11+00:00",
      "link": "https://arxiv.org/pdf/2602.08733v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.07834v1",
      "title": "Interpretable Analytic Calabi-Yau Metrics via Symbolic Distillation",
      "abstract": "Calabi--Yau manifolds are essential for string theory but require computing intractable metrics. Here we show that symbolic regression can distill neural approximations into simple, interpretable formulas. Our five-term expression matches neural accuracy ($R^2 = 0.9994$) with 3,000-fold fewer parameters. Multi-seed validation confirms that geometric constraints select essential features, specifically power sums and symmetric polynomials, while permitting structural diversity. The functional form can be maintained across the studied moduli range ($ψ\\in [0, 0.8]$) with coefficients varying smoothly; we interpret these trends as empirical hypotheses within the accuracy regime of the locally-trained teachers ($σ\\approx 8-9\\%$ at $ψ\\neq 0$). The formula reproduces physical observables -- volume integrals and Yukawa couplings -- validating that symbolic distillation recovers compact, interpretable models for quantities previously accessible only to black-box networks.",
      "authors": [
        "D Yang Eng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.DG"
      ],
      "published": "2026-02-08T05:51:35+00:00",
      "link": "https://arxiv.org/pdf/2602.07834v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.12870v1",
      "title": "GAME: Genetic Algorithms with Marginalised Ensembles for model-independent reconstruction of cosmological quantities",
      "abstract": "Genetic Algorithms (GA) are a powerful tool for stochastic optimisation and non-parametric symbolic regression, already widely used in cosmology. They are capable of reconstructing analytical functions directly from data points without introducing new physical models. A limitation of this approach is that while the reconstructed function is very efficient at reproducing the behaviour of the data points, non-observable quantities involving derivatives are particularly sensitive to stochasticity, hyperparameters, and to the choice of the best-fit function obtained by the GA, which implies the risk of the algorithm getting stuck in a local minimum. In this work we propose an update to the GA methodology for the reconstruction of analytical functions that involves computing a weighted average of an ensemble of GA configurations (\\texttt{GAME}). We define the weights via a quantity that accounts for both the goodness-of-fit of the points and the smoothness of the resulting function. We also present a practical method to analytically estimate and correct the errors on the averaged function by combining a path-integral approach with an ensemble variance. We demonstrate the improvement offered by \\texttt{GAME} methodology on a generic test function. We then apply the new methodology to a non-parametric reconstruction of the Hubble rate $H(z)$ using Cosmic Chronometers data and, assuming a flat Friedmann-Lemaître-Robertson-Walker background and General Relativity, we infer the corresponding dark energy equation of state $w(z)$. Through consistency tests, we show that current data produces results compatible with $Λ$CDM, and that Stage IV cosmology surveys will allow GA reinforced with \\texttt{GAME} methodology to become an even more competitive tool for discriminating between different models.",
      "authors": [
        "Matteo Peronaci",
        "Matteo Martinelli",
        "Savvas Nesseris"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO"
      ],
      "published": "2026-02-13T12:20:46+00:00",
      "link": "https://arxiv.org/pdf/2602.12870v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.14288v1",
      "title": "DeepInflation: an AI agent for research and model discovery of inflation",
      "abstract": "We present \\textbf{DeepInflation}, an AI agent designed for research and model discovery in inflationary cosmology. Built upon a multi-agent architecture, \\textbf{DeepInflation} integrates Large Language Models (LLMs) with a symbolic regression (SR) engine and a retrieval-augmented generation (RAG) knowledge base. This framework enables the agent to automatically explore and verify the vast landscape of inflationary potentials while grounding its outputs in established theoretical literature. We demonstrate that \\textbf{DeepInflation} can successfully discover simple and viable single-field slow-roll inflationary potentials consistent with the latest observations (here ACT DR6 results as example) or any given $n_s$ and $r$, and provide accurate theoretical context for obscure inflationary scenarios. \\textbf{DeepInflation} serves as a prototype for a new generation of autonomous scientific discovery engines in cosmology, which enables researchers and non-experts alike to explore the inflationary landscape using natural language. This agent is available at https://github.com/pengzy-cosmo/DeepInflation.",
      "authors": [
        "Ze-Yu Peng",
        "Hao-Shi Yuan",
        "Qi Lai",
        "Jun-Qian Jiang",
        "Gen Ye",
        "Jun Zhang",
        "Yun-Song Piao"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO",
        "cs.AI",
        "cs.CE",
        "gr-qc",
        "hep-th"
      ],
      "published": "2026-01-14T09:41:01+00:00",
      "link": "https://arxiv.org/pdf/2601.14288v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.16166v1",
      "title": "Discovering Unknown Inverter Governing Equations via Physics-Informed Sparse Machine Learning",
      "abstract": "Discovering the unknown governing equations of grid-connected inverters from external measurements holds significant attraction for analyzing modern inverter-intensive power systems. However, existing methods struggle to balance the identification of unmodeled nonlinearities with the preservation of physical consistency. To address this, this paper proposes a Physics-Informed Sparse Machine Learning (PISML) framework. The architecture integrates a sparse symbolic backbone to capture dominant model skeletons with a neural residual branch that compensates for complex nonlinear control logic. Meanwhile, a Jacobian-regularized physics-informed training mechanism is introduced to enforce multi-scale consistency including large/small-scale behaviors. Furthermore, by performing symbolic regression on the neural residual branch, PISML achieves a tractable mapping from black-box data to explicit control equations. Experimental results on a high-fidelity Hardware-in-the-Loop platform demonstrate the framework's superior performance. It not only achieves high-resolution identification by reducing error by over 340 times compared to baselines but also realizes the compression of heavy neural networks into compact explicit forms. This restores analytical tractability for rigorous stability analysis and reduces computational complexity by orders of magnitude. It also provides a unified pathway to convert structurally inaccessible devices into explicit mathematical models, enabling stability analysis of power systems with unknown inverter governing equations.",
      "authors": [
        "Jialin Zheng",
        "Ruhaan Batta",
        "Zhong Liu",
        "Xiaonan Lu"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-18T03:46:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16166v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.01149v1",
      "title": "Robust Machine Learning Framework for Reliable Discovery of High-Performance Half-Heusler Thermoelectrics",
      "abstract": "Machine learning (ML) can facilitate efficient thermoelectric (TE) material discovery essential to address the environmental crisis. However, ML models often suffer from poor experimental generalizability despite high metrics. This study presents a robust workflow, applied to the half-Heusler (hH) structural prototype, for figure of merit (zT) prediction, to improve the generalizability of ML models. To resolve challenges in dataset handling and feature filtering, we first introduce a rigorous PCA-based splitting method that ensures training and test sets are unbiased and representative of the full chemical space. We then integrate Bayesian hyperparameter optimization with k-best feature filtering across three architectures-Random Forest, XGBoost, and Neural Networks - while employing SISSO symbolic regression for physical insight and comparison. Using SHAP and SISSO analysis, we identify A-site dopant concentration (xA'), and A-site Heat of Vaporization (HVA) as the primary drivers of zT besides Temperature (T). Finally, a high-throughput screening of approximately 6.6x10^8 potential compositions, filtered by stability constraints, yielded several novel high-zT candidates. Breaking from the traditional focus of improving test RMSE/R^2 values of the models, this work shifts the attention on establishing the test set a true proxy for model generalizability and strengthening the often neglected modules of the existing ML workflows for the data-driven design of next-generation thermoelectric materials.",
      "authors": [
        "Shoeb Athar",
        "Adrien Mecibah",
        "Philippe Jund"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG"
      ],
      "published": "2026-02-01T10:50:42+00:00",
      "link": "https://arxiv.org/pdf/2602.01149v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.02886v1",
      "title": "Mixture of Concept Bottleneck Experts",
      "abstract": "Concept Bottleneck Models (CBMs) promote interpretability by grounding predictions in human-understandable concepts. However, existing CBMs typically fix their task predictor to a single linear or Boolean expression, limiting both predictive accuracy and adaptability to diverse user needs. We propose Mixture of Concept Bottleneck Experts (M-CBEs), a framework that generalizes existing CBMs along two dimensions: the number of experts and the functional form of each expert, exposing an underexplored region of the design space. We investigate this region by instantiating two novel models: Linear M-CBE, which learns a finite set of linear expressions, and Symbolic M-CBE, which leverages symbolic regression to discover expert functions from data under user-specified operator vocabularies. Empirical evaluation demonstrates that varying the mixture size and functional form provides a robust framework for navigating the accuracy-interpretability trade-off, adapting to different user and task needs.",
      "authors": [
        "Francesco De Santis",
        "Gabriele Ciravegna",
        "Giovanni De Felice",
        "Arianna Casanova",
        "Francesco Giannini",
        "Michelangelo Diligenti",
        "Mateo Espinosa Zarlenga",
        "Pietro Barbiero",
        "Johannes Schneider",
        "Danilo Giordano"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02T22:44:42+00:00",
      "link": "https://arxiv.org/pdf/2602.02886v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.10576v1",
      "title": "LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization",
      "abstract": "Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.",
      "authors": [
        "Boxiao Wang",
        "Kai Li",
        "Tianyi Liu",
        "Chen Li",
        "Junzhe Wang",
        "Yifan Zhang",
        "Jian Cheng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11T07:02:23+00:00",
      "link": "https://arxiv.org/pdf/2602.10576v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.12109v1",
      "title": "A critical assessment of bonding descriptors for predicting materials properties",
      "abstract": "Most machine learning models for materials science rely on descriptors based on materials compositions and structures, even though the chemical bond has been proven to be a valuable concept for predicting materials properties. Over the years, various theoretical frameworks have been developed to characterize bonding in solid-state materials. However, integrating bonding information from these frameworks into machine learning pipelines at scale has been limited by the lack of a systematically generated and validated database. Recent advances in high-throughput bonding analysis workflows have addressed this issue, and our previously computed Quantum-Chemical Bonding Database for Solid-State Materials was extended to include approximately 13,000 materials. This database is then used to derive a new set of quantum-chemical bonding descriptors. A systematic assessment is performed using statistical significance tests to evaluate how the inclusion of these descriptors influences the performance of machine-learning models that otherwise rely solely on structure- and composition-derived features. Models are built to predict elastic, vibrational, and thermodynamic properties typically associated with chemical bonding in materials. The results demonstrate that incorporating quantum-chemical bonding descriptors not only improves predictive performance but also helps identify intuitive expressions for properties such as the projected force constant and lattice thermal conductivity via symbolic regression.",
      "authors": [
        "Aakash Ashok Naik",
        "Nidal Dhamrait",
        "Katharina Ueltzen",
        "Christina Ertural",
        "Philipp Benner",
        "Gian-Marco Rignanese",
        "Janine George"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "physics.chem-ph",
        "physics.comp-ph"
      ],
      "published": "2026-02-12T16:00:12+00:00",
      "link": "https://arxiv.org/pdf/2602.12109v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.07651v1",
      "title": "Cosmology with one galaxy: An analytic formula relating $Ω_{\\rm m}$ with galaxy properties",
      "abstract": "Standard cosmological analyses typically treat galaxy formation and cosmological parameter inference as decoupled problems, relying on population-level statistics such as clustering, lensing, or halo abundances. However, classical studies of baryon fractions in massive galaxy clusters have long suggested that gravitationally bound systems may retain cosmological information through their baryonic content. Building on this insight, we present the first analytic and physically interpretable cosmological tracer that links the matter density parameter, $Ω_m$, directly to intrinsic galaxy-scale observables, demonstrating that cosmological information can be extracted from individual galaxies. Using symbolic regression applied to state-of-the-art hydrodynamical simulations from the CAMELS project, we identify a compact functional form that robustly recovers $Ω_m$ across multiple simulation suites (IllustrisTNG, ASTRID, SIMBA, and Swift-EAGLE), requiring only modest recalibration of a small number of coefficients. The resulting expression admits a transparent physical interpretation in terms of baryonic retention and enrichment efficiency regulated by gravitational potential depth, providing a clear explanation for why $Ω_m$ is locally encoded in galaxy properties. Our work establishes a direct, interpretable bridge between small-scale galaxy physics and large-scale cosmology, opening a complementary pathway to cosmological inference that bypasses traditional clustering-based statistics and enables new synergies between galaxy formation theory and precision cosmology.",
      "authors": [
        "Kito Liao",
        "Francisco Villaescusa-Navarro",
        "Romain Teysser",
        "Natalí S. M. de Santi"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO",
        "astro-ph.GA"
      ],
      "published": "2026-02-07T18:23:07+00:00",
      "link": "https://arxiv.org/pdf/2602.07651v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.05894v1",
      "title": "Learning microstructure in active matter",
      "abstract": "Understanding microstructure in terms of closed-form expressions is an open challenge in nonequilibrium statistical physics. We propose a simple and generic method that combines particle-resolved simulations, deep neural networks and symbolic regression to predict the pair-correlation function of passive and active particles. Our analytical closed-form results closely agree with Brownian dynamics simulations, even at relatively large packing fractions and for strong activity. The proposed method is broadly applicable, computationally efficient, and can be used to enhance the predictive power of nonequilibrium continuum theories and for designing pattern formation.",
      "authors": [
        "Writu Dasgupta",
        "Suvendu Mandal",
        "Aritra K. Mukhopadhyay",
        "Benno Liebchen"
      ],
      "primary_category": "cond-mat.soft",
      "categories": [
        "cond-mat.soft",
        "cond-mat.dis-nn",
        "cond-mat.stat-mech"
      ],
      "published": "2026-01-09T16:11:38+00:00",
      "link": "https://arxiv.org/pdf/2601.05894v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL",
        "query:SR"
      ]
    },
    {
      "id": "2601.10379v1",
      "title": "Online identification of nonlinear time-varying systems with uncertain information",
      "abstract": "Digital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning.",
      "authors": [
        "He Ren",
        "Gaowei Yan",
        "Hang Liu",
        "Lifeng Cao",
        "Zhijun Zhao",
        "Gang Dang"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2026-01-15T13:33:48+00:00",
      "link": "https://arxiv.org/pdf/2601.10379v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.12979v2",
      "title": "The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check",
      "abstract": "The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a \"bitter lesson\": current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.",
      "authors": [
        "Qingyu Lu",
        "Liang Ding",
        "Kanjian Zhang",
        "Jinxia Zhang",
        "Dacheng Tao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-19T11:45:39+00:00",
      "link": "https://arxiv.org/pdf/2601.12979v2",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS",
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.07310v1",
      "title": "Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing",
      "abstract": "Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.",
      "authors": [
        "Kyle Williams",
        "Andrew Seltzman"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-07T01:59:10+00:00",
      "link": "https://arxiv.org/pdf/2602.07310v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.14485v1",
      "title": "Scalable Knee-Point Guided Activity Group Selection in Multi-Tree Genetic Programming for Dynamic Multi-Mode Project Scheduling",
      "abstract": "The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.",
      "authors": [
        "Yuan Tian",
        "Yi Mei",
        "Mengjie Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-20T21:13:11+00:00",
      "link": "https://arxiv.org/pdf/2601.14485v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15070v1",
      "title": "An effective Genetic Programming Hyper-Heuristic for Uncertain Agile Satellite Scheduling",
      "abstract": "This paper investigates a novel problem, namely the Uncertain Agile Earth Observation Satellite Scheduling Problem (UAEOSSP). Unlike the static AEOSSP, it takes into account a range of uncertain factors (e.g., task profit, resource consumption, and task visibility) in order to reflect the reality that the actual information is inherently unknown beforehand. An effective Genetic Programming Hyper-Heuristic (GPHH) is designed to automate the generation of scheduling policies. The evolved scheduling policies can be utilized to adjust plans in real time and perform exceptionally well. Experimental results demonstrate that evolved scheduling policies significantly outperform both well-designed Look-Ahead Heuristics (LAHs) and Manually Designed Heuristics (MDHs). Specifically, the policies generated by GPHH achieve an average improvement of 5.03% compared to LAHs and 8.14% compared to MDHs.",
      "authors": [
        "Yuning Chen",
        "Junhua Xue",
        "Wangqi Gu",
        "Mingyan Shao"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "published": "2026-02-15T02:09:57+00:00",
      "link": "https://arxiv.org/pdf/2602.15070v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.15717v1",
      "title": "Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling",
      "abstract": "Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.",
      "authors": [
        "Luyao Zhu",
        "Fangfang Zhang",
        "Yi Mei",
        "Mengjie Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22T07:38:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15717v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.07659v1",
      "title": "Continuous Program Search",
      "abstract": "Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.   We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.   Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.",
      "authors": [
        "Matthew Siper",
        "Muhammad Umair Nasir",
        "Ahmed Khalifa",
        "Lisa Soros",
        "Jay Azhang",
        "Julian Togelius"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.ST"
      ],
      "published": "2026-02-07T18:41:14+00:00",
      "link": "https://arxiv.org/pdf/2602.07659v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09772v1",
      "title": "Design and Evaluation of an Assisted Programming Interface for Behavior Trees in Robotics",
      "abstract": "The possibility to create reactive robot programs faster without the need for extensively trained programmers is becoming increasingly important. So far, it has not been explored how various techniques for creating Behavior Tree (BT) program representations could be combined with complete graphical user interfaces (GUIs) to allow a human user to validate and edit trees suggested by automated methods. In this paper, we introduce BEhavior TRee GUI (BETR-GUI) for creating BTs with the help of an AI assistant that combines methods using large language models, planning, genetic programming, and Bayesian optimization with a drag-and-drop editor. A user study with 60 participants shows that by combining different assistive methods, BETR-GUI enables users to perform better at solving the robot programming tasks. The results also show that humans using the full variant of BETR-GUI perform better than the AI assistant running on its own.",
      "authors": [
        "Jonathan Styrud",
        "Matteo Iovino",
        "Rebecca Stower",
        "Mart Kartašev",
        "Mikael Norrlöf",
        "Mårten Björkman",
        "Christian Smith"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-10T13:34:00+00:00",
      "link": "https://arxiv.org/pdf/2602.09772v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.15738v1",
      "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling",
      "abstract": "Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.",
      "authors": [
        "Junhao Qiu",
        "Haoyang Zhuang",
        "Fei Liu",
        "Jianjun Liu",
        "Qingfu Zhang"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-22T08:06:40+00:00",
      "link": "https://arxiv.org/pdf/2601.15738v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10891v1",
      "title": "Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search",
      "abstract": "Multi-task policy search is a challenging problem because policies are required to generalize beyond training cases. Curriculum learning has proven to be effective in this setting, as it introduces complexity progressively. However, designing effective curricula is labor-intensive and requires extensive domain expertise. LLM-based curriculum generation has only recently emerged as a potential solution, but was limited to operate in static, offline modes without leveraging real-time feedback from the optimizer. Here we propose an interactive LLM-assisted framework for online curriculum generation, where the LLM adaptively designs training cases based on real-time feedback from the evolutionary optimization process. We investigate how different feedback modalities, ranging from numeric metrics alone to combinations with plots and behavior visualizations, influence the LLM ability to generate meaningful curricula. Through a 2D robot navigation case study, tackled with genetic programming as optimizer, we evaluate our approach against static LLM-generated curricula and expert-designed baselines. We show that interactive curriculum generation outperforms static approaches, with multimodal feedback incorporating both progression plots and behavior visualizations yielding performance competitive with expert-designed curricula. This work contributes to understanding how LLMs can serve as interactive curriculum designers for embodied AI systems, with potential extensions to broader evolutionary robotics applications.",
      "authors": [
        "Berfin Sakallioglu",
        "Giorgia Nadizar",
        "Eric Medvet"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "published": "2026-02-11T14:21:52+00:00",
      "link": "https://arxiv.org/pdf/2602.10891v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.03840v1",
      "title": "Investigating Quantum Circuit Designs Using Neuro-Evolution",
      "abstract": "Designing effective quantum circuits remains a central challenge in quantum computing, as circuit structure strongly influences expressivity, trainability, and hardware feasibility. Current approaches, whether using manually designed circuit templates, fixed heuristics, or automated rules, face limitations in scalability, flexibility, and adaptability, often producing circuits that are poorly matched to the specific problem or quantum hardware. In this work, we propose the Evolutionary eXploration of Augmenting Quantum Circuits (EXAQC), an evolutionary approach to the automated design and training of parameterized quantum circuits (PQCs) which leverages and extends on strategies from neuroevolution and genetic programming. The proposed method jointly searches over gate types, qubit connectivity, parameterization, and circuit depth while respecting hardware and noise constraints. The method supports both Qiskit and Pennylane libraries, allowing the user to configure every aspect. This work highlights evolutionary search as a critical tool for advancing quantum machine learning and variational quantum algorithms, providing a principled pathway toward scalable, problem-aware, and hardware-efficient quantum circuit design. Preliminary results demonstrate that circuits evolved on classification tasks are able to achieve over 90% accuracy on most of the benchmark datasets with a limited computational budget, and are able to emulate target circuit quantum states with high fidelity scores.",
      "authors": [
        "Devroop Kar",
        "Daniel Krutz",
        "Travis Desell"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "published": "2026-02-03T18:57:39+00:00",
      "link": "https://arxiv.org/pdf/2602.03840v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04529v1",
      "title": "Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization",
      "abstract": "The advent of Large Language Models (LLMs) has opened new frontiers in automated algorithm design, giving rise to numerous powerful methods. However, these approaches retain critical limitations: they require extensive evaluation of the target problem to guide the search process, making them impractical for real-world optimization tasks, where each evaluation consumes substantial computational resources. This research proposes an innovative and efficient framework that decouples algorithm discovery from high-cost evaluation. Our core innovation lies in combining a Genetic Programming (GP) function generator with an LLM-driven evolutionary algorithm designer. The evolutionary direction of the GP-based function generator is guided by the similarity between the landscape characteristics of generated proxy functions and those of real-world problems, ensuring that algorithms discovered via proxy functions exhibit comparable performance on real-world problems. Our method enables deep exploration of the algorithmic space before final validation while avoiding costly real-world evaluations. We validated the framework's efficacy across multiple real-world problems, demonstrating its ability to discover high-performance algorithms while substantially reducing expensive evaluations. This approach shows a path to apply LLM-based automated algorithm design to computationally intensive real-world optimization challenges.",
      "authors": [
        "Haoran Yin",
        "Shuaiqun Pan",
        "Zhao Wei",
        "Jian Cheng Wong",
        "Yew-Soon Ong",
        "Anna V. Kononova",
        "Thomas Bäck",
        "Niki van Stein"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-04T13:18:45+00:00",
      "link": "https://arxiv.org/pdf/2602.04529v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.00843v1",
      "title": "NegaBent, No Regrets: Evolving Spectrally Flat Boolean Functions",
      "abstract": "Negabent Boolean functions are defined by having a flat magnitude spectrum under the nega-Hadamard transform. They exist in both even and odd dimensions, and the subclass of functions that are simultaneously bent and negabent (bent-negabent) has attracted interest due to the combined optimal periodic and negaperiodic spectral properties. In this work, we investigate how evolutionary algorithms can be used to evolve (bent-)negabent Boolean functions. Our experimental results indicate that evolutionary algorithms, especially genetic programming, are a suitable approach for evolving negabent Boolean functions, and we successfully evolve such functions in all dimensions we consider.",
      "authors": [
        "Claude Carlet",
        "Marko Ðurasevic",
        "Ermes Franch",
        "Domagoj Jakobovic",
        "Luca Mariot",
        "Stjepan Picek"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.CR"
      ],
      "published": "2026-01-31T18:13:03+00:00",
      "link": "https://arxiv.org/pdf/2602.00843v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.00755v1",
      "title": "Evolving Interpretable Constitutions for Multi-Agent Coordination",
      "abstract": "Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles (\"be helpful, harmless, honest\") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.",
      "authors": [
        "Ujwal Kumar",
        "Alice Saito",
        "Hershraj Niranjani",
        "Rayan Yessou",
        "Phan Xuan Tan"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.NE"
      ],
      "published": "2026-01-31T14:41:43+00:00",
      "link": "https://arxiv.org/pdf/2602.00755v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.08657v1",
      "title": "NEVO-GSPT: Population-Based Neural Network Evolution Using Inflate and Deflate Operators",
      "abstract": "Evolving neural network architectures is a computationally demanding process. Traditional methods often require an extensive search through large architectural spaces and offer limited understanding of how structural modifications influence model behavior. This paper introduces \\gls{ngspt}, a novel Neuroevolution algorithm based on two key innovations. First, we adapt geometric semantic operators~(GSOs) from genetic programming to neural network evolution, ensuring that architectural changes produce predictable effects on network semantics within a unimodal error surface. Second, we introduce a novel operator (DGSM) that enables controlled reduction of network size, while maintaining the semantic properties of~GSOs. Unlike traditional approaches, \\gls{ngspt}'s efficient evaluation mechanism, which only requires computing the semantics of newly added components, allows for efficient population-based training, resulting in a comprehensive exploration of the search space at a fraction of the computational cost. Experimental results on four regression benchmarks show that \\gls{ngspt} consistently evolves compact neural networks that achieve performance comparable to or better than established methods in the literature, such as standard neural networks, SLIM-GSGP, TensorNEAT, and SLM.",
      "authors": [
        "Davide Farinati",
        "Frederico J. J. B. Santos",
        "Leonardo Vanneschi",
        "Mauro Castelli"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-13T15:35:16+00:00",
      "link": "https://arxiv.org/pdf/2601.08657v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13864v1",
      "title": "Evolving Multi-Channel Confidence-Aware Activation Functions for Missing Data with Channel Propagation",
      "abstract": "Learning in the presence of missing data can result in biased predictions and poor generalizability, among other difficulties, which data imputation methods only partially address. In neural networks, activation functions significantly affect performance yet typical options (e.g., ReLU, Swish) operate only on feature values and do not account for missingness indicators or confidence scores. We propose Three-Channel Evolved Activations (3C-EA), which we evolve using Genetic Programming to produce multivariate activation functions f(x, m, c) in the form of trees that take (i) the feature value x, (ii) a missingness indicator m, and (iii) an imputation confidence score c. To make these activations useful beyond the input layer, we introduce ChannelProp, an algorithm that deterministically propagates missingness and confidence values via linear layers based on weight magnitudes, retaining reliability signals throughout the network. We evaluate 3C-EA and ChannelProp on datasets with natural and injected (MCAR/MAR/MNAR) missingness at multiple rates under identical preprocessing and splits. Results indicate that integrating missingness and confidence inputs into the activation search improves classification performance under missingness.",
      "authors": [
        "Naeem Shahabi Sani",
        "Ferial Najiantabriz",
        "Shayan Shafaei",
        "Dean F. Hougen"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "published": "2026-02-14T19:52:10+00:00",
      "link": "https://arxiv.org/pdf/2602.13864v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.10740v1",
      "title": "Neuro-Symbolic Activation Discovery: Transferring Mathematical Structures from Physics to Ecology for Parameter-Efficient Neural Networks",
      "abstract": "Modern neural networks rely on generic activation functions (ReLU, GELU, SiLU) that ignore the mathematical structure inherent in scientific data. We propose Neuro-Symbolic Activation Discovery, a framework that uses Genetic Programming to extract interpretable mathematical formulas from data and inject them as custom activation functions. Our key contribution is the discovery of a Geometric Transfer phenomenon: activation functions learned from particle physics data successfully generalize to ecological classification, outperforming standard activations (ReLU, GELU, SiLU) in both accuracy and parameter efficiency. On the Forest Cover dataset, our Hybrid Transfer model achieves 82.4% accuracy with only 5,825 parameters, compared to 83.4% accuracy requiring 31,801 parameters for a conventional heavy network -- a 5.5x parameter reduction with only 1% accuracy loss. We introduce a Parameter Efficiency Score ($E_{param} = AUC / \\log_{10}(Params)$) and demonstrate that lightweight hybrid architectures consistently achieve 18-21% higher efficiency than over-parameterized baselines. Crucially, we establish boundary conditions: while Physics to Ecology transfer succeeds (both involve continuous Euclidean measurements), Physics to Text transfer fails (discrete word frequencies require different mathematical structures). Our work opens pathways toward domain-specific activation libraries for efficient scientific machine learning.",
      "authors": [
        "Anas Hajbi"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-10T02:49:32+00:00",
      "link": "https://arxiv.org/pdf/2601.10740v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13410v1",
      "title": "Evolutionary design of thermodynamic logic gates and their heat emission",
      "abstract": "Landauer's principle bounds the heat generated by logical operations, but in practice the thermodynamic cost of computation is dominated by the control systems that implement logic. CMOS gates dissipate energy far above the Landauer bound, while laboratory demonstrations of near-Landauer erasure rely on external measurement or feedback systems whose energy costs exceed that of the logic operation by many orders of magnitude. Here we use simulations to show that a genetic algorithm can program a thermodynamic computer to implement logic operations in which the total heat emitted by the control system is of a similar order of magnitude to that of the information-bearing degrees of freedom. Moreover, the computer can be programmed so that heat is drawn away from the information-bearing degrees of freedom and dissipated within the control unit, suggesting the possibility of computing architectures in which heat management is an integral part of the program design.",
      "authors": [
        "Stephen Whitelam"
      ],
      "primary_category": "cond-mat.stat-mech",
      "categories": [
        "cond-mat.stat-mech",
        "cs.NE"
      ],
      "published": "2026-02-13T19:17:56+00:00",
      "link": "https://arxiv.org/pdf/2602.13410v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.14480v1",
      "title": "A benchmarking framework for PON-based fronthaul network design",
      "abstract": "As mobile networks transition toward 5G and 6G RAN architectures, Passive Optical Networks (PONs) offer a critical solution for cost-effective fronthaul transport. However, the lack of standardized evaluation models in current literature makes an objective comparison of diverse optimization strategies difficult. This paper addresses this gap by proposing a unified benchmarking framework that standardizes cost catalogs and deployment scenarios. We formulate the network design problem using Integer Linear Programming (ILP) to establish optimality bounds and evaluate three scalable heuristic strategies: a Genetic Algorithm, K-Means Clustering (KMC+), and a graph-based Randomized Successive Splitter Assignment (RSSA+) algorithm. Simulation results show that a time-limited ILP remains a strong reference point, even when optimality is not reached. Despite being rarely used in prior fronthaul planning studies, it consistently yields solutions superior to those produced by standard heuristic methods. Among scalable approaches, RSSA+ reliably attains near-ILP performance while ensuring feasibility across all evaluated scenarios, which underscores the importance of advanced, constraint-aware algorithmic designs over simpler heuristics. The complete benchmarking framework and datasets are publicly shared in [1].",
      "authors": [
        "Egemen Erbayat",
        "Gustavo B. Figueiredo",
        "Shih-Chun Lin",
        "Motoharu Matsuura",
        "Hiroshi Hasegawa",
        "Suresh Subramaniam"
      ],
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI"
      ],
      "published": "2026-01-20T21:03:15+00:00",
      "link": "https://arxiv.org/pdf/2601.14480v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04901v1",
      "title": "Beyond Independent Genes: Learning Module-Inductive Representations for Gene Perturbation Prediction",
      "abstract": "Predicting transcriptional responses to genetic perturbations is a central problem in functional genomics. In practice, perturbation responses are rarely gene-independent but instead manifest as coordinated, program-level transcriptional changes among functionally related genes. However, most existing methods do not explicitly model such coordination, due to gene-wise modeling paradigms and reliance on static biological priors that cannot capture dynamic program reorganization. To address these limitations, we propose scBIG, a module-inductive perturbation prediction framework that explicitly models coordinated gene programs. scBIG induces coherent gene programs from data via Gene-Relation Clustering, captures inter-program interactions through a Gene-Cluster-Aware Encoder, and preserves modular coordination using structure-aware alignment objectives. These structured representations are then modeled using conditional flow matching to enable flexible and generalizable perturbation prediction. Extensive experiments on multiple single-cell perturbation benchmarks show that scBIG consistently outperforms state-of-the-art methods, particularly on unseen and combinatorial perturbation settings, achieving an average improvement of 6.7% over the strongest baselines.",
      "authors": [
        "Jiafa Ruan",
        "Ruijie Quan",
        "Zongxin Yang",
        "Liyang Xu",
        "Yi Yang"
      ],
      "primary_category": "q-bio.GN",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "published": "2026-02-03T16:43:40+00:00",
      "link": "https://arxiv.org/pdf/2602.04901v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.12274v1",
      "title": "Hybrid Concolic Testing with Large Language Models for Guided Path Exploration",
      "abstract": "Concolic testing, a powerful hybrid software testing technique, has historically been plagued by fundamental limitations such as path explosion and the high cost of constraint solving, which hinder its practical application in large-scale, real-world software systems. This paper introduces a novel algorithmic framework that synergistically integrates concolic execution with Large Language Models (LLMs) to overcome these challenges. Our hybrid approach leverages the semantic reasoning capabilities of LLMs to guide path exploration, prioritize interesting execution paths, and assist in constraint solving. We formally define the system architecture and algorithms that constitute this new paradigm. Through a series of experiments on both synthetic and real-world Fintech applications, we demonstrate that our approach significantly outperforms traditional concolic testing, random testing, and genetic algorithm-based methods in terms of branch coverage, path coverage, and time-to-coverage. The results indicate that by combining the strengths of both concolic execution and LLMs, our method achieves a more efficient and effective exploration of the program state space, leading to improved bug detection capabilities.",
      "authors": [
        "Mahdi Eslamimehr"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-01-18T06:09:18+00:00",
      "link": "https://arxiv.org/pdf/2601.12274v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11487v1",
      "title": "Search-Based Quantum Program Testing via Commuting Pauli String",
      "abstract": "Quantum software testing is important for reliable quantum software engineering. Despite recent advances, existing quantum software testing approaches rely on simple test inputs and statistical oracles, costly program specifications, and limited validation on real quantum computers. To address these challenges, we propose SB-QOPS, a search-based quantum program testing approach via commuting Pauli strings. SB-QOPS, as a direct extension to a previously proposed QOPS approach, redefines test cases in terms of Pauli strings and introduces a measurement-centric oracle that exploits their commutation properties, enabling effective testing of quantum programs while reducing the need for full program specifications. By systematically exploring the search space through an expectation-value-based fitness function, SB-QOPS improves test budget utilization and increases the likelihood of uncovering subtle faults. We conduct a large-scale empirical evaluation on quantum circuits of up to 29 qubits on real quantum computers and emulators. We assess three search strategies: Genetic Algorithm, Hill Climbing, and the (1+1) Evolutionary Algorithm, and evaluate SB-QOPS under both simulated and real noisy conditions. Experiments span three quantum computing platforms: IBM, IQM, and Quantinuum. Results show that SB-QOPS significantly outperforms QOPS, achieving a fault-detection score of 100% for circuits up to 29 qubits, and demonstrating portability across quantum platforms.",
      "authors": [
        "Asmar Muqeet",
        "Shaukat Ali",
        "Paolo Arcaini"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-12T02:13:12+00:00",
      "link": "https://arxiv.org/pdf/2602.11487v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.08884v1",
      "title": "Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting",
      "abstract": "OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the \"nano\"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.",
      "authors": [
        "Samyak Jhaveri",
        "Cristina V. Lopes"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-01-12T23:54:08+00:00",
      "link": "https://arxiv.org/pdf/2601.08884v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.06820v1",
      "title": "Bgolearn: a Unified Bayesian Optimization Framework for Accelerating Materials Discovery",
      "abstract": "Efficient exploration of vast compositional and processing spaces is essential for accelerated materials discovery. Bayesian optimization (BO) provides a principled strategy for identifying optimal materials with minimal experiments, yet its adoption in materials science is hindered by implementation complexity and limited domain-specific tools. Here, we present Bgolearn, a comprehensive Python framework that makes BO accessible and practical for materials research through an intuitive interface, robust algorithms, and materials-oriented workflows. Bgolearn supports both single-objective and multi-objective Bayesian optimization with multiple acquisition functions (e.g., expected improvement, upper confidence bound, probability of improvement, and expected hypervolume improvement etc.), diverse surrogate models (including Gaussian processes, random forests, and gradient boosting etc.), and bootstrap-based uncertainty quantification. Benchmark studies show that Bgolearn reduces the number of required experiments by 40-60% compared with random search, grid search, and genetic algorithms, while maintaining comparable or superior solution quality. Its effectiveness is demonstrated not only through the studies presented in this paper, such as the identification of maximum-elastic-modulus triply periodic minimal surface structures, ultra-high-hardness high-entropy alloys, and high-strength, high-ductility medium-Mn steels, but also by numerous publications that have proven its impact in material discovery. With a modular architecture that integrates seamlessly into existing materials workflows and a graphical user interface (BgoFace) that removes programming barriers, Bgolearn establishes a practical and reliable platform for Bayesian optimization in materials science, and is openly available at https://github.com/Bin-Cao/Bgolearn.",
      "authors": [
        "Bin Cao",
        "Jie Xiong",
        "Jiaxuan Ma",
        "Yuan Tian",
        "Yirui Hu",
        "Mengwei He",
        "Longhan Zhang",
        "Jiayu Wang",
        "Jian Hui",
        "Li Liu",
        "Dezhen Xue",
        "Turab Lookman",
        "Tong-Yi Zhang"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-01-11T09:09:21+00:00",
      "link": "https://arxiv.org/pdf/2601.06820v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.01209v1",
      "title": "Heuristics for the Worst Optimal Value of Interval Transportation Problems",
      "abstract": "An interval transportation problem represents a model for a transportation problem in which the values of supply, demand, and transportation costs are affected by uncertainty and can vary independently within given interval ranges. One of the main tasks of solving interval programming models is computing the best and worst optimal value over all possible choices of the interval data. Although the best optimal value of an interval transportation problem can be computed in polynomial time, computing the worst (finite) optimal value was proved to be NP-hard. In this paper, we strengthen a previous result showing a quasi-extreme decomposition for finding the worst optimal value, and building on the result, we design heuristics for efficiently approximating the value. Using a simplified encoding of the scenarios, we first derive a local search method and a genetic algorithm for approximating the worst optimal value. Then, we integrate the two methods into a memetic algorithm, which combines the evolutionary improvement of a genetic algorithm with individual learning implemented via local search. Moreover, we include numerical experiments for a practical comparison of the three different approaches. We also show that the proposed memetic algorithm is competitive with the available state-of-the-art methods for approximating the worst optimal value of interval transportation problems, this is demonstrated by finding the new best solutions for several instances, among others.",
      "authors": [
        "Elif Radová Garajová",
        "Miroslav Rada"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-01T13:01:38+00:00",
      "link": "https://arxiv.org/pdf/2602.01209v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.20693v1",
      "title": "Drone-Aided Blood Collection Routing Problem: A Column Generation Approach",
      "abstract": "Platelet extraction requires whole blood to be processed within six hours of donation. To meet this deadline, blood collection organizations must optimally route a fleet of vehicles to pick up blood units from donation sites and deliver them to a processing center. This paper introduces a drone-aided blood collection routing problem in which a fleet of trucks, each equipped with a drone, operates in a synchronized manner to collect blood units before their processing time limit expires. Each truck-drone tandem can perform multiple trips throughout the planning horizon, allowing donation sites to be visited repeatedly as new blood units become available over time. We formulate this problem as a mixed-integer linear program that jointly optimizes the routing of trucks and drones, pickup schedules, and timing decisions to maximize the total number of viable blood units collected. We also develop a column generation approach that decomposes the problem into a master problem to select the optimal set of truck-drone tours and a pricing subproblem, which is solved using a tailored memetic algorithm to generate promising new columns. Through a comprehensive computational study, we show the operational benefits of integrating drones into the blood collection system. In addition, we demonstrate the superior performance of the proposed algorithm over Gurobi and two metaheuristics from the literature, namely the hybrid genetic algorithm and the invasive weed optimization, in both the drone-aided and truck-only settings.",
      "authors": [
        "Amirhossein Abbaszadeh",
        "Hossein Hashemi Doulabi"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-28T15:23:05+00:00",
      "link": "https://arxiv.org/pdf/2601.20693v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.00429v1",
      "title": "A Hybrid Relaxation-Heuristic Framework for Solving MIP with Binary Variables",
      "abstract": "Mixed-Integer Programming (MIP), particularly Mixed-Integer Linear Programming (MILP) and Mixed-Integer Quadratic Programming (MIQP), has found extensive applications in domains such as portfolio optimization and network flow control, which inclusion of integer variables or cardinality constraints renders these problems NP-hard, posing significant computational challenges. While traditional approaches have explored approximation methods like heuristics and relaxation techniques (e.g. Lagrangian dual relaxation), the integration of these strategies within a unified hybrid framework remains underexplored. In this paper, we propose a generalized hybrid framework to address MIQP problems with binary variables, which consists of two phases: (1) a Mixed Relaxation Phase, which employs Linear Relaxation, Duality Relaxation, and Augmented Relaxation with randomized sampling to generate a diverse pre-solution pool, and (2) a Heuristic Optimization Phase, which refines the pool using Genetic Algorithms and Variable Neighborhood Search (VNS) to approximate binary solutions effectively. Becuase of the page limit, we will only detailedly evaluate the proposed framework on portfolio optimization problems using benchmark datasets from the OR Library, where the experimental results demonstrate state-of-the-art performance, highlighting the framework's ability to solve larger and more complex MIP problems efficiently. This study offers a robust and flexible methodology that bridges relaxation techniques and heuristic optimization, advancing the practical solvability of challenging MIP problems.",
      "authors": [
        "Zayn Wang"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-31T00:50:30+00:00",
      "link": "https://arxiv.org/pdf/2602.00429v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.13407v2",
      "title": "A Joint Survival Modeling and Therapy Knowledge Graph Framework to Characterize Opioid Use Disorder Trajectories",
      "abstract": "Motivation: Opioid use disorder (OUD) often arises after prescription opioid exposure and follows transitions among onset, remission, and relapse. Linked EHR-survey resources such as the All of Us Research Program enable stage-specific risk modeling and connection to intervention options. Results: We built a multi-stage framework to model time-to-onset, time-to-remission, and time-to-relapse after remission using All of Us EHR and survey data. For each participant we derived longitudinal predictors from clinical conditions and survey concepts, including recent (1/3/12-month) event counts, cumulative exposures, and time since last event. We fit regularized Cox models for each transition and aggregated selection frequencies and hazard ratios to identify a compact set of high-confidence predictors. Pain, mental health, and polysubstance use contributed across stages: chronic pain syndromes, tobacco/nicotine dependence, anxiety and depressive disorders, and cannabis dependence prominently predicted onset and relapse, whereas tobacco dependence during remission and other remission-coded conditions were strongly associated with transition to remission. To support therapeutic prioritization, we constructed a therapy knowledge graph integrating genetic targets, biological pathways, and published evidence to map identified risk factors to candidate treatments in recent OUD studies and clinical guidelines.",
      "authors": [
        "Mengman Wei",
        "Stanislav Listopad",
        "Qian Peng"
      ],
      "primary_category": "q-bio.QM",
      "categories": [
        "q-bio.QM"
      ],
      "published": "2026-01-19T21:34:10+00:00",
      "link": "https://arxiv.org/pdf/2601.13407v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.02710v1",
      "title": "Maximum Likelihood Reinforcement Learning",
      "abstract": "Reinforcement learning is the method of choice to train models in sampling-based setups with binary outcome feedback, such as navigation, code generation, and mathematical problem solving. In such settings, models implicitly induce a likelihood over correct rollouts. However, we observe that reinforcement learning does not maximize this likelihood, and instead optimizes only a lower-order approximation. Inspired by this observation, we introduce Maximum Likelihood Reinforcement Learning (MaxRL), a sampling-based framework to approximate maximum likelihood using reinforcement learning techniques. MaxRL addresses the challenges of non-differentiable sampling by defining a compute-indexed family of sample-based objectives that interpolate between standard reinforcement learning and exact maximum likelihood as additional sampling compute is allocated. The resulting objectives admit a simple, unbiased policy-gradient estimator and converge to maximum likelihood optimization in the infinite-compute limit. Empirically, we show that MaxRL Pareto-dominates existing methods in all models and tasks we tested, achieving up to 20x test-time scaling efficiency gains compared to its GRPO-trained counterpart. We also observe MaxRL to scale better with additional data and compute. Our results suggest MaxRL is a promising framework for scaling RL training in correctness based settings.",
      "authors": [
        "Fahim Tajwar",
        "Guanning Zeng",
        "Yueer Zhou",
        "Yuda Song",
        "Daman Arora",
        "Yiding Jiang",
        "Jeff Schneider",
        "Ruslan Salakhutdinov",
        "Haiwen Feng",
        "Andrea Zanette"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-02T19:23:42+00:00",
      "link": "https://arxiv.org/pdf/2602.02710v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13949v1",
      "title": "Experiential Reinforcement Learning",
      "abstract": "Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.",
      "authors": [
        "Taiwei Shi",
        "Sihao Chen",
        "Bowen Jiang",
        "Linxin Song",
        "Longqi Yang",
        "Jieyu Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15T01:23:48+00:00",
      "link": "https://arxiv.org/pdf/2602.13949v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.07948v1",
      "title": "Reinforcement Learning Methods for Neighborhood Selection in Local Search",
      "abstract": "Reinforcement learning has recently gained traction as a means to improve combinatorial optimization methods, yet its effectiveness within local search metaheuristics specifically remains comparatively underexamined. In this study, we evaluate a range of reinforcement learning-based neighborhood selection strategies -- multi-armed bandits (upper confidence bound, $ε$-greedy) and deep reinforcement learning methods (proximal policy optimization, double deep $Q$-network) -- and compare them against multiple baselines across three different problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. We show how search-specific characteristics, particularly large variations in cost due to constraint violation penalties, necessitate carefully designed reward functions to provide stable and informative learning signals. Our extensive experiments reveal that algorithm performance varies substantially across problems, although that $ε$-greedy consistently ranks among the best performers. In contrast, the computational overhead of deep reinforcement learning approaches only makes them competitive with a substantially longer runtime. These findings highlight both the promise and the practical limitations of deep reinforcement learning in local search.",
      "authors": [
        "Yannick Molinghen",
        "Augustin Delecluse",
        "Renaud De Landtsheer",
        "Stefano Michelini"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-12T19:25:29+00:00",
      "link": "https://arxiv.org/pdf/2601.07948v1",
      "tags": [
        "keyword:SR",
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.16543v1",
      "title": "Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning",
      "abstract": "Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy's gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy's internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.",
      "authors": [
        "Jialiang Fan",
        "Shixiong Jiang",
        "Mengyu Liu",
        "Fanxin Kong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18T15:43:36+00:00",
      "link": "https://arxiv.org/pdf/2602.16543v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.07719v1",
      "title": "Efficient Planning in Reinforcement Learning via Model Introspection",
      "abstract": "Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.",
      "authors": [
        "Gabriel Stella"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-07T21:49:21+00:00",
      "link": "https://arxiv.org/pdf/2602.07719v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.18419v1",
      "title": "Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication",
      "abstract": "Emergent cooperation in classical Multi-Agent Reinforcement Learning has gained significant attention, particularly in the context of Sequential Social Dilemmas (SSDs). While classical reinforcement learning approaches have demonstrated capability for emergent cooperation, research on extending these methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication. In this paper, we apply communication approaches to quantum Q-Learning agents: the Mutual Acknowledgment Token Exchange (MATE) protocol, its extension Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange (MEDIATE), the peer rewarding mechanism Gifting, and Reinforced Inter-Agent Learning (RIAL). We evaluate these approaches in three SSDs: the Iterated Prisoner's Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. Our experimental results show that approaches using MATE with temporal-difference measure (MATE\\textsubscript{TD}), AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all dilemmas, demonstrating that communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning.",
      "authors": [
        "Michael Kölle",
        "Christian Reff",
        "Leo Sünkel",
        "Julian Hager",
        "Gerhard Stenzel",
        "Claudia Linnhoff-Popien"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2026-01-26T12:21:05+00:00",
      "link": "https://arxiv.org/pdf/2601.18419v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.18953v1",
      "title": "Reinforcement Learning for Quantum Technology",
      "abstract": "Many challenges arising in Quantum Technology can be successfully addressed using a set of machine learning algorithms collectively known as reinforcement learning (RL), based on adaptive decision-making through interaction with the quantum device. After a concise and intuitive introduction to RL aimed at a broad physics readership, we discuss the key ideas and core concepts in reinforcement learning with a particular focus on quantum systems. We then survey recent progress in RL in all relevant areas. We discuss state preparation in few- and many-body quantum systems, the design and optimization of high-fidelity quantum gates, and the automated construction of quantum circuits, including applications to variational quantum eigensolvers and architecture search. We further highlight the interactive capabilities of RL agents, emphasizing recent progress in quantum feedback control and quantum error correction, and briefly discuss quantum reinforcement learning as well as applications to quantum metrology. The review concludes with a discussion of open challenges -- such as scalability, interpretability, and integration with experimental platforms -- and outlines promising directions for future research. Throughout, we highlight experimental implementations that exemplify the increasing role of reinforcement learning in shaping the development of quantum technologies.",
      "authors": [
        "Marin Bukov",
        "Florian Marquardt"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cond-mat.quant-gas",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-26T20:47:48+00:00",
      "link": "https://arxiv.org/pdf/2601.18953v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16475v1",
      "title": "Certifying Hamilton-Jacobi Reachability Learned via Reinforcement Learning",
      "abstract": "We present a framework to \\emph{certify} Hamilton--Jacobi (HJ) reachability learned by reinforcement learning (RL). Building on a discounted initial time \\emph{travel-cost} formulation that makes small-step RL value iteration provably equivalent to a forward Hamilton--Jacobi (HJ) equation with damping, we convert certified learning errors into calibrated inner/outer enclosures of strict backward reachable tube. The core device is an additive-offset identity: if $W_λ$ solves the discounted travel-cost Hamilton--Jacobi--Bellman (HJB) equation, then $W_\\varepsilon:=W_λ+ \\varepsilon$ solves the same PDE with a constant offset $λ\\varepsilon$. This means that a uniform value error is \\emph{exactly} equal to a constant HJB offset. We establish this uniform value error via two routes: (A) a Bellman operator-residual bound, and (B) a HJB PDE-slack bound. Our framework preserves HJ-level safety semantics and is compatible with deep RL. We demonstrate the approach on a double-integrator system by formally certifying, via satisfiability modulo theories (SMT), a value function learned through reinforcement learning to induce provably correct inner and outer backward-reachable set enclosures over a compact region of interest.",
      "authors": [
        "Prashant Solanki",
        "Isabelle El-Hajj",
        "Jasper J. van Beers",
        "Erik-Jan van Kampen",
        "Coen C. de Visser"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-18T14:05:17+00:00",
      "link": "https://arxiv.org/pdf/2602.16475v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.07821v1",
      "title": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation",
      "abstract": "Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.",
      "authors": [
        "Huanyu Li",
        "Kun Lei",
        "Sheng Zang",
        "Kaizhe Hu",
        "Yongyuan Liang",
        "Bo An",
        "Xiaoli Li",
        "Huazhe Xu"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-12T18:53:11+00:00",
      "link": "https://arxiv.org/pdf/2601.07821v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.06960v2",
      "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
      "abstract": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
      "authors": [
        "Yuchen Yan",
        "Liang Jiang",
        "Jin Jiang",
        "Shuaicheng Li",
        "Zujie Wen",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Jian Shao",
        "Yueting Zhuang",
        "Yongliang Shen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-06T18:59:27+00:00",
      "link": "https://arxiv.org/pdf/2602.06960v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.01260v1",
      "title": "Sample Efficient Active Algorithms for Offline Reinforcement Learning",
      "abstract": "Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.",
      "authors": [
        "Soumyadeep Roy",
        "Shashwat Kushwaha",
        "Ambedkar Dukkipati"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-01T14:38:07+00:00",
      "link": "https://arxiv.org/pdf/2602.01260v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12099v1",
      "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
      "abstract": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}.",
      "authors": [
        "GigaBrain Team",
        "Boyuan Wang",
        "Chaojun Ni",
        "Guan Huang",
        "Guosheng Zhao",
        "Hao Li",
        "Jie Li",
        "Jindi Lv",
        "Jingyu Liu",
        "Lv Feng",
        "Mingming Yu",
        "Peng Li",
        "Qiuping Deng",
        "Tianze Liu",
        "Xinyu Zhou",
        "Xinze Chen",
        "Xiaofeng Wang",
        "Yang Wang",
        "Yifan Li",
        "Yifei Nie",
        "Yilong Li",
        "Yukun Zhou",
        "Yun Ye",
        "Zhichao Liu",
        "Zheng Zhu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-12T15:55:19+00:00",
      "link": "https://arxiv.org/pdf/2602.12099v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.08244v1",
      "title": "Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers",
      "abstract": "In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.",
      "authors": [
        "Juncheng Dong",
        "Bowen He",
        "Moyang Guo",
        "Ethan X. Fang",
        "Zhuoran Yang",
        "Vahid Tarokh"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09T03:42:16+00:00",
      "link": "https://arxiv.org/pdf/2602.08244v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.12886v1",
      "title": "Communication Methods in Multi-Agent Reinforcement Learning",
      "abstract": "Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.",
      "authors": [
        "Christoph Wittner"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-19T09:39:00+00:00",
      "link": "https://arxiv.org/pdf/2601.12886v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.23058v1",
      "title": "From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning",
      "abstract": "Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.",
      "authors": [
        "Wenzhe Niu",
        "Wei He",
        "Zongxia Xie",
        "Jinpeng Ou",
        "Huichuan Fan",
        "Yuchen Ge",
        "Yanru Sun",
        "Ziyin Wang",
        "Yizhao Sun",
        "Chengshun Shi",
        "Jiuchong Gao",
        "Jinghua Hao",
        "Renqing He"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-30T15:07:06+00:00",
      "link": "https://arxiv.org/pdf/2601.23058v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.21912v1",
      "title": "ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation",
      "abstract": "Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to \"process hallucinations\", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.",
      "authors": [
        "Zhao Wang",
        "Ziliang Zhao",
        "Zhicheng Dou"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-01-29T16:04:59+00:00",
      "link": "https://arxiv.org/pdf/2601.21912v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.18533v1",
      "title": "From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.",
      "authors": [
        "Yuxin Jiang",
        "Yufei Wang",
        "Qiyuan Zhang",
        "Xingshan Zeng",
        "Liangyou Li",
        "Jierun Chen",
        "Chaofan Tao",
        "Haoli Bai",
        "Lifeng Shang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-26T14:39:58+00:00",
      "link": "https://arxiv.org/pdf/2601.18533v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.21312v1",
      "title": "Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach",
      "abstract": "With the rapid expansion of electric vehicles (EVs) and charging infrastructure, the effective management of Autonomous Electric Taxi (AET) fleets faces a critical challenge in environments with dynamic and uncertain charging availability. While most existing research assumes a static charging network, this simplification creates a significant gap between theoretical models and real-world operations. To bridge this gap, we propose GAT-PEARL, a novel meta-reinforcement learning framework that learns an adaptive operational policy. Our approach integrates a graph attention network (GAT) to effectively extract robust spatial representations under infrastructure layouts and model the complex spatiotemporal relationships of the urban environment, and employs probabilistic embeddings for actor-critic reinforcement learning (PEARL) to enable rapid, inference-based adaptation to changes in charging network layouts without retraining. Through extensive simulations on real-world data in Chengdu, China, we demonstrate that GAT-PEARL significantly outperforms conventional reinforcement learning baselines, showing superior generalization to unseen infrastructure layouts and achieving higher overall operational efficiency in dynamic settings.",
      "authors": [
        "Xiaozhuang Li",
        "Xindi Tang",
        "Fang He"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-29T06:16:34+00:00",
      "link": "https://arxiv.org/pdf/2601.21312v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.20802v1",
      "title": "Reinforcement Learning via Self-Distillation",
      "abstract": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.",
      "authors": [
        "Jonas Hübotter",
        "Frederike Lübeck",
        "Lejs Behric",
        "Anton Baumann",
        "Marco Bagatella",
        "Daniel Marta",
        "Ido Hakimi",
        "Idan Shenfeld",
        "Thomas Kleine Buening",
        "Carlos Guestrin",
        "Andreas Krause"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-28T17:45:12+00:00",
      "link": "https://arxiv.org/pdf/2601.20802v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.22149v1",
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.",
      "authors": [
        "Hang Ding",
        "Peidong Liu",
        "Junqiao Wang",
        "Ziwei Ji",
        "Meng Cao",
        "Rongzhao Zhang",
        "Lynn Ai",
        "Eric Yang",
        "Tianyu Shi",
        "Lei Yu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-29T18:59:07+00:00",
      "link": "https://arxiv.org/pdf/2601.22149v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10894v1",
      "title": "Resource-Efficient Model-Free Reinforcement Learning for Board Games",
      "abstract": "Board games have long served as complex decision-making benchmarks in artificial intelligence. In this field, search-based reinforcement learning methods such as AlphaZero have achieved remarkable success. However, their significant computational demands have been pointed out as barriers to their reproducibility. In this study, we propose a model-free reinforcement learning algorithm designed for board games to achieve more efficient learning. To validate the efficiency of the proposed method, we conducted comprehensive experiments on five board games: Animal Shogi, Gardner Chess, Go, Hex, and Othello. The results demonstrate that the proposed method achieves more efficient learning than existing methods across these environments. In addition, our extensive ablation study shows the importance of core techniques used in the proposed method. We believe that our efficient algorithm shows the potential of model-free reinforcement learning in domains traditionally dominated by search-based methods.",
      "authors": [
        "Kazuki Ota",
        "Takayuki Osa",
        "Motoki Omura",
        "Tatsuya Harada"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11T14:25:38+00:00",
      "link": "https://arxiv.org/pdf/2602.10894v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.20688v1",
      "title": "Grover's Search-Inspired Quantum Reinforcement Learning for Massive MIMO User Scheduling",
      "abstract": "The efficient user scheduling policy in the massive Multiple Input Multiple Output (mMIMO) system remains a significant challenge in the field of 5G and Beyond 5G (B5G) due to its high computational complexity, scalability, and Channel State Information (CSI) overhead. This paper proposes a novel Grover's search-inspired Quantum Reinforcement Learning (QRL) framework for mMIMO user scheduling. The QRL agent can explore the exponentially large scheduling space effectively by applying Grover's search to the reinforcement learning process. The model is implemented using our designed quantum-gate-based circuit, which imitates the layered architecture of reinforcement learning, where quantum operations act as policy updates and decision-making units. Moreover, the simulation results demonstrate that the proposed method achieves proper convergence and significantly outperforms classical Convolutional Neural Networks (CNN) and Quantum Deep Learning (QDL) benchmarks.",
      "authors": [
        "Ruining Fan",
        "Xingyu Huang",
        "Mouli Chakraborty",
        "Avishek Nag",
        "Anshu Mukherjee"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-01-28T15:14:51+00:00",
      "link": "https://arxiv.org/pdf/2601.20688v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.13284v1",
      "title": "Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning",
      "abstract": "Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR's failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR's accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.",
      "authors": [
        "Duygu Nur Yaldiz",
        "Evangelia Spiliopoulou",
        "Zheng Qi",
        "Siddharth Varia",
        "Srikanth Doss",
        "Nikolaos Pappas"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-19T18:31:31+00:00",
      "link": "https://arxiv.org/pdf/2601.13284v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.05578v1",
      "title": "Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection",
      "abstract": "E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.",
      "authors": [
        "Cooper Lin",
        "Yanting Zhang",
        "Maohao Ran",
        "Wei Xue",
        "Hongwei Fan",
        "Yibo Xu",
        "Zhenglin Wan",
        "Sirui Han",
        "Yike Guo",
        "Jun Song"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "published": "2026-01-09T06:56:27+00:00",
      "link": "https://arxiv.org/pdf/2601.05578v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12146v1",
      "title": "Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning",
      "abstract": "Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.",
      "authors": [
        "Mahdi Khodabandeh",
        "Ghazal Shabani",
        "Arash Yousefi Jordehi",
        "Seyed Abolghasem Mirroshandel"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IT"
      ],
      "published": "2026-02-12T16:30:55+00:00",
      "link": "https://arxiv.org/pdf/2602.12146v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.17275v1",
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.",
      "authors": [
        "Lianlei Shan",
        "Han Chen",
        "Yixuan Wang",
        "Zhenjie Liu",
        "Wei Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-24T03:18:22+00:00",
      "link": "https://arxiv.org/pdf/2601.17275v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14697v1",
      "title": "Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs",
      "abstract": "Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL",
      "authors": [
        "Lunjun Zhang",
        "Ryan Chen",
        "Bradly C. Stadie"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-16T12:34:27+00:00",
      "link": "https://arxiv.org/pdf/2602.14697v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.06603v2",
      "title": "The hidden risks of temporal resampling in clinical reinforcement learning",
      "abstract": "Offline reinforcement learning (ORL) has shown potential for improving decision-making in healthcare. However, contemporary research typically aggregates patient data into fixed time intervals, simplifying their mapping to standard ORL frameworks. The impact of these temporal manipulations on model safety and efficacy remains poorly understood. In this work, using both a gridworld navigation task and the UVA/Padova clinical diabetes simulator, we demonstrate that temporal resampling significantly degrades the performance of offline reinforcement learning algorithms during live deployment. We propose three mechanisms that drive this failure: (i) the generation of counterfactual trajectories, (ii) the distortion of temporal expectations, and (iii) the compounding of generalisation errors. Crucially, we find that standard off-policy evaluation metrics can fail to detect these drops in performance. Our findings reveal a fundamental risk in current healthcare ORL pipelines and emphasise the need for methods that explicitly handle the irregular timing of clinical decision-making.",
      "authors": [
        "Thomas Frost",
        "Hrisheekesh Vaidya",
        "Steve Harris"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-06T11:02:06+00:00",
      "link": "https://arxiv.org/pdf/2602.06603v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14338v1",
      "title": "Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.",
      "authors": [
        "Zhi Zhang",
        "Zhen Han",
        "Costas Mavromatis",
        "Qi Zhu",
        "Yunyi Zhang",
        "Sheng Guan",
        "Dingmin Wang",
        "Xiong Zhou",
        "Shuai Wang",
        "Soji Adeshina",
        "Vassilis Ioannidis",
        "Huzefa Rangwala"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15T23:14:05+00:00",
      "link": "https://arxiv.org/pdf/2602.14338v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.06440v1",
      "title": "TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking",
      "abstract": "Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.",
      "authors": [
        "Sung-Hoon Yoon",
        "Ruizhi Qian",
        "Minda Zhao",
        "Weiyue Li",
        "Mengyu Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "published": "2026-02-06T07:11:10+00:00",
      "link": "https://arxiv.org/pdf/2602.06440v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.03048v2",
      "title": "CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning. However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.",
      "authors": [
        "Zhiyuan Yao",
        "Yi-Kai Zhang",
        "Yuxin Chen",
        "Yueqing Sun",
        "Zishan Xu",
        "Yu Yang",
        "Tianhao Hu",
        "Qi Gu",
        "Hui Su",
        "Xunliang Cai"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-03T03:14:36+00:00",
      "link": "https://arxiv.org/pdf/2602.03048v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.03048v3",
      "title": "CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning. However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.",
      "authors": [
        "Zhiyuan Yao",
        "Yi-Kai Zhang",
        "Yuxin Chen",
        "Yueqing Sun",
        "Zishan Xu",
        "Yu Yang",
        "Tianhao Hu",
        "Qi Gu",
        "Hui Su",
        "Xunliang Cai"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-03T03:14:36+00:00",
      "link": "https://arxiv.org/pdf/2602.03048v3",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.00400v1",
      "title": "KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning",
      "abstract": "Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.",
      "authors": [
        "Fan Yang",
        "Rui Meng",
        "Trudi Di Qi",
        "Ali Ezzati",
        "Yuxin Wen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-30T23:28:37+00:00",
      "link": "https://arxiv.org/pdf/2602.00400v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14468v1",
      "title": "LACONIC: Length-Aware Constrained Reinforcement Learning for LLM",
      "abstract": "Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.",
      "authors": [
        "Chang Liu",
        "Yiran Zhao",
        "Lawrence Liu",
        "Yaoqi Ye",
        "Csaba Szepesvári",
        "Lin F. Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16T05:09:40+00:00",
      "link": "https://arxiv.org/pdf/2602.14468v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11455v1",
      "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.",
      "authors": [
        "Zhengbo Jiao",
        "Shaobo Wang",
        "Zifan Zhang",
        "Wei Wang",
        "Bing Zhao",
        "Hu Wei",
        "Linfeng Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12T00:20:54+00:00",
      "link": "https://arxiv.org/pdf/2602.11455v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04879v1",
      "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.",
      "authors": [
        "Penghui Qi",
        "Xiangxin Zhou",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Min Lin",
        "Wee Sun Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T18:59:04+00:00",
      "link": "https://arxiv.org/pdf/2602.04879v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.17038v1",
      "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \\emph{single} policy network, causing \\emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \\textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \\emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.",
      "authors": [
        "Shengtian Yang",
        "Yu Li",
        "Shuo He",
        "Yewen Li",
        "Qingpeng Cai",
        "Peng Jiang",
        "Lei Feng"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-19T03:18:30+00:00",
      "link": "https://arxiv.org/pdf/2602.17038v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.02192v3",
      "title": "ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.",
      "authors": [
        "Jie Xiao",
        "Meng Chen",
        "Qingnan Ren",
        "Jingwei Song",
        "Jiaqi Huang",
        "Yangshen Deng",
        "Chris Tong",
        "Wanyi Chen",
        "Suli Wang",
        "Ziqian Bi",
        "Shuo Lu",
        "Yiqun Duan",
        "Xu Wang",
        "Rymon Yu",
        "Ween Yang",
        "Lynn Ai",
        "Eric Yang",
        "Bill Shi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published": "2026-02-02T14:57:53+00:00",
      "link": "https://arxiv.org/pdf/2602.02192v3",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.02192v2",
      "title": "ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.",
      "authors": [
        "Jie Xiao",
        "Meng Chen",
        "Qingnan Ren",
        "Song Jingwei",
        "Jiaqi Huang",
        "Yangshen Deng",
        "Chris Tong",
        "Wanyi Chen",
        "Suli Wang",
        "Ziqian Bi",
        "Shuo Lu",
        "Yiqun Duan",
        "Xu Wang",
        "Rymon Yu",
        "Ween Yang",
        "Lynn Ai",
        "Eric Yang",
        "Bill Shi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published": "2026-02-02T14:57:53+00:00",
      "link": "https://arxiv.org/pdf/2602.02192v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2601.18150v1",
      "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.",
      "authors": [
        "Zhaopeng Qiu",
        "Shuang Yu",
        "Jingqi Zhang",
        "Shuai Zhang",
        "Xue Huang",
        "Jingyi Yang",
        "Junjie Lai"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-26T05:12:05+00:00",
      "link": "https://arxiv.org/pdf/2601.18150v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.03352v1",
      "title": "PEGRL: Improving Machine Translation by Post-Editing Guided Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) has shown strong promise for LLM-based machine translation, with recent methods such as GRPO demonstrating notable gains; nevertheless, translation-oriented RL remains challenged by noisy learning signals arising from Monte Carlo return estimation, as well as a large trajectory space that favors global exploration over fine-grained local optimization. We introduce \\textbf{PEGRL}, a \\textit{two-stage} RL framework that uses post-editing as an auxiliary task to stabilize training and guide overall optimization. At each iteration, translation outputs are sampled to construct post-editing inputs, allowing return estimation in the post-editing stage to benefit from conditioning on the current translation behavior, while jointly supporting both global exploration and fine-grained local optimization. A task-specific weighting scheme further balances the contributions of translation and post-editing objectives, yielding a biased yet more sample-efficient estimator. Experiments on English$\\to$Finnish, English$\\to$Turkish, and English$\\leftrightarrow$Chinese show consistent gains over RL baselines, and for English$\\to$Turkish, performance on COMET-KIWI is comparable to advanced LLM-based systems (DeepSeek-V3.2).",
      "authors": [
        "Yunzhi Shen",
        "Hao Zhou",
        "Xin Huang",
        "Xue Han",
        "Junlan Feng",
        "Shujian Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-03T10:22:55+00:00",
      "link": "https://arxiv.org/pdf/2602.03352v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.00759v1",
      "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.",
      "authors": [
        "Zhipeng Chen",
        "Xiaobo Qin",
        "Wayne Xin Zhao",
        "Youbin Wu",
        "Ji-Rong Wen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-31T14:48:23+00:00",
      "link": "https://arxiv.org/pdf/2602.00759v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.06107v1",
      "title": "Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \\sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.",
      "authors": [
        "Zhuoming Chen",
        "Hongyi Liu",
        "Yang Zhou",
        "Haizhong Zheng",
        "Beidi Chen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T18:57:01+00:00",
      "link": "https://arxiv.org/pdf/2602.06107v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15245v1",
      "title": "MyoInteract: A Framework for Fast Prototyping of Biomechanical HCI Tasks using Reinforcement Learning",
      "abstract": "Reinforcement learning (RL)-based biomechanical simulations have the potential to revolutionise HCI research and interaction design, but currently lack usability and interpretability. Using the Human Action Cycle as a design lens, we identify key limitations of biomechanical RL frameworks and develop MyoInteract, a novel framework for fast prototyping of biomechanical HCI tasks. MyoInteract allows designers to setup tasks, user models, and training parameters from an easy-to-use GUI within minutes. It trains and evaluates muscle-actuated simulated users within minutes, reducing training times by up to 98%. A workshop study with 12 interaction designers revealed that MyoInteract allowed novices in biomechanical RL to successfully setup, train, and assess goal-directed user movements within a single session. By transforming biomechanical RL from a days-long expert task into an accessible hour-long workflow, this work significantly lowers barriers to entry and accelerates iteration cycles in HCI biomechanics research.",
      "authors": [
        "Ankit Bhattarai",
        "Hannah Selder",
        "Florian Fischer",
        "Arthur Fleig",
        "Per Ola Kristensson"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2026-02-16T22:51:57+00:00",
      "link": "https://arxiv.org/pdf/2602.15245v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10019v1",
      "title": "ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning",
      "abstract": "Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time. This limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively. To address this problem, we introduce \\textbf{ADORA} (\\textbf{A}dvantage \\textbf{D}ynamics via \\textbf{O}nline \\textbf{R}ollout \\textbf{A}daptation), a novel framework for policy optimization. ADORA dynamically adjusts the advantage function's weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates. Extensive evaluations across diverse model families and varying data scales demonstrate that ADORA is a robust and efficient framework. It significantly enhances long reasoning in both geometric and mathematical tasks, consistently achieving notable performance gains without requiring sensitive hyperparameter tuning.",
      "authors": [
        "Qingnan Ren",
        "Shiting Huang",
        "Zhen Fang",
        "Zehui Chen",
        "Lin Chen",
        "Lijun Li",
        "Feng Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-10T17:40:39+00:00",
      "link": "https://arxiv.org/pdf/2602.10019v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09207v1",
      "title": "CausalGDP: Causality-Guided Diffusion Policies for Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) has achieved remarkable success in a wide range of sequential decision-making problems. Recent diffusion-based policies further improve RL by modeling complex, high-dimensional action distributions. However, existing diffusion policies primarily rely on statistical associations and fail to explicitly account for causal relationships among states, actions, and rewards, limiting their ability to identify which action components truly cause high returns. In this paper, we propose Causality-guided Diffusion Policy (CausalGDP), a unified framework that integrates causal reasoning into diffusion-based RL. CausalGDP first learns a base diffusion policy and an initial causal dynamical model from offline data, capturing causal dependencies among states, actions, and rewards. During real-time interaction, the causal information is continuously updated and incorporated as a guidance signal to steer the diffusion process toward actions that causally influence future states and rewards. By explicitly considering causality beyond association, CausalGDP focuses policy optimization on action components that genuinely drive performance improvements. Experimental results demonstrate that CausalGDP consistently achieves competitive or superior performance over state-of-the-art diffusion-based and offline RL methods, especially in complex, high-dimensional control tasks.",
      "authors": [
        "Xiaofeng Xiao",
        "Xiao Hu",
        "Yang Ye",
        "Xubo Yue"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09T21:18:32+00:00",
      "link": "https://arxiv.org/pdf/2602.09207v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.01388v2",
      "title": "The Enhanced Physics-Informed Kolmogorov-Arnold Networks: Applications of Newton's Laws in Financial Deep Reinforcement Learning (RL) Algorithms",
      "abstract": "Deep Reinforcement Learning (DRL), a subset of machine learning focused on sequential decision-making, has emerged as a powerful approach for tackling financial trading problems. In finance, DRL is commonly used either to generate discrete trade signals or to determine continuous portfolio allocations. In this work, we propose a novel reinforcement learning framework for portfolio optimization that incorporates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into several DRL algorithms. The approach replaces conventional multilayer perceptrons with Kolmogorov-Arnold Networks (KANs) in both actor and critic components-utilizing learnable B-spline univariate functions to achieve parameter-efficient and more interpretable function approximation. During actor updates, we introduce a physics-informed regularization loss that promotes second-order temporal consistency between observed return dynamics and the action-induced portfolio adjustments. The proposed framework is evaluated across three equity markets-China, Vietnam, and the United States, covering both emerging and developed economies. Across all three markets, PIKAN-based agents consistently deliver higher cumulative and annualized returns, superior Sharpe and Calmar ratios, and more favorable drawdown characteristics compared to both standard DRL baselines and classical online portfolio-selection methods. This yields more stable training, higher Sharpe ratios, and superior performance compared to traditional DRL counterparts. The approach is particularly valuable in highly dynamic and noisy financial markets, where conventional DRL often suffers from instability and poor generalization.",
      "authors": [
        "Trang Thoi",
        "Hung Tran",
        "Tram Thoi",
        "Huaiyang Zhong"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE",
        "cs.LG"
      ],
      "published": "2026-02-01T18:48:33+00:00",
      "link": "https://arxiv.org/pdf/2602.01388v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13953v1",
      "title": "QuRL: Efficient Reinforcement Learning with Quantized Rollout",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.",
      "authors": [
        "Yuhang Li",
        "Reena Elangovan",
        "Xin Dong",
        "Priyadarshini Panda",
        "Brucek Khailany"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-15T01:48:10+00:00",
      "link": "https://arxiv.org/pdf/2602.13953v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10539v1",
      "title": "What Makes Value Learning Efficient in Residual Reinforcement Learning?",
      "abstract": "Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.",
      "authors": [
        "Guozheng Ma",
        "Lu Li",
        "Haoyu Wang",
        "Zixuan Liu",
        "Pierre-Luc Bacon",
        "Dacheng Tao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11T05:25:39+00:00",
      "link": "https://arxiv.org/pdf/2602.10539v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.01156v1",
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.",
      "authors": [
        "Shunpeng Yang",
        "Ben Liu",
        "Hua Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "published": "2026-02-01T11:08:09+00:00",
      "link": "https://arxiv.org/pdf/2602.01156v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.08267v1",
      "title": "Inverting Data Transformations via Diffusion Sampling",
      "abstract": "We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scientific modeling, where they can significantly distort observations. We take a probabilistic view and model the posterior over transformations as a Boltzmann distribution defined by an energy function on data space. To sample from this posterior, we introduce a diffusion process on Lie groups that keeps all updates on-manifold and only requires computations in the associated Lie algebra. Our method, Transformation-Inverting Energy Diffusion (TIED), relies on a new trivialized target-score identity that enables efficient score-based sampling of the transformation posterior. As a key application, we focus on test-time equivariance, where the objective is to improve the robustness of pretrained neural networks to input transformations. Experiments on image homographies and PDE symmetries demonstrate that TIED can restore transformed inputs to the training distribution at test time, showing improved performance over strong canonicalization and sampling baselines. Code is available at https://github.com/jw9730/tied.",
      "authors": [
        "Jinwoo Kim",
        "Sékou-Oumar Kaba",
        "Jiyun Park",
        "Seunghoon Hong",
        "Siamak Ravanbakhsh"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09T04:58:34+00:00",
      "link": "https://arxiv.org/pdf/2602.08267v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.08857v1",
      "title": "Discovering Interpretable Algorithms by Decompiling Transformers to RASP",
      "abstract": "Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.",
      "authors": [
        "Xinting Huang",
        "Aleksandra Bakalova",
        "Satwik Bhattamishra",
        "William Merrill",
        "Michael Hahn"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-09T16:22:29+00:00",
      "link": "https://arxiv.org/pdf/2602.08857v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.06300v1",
      "title": "Accelerating Vision Transformers on Brain Processing Unit",
      "abstract": "With the advancement of deep learning technologies, specialized neural processing hardware such as Brain Processing Units (BPUs) have emerged as dedicated platforms for CNN acceleration, offering optimized INT8 computation capabilities for convolutional operations. Meanwhile, Vision Transformer (ViT) models, such as the Data-efficient Image Transformer (DeiT), have demonstrated superior performance and play increasingly crucial roles in computer vision tasks. However, due to the architectural mismatch between CNN-optimized hardware and Vision Transformer computation characteristics--namely, that linear layers in Transformers operate on three-dimensional data while BPU acceleration is designed for four-dimensional convolution operations-it is difficult or even impossible to leverage BPU's advantages when deploying Vision Transformers. To address this challenge, we propose a novel approach that restructures the Vision Transformer by replacing linear layers and layer normalization operations with carefully designed convolutional operators. This enables DeiT to fully utilize the acceleration capabilities of BPUs, while allowing the original weight parameters to be inherited by the restructured models without retraining or fine-tuning. To the best of our knowledge, this is the first successful deployment of Vision Transformers that fully leverages BPU classification datasets demonstrate the effectiveness of our approach. Specifically, the quantized DeiT-Base model achieves 80.4% accuracy on ImageNet, compared to the original 81.8%, while obtaining up to a 3.8* inference speedup. Our finetuned DeiT model on the flower classification dataset also achieves excellent performance, with only a 0.5% accuracy drop for the DeiT-Base model, further demonstrating the effectiveness of our method.",
      "authors": [
        "Jinchi Tang",
        "Yan Guo"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-06T01:48:54+00:00",
      "link": "https://arxiv.org/pdf/2602.06300v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.08695v1",
      "title": "Trapped by simplicity: When Transformers fail to learn from noisy features",
      "abstract": "Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.",
      "authors": [
        "Evan Peters",
        "Ando Deng",
        "Matheus H. Zambianco",
        "Devin Blankespoor",
        "Achim Kempf"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09T14:14:39+00:00",
      "link": "https://arxiv.org/pdf/2602.08695v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.12681v1",
      "title": "Fool Me If You Can: On the Robustness of Binary Code Similarity Detection Models against Semantics-preserving Transformations",
      "abstract": "Binary code analysis plays an essential role in cybersecurity, facilitating reverse engineering to reveal the inner workings of programs in the absence of source code. Traditional approaches, such as static and dynamic analysis, extract valuable insights from stripped binaries, but often demand substantial expertise and manual effort. Recent advances in deep learning have opened promising opportunities to enhance binary analysis by capturing latent features and disclosing underlying code semantics. Despite the growing number of binary analysis models based on machine learning, their robustness to adversarial code transformations at the binary level remains underexplored. We evaluate the robustness of deep learning models for the task of binary code similarity detection (BCSD) under semantics-preserving transformations. The unique nature of machine instructions presents distinct challenges compared to the typical input perturbations found in other domains. We introduce asmFooler, a system that evaluates the resilience of BCSD models using a diverse set of adversarial code transformations that preserve functional semantics. We construct a dataset of 9,565 binary variants from 620 baseline samples by applying eight semantics-preserving transformations across six representative BCSD models. Our major findings highlight several key insights: i) model robustness relies on the processing pipeline, including code pre-processing, architecture, and feature selection; ii) adversarial transformation effectiveness is bounded by a budget shaped by model-specific constraints like input size and instruction expressive capacity; iii) well-crafted transformations can be highly effective with minimal perturbations; and iv) such transformations efficiently disrupt model decisions (e.g., misleading to false positives or false negatives) by focusing on semantically significant instructions.",
      "authors": [
        "Jiyong Uhm",
        "Minseok Kim",
        "Michalis Polychronakis",
        "Hyungjoon Koo"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published": "2026-02-13T07:23:15+00:00",
      "link": "https://arxiv.org/pdf/2602.12681v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.16450v1",
      "title": "On the Expressive Power of Floating-Point Transformers",
      "abstract": "The study on the expressive power of transformers shows that transformers are permutation equivariant, and they can approximate all permutation-equivariant continuous functions on a compact domain. However, these results are derived under real parameters and exact operations, while real implementations on computers can only use a finite set of numbers and inexact machine operations with round-off errors. In this work, we investigate the representability of floating-point transformers that use floating-point parameters and floating-point operations. Unlike existing results under exact operations, we first show that floating-point transformers can represent a class of non-permutation-equivariant functions even without positional encoding. Furthermore, we prove that floating-point transformers can represent all permutation-equivariant functions when the sequence length is bounded, but they cannot when the sequence length is large. We also found the minimal equivariance structure in floating-point transformers, and show that all non-trivial additive positional encoding can harm the representability of floating-point transformers.",
      "authors": [
        "Sejun Park",
        "Yeachan Park",
        "Geonho Hwang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-23T05:03:00+00:00",
      "link": "https://arxiv.org/pdf/2601.16450v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.10519v1",
      "title": "Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models",
      "abstract": "Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.",
      "authors": [
        "Andrea Melis",
        "Andrea Piroddi",
        "Roberto Girau"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-15T15:46:22+00:00",
      "link": "https://arxiv.org/pdf/2601.10519v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.07070v1",
      "title": "Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures",
      "abstract": "Standard Transformer architectures rely heavily on dense linear transformations, treating feature projection as a monolithic, full-rank operation. We argue that this formulation is inefficient and lacks the structural inductive bias necessary for distinguishing between local feature preservation and global context integration. To address this, we introduce the Hybrid Dual-Path Linear (HDPL) operator, which decomposes the affine transformation into two topologically distinct pathways: a sparse block-diagonal component for high-rank local processing, and a low-rank Variational Autoencoder (VAE) bottleneck for global context regularization. By \"surgically\" replacing specific projections (Query, Key, Value, Gate, Up) with HDPL operators while retaining standard dense layers for aggregation (Output, Down), we achieve a superior balance of efficiency and representational power. Experiments on the FineWeb-Edu dataset demonstrate that the HDPL architecture outperforms a standard Llama-style baseline, reducing validation loss while simultaneously reducing parameter count by 6.8%. Beyond immediate performance gains, we discuss how the explicit materialization of a probabilistic latent space within the Transformer backbone serves as a vital architectural affordance, offering new pathways for inference-time or hypernetwork induced control, continual adaptation, interpretability, and cross-model or cross-modal synchronization. The code is available at https://github.com/VladimerKhasia/HDPL",
      "authors": [
        "Vladimer Khasia"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T20:16:10+00:00",
      "link": "https://arxiv.org/pdf/2602.07070v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.14318v1",
      "title": "In Transformer We Trust? A Perspective on Transformer Architecture Failure Modes",
      "abstract": "Transformer architectures have revolutionized machine learning across a wide range of domains, from natural language processing to scientific computing. However, their growing deployment in high-stakes applications, such as computer vision, natural language processing, healthcare, autonomous systems, and critical areas of scientific computing including climate modeling, materials discovery, drug discovery, nuclear science, and robotics, necessitates a deeper and more rigorous understanding of their trustworthiness. In this work, we critically examine the foundational question: \\textitHow trustworthy are transformer models?} We evaluate their reliability through a comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy. We systematically examine the trustworthiness of transformer-based models in safety-critical applications spanning natural language processing, computer vision, and science and engineering domains, including robotics, medicine, earth sciences, materials science, fluid dynamics, nuclear science, and automated theorem proving; highlighting high-impact areas where these architectures are central and analyzing the risks associated with their deployment. By synthesizing insights across these diverse areas, we identify recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit the reliable deployment of transformers.",
      "authors": [
        "Trishit Mondal",
        "Ameya D. Jagtap"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-15T21:57:14+00:00",
      "link": "https://arxiv.org/pdf/2602.14318v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.13224v1",
      "title": "Functional Logic Program Transformations",
      "abstract": "Many tools used to process programs, like compilers, analyzers, or verifiers, perform transformations on their intermediate program representation, like abstract syntax trees. Implementing such program transformations is a non-trivial task, since it is necessary to iterate over the complete syntax tree and apply various transformations at nodes in a tree. In this paper we show how the features of functional logic programming are useful to implement program transformations in a compact and comprehensible manner. For this purpose, we propose to write program transformations as partially defined and non-deterministic operations. Since the implementation of non-determinism usually causes some overhead compared to deterministically defined operations, we compare our approach to a deterministic transformation method. We evaluate these alternatives for the functional logic language Curry and its intermediate representation FlatCurry which is used in various analysis and verification tools and compilers.",
      "authors": [
        "Michael Hanus",
        "Steven Libby"
      ],
      "primary_category": "cs.PL",
      "categories": [
        "cs.PL"
      ],
      "published": "2026-01-19T16:59:12+00:00",
      "link": "https://arxiv.org/pdf/2601.13224v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.14803v3",
      "title": "A physics inspired and efficient transform for optoacoustic systems",
      "abstract": "Optoacoustic imaging technologies require fast and accurate signal pre-processing algorithms to enable widespread deployment in clinical and home-care settings. However, they still rely on the Discrete Fourier Transform (DFT) as the default tool for essential signal-conditioning operations, which imposes hard limits on both execution speed and signal-retrieval accuracy. Here, we present a new transform whose building blocks are directly inspired by the physics of optoacoustic signal generation. We compared its performance with the DFT and other classical transforms on common signal-processing tasks using both simulations and experimental datasets. Our results indicate that the proposed transform not only sets a new lower bound on computational complexity relative to the DFT, but also substantially outperforms classical transforms on basic signal-processing operations in terms of accuracy. We expect this transform to catalyze broader adoption of optoacoustic methods.",
      "authors": [
        "Maria Rodriguez Saenz de Tejada",
        "Alvaro Jimenez",
        "Rodrigo Rojo",
        "Sergio Contador",
        "Juan Aguirre"
      ],
      "primary_category": "physics.med-ph",
      "categories": [
        "physics.med-ph"
      ],
      "published": "2026-02-16T14:54:00+00:00",
      "link": "https://arxiv.org/pdf/2602.14803v3",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.10876v1",
      "title": "Efficient Quantum Circuits for the Hilbert Transform",
      "abstract": "The quantum Fourier transform and quantum wavelet transform have been cornerstones of quantum information processing. However, for non-stationary signals and anomaly detection, the Hilbert transform can be a more powerful tool, yet no prior work has provided efficient quantum implementations for the discrete Hilbert transform. This letter presents a novel construction for a quantum Hilbert transform in polylogarithmic size and logarithmic depth for a signal of length $N$, exponentially fewer operations than classical algorithms for the same mapping. We generalize this algorithm to create any $d$-dimensional Hilbert transform in depth $O(d\\log N)$. Simulations demonstrate effectiveness for tasks such as power systems control and image processing, with exact agreement with classical results.",
      "authors": [
        "Henry Zhang",
        "Joseph Li"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "eess.SP"
      ],
      "published": "2026-01-15T22:02:32+00:00",
      "link": "https://arxiv.org/pdf/2601.10876v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.05618v1",
      "title": "Boundedness of the discrete Hilbert transform on discrete weighted Morrey spaces",
      "abstract": "The Hilbert transform is a multiplier operator and is widely used in the theory of Fourier transforms. The Hilbert transform was the motivation for the development of modern harmonic analysis. Its discrete version is also widely used in many areas of science and technology and plays an important role in digital signal processing. The essential motivation behind thinking about discrete transforms is that experimental data are most often not taken in a continuous manner but sampled at discrete time values. Since much of the data collected in both the physical sciences and engineering are discrete, the discrete Hilbert transform is a rather useful tool in these areas for the general analysis of this type of data. In this paper, we discuss the discrete Hilbert transform on discrete Weighted Morrey spaces and obtain its boundedness in these spaces.",
      "authors": [
        "Rashid Aliev",
        "Amil Jabiyev"
      ],
      "primary_category": "math.FA",
      "categories": [
        "math.FA"
      ],
      "published": "2026-01-09T08:21:47+00:00",
      "link": "https://arxiv.org/pdf/2601.05618v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.18274v1",
      "title": "TEFormer: Structured Bidirectional Temporal Enhancement Modeling in Spiking Transformers",
      "abstract": "In recent years, Spiking Neural Networks (SNNs) have achieved remarkable progress, with Spiking Transformers emerging as a promising architecture for energy-efficient sequence modeling. However, existing Spiking Transformers still lack a principled mechanism for effective temporal fusion, limiting their ability to fully exploit spatiotemporal dependencies. Inspired by feedforward-feedback modulation in the human visual pathway, we propose TEFormer, the first Spiking Transformer framework that achieves bidirectional temporal fusion by decoupling temporal modeling across its core components. Specifically, TEFormer employs a lightweight and hyperparameter-free forward temporal fusion mechanism in the attention module, enabling fully parallel computation, while incorporating a backward gated recurrent structure in the MLP to aggregate temporal information in reverse order and reinforce temporal consistency. Extensive experiments across a wide range of benchmarks demonstrate that TEFormer consistently and significantly outperforms strong SNN and Spiking Transformer baselines under diverse datasets. Moreover, through the first systematic evaluation of Spiking Transformers under different neural encoding schemes, we show that the performance gains of TEFormer remain stable across encoding choices, indicating that the improved temporal modeling directly translates into reliable accuracy improvements across varied spiking representations. These results collectively establish TEFormer as an effective and general framework for temporal modeling in Spiking Transformers.",
      "authors": [
        "Sicheng Shen",
        "Mingyang Lv",
        "Bing Han",
        "Dongcheng Zhao",
        "Guobin Shen",
        "Feifei Zhao",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-26T08:58:36+00:00",
      "link": "https://arxiv.org/pdf/2601.18274v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.16608v1",
      "title": "Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models",
      "abstract": "Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \\textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.",
      "authors": [
        "Melkamu Abay Mersha",
        "Jugal Kalita"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-18T17:03:10+00:00",
      "link": "https://arxiv.org/pdf/2602.16608v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.16914v1",
      "title": "A statistical perspective on transformers for small longitudinal cohort data",
      "abstract": "Modeling of longitudinal cohort data typically involves complex temporal dependencies between multiple variables. There, the transformer architecture, which has been highly successful in language and vision applications, allows us to account for the fact that the most recently observed time points in an individual's history may not always be the most important for the immediate future. This is achieved by assigning attention weights to observations of an individual based on a transformation of their values. One reason why these ideas have not yet been fully leveraged for longitudinal cohort data is that typically, large datasets are required. Therefore, we present a simplified transformer architecture that retains the core attention mechanism while reducing the number of parameters to be estimated, to be more suitable for small datasets with few time points. Guided by a statistical perspective on transformers, we use an autoregressive model as a starting point and incorporate attention as a kernel-based operation with temporal decay, where aggregation of multiple transformer heads, i.e. different candidate weighting schemes, is expressed as accumulating evidence on different types of underlying characteristics of individuals. This also enables a permutation-based statistical testing procedure for identifying contextual patterns. In a simulation study, the approach is shown to recover contextual dependencies even with a small number of individuals and time points. In an application to data from a resilience study, we identify temporal patterns in the dynamics of stress and mental health. This indicates that properly adapted transformers can not only achieve competitive predictive performance, but also uncover complex context dependencies in small data settings.",
      "authors": [
        "Kiana Farhadyar",
        "Maren Hackenberg",
        "Kira Ahrens",
        "Charlotte Schenk",
        "Bianca Kollmann",
        "Oliver Tüscher",
        "Klaus Lieb",
        "Michael M. Plichta",
        "Andreas Reif",
        "Raffael Kalisch",
        "Martin Wolkewitz",
        "Moritz Hess",
        "Harald Binder"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-18T22:03:59+00:00",
      "link": "https://arxiv.org/pdf/2602.16914v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.07930v1",
      "title": "Transformer-Based Approach for Automated Functional Group Replacement in Chemical Compounds",
      "abstract": "Functional group replacement is a pivotal approach in cheminformatics to enable the design of novel chemical compounds with tailored properties. Traditional methods for functional group removal and replacement often rely on rule-based heuristics, which can be limited in their ability to generate diverse and novel chemical structures. Recently, transformer-based models have shown promise in improving the accuracy and efficiency of molecular transformations, but existing approaches typically focus on single-step modeling, lacking the guarantee of structural similarity. In this work, we seek to advance the state of the art by developing a novel two-stage transformer model for functional group removal and replacement. Unlike one-shot approaches that generate entire molecules in a single pass, our method generates the functional group to be removed and appended sequentially, ensuring strict substructure-level modifications. Using a matched molecular pairs (MMPs) dataset derived from ChEMBL, we trained an encoder-decoder transformer model with SMIRKS-based representations to capture transformation rules effectively. Extensive evaluations demonstrate our method's ability to generate chemically valid transformations, explore diverse chemical spaces, and maintain scalability across varying search sizes.",
      "authors": [
        "Bo Pan",
        "Zhiping Zhang",
        "Kevin Spiekermann",
        "Tianchi Chen",
        "Xiang Yu",
        "Liying Zhang",
        "Liang Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-12T19:01:11+00:00",
      "link": "https://arxiv.org/pdf/2601.07930v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.12571v1",
      "title": "Self-avoiding walk, connective constant, cubic graph, Fisher transformation, quasi-transitive graph",
      "abstract": "We study self-avoiding walks (SAWs) on infinite quasi-transitive cubic graphs under \\emph{local transformations} that replace each degree-$3$ vertex by a finite, symmetric three-port gadget. To each gadget we associate a two-port SAW generating function $g(x)$, defined by counting SAWs that enter and exit the gadget through prescribed ports. Our first main result shows that, if $G$ is cubic and $G_1=φ(G)$ is obtained by applying the local transformation at every vertex, then the connective constants $μ(G)$ and $μ(G_1)$ satisfy the functional relation \\[ μ(G)^{-1}=g\\bigl(μ(G_1)^{-1}\\bigr). \\] We next consider critical exponents defined via susceptibility-type series that do not rely on an ambient Euclidean dimension, and prove that the exponents $γ$ and $η$ are invariant under local transformations; moreover $ν$ is invariant under a standard regularity hypothesis on SAW counts (a common slowly varying function).   Our second set of results concerns bipartite graphs, where the local transformation is applied to one colour class (or to both classes, possibly with different gadgets). In this setting we obtain an analogous relation \\[ μ(G)^{-2}=h\\bigl(μ(G_{\\mathrm e})^{-1}\\bigr), \\] with $h(x)=xg(x)$ when only one class is transformed and $h(x)=g_{φ_1}(x)\\,g_{φ_2}(x)$ when both are transformed. We further present explicit families of examples, including replacing each degree-3 vertex by a complete-graph gadget $K_N$.",
      "authors": [
        "Benjamin Grant",
        "Zhongyang Li"
      ],
      "primary_category": "math.CO",
      "categories": [
        "math.CO",
        "math-ph",
        "math.PR"
      ],
      "published": "2026-01-18T20:25:37+00:00",
      "link": "https://arxiv.org/pdf/2601.12571v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.08920v1",
      "title": "Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration",
      "abstract": "Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.",
      "authors": [
        "Manh Cuong Dao",
        "Quang Hung Pham",
        "Phi Le Nguyen",
        "Thao Nguyen Truong",
        "Bryan Kian Hsiang Low",
        "Trong Nghia Hoang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09T17:24:47+00:00",
      "link": "https://arxiv.org/pdf/2602.08920v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.10434v1",
      "title": "The transformation mechanisms among cuboctahedra, Ino's decahedra and icosahedra structures of magic-size gold nanoclusters",
      "abstract": "Gold nanoclusters possess multiple competing structural motifs with small energy differences, enabling structural coexistence and interconversion. Using a high-accuracy machine learned potential trained on some 20'000 density functional theory reference data points, we investigate transformation pathways connecting both high-symmetry and amorphous cuboctahedra, Ino's decahedra and icosahedra for Au55, Au147, Au309 and Au561 nanoclusters. Our saddle point searches reveal that high-symmetry transformations from cuboctahedra and Ino's decahedra to icosahedra proceed through a single barrier and represent soft-mode-driven jitterbug-type and slip-dislocation motions. In addition, we identify lower-barrier asymmetric transformation pathways that drive the system into disordered, Jahn-Teller-stabilized amorphous icosahedra. Minima Hopping sampling further uncovers, in this context, many such low-symmetry minima. Some of the newly identified global minima for Au309 and Au561 have energies that are up to 2.8 eV lower than the previously reported global minima. Hence, both the shapes and the transformation pathways studied in previous investigations are not the physically relevant ones. In contrast to the previously studied pathways, our transformation pathways give reasonable transformation times that are in rough agreement with experiments.",
      "authors": [
        "Ehsan Rahmatizad Khajehpasha",
        "Mohammad Ismaeil Safa",
        "Nasrin Eyvazi",
        "Marco Krummenacher",
        "Stefan Goedecker"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "physics.comp-ph"
      ],
      "published": "2026-01-15T14:29:17+00:00",
      "link": "https://arxiv.org/pdf/2601.10434v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.16264v1",
      "title": "Prediction of Major Solar Flares Using Interpretable Class-dependent Reward Framework with Active Region Magnetograms and Domain Knowledge",
      "abstract": "In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results. (2)Transformer achieves better performance with combined LOS and vector magnetic field data than with either alone. (3)Models using knowledge-informed features outperform those using magnetograms. (4)While CNN and CNN-BiLSTM outperform their CDR counterparts on magnetograms, CDR-Transformer is slightly superior to its deep learning counterpart when using knowledge-informed features. Among all models, CDR-Transformer achieves the best performance. (5)The predictive performance of the CDR models is not overly sensitive to the reward choices.(6)Through SHAP analysis, the CDR model tends to regard TOTUSJH as more important, while the Transformer tends to prioritize R_VALUE more.(7)Under identical prediction time and active region (AR) number, the CDR-Transformer shows superior predictive capabilities compared to NASA/CCMC.",
      "authors": [
        "Zixian Wu",
        "Xuebao Li",
        "Yanfang Zheng",
        "Rui Wang",
        "Shunhuang Zhang",
        "Jinfang Wei",
        "Yongshang Lv",
        "Liang Dong",
        "Zamri Zainal Abidin",
        "Noraisyah Mohamed Shah",
        "Hongwei Ye",
        "Pengchao Yan",
        "Xuefeng Li",
        "Xiaojia Ji",
        "Xusheng Huang",
        "Xiaotian Wang",
        "Honglei Jin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "astro-ph.SR"
      ],
      "published": "2026-02-18T08:30:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16264v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.15509v1",
      "title": "The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers",
      "abstract": "The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.",
      "authors": [
        "Prasanna Kumar"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-21T22:40:47+00:00",
      "link": "https://arxiv.org/pdf/2601.15509v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.09467v1",
      "title": "Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting",
      "abstract": "Accurate global medium-range weather forecasting is fundamental to Earth system science. Most existing Transformer-based forecasting models adopt vision-centric architectures that neglect the Earth's spherical geometry and zonal periodicity. In addition, conventional autoregressive training is computationally expensive and limits forecast horizons due to error accumulation. To address these challenges, we propose the Shifted Earth Transformer (Searth Transformer), a physics-informed architecture that incorporates zonal periodicity and meridional boundaries into window-based self-attention for physically consistent global information exchange. We further introduce a Relay Autoregressive (RAR) fine-tuning strategy that enables learning long-range atmospheric evolution under constrained memory and computational budgets. Based on these methods, we develop YanTian, a global medium-range weather forecasting model. YanTian achieves higher accuracy than the high-resolution forecast of the European Centre for Medium-Range Weather Forecasts and performs competitively with state-of-the-art AI models at one-degree resolution, while requiring roughly 200 times lower computational cost than standard autoregressive fine-tuning. Furthermore, YanTian attains a longer skillful forecast lead time for Z500 (10.3 days) than HRES (9 days). Beyond weather forecasting, this work establishes a robust algorithmic foundation for predictive modeling of complex global-scale geophysical circulation systems, offering new pathways for Earth system science.",
      "authors": [
        "Tianye Li",
        "Qi Liu",
        "Hao Li",
        "Lei Chen",
        "Wencong Cheng",
        "Fei Zheng",
        "Xiangao Xia",
        "Ya Wang",
        "Gang Huang",
        "Weiwei Wang",
        "Xuan Tong",
        "Ziqing Zu",
        "Yi Fang",
        "Shenming Fu",
        "Jiang Jiang",
        "Haochen Li",
        "Mingxing Li",
        "Jiangjiang Xia"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "published": "2026-01-14T13:20:17+00:00",
      "link": "https://arxiv.org/pdf/2601.09467v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.07677v1",
      "title": "Affine Transformable Unmanned Ground Vehicle",
      "abstract": "This paper develops the proof of concept for a novel affine transformable unmanned ground vehicle (ATUGV) with the capability of safe and aggressive deformation while carrying multiple payloads. The ATUGV is a multi-body system with mobile robots that can be used to power the ATUGV morphable motion, powered cells to enclose the mobile robots, unpowered cells to contain payloads, and a deformable structure to integrate cells through bars and joints. The objective is that all powered and unpowered cells motion can safely track a desired affine transformation, where an affine transformation can be decomposed into translation, rigid body rotation, and deformation. To this end, the paper first uses a deep neural network to structure cell interconnection in such a way that every cell can freely move over the deformation plane, and the entire structure can reconfigurably deform to track a desired affine transformation. Then, the mobile robots, contained by the powered cells and stepper motors, regulating the connections of the powered and unpowered cells, design the proper controls so that all cells safely track the desired affine transformation. The functionality of the proposed ATUGV is validated through hardware experimentation and simulation.",
      "authors": [
        "Aron Mathias",
        "Mohammad Ghufran",
        "Jack Hughes",
        "Hossein Rastgoftar"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2026-02-07T19:56:27+00:00",
      "link": "https://arxiv.org/pdf/2602.07677v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.21069v2",
      "title": "CompSRT: Quantization and Pruning for Image Super Resolution Transformers",
      "abstract": "Model compression has become an important tool for making image super resolution models more efficient. However, the gap between the best compressed models and the full precision model still remains large and a need for deeper understanding of compression theory on more performant models remains. Prior research on quantization of LLMs has shown that Hadamard transformations lead to weights and activations with reduced outliers, which leads to improved performance. We argue that while the Hadamard transform does reduce the effect of outliers, an empirical analysis on how the transform functions remains needed. By studying the distributions of weights and activations of SwinIR-light, we show with statistical analysis that lower errors is caused by the Hadamard transforms ability to reduce the ranges, and increase the proportion of values around $0$. Based on these findings, we introduce CompSRT, a more performant way to compress the image super resolution transformer network SwinIR-light. We perform Hadamard-based quantization, and we also perform scalar decomposition to introduce two additional trainable parameters. Our quantization performance statistically significantly surpasses the SOTA in metrics with gains as large as 1.53 dB, and visibly improves visual quality by reducing blurriness at all bitwidths. At $3$-$4$ bits, to show our method is compatible with pruning for increased compression, we also prune $40\\%$ of weights and show that we can achieve $6.67$-$15\\%$ reduction in bits per parameter with comparable performance to SOTA.",
      "authors": [
        "Dorsa Zeinali",
        "Hailing Wang",
        "Yitian Zhang",
        "Yun Fu"
      ],
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV"
      ],
      "published": "2026-01-28T21:52:29+00:00",
      "link": "https://arxiv.org/pdf/2601.21069v2",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.00856v1",
      "title": "Higher-order transformations of bidirectional quantum processes",
      "abstract": "Bidirectional devices are devices for which the roles of the input and output ports can be exchanged. Mathematically, these devices are described by bistochastic quantum channels, namely completely positive linear maps that are both trace-preserving and identity-preserving. Recently, it has been shown that bidirectional quantum devices can, in principle, be used in ways that are incompatible with a definite input-output direction, giving rise to a new phenomenon called input-output indefiniteness. Here we characterize the most general forms of input-output indefiniteness, associated with a hierarchy of higher-order transformations built from transformations of bistochastic quantum channels. Some levels of the hierarchy correspond to transformations that combine bistochastic channels in a definite causal order, while generally using each channel in an indefinite input-output direction. For other levels of the hierarchy, the indefiniteness can involve both the local input-output direction of each process and the global causal order among the processes. On the foundational side, the hierarchy of higher-order transformations characterized here can be regarded as the largest set of physical processes compatible with a time-symmetric variant of quantum theory, where the possible state transformations are restricted to bistochastic channels.",
      "authors": [
        "Luca Apadula",
        "Alessandro Bisio",
        "Giulio Chiribella",
        "Paolo Perinotti",
        "Kyrylo Simonov"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "math-ph"
      ],
      "published": "2026-01-31T18:39:36+00:00",
      "link": "https://arxiv.org/pdf/2602.00856v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.14875v1",
      "title": "GAT-NeRF: Geometry-Aware-Transformer Enhanced Neural Radiance Fields for High-Fidelity 4D Facial Avatars",
      "abstract": "High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications. While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement. To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline. GAT-NeRF synergistically combines a coordinate-aligned Multilayer Perceptron (MLP) with a lightweight Transformer module, termed as Geometry-Aware-Transformer (GAT) due to its processing of multi-modal inputs containing explicit geometric priors. The GAT module is enabled by fusing multi-modal input features, including 3D spatial coordinates, 3D Morphable Model (3DMM) expression parameters, and learnable latent codes to effectively learn and enhance feature representations pertinent to fine-grained geometry. The Transformer's effective feature learning capabilities are leveraged to significantly augment the modeling of complex local facial patterns like dynamic wrinkles and acne scars. Comprehensive experiments unequivocally demonstrate GAT-NeRF's state-of-the-art performance in visual fidelity and high-frequency detail recovery, forging new pathways for creating realistic dynamic digital humans for multimedia applications.",
      "authors": [
        "Zhe Chang",
        "Haodong Jin",
        "Ying Sun",
        "Yan Song",
        "Hui Yu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-21T11:05:13+00:00",
      "link": "https://arxiv.org/pdf/2601.14875v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.06597v1",
      "title": "DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters",
      "abstract": "While generative modeling on time series facilitates more capable and flexible probabilistic forecasting, existing generative time series models do not address the multi-dimensional properties of time series data well. The prevalent architecture of Diffusion Transformers (DiT), which relies on simplistic conditioning controls and a single-stream Transformer backbone, tends to underutilize cross-variate dependencies in covariate-aware forecasting. Inspired by Multimodal Diffusion Transformers that integrate textual guidance into video generation, we propose Diffusion Transformers for Time Series (DiTS), a general-purpose architecture that frames endogenous and exogenous variates as distinct modalities. To better capture both inter-variate and intra-variate dependencies, we design a dual-stream Transformer block tailored for time-series data, comprising a Time Attention module for autoregressive modeling along the temporal dimension and a Variate Attention module for cross-variate modeling. Unlike the common approach for images, which flattens 2D token grids into 1D sequences, our design leverages the low-rank property inherent in multivariate dependencies, thereby reducing computational costs. Experiments show that DiTS achieves state-of-the-art performance across benchmarks, regardless of the presence of future exogenous variate observations, demonstrating unique generative forecasting strengths over traditional deterministic deep forecasting models.",
      "authors": [
        "Haoran Zhang",
        "Haixuan Liu",
        "Yong Liu",
        "Yunzhong Qiu",
        "Yuxuan Wang",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-06T10:48:13+00:00",
      "link": "https://arxiv.org/pdf/2602.06597v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.13188v1",
      "title": "Diamond-to-graphite transformation under hypersonic impact",
      "abstract": "Diamond to graphite transformation is a complex kinetically driven process which has been studied under various conditions for its fundamental importance. We report the transformation of diamond embedded ceramic matrix composites during hypersonic impact. Diamond particles embedded in cubic boron nitride matrix provide a superhard composite that was subjected to high impact collisions of metal projectiles travelling at speeds reaching Mach 8.45. Our observations suggest that the energy absorption and fracture of the composite is primarily enabled via the phase change of diamond into graphite. Characterization of the impact-fractured composite shows transformed diamond particles and provides details of the shock-induced phase transformation and the nature of diamond-graphite interfaces formed during rapid phase change. The study provides new understanding of phase transformation of diamond under extreme conditions.",
      "authors": [
        "Abhijit Biswas",
        "Aniket Mote",
        "Rajib Sahu",
        "Marcelo Lopes Pereira Junior",
        "Shuo Yang",
        "Sudaice Kazibwe",
        "Jishnu Murukeshan",
        "Raphael Benjamin de Oliveira",
        "Guilherme da Silva Lopes Fabris",
        "Shreyasi Chattopadhyay",
        "Gelu Costin",
        "Jianhua Li",
        "Robert Vajtai",
        "Ching-Wu Chu",
        "Lizhong Lang",
        "Yu Zou",
        "Liangzi Deng",
        "Tobin Filleter",
        "Douglas Soares Galvão",
        "Christian Kübel",
        "Thomas E Lacy",
        "Pulickel M. Ajayan"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-02-13T18:55:16+00:00",
      "link": "https://arxiv.org/pdf/2602.13188v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.17307v1",
      "title": "Security of the Fischlin Transform in Quantum Random Oracle Model",
      "abstract": "The Fischlin transform yields non-interactive zero-knowledge proofs with straight-line extractability in the classical random oracle model. This is done by forcing a prover to generate multiple accepting transcripts through a proof-of-work mechanism. Whether the Fischlin transform is straight-line extractable against quantum adversaries has remained open due to the difficulty of reasoning about the likelihood of query transcripts in the quantum-accessible random oracle model (QROM), even when using the compressed oracle methodology. In this work, we prove that the Fischlin transform remains straight-line extractable in the QROM, via an extractor based on the compressed oracle. This establishes the post-quantum security of the Fischlin transform, providing a post-quantum straight-line extractable NIZK alternative to Pass' transform with smaller proof size. Our techniques include tail bounds for sums of independent random variables and for martingales as well as symmetrization, query amplitude and quantum union bound arguments.",
      "authors": [
        "Christian Majenz",
        "Jaya Sharma"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR"
      ],
      "published": "2026-02-19T12:18:28+00:00",
      "link": "https://arxiv.org/pdf/2602.17307v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.05770v1",
      "title": "Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer",
      "abstract": "Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.",
      "authors": [
        "Yifan Zhang",
        "Wei Bi",
        "Kechi Zhang",
        "Dongming Jin",
        "Jie Fu",
        "Zhi Jin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-09T12:49:41+00:00",
      "link": "https://arxiv.org/pdf/2601.05770v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.18385v1",
      "title": "Estimation of geometric transformation matrices using grid-shaped pilot signals",
      "abstract": "Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.",
      "authors": [
        "Rinka Kawano",
        "Masaki Kawamura"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-26T11:33:01+00:00",
      "link": "https://arxiv.org/pdf/2601.18385v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.12515v1",
      "title": "Matching of SAR and optical images based on transformation to shared modality",
      "abstract": "Significant differences in optical images and Synthetic Aperture Radar (SAR) images are caused by fundamental differences in the physical principles underlying their acquisition by Earth remote sensing platforms. These differences make precise image matching (co-registration) of these two types of images difficult. In this paper, we propose a new approach to image matching of optical and SAR images, which is based on transforming the images to a new modality. The new image modality is common to both optical and SAR images and satisfies the following conditions. First, the transformed images must have an equal pre-defined number of channels. Second, the transformed and co-registered images must be as similar as possible. Third, the transformed images must be non-degenerate, meaning they must preserve the significant features of the original images. To further match images transformed to this shared modality, we train the RoMa image matching model, which is one of the leading solutions for matching of regular digital photographs. We evaluated the proposed approach on the publicly available MultiSenGE dataset containing both optical and SAR images. We demonstrated its superiority over alternative approaches based on image translation between original modalities and various feature matching algorithms. The proposed solution not only provides better quality of matching, but is also more versatile. It enables the use of ready-made RoMa and DeDoDe models, pre-trained for regular images, without retraining for a new modality, while maintaining high-quality matching of optical and SAR images.",
      "authors": [
        "Alexey Borisov",
        "Evgeny Myasnikov",
        "Vladislav Myasnikov"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13T01:41:24+00:00",
      "link": "https://arxiv.org/pdf/2602.12515v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.21942v1",
      "title": "Clustering in Deep Stochastic Transformers",
      "abstract": "Transformers have revolutionized deep learning across various domains but understanding the precise token dynamics remains a theoretical challenge. Existing theories of deep Transformers with layer normalization typically predict that tokens cluster to a single point; however, these results rely on deterministic weight assumptions, which fail to capture the standard initialization scheme in Transformers. In this work, we show that accounting for the intrinsic stochasticity of random initialization alters this picture. More precisely, we analyze deep Transformers where noise arises from the random initialization of value matrices. Under diffusion scaling and token-wise RMS normalization, we prove that, as the number of Transformer layers goes to infinity, the discrete token dynamics converge to an interacting-particle system on the sphere where tokens are driven by a \\emph{common} matrix-valued Brownian noise. In this limit, we show that initialization noise prevents the collapse to a single cluster predicted by deterministic models. For two tokens, we prove a phase transition governed by the interaction strength and the token dimension: unlike deterministic attention flows, antipodal configurations become attracting with positive probability. Numerical experiments confirm the predicted transition, reveal that antipodal formations persist for more than two tokens, and demonstrate that suppressing the intrinsic noise degrades accuracy.",
      "authors": [
        "Lev Fedorov",
        "Michaël E. Sander",
        "Romuald Elie",
        "Pierre Marion",
        "Mathieu Laurière"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-01-29T16:28:13+00:00",
      "link": "https://arxiv.org/pdf/2601.21942v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.20854v1",
      "title": "Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation",
      "abstract": "Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.",
      "authors": [
        "Aníbal Silva",
        "Moisés Santos",
        "André Restivo",
        "Carlos Soares"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-28T18:54:27+00:00",
      "link": "https://arxiv.org/pdf/2601.20854v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.05896v1",
      "title": "Parity, Sensitivity, and Transformers",
      "abstract": "The transformer architecture is almost a decade old. Despite that, we still have a limited understanding of what this architecture can or cannot compute. For instance, can a 1-layer transformer solve PARITY -- or more generally -- which kinds of transformers can do it? Known constructions for PARITY have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.   We give a new construction of a transformer for PARITY with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. We also give the first lower bound for transformers solving PARITY -- by showing that it cannot be done with only one layer and one head.",
      "authors": [
        "Alexander Kozachinskiy",
        "Tomasz Steifer",
        "Przemysław Wałȩga"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T17:14:33+00:00",
      "link": "https://arxiv.org/pdf/2602.05896v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.05523v1",
      "title": "Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations",
      "abstract": "Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.",
      "authors": [
        "Shahin Honarvar",
        "Amber Gorzynski",
        "James Lee-Jones",
        "Harry Coppock",
        "Marek Rei",
        "Joseph Ryan",
        "Alastair F. Donaldson"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-02-05T10:30:57+00:00",
      "link": "https://arxiv.org/pdf/2602.05523v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.22081v1",
      "title": "Accessibility-Driven Information Transformations in Mixed-Visual Ability Work Teams",
      "abstract": "Blind and low-vision (BLV) employees in mixed-visual ability teams often encounter information (e.g., PDFs, diagrams) in inaccessible formats. To enable teamwork, teams must transform these representations by modifying or re-creating them into accessible forms. However, these transformations are frequently overlooked, lack infrastructural support, and cause additional labour. To design systems that move beyond one-off accommodations to effective mixed-ability collaboration, we need a deeper understanding of the representations, their transformations and how they occur. We conducted a week-long diary study with follow-up interviews with 23 BLV and sighted professionals from five legal, non-profit, and consulting teams, documenting 36 transformation cases. Our analysis characterizes how teams perform representational transformations for accessibility: how they are triggered proactively or reactively, how they simplify or enhance, and four common patterns in which workers coordinate with each other to address representational incompatibility. Our findings uncover opportunities for designing systems that can better support mixed-visual ability work.",
      "authors": [
        "Yichun Zhao",
        "Miguel A. Nacenta",
        "Mahadeo A. Sukhai",
        "Sowmya Somanath"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-01-29T18:20:27+00:00",
      "link": "https://arxiv.org/pdf/2601.22081v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.17724v1",
      "title": "Quantum-Inspired Algorithms beyond Unitary Circuits: the Laplace Transform",
      "abstract": "Quantum-inspired algorithms can deliver substantial speedups over classical state-of-the-art methods by executing quantum algorithms with tensor networks on conventional hardware. Unlike circuit models restricted to unitary gates, tensor networks naturally accommodate non-unitary maps. This flexibility lets us design quantum-inspired methods that start from a quantum algorithmic structure, yet go beyond unitarity to achieve speedups. Here we introduce a tensor-network approach to compute the discrete Laplace transform, a non-unitary, aperiodic transform (in contrast to the Fourier transform). We encode a length-$N$ signal on two paired $n$-qubit registers and decompose the overall map into a non-unitary exponential Damping Transform followed by a Quantum Fourier Transform, both compressed in a single matrix-product operator. This decomposition admits strong MPO compression to low bond dimension resulting in significant acceleration. We demonstrate simulations up to $N=2^{30}$ input data points, with up to $2^{60}$ output data points, and quantify how bond dimension controls runtime and accuracy, including precise and efficient pole identification.",
      "authors": [
        "Noufal Jaseem",
        "Sergi Ramos-Calderer",
        "Gauthameshwar S.",
        "Dingzu Wang",
        "José Ignacio Latorre",
        "Dario Poletti"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "math-ph",
        "physics.data-an"
      ],
      "published": "2026-01-25T07:19:56+00:00",
      "link": "https://arxiv.org/pdf/2601.17724v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.20796v1",
      "title": "Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers",
      "abstract": "Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.",
      "authors": [
        "Yiran Huang",
        "Karsten Roth",
        "Quentin Bouniot",
        "Wenjia Xu",
        "Zeynep Akata"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-01-28T17:37:28+00:00",
      "link": "https://arxiv.org/pdf/2601.20796v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.06246v1",
      "title": "Adaptive Sparse Möbius Transforms for Learning Polynomials",
      "abstract": "We consider the problem of exactly learning an $s$-sparse real-valued Boolean polynomial of degree $d$ of the form $f:\\{ 0,1\\}^n \\rightarrow \\mathbb{R}$. This problem corresponds to decomposing functions in the AND basis and is known as taking a Möbius transform. While the analogous problem for the parity basis (Fourier transform) $f: \\{-1,1 \\}^n \\rightarrow \\mathbb{R}$ is well-understood, the AND basis presents a unique challenge: the basis vectors are coherent, precluding standard compressed sensing methods. We overcome this challenge by identifying that we can exploit adaptive group testing to provide a constructive, query-efficient implementation of the Möbius transform (also known as Möbius inversion) for sparse functions. We present two algorithms based on this insight. The Fully-Adaptive Sparse Möbius Transform (FASMT) uses $O(sd \\log(n/d))$ adaptive queries in $O((sd + n) sd \\log(n/d))$ time, which we show is near-optimal in query complexity. Furthermore, we also present the Partially-Adaptive Sparse Möbius Transform (PASMT), which uses $O(sd^2\\log(n/d))$ queries, trading a factor of $d$ to reduce the number of adaptive rounds to $O(d^2\\log(n/d))$, with no dependence on $s$. When applied to hypergraph reconstruction from edge-count queries, our results improve upon baselines by avoiding the combinatorial explosion in the rank $d$. We demonstrate the practical utility of our method for hypergraph reconstruction by applying it to learning real hypergraphs in simulations.",
      "authors": [
        "Yigit Efe Erginbas",
        "Justin Singh Kang",
        "Elizabeth Polito",
        "Kannan Ramchandran"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T22:50:49+00:00",
      "link": "https://arxiv.org/pdf/2602.06246v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.11145v1",
      "title": "SCRAPL: Scattering Transform with Random Paths for Machine Learning",
      "abstract": "The Euclidean distance between wavelet scattering transform coefficients (known as paths) provides informative gradients for perceptual quality assessment of deep inverse problems in computer vision, speech, and audio processing. However, these transforms are computationally expensive when employed as differentiable loss functions for stochastic gradient descent due to their numerous paths, which significantly limits their use in neural network training. Against this problem, we propose \"Scattering transform with Random Paths for machine Learning\" (SCRAPL): a stochastic optimization scheme for efficient evaluation of multivariable scattering transforms. We implement SCRAPL for the joint time-frequency scattering transform (JTFS) which demodulates spectrotemporal patterns at multiple scales and rates, allowing a fine characterization of intermittent auditory textures. We apply SCRAPL to differentiable digital signal processing (DDSP), specifically, unsupervised sound matching of a granular synthesizer and the Roland TR-808 drum machine. We also propose an initialization heuristic based on importance sampling, which adapts SCRAPL to the perceptual content of the dataset, improving neural network convergence and evaluation performance. We make our code and audio samples available and provide SCRAPL as a Python package.",
      "authors": [
        "Christopher Mitcheltree",
        "Vincent Lostanlen",
        "Emmanouil Benetos",
        "Mathieu Lagrange"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "published": "2026-02-11T18:57:08+00:00",
      "link": "https://arxiv.org/pdf/2602.11145v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.13067v1",
      "title": "SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery",
      "abstract": "This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value\" features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.",
      "authors": [
        "Chunming Li",
        "Shidong Wang",
        "Tong Xin",
        "Haofeng Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13T16:22:31+00:00",
      "link": "https://arxiv.org/pdf/2602.13067v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.20116v1",
      "title": "In-Context Reinforcement Learning From Suboptimal Historical Data",
      "abstract": "Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.",
      "authors": [
        "Juncheng Dong",
        "Moyang Guo",
        "Ethan X. Fang",
        "Zhuoran Yang",
        "Vahid Tarokh"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-27T23:13:06+00:00",
      "link": "https://arxiv.org/pdf/2601.20116v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.14522v1",
      "title": "On the Runway Cascade of Transformers for Language Modeling",
      "abstract": "In decoder-only (causal) transformers, the computation graph created by causal masking routes information through both direct-path attention and indirect paths formed by intermediate tokens. We denote these indirect paths between token pairs as their runways. We argue that certain failure modes of causal transformers as observed by a growing body of recent works are likely exacerbated by a misalignment between these two information propagation modes. We formalize runway cascade as a phenomenon whereby this misalignment results in redundancies and irrelevant information cascading to token representations despite adequately learned attention patterns. As a solution, we propose runway-aware rewiring as a more explicit way of incorporating runway context directly into each token's direct-path attention. This mechanism re-wires the attention pattern for each token based on a summary of its runway landscape, enabling awareness of accumulating representational influences and allowing for more balanced information propagation. Our proposed methodology introduces no additional parameters and can seamlessly be integrated into standard attention mechanism. Empirically, our rewired transformer results in steady improvements in general language modeling as well as noticeably stronger information retrieval and extrapolation abilities compared to standard transformers.",
      "authors": [
        "Hunjae Lee",
        "Corey Clark"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-20T22:29:34+00:00",
      "link": "https://arxiv.org/pdf/2601.14522v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.06365v1",
      "title": "Dynamic nanoscale spatial heterogeneity in a perovskite to brownmillerite topotactic phase transformation",
      "abstract": "Phase transitions are omnipresent in modern condensed matter physics and its applications. In solids, phase transformations typically occur by nucleation and growth under non-equilibrium conditions. Under constant external conditions, $\\textit{e.g.}$, constant heating temperature and pressure, the nucleation and growth dynamics are often thought of as spatially and temporally independent. Here, $\\textit{in-situ}$ Bragg X-ray photon correlation spectroscopy (XPCS) reveals nanoscale spatial and dynamical heterogeneity in the perovskite to brownmillerite topotactic phase transformation in La$_{0.7}$Sr$_{0.3}$CoO$_3$ (LSCO) thin films under constant reducing conditions over a time-span of multiple hours. Specifically, a timescale associated with domain growth remains stable, with a corresponding domain wall speed of $v_d = 6 \\pm 0.5 \\times10^{-4}$ nm/s ($2 \\pm 0.2$ nm/h), while a slower timescale, associated with temperature driven de-pinning of domains, leads to accelerating dynamics with timescales following an aging power law with exponent $-2.2 \\pm 0.5$. The experiment demonstrates that Bragg XPCS is a powerful tool to study nanoscale dynamics in phase transformations. The results are relevant for phase engineering of phase-change devices, as they show that nanoscale dynamics, linked to domain and domain-wall motion, can continuously evolve and speed up with time, even hours after the initiation of the phase transformation, with potential repercussions on electrical performance.",
      "authors": [
        "Nicolò D'Anna",
        "Erik S. Lamb",
        "Robin Glefke",
        "Daseul Ham",
        "Ishmam Nihal",
        "Su Yong Lee",
        "Yayoi Takamura",
        "Oleg Shpyrko"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.mes-hall"
      ],
      "published": "2026-01-10T00:33:05+00:00",
      "link": "https://arxiv.org/pdf/2601.06365v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.12480v1",
      "title": "MXFormer: A Microscaling Floating-Point Charge-Trap Transistor Compute-in-Memory Transformer Accelerator",
      "abstract": "The proliferation of Transformer models is often constrained by the significant computational and memory bandwidth demands of deployment. To address this, we present MXFormer, a novel, hybrid, weight-stationary Compute-in-Memory (CIM) accelerator that provides high throughput and efficiency for fixed-model inference on large short-sequence Transformers. Our architecture's foundation is the use of ultra-dense Charge-Trap Transistors (CTTs) in Microscaling MXFP4 CIM arrays, uniquely enabling the on-chip storage of up to hundreds of millions of parameters in Fully Weight Stationary (FWS) fashion.   We introduce a statically partitioned design with 12 Transformer blocks connected by a deeply pipelined dataflow. Static-weight layers (MLPs and linear projections) execute on highly parallel analog CTT arrays using an MXFP4-native flow with per-block exponent alignment and a 10-bit SAR ADC. Dynamic computations are handled in fully accurate digital blocks that utilize MXFP-enabled systolic arrays for scaled dot-product attention and vector units for LayerNorm and FlashAttention-style Softmax.   By eliminating all weight movement, the deeply pipelined MXFormer architecture yields very high single-stream throughput and efficiency, processing 58275 FPS on ViT-L/32 (dual-chip) or 41269 FPS on ViT-B/16 (single chip). MXFormer outperforms comparable state-of-the-art non-FWS digital, hybrid and photonic Transformer accelerators ~3.3x-60.5x in compute density and ~1.7x-2.5x in energy efficiency. Against FWS accelerators, MXFormer improves compute density by ~20.9x and resident weight storage density by ~2x, while preserving near-digital accuracy (drop of <1%) without any model retraining.",
      "authors": [
        "George Karfakis",
        "Samyak Chakrabarty",
        "Vinod Kurian Jacob",
        "Siyun Qiao",
        "Subramanian S. Iyer",
        "Sudhakar Pamarti",
        "Puneet Gupta"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR"
      ],
      "published": "2026-02-12T23:38:08+00:00",
      "link": "https://arxiv.org/pdf/2602.12480v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.15348v1",
      "title": "Abusive music and song transformation using GenAI and LLMs",
      "abstract": "Repeated exposure to violence and abusive content in music and song content can influence listeners' emotions and behaviours, potentially normalising aggression or reinforcing harmful stereotypes. In this study, we explore the use of generative artificial intelligence (GenAI) and Large Language Models (LLMs) to automatically transform abusive words (vocal delivery) and lyrical content in popular music. Rather than simply muting or replacing a single word, our approach transforms the tone, intensity, and sentiment, thus not altering just the lyrics, but how it is expressed. We present a comparative analysis of four selected English songs and their transformed counterparts, evaluating changes through both acoustic and sentiment-based lenses. Our findings indicate that Gen-AI significantly reduces vocal aggressiveness, with acoustic analysis showing improvements in Harmonic to Noise Ratio, Cepstral Peak Prominence, and Shimmer. Sentiment analysis reduced aggression by 63.3-85.6\\% across artists, with major improvements in chorus sections (up to 88.6\\% reduction). The transformed versions maintained musical coherence while mitigating harmful content, offering a promising alternative to traditional content moderation that avoids triggering the \"forbidden fruit\" effect, where the censored content becomes more appealing simply because it is restricted. This approach demonstrates the potential for GenAI to create safer listening experiences while preserving artistic expression.",
      "authors": [
        "Jiyang Choi",
        "Rohitash Chandra"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-21T02:56:45+00:00",
      "link": "https://arxiv.org/pdf/2601.15348v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.11237v1",
      "title": "Likelihood-Based Ergodicity Transformations in Time Series Analysis",
      "abstract": "Time series often exhibit non-ergodic behaviour that complicates forecasting and inference. This article proposes a likelihood-based approach for estimating ergodicity transformations that addresses such challenges. The method is broadly compatible with standard models, including Gaussian processes, ARMA, and GARCH. A detailed simulation study using geometric and arithmetic Brownian motion demonstrates the ability of the approach to recover known ergodicity transformations. A further case study on the large macroeconomic database FRED-QD shows that incorporating ergodicity transformations can provide meaningful improvements over conventional transformations or naive specifications in applied work.",
      "authors": [
        "Anthony Britto"
      ],
      "primary_category": "econ.EM",
      "categories": [
        "econ.EM",
        "stat.ME"
      ],
      "published": "2026-01-16T12:30:51+00:00",
      "link": "https://arxiv.org/pdf/2601.11237v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.15158v3",
      "title": "Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data",
      "abstract": "Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive policy gradient to discover such systematic reasoning remains poorly understood. We address this by analyzing the policy gradient dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, policy gradient drives the Transformer to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of \"simple examples\": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler examples, the Transformer learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, policy gradient learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.",
      "authors": [
        "Yuval Ran-Milo",
        "Yotam Alexander",
        "Shahar Mendel",
        "Nadav Cohen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-21T16:36:19+00:00",
      "link": "https://arxiv.org/pdf/2601.15158v3",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.13513v2",
      "title": "Learning Gradient Flow: Using Equation Discovery to Accelerate Engineering Optimization",
      "abstract": "In this work, we investigate the use of data-driven equation discovery for dynamical systems to model and forecast continuous-time dynamics of unconstrained optimization problems. To avoid expensive evaluations of the objective function and its gradient, we leverage trajectory data on the optimization variables to learn the continuous-time dynamics associated with gradient descent, Newton's method, and ADAM optimization. The discovered gradient flows are then solved as a surrogate for the original optimization problem. To this end, we introduce the Learned Gradient Flow (LGF) optimizer, which is equipped to build surrogate models of variable polynomial order in full- or reduced-dimensional spaces at user-defined intervals in the optimization process. We demonstrate the efficacy of this approach on several standard problems from engineering mechanics and scientific machine learning, including two inverse problems, structural topology optimization, and two forward solves with different discretizations. Our results suggest that the learned gradient flows can significantly expedite convergence by capturing critical features of the optimization trajectory while avoiding expensive evaluations of the objective and its gradient.",
      "authors": [
        "Grant Norman",
        "Conor Rowan",
        "Kurt Maute",
        "Alireza Doostan"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.CE",
        "cs.LG",
        "math.DS",
        "math.NA"
      ],
      "published": "2026-02-13T22:44:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13513v2",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.07970v1",
      "title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
      "abstract": "Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.",
      "authors": [
        "Zheyuan Hu",
        "Weitao Chen",
        "Cengiz Öztireli",
        "Chenliang Zhou",
        "Fangcheng Zhong"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-08T13:44:36+00:00",
      "link": "https://arxiv.org/pdf/2602.07970v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.04114v1",
      "title": "Turning mechanistic models into forecasters by using machine learning",
      "abstract": "The equations of complex dynamical systems may not be identified by expert knowledge, especially if the underlying mechanisms are unknown. Data-driven discovery methods address this challenge by inferring governing equations from time-series data using a library of functions constructed from the measured variables. However, these methods typically assume time-invariant coefficients, which limits their ability to capture evolving system dynamics. To overcome this limitation, we allow some of the parameters to vary over time, learn their temporal evolution directly from data, and infer a system of equations that incorporates both constant and time-varying parameters. We then transform this framework into a forecasting model by predicting the time-varying parameters and substituting these predictions into the learned equations. The model is validated using datasets for Susceptible-Infected-Recovered, Consumer--Resource, greenhouse gas concentration, and Cyanobacteria cell count. By dynamically adapting to temporal shifts, our proposed model achieved a mean absolute error below 3\\% for learning a time series and below 6\\% for forecasting up to a month ahead. We additionally compare forecasting performance against CNN-LSTM and Gradient Boosting Machine (GBM), and show that our model outperforms these methods across most datasets. Our findings demonstrate that integrating time-varying parameters into data-driven discovery of differential equations improves both modeling accuracy and forecasting performance.",
      "authors": [
        "Amit K. Chakraborty",
        "Hao Wang",
        "Pouria Ramazi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.DS"
      ],
      "published": "2026-02-04T01:00:08+00:00",
      "link": "https://arxiv.org/pdf/2602.04114v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.14779v2",
      "title": "Integrating the probe and singular sources methods: IV. IPS function for the Schrödinger equation",
      "abstract": "The integrated theory of the probe and singular sources methods (IPS) is developed for an inverse obstacle problem governed by the stationary Schrödinger equation in a bounded domain. The unknown obstacles are penetrable, and their surface is modeled by a part of the support of the potential in the governing equation. The main results concern an analytical detection method for these obstacles from the Dirichlet-to-Neumann map. They consist of three parts: a singular sources method via the probe method using a solution with higher-order singularity for the governing equation of the background medium; the discovery of an IPS function whose two ways of decomposition give us the indicator functions for both the probe and singular sources methods; a completely integrated version of both methods, which means their indicator functions coincide. Furthermore, a result on Side B of IPS is also given, concerning the blowing-up property of a sequence calculated from the Dirichlet-to-Neumann map.",
      "authors": [
        "Masaru Ikehata"
      ],
      "primary_category": "math.AP",
      "categories": [
        "math.AP"
      ],
      "published": "2026-01-21T08:59:16+00:00",
      "link": "https://arxiv.org/pdf/2601.14779v2",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.04907v1",
      "title": "Physics as the Inductive Bias for Causal Discovery",
      "abstract": "Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.",
      "authors": [
        "Jianhong Chen",
        "Naichen Shi",
        "Xubo Yue"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "published": "2026-02-03T23:42:01+00:00",
      "link": "https://arxiv.org/pdf/2602.04907v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.05632v1",
      "title": "LLM-DMD: Large Language Model-based Power System Dynamic Model Discovery",
      "abstract": "Current model structural discovery methods for power system dynamics impose rigid priors on the basis functions and variable sets of dynamic models while often neglecting algebraic constraints, thereby limiting the formulation of high-fidelity models required for precise simulation and analysis. This letter presents a novel large language model (LLM)-based framework for dynamic model discovery (LLM-DMD) which integrates the reasoning and code synthesis capabilities of LLMs to discover dynamic equations and enforce algebraic constraints through two sequential loops: the differential-equation loop that identifies state dynamics and associated variables, and the algebraic-equation loop that formulates algebraic constraints on the identified algebraic variables. In each loop, executable skeletons of power system dynamic equations are generated by the LLM-based agent and evaluated via gradient-based optimizer. Candidate models are stored in an island-based archive to guide future iterations, and evaluation stagnation activates a variable extension mechanism that augments the model with missing algebraic or input variables, such as stator currents to refine the model. Validation on synchronous generator benchmarks of the IEEE 39-bus system demonstrates the superiority of LLM-DMD in complete dynamic model discovery.",
      "authors": [
        "Chao Shen",
        "Zihan Guo",
        "Ke Zuo",
        "Wenqi Huang",
        "Mingyang Sun"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-01-09T08:40:45+00:00",
      "link": "https://arxiv.org/pdf/2601.05632v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.04498v1",
      "title": "Painleve solitons of AKNS system and irrational algebraic solitons of NLS equations",
      "abstract": "A novel symmetry decomposition approach is introduced to derive the so-called ``Painleve solitons'' of the Ablowitz-Kaup-Newell-Segur (AKNS) system. These Painleve solitons propagate against a background governed by a Painleve transcendent, establishing a fundamental generalization of the well-known elliptic solitons concept. We demonstrate that while elliptic solitons arise from the combination of translation invariance and square eigenfunction symmetry, a different symmetry combination-scaling invariance, Galilean invariance, and square eigenfunction symmetry-generates ``Painleve IV solitons'' for the AKNS system. This discovery represents a significant theoretical advance in integrable systems theory. By selecting special solutions of the Painleve IV equation, we obtain explicit forms of several previously unknown classes of solutions for the AKNS system and the nonlinear Schrodinger (NLS) equation: irrational algebraic solitons, rational algebraic solitons, and parabolic cylindrical function solitons. These results dramatically expand the known solution landscape of one of the most important integrable models in mathematical physics, with broad implications for nonlinear wave phenomena across multiple physical disciplines including optics, Bose-Einstein condensates, and fluid dynamics.",
      "authors": [
        "Man Jia",
        "Xia-Zhi Hao",
        "Ruo-Xia Yao",
        "Fa-Ren Wang",
        "S. Y. Lou"
      ],
      "primary_category": "nlin.SI",
      "categories": [
        "nlin.SI",
        "math-ph",
        "nlin.PS",
        "physics.class-ph"
      ],
      "published": "2026-02-04T12:44:18+00:00",
      "link": "https://arxiv.org/pdf/2602.04498v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.04498v2",
      "title": "Painleve solitons of AKNS system and irrational algebraic solitons of NLS equations",
      "abstract": "A novel symmetry decomposition approach is introduced to derive the so-called ``Painlevé solitons'' of the Ablowitz-Kaup-Newell-Segur (AKNS) system. These Painlevé solitons propagate against a background governed by a Painlevé transcendent, establishing a fundamental generalization of the well-known elliptic solitons concept. We demonstrate that while elliptic solitons arise from the combination of translation invariance and square eigenfunction symmetry, a \\textit{different} symmetry combination-scaling invariance, Galilean invariance, and square eigenfunction symmetry-generates ``Painlevé IV solitons'' for the AKNS system. This discovery represents a significant theoretical advance in integrable systems theory. By selecting special solutions of the Painlevé IV equation, we obtain explicit forms of several previously unknown classes of solutions for the AKNS system and the nonlinear Schrödinger (NLS) equation: irrational algebraic solitons, rational algebraic solitons, and parabolic cylindrical function solitons. These results dramatically expand the known solution landscape of one of the most important integrable models in mathematical physics, with broad implications for nonlinear wave phenomena across multiple physical disciplines including optics, Bose-Einstein condensates, and fluid dynamics.",
      "authors": [
        "Man Jia",
        "Xia-Zhi Hao",
        "Ruo-Xia Yao",
        "Fa-Ren Wang",
        "S. Y. Lou"
      ],
      "primary_category": "nlin.SI",
      "categories": [
        "nlin.SI",
        "math-ph",
        "nlin.PS",
        "physics.class-ph"
      ],
      "published": "2026-02-04T12:44:18+00:00",
      "link": "https://arxiv.org/pdf/2602.04498v2",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.07733v1",
      "title": "Data-Driven Discovery of Sign-Indefinite Artificial Viscosity for Linear Convection -- A Space-Time Reconvolution Perspective",
      "abstract": "Artificial viscosity is traditionally interpreted as a positive, spatially acting regularization introduced to stabilize numerical discretizations of hyperbolic conservation laws. In this work, we report a data-driven discovery that motivates a reinterpretation of this classical view. We consider the linear convection equation discretized using an unstable FTCS scheme augmented with a learnable artificial viscosity. Using automatic differentiation and gradient-based optimization, the viscosity field is inferred by minimizing the error with respect to the exact solution, without imposing any sign constraints. The optimized viscosity consistently becomes locally negative near extrema, while the numerical solution remains stable and nearly exact. This behavior is not readily explained within classical modified equation analysis and Lax-Wendroff-type arguments, which predict a strictly positive effective viscosity. To resolve this apparent contradiction, we reinterpret artificial viscosity as a space-time closure that compensates unresolved truncation errors while enforcing entropy stability through global dissipation balance rather than pointwise positivity. Within this framework, the Lax-Wendroff scheme corresponds to a degenerate projection in which temporal truncation errors are eliminated and reintroduced as spatial diffusion. We show that entropy stability constrains the integrated dissipation budget rather than the pointwise sign of spatial viscosity. As a result, locally negative viscosity naturally emerges as a numerical reconvolution operator that compensates for dispersive truncation errors. Negative viscosity is therefore not an unphysical diffusion process, but a scheme- and grid-dependent correction mechanism.",
      "authors": [
        "Arun Govind Neelan"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-08T00:00:05+00:00",
      "link": "https://arxiv.org/pdf/2602.07733v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.19223v1",
      "title": "Nonlocal Kramers-Moyal formulas and data-driven discovery of stochastic dynamical systems with multiplicative Lévy noise",
      "abstract": "Traditional data-driven methods, effective for deterministic systems or stochastic differential equations (SDEs) with Gaussian noise, fail to handle the discontinuous sample paths and heavy-tailed fluctuations characteristic of Lévy processes, particularly when the noise is state-dependent. To bridge this gap, we establish nonlocal Kramers-Moyal formulas, rigorously generalizing the classical Kramers-Moyal relations to SDEs with multiplicative Lévy noise. These formulas provide a direct link between short-time transition probability densities (or sample path statistics) and the underlying SDE coefficients: the drift vector, diffusion matrix, Lévy jump measure kernel, and Lévy noise intensity functions. Leveraging these theoretical foundations, we develop novel data-driven algorithms capable of simultaneously identifying all governing components from data and establish convergence results and error analysis for the algorithms. We validate the framework through extensive numerical experiments on prototypical systems. This work provides a principled and practical toolbox for discovering interpretable SDE models governing complex systems influenced by discontinuous, heavy-tailed, state-dependent fluctuations, with broad applicability in climate science, neuroscience, epidemiology, finance, and biological physics.",
      "authors": [
        "Yang Li",
        "Jinqiao Duan"
      ],
      "primary_category": "math.DS",
      "categories": [
        "math.DS",
        "stat.ML"
      ],
      "published": "2026-01-27T05:44:50+00:00",
      "link": "https://arxiv.org/pdf/2601.19223v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.11849v1",
      "title": "Data-driven discovery of chemical reaction networks",
      "abstract": "We propose a unified framework that allows for the full mechanistic reconstruction of chemical reaction networks (CRNs) from concentration data. The framework utilizes an integral formulation of the differential equations governing the chemical reactions, followed by an automatic procedure to recover admissible mass-action mechanisms from the equations. We provide theoretical justification for the use of integral formulations using analytical and numerical error bounds. The integral formulation is demonstrated to offer superior robustness to noise and improved accuracy in both rate-law and graph recovery when compared to other commonly used formulations. Together, our developments advance the goal of fully automated, data-driven chemical mechanism discovery.",
      "authors": [
        "Abraham Reyes-Velazquez",
        "Stefan Güttel",
        "Igor Larrosa",
        "Jonas Latz"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-12T11:41:42+00:00",
      "link": "https://arxiv.org/pdf/2602.11849v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.10632v1",
      "title": "The Neurosymbolic Frontier of Nonuniform Ellipticity: Formalizing Sharp Schauder Theory via Topos-Theoretic Reasoning Models",
      "abstract": "This white paper presents a critical synthesis of the recent breakthrough in nonuniformly elliptic regularity theory and the burgeoning field of neurosymbolic large reasoning models (LRMs). We explore the resolution of the long-standing sharp growth rate conjecture in Schauder theory, achieved by Cristiana De Filippis and Giuseppe Mingione, which identifies the exact threshold $q/p < 1 + α/n$ for gradient Hölder continuity. Central to this mathematical achievement is the ``ghost equation'' methodology, a sophisticated auxiliary derivation that bypasses the non-differentiability of classical Euler-Lagrange systems. We propose that the next era of mathematical discovery lies in the integration of these pure analytical constructs with LRMs grounded in topos theory and formal verification frameworks such as Safe and Typed Chain-of-Thought (PC-CoT). By modeling the reasoning process as a categorical colimit in a slice topos, we demonstrate how LRMs can autonomously navigate the ``Dark Side'' of the calculus of variations, providing machine-checkable proofs for regularity bounds in complex, multi-phase physical systems.",
      "authors": [
        "Suyash Mishra"
      ],
      "primary_category": "cs.SC",
      "categories": [
        "cs.SC",
        "cs.AI"
      ],
      "published": "2026-02-11T08:24:57+00:00",
      "link": "https://arxiv.org/pdf/2602.10632v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.19838v1",
      "title": "Modified splitting methods for Gross-Pitaevskii systems modelling Bose-Einstein condensates: Time evolution and ground state computation",
      "abstract": "The year 2025 marks the 100 and 30 years anniversaries of the discovery of Bose--Einstein condensation and its successful experimental realisation. Inspired by these important research achievements, a conceptually simple approach is proposed to facilitate reliable and efficient numerical simulations. The structure of the underlying systems of coupled Gross--Pitaevskii equations suggests the use of optimised high-order operator splitting methods for dynamical evolution and ground state computation. A second-order barrier, however, prevents the applicability of standard operator splitting methods for both, time evolution as well as imaginary time propagation. An innovative alternative approach accomplishes the design of novel modified operator splitting methods that remain stable under moderate smallness assumptions on the time increments. The core idea is to incorporate commutators of the defining differential and nonlinear multiplication operators, since this permits to fulfill the basic stability requirement of positive method coefficients. Further improvements with respect to convergence at the targeted precision arise from automatic adjustments of the time stepsizes by an inexpensive local error control. The presented numerical experiments confirm the favourable performance of a specific fourth-order modified operator splitting method. Amongst others, it is demonstrated that the excellent mass and energy conservation in long-term evolutions, intrinsic attributes of geometric numerical integrators for Hamiltonian systems, is maintained for a sensible variation of the time stepsizes. Moreover, the benefits of adaptive higher-order approximations in ground state computations are illustrated.",
      "authors": [
        "Mechthild Thalhammer",
        "Gregor Thalhammer-Thurner"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-01-27T17:44:03+00:00",
      "link": "https://arxiv.org/pdf/2601.19838v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.14961v1",
      "title": "Power-Law Scaling in the Classification Performance of Small-Scale Spiking Neural Networks",
      "abstract": "This paper investigates the classification capability of small-scale spiking neural networks based on the Leaky Integrate-and-Fire (LIF) neuron model. We analyze the relationship between classification accuracy and three factors: the number of neurons, the number of stimulus nodes, and the number of classification categories. Notably, we employ a large language model (LLM) to assist in discovering the underlying functional relationships among these variables, and compare its performance against traditional methods such as linear and polynomial fitting. Experimental results show that classification accuracy follows a power-law scaling primarily with the number of categories, while the effects of neuron count and stimulus nodes are relatively minor. A key advantage of the LLM-based approach is its ability to propose plausible functional forms beyond pre-defined equation templates, often leading to more concise or accurate mathematical descriptions of the observed scaling laws. This finding has important implications for understanding efficient computation in biological neural systems and for pioneering new paradigms in AI-aided scientific discovery.",
      "authors": [
        "Zhengdi Zhang",
        "Cong Han",
        "Wenjun Xia"
      ],
      "primary_category": "q-bio.NC",
      "categories": [
        "q-bio.NC"
      ],
      "published": "2026-01-21T13:05:34+00:00",
      "link": "https://arxiv.org/pdf/2601.14961v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.19091v1",
      "title": "Out-of-Distribution Generalization for Neural Physics Solvers",
      "abstract": "Neural physics solvers are increasingly used in scientific discovery, given their potential for rapid in silico insights into physical, materials, or biological systems and their long-time evolution. However, poor generalization beyond their training support limits exploration of novel designs and long-time horizon predictions. We introduce NOVA, a route to generalizable neural physics solvers that can provide rapid, accurate solutions to scenarios even under distributional shifts in partial differential equation parameters, geometries and initial conditions. By learning physics-aligned representations from an initial sparse set of scenarios, NOVA consistently achieves 1-2 orders of magnitude lower out-of-distribution errors than data-driven baselines across complex, nonlinear problems including heat transfer, diffusion-reaction and fluid flow. We further showcase NOVA's dual impact on stabilizing long-time dynamical rollouts and improving generative design through application to the simulation of nonlinear Turing systems and fluidic chip optimization. Unlike neural physics solvers that are constrained to retrieval and/or emulation within an a priori space, NOVA enables reliable extrapolation beyond known regimes, a key capability given the need for exploration of novel hypothesis spaces in scientific discovery",
      "authors": [
        "Zhao Wei",
        "Chin Chun Ooi",
        "Jian Cheng Wong",
        "Abhishek Gupta",
        "Pao-Hsiung Chiu",
        "Yew-Soon Ong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-27T01:57:14+00:00",
      "link": "https://arxiv.org/pdf/2601.19091v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.10038v1",
      "title": "What Understanding Means in AI-Laden Astronomy",
      "abstract": "Artificial intelligence is rapidly transforming astronomical research, yet the scientific community has largely treated this transformation as an engineering challenge rather than an epistemological one. This perspective article argues that philosophy of science offers essential tools for navigating AI's integration into astronomy--conceptual clarity about what \"understanding\" means, critical examination of assumptions about data and discovery, and frameworks for evaluating AI's roles across different research contexts. Drawing on an interdisciplinary workshop convening astronomers, philosophers, and computer scientists, we identify several tensions. First, the narrative that AI will \"derive fundamental physics\" from data misconstrues contemporary astronomy as equation-derivation rather than the observation-driven enterprise it is. Second, scientific understanding involves more than prediction--it requires narrative construction, contextual judgment, and communicative achievement that current AI architectures struggle to provide. Third, because narrative and judgment matter, human peer review remains essential--yet AI-generated content flooding the literature threatens our capacity to identify genuine insight. Fourth, while AI excels at well-defined problem-solving, the ill-defined problem-finding that drives breakthroughs appears to require capacities beyond pattern recognition. Fifth, as AI accelerates what is feasible, pursuitworthiness criteria risk shifting toward what AI makes easy rather than what is genuinely important. We propose \"pragmatic understanding\" as a framework for integration--recognizing AI as a tool that extends human cognition while requiring new norms for validation and epistemic evaluation. Engaging with these questions now may help the community shape the transformation rather than merely react to it.",
      "authors": [
        "Yuan-Sen Ting",
        "André Curtis-Trudel",
        "Siyu Yao"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-15T03:28:38+00:00",
      "link": "https://arxiv.org/pdf/2601.10038v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.09093v1",
      "title": "Predicting magnetism with first-principles AI",
      "abstract": "Computational discovery of magnetic materials remains challenging because magnetism arises from the competition between kinetic energy and Coulomb interaction that is often beyond the reach of standard electronic-structure methods. Here we tackle this challenge by directly solving the many-electron Schrödinger equation with neural-network variational Monte Carlo, which provides a highly expressive variational wavefunction for strongly correlated systems. Applying this technique to transition metal dichalcogenide moiré semicondutors, we predict itinerant ferromagnetism in WSe$_2$/WS$_2$ and an antiferromagnetic insulator in twisted $Γ$-valley homobilayer, using the same neural network without any physics input beyond the microscopic Hamiltonian. Crucially, both types of magnetic states are obtained from a single calculation within the $S_z=0$ sector, removing the need to compute and compare multiple $S_z$ sectors. This significantly reduces computational cost and paves the way for faster and more reliable magnetic material design.",
      "authors": [
        "Max Geier",
        "Liang Fu"
      ],
      "primary_category": "cond-mat.str-el",
      "categories": [
        "cond-mat.str-el",
        "cs.LG"
      ],
      "published": "2026-02-09T19:00:01+00:00",
      "link": "https://arxiv.org/pdf/2602.09093v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.12320v1",
      "title": "Molecular pentaquarks composed of a ground octet baryon and a $P-$wave anti-charmed meson",
      "abstract": "In this work, we investigate the interactions between an excited anti-charm meson doublet $(\\bar{D}_1, \\bar{D}_2^*)$ and ground-state octet baryons $(N, Λ, Σ, Ξ)$ with the aim of identifying possible molecular pentaquark states. A systematic analysis is performed within the one-boson-exchange model, which incorporates both $S$-wave and $P$-wave interactions, $S$-$D$ wave mixing, and coupled-channel effects. By solving the Schrödinger equations, we can predict a rich spectrum of loosely bound anti-charm molecular pentaquarks with strangeness $|S| = 0, 1, 2$. Our results provide specific quantum number assignments and mass range predictions to guide future experimental searches at facilities such as LHCb and Belle II. The discovery of such states would significantly enrich the hadron spectrum and serve as a critical test of theoretical models for hadronic interactions.",
      "authors": [
        "Yu-Yue Cui",
        "Rui Chen",
        "Qi Huang"
      ],
      "primary_category": "hep-ph",
      "categories": [
        "hep-ph"
      ],
      "published": "2026-01-18T09:05:04+00:00",
      "link": "https://arxiv.org/pdf/2601.12320v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.07939v1",
      "title": "New solution to the hyperon puzzle of neutron stars: Quantum many-body effects",
      "abstract": "The hyperon puzzle refers to the challenge of reconciling the existence of hyperons in neutron star cores and the observed high masses of neutron stars. The recent discovery of PSR J0952-0607 ($2.35\\pm0.17 M_{\\odot}$) has intensified this challenge. Existing solutions fail to achieve such a high mass, and often predict unrealistically fast cooling that is at odds with observations. Here, we propose a novel solution to the hyperon puzzle. Using the Dyson-Schwinger equation approach, we incorporate the quantum many-body effects caused by strong baryon-meson interactions into the equation of state for cold baryonic matter and find it stiff enough to support a maximum hyperon-star mass of $M_{\\mathrm{max}} \\approx 2.59 M_{\\odot}$, which can explain all the observed high neutron-star masses. The resulting proton and hyperon fractions are remarkably low, thus the nucleonic and hyperonic direct Urca processes are significantly suppressed. As a result, fast cooling typically does not occur in ordinary neutron stars.",
      "authors": [
        "Hao-Fu Zhu",
        "Guo-Zhu Liu",
        "Xufen Wu",
        "Ye-Fei Yuan"
      ],
      "primary_category": "nucl-th",
      "categories": [
        "nucl-th",
        "astro-ph.HE",
        "cond-mat.str-el"
      ],
      "published": "2026-02-08T12:13:06+00:00",
      "link": "https://arxiv.org/pdf/2602.07939v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.08269v1",
      "title": "Quantization-aware Photonic Homodyne computing for Accelerated Artificial Intelligence and Scientific Simulation",
      "abstract": "Modern problems in high-performance computing, ranging from training and inferencing deep learning models in computer vision and language models to simulating complex physical systems with nonlinearly-coupled equations, require exponential growth of computational resources. Photonic analog systems are emerging with solutions of intrinsic parallelism, high bandwidth, and low propagation loss. However, their application has been hindered by the low analog accuracy due to the electro-optic distortion, material nonlinearities, and signal-to-noise ratios. Here we overcome this barrier with a quantization-aware digital-photonic mixed-precision framework across chiplets for accelerated AI processing and physical simulation. Using Lithium Niobate photonics with channel equalization techniques, we demonstrate linear multiplication (9-bit amplitude-phase decoupling) in homodyne optical logics with 6-bit precision at the clock rate of 128 giga-symbol-per-second (128 GS/s), enabling AI processing with 6 ns latency. Codesign hardware-algorithms, including iterative solvers, sparse-dense quantization, and bit-sliced matrix multiplication, explore photonic amplitude and phase coherence for complex-valued, physics-inspired computation. In electromagnetic problems, our approach yields 12-bit solutions for partial differential equations (PDEs) in scattering problems that would conventionally require up to 32-bit and often even 64-bit precision. These results preserve digital-level fidelity while leveraging the high-speed low-energy photonic hardware, establishing a pathway toward general-purpose optical acceleration for generative artificial intelligence, real-time robotics, and accurate simulation for climate challenges and biological discoveries.",
      "authors": [
        "Lian Zhou",
        "Kaiwen Xue",
        "Amirhossein Fallah",
        "Lijin Liu",
        "Chun-Ho Lee",
        "Kiwon Kwon",
        "Clayton Cheung",
        "Yuan Li",
        "Yue Yu",
        "Yun-Jhu Lee",
        "Songlin Zhao",
        "Ryan Hamerly",
        "Edo Waks",
        "Dirk Englund",
        "Constantine Sideris",
        "Mengjie Yu",
        "Zaijun Chen"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET"
      ],
      "published": "2026-02-09T05:08:03+00:00",
      "link": "https://arxiv.org/pdf/2602.08269v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.17493v1",
      "title": "Learning with Boolean threshold functions",
      "abstract": "We develop a method for training neural networks on Boolean data in which the values at all nodes are strictly $\\pm 1$, and the resulting models are typically equivalent to networks whose nonzero weights are also $\\pm 1$. The method replaces loss minimization with a nonconvex constraint formulation. Each node implements a Boolean threshold function (BTF), and training is expressed through a divide-and-concur decomposition into two complementary constraints: one enforces local BTF consistency between inputs, weights, and output; the other imposes architectural concurrence, equating neuron outputs with downstream inputs and enforcing weight equality across training-data instantiations of the network. The reflect-reflect-relax (RRR) projection algorithm is used to reconcile these constraints.   Each BTF constraint includes a lower bound on the margin. When this bound is sufficiently large, the learned representations are provably sparse and equivalent to networks composed of simple logical gates with $\\pm 1$ weights. Across a range of tasks -- including multiplier-circuit discovery, binary autoencoding, logic-network inference, and cellular automata learning -- the method achieves exact solutions or strong generalization in regimes where standard gradient-based methods struggle. These results demonstrate that projection-based constraint satisfaction provides a viable and conceptually distinct foundation for learning in discrete neural systems, with implications for interpretability and efficient inference.",
      "authors": [
        "Veit Elser",
        "Manish Krishan Lal"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-19T16:07:25+00:00",
      "link": "https://arxiv.org/pdf/2602.17493v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.04806v2",
      "title": "A Two-Dimensional Analytic Solution for the Generation of Hyperbolic Trajectories Via A Single Close Encounter with Applications To Interstellar Objects",
      "abstract": "The discovery of interstellar interlopers such as 1I/`Oumuamua, 2I/Borisov, and 3I/ATLAS have highlighted the necessity of understanding the dynamical pathways that eject small bodies from planetary systems into hyperbolic trajectories. In this paper we examine the orbital elements of particles in the restricted three-body problem prior to and post scattering onto hyperbolic trajectories by massive perturbers. Building on previous work, we calculate closed-form -- but approximate -- analytic criteria that map pre- to post-encounter orbital elements. An application of these equations demonstrates that ejection occurs most efficiently when the orbital eccentricity of the massless test particle exceeds a minimum threshold, $e\\gtrsim0.4$. The primary driver of the final eccentricity is the component of the perturber-centric velocity projected along the direction of motion of the perturber. These analytic criteria are then benchmarked and validated against numerical simulations which demonstrate that they provide a reasonably good zeroth-order approximation for ejection behavior. However, system-specific cases will generally require numerical simulations in addition to this analytic construction. The methodology is applied to (i) the solar system and exoplanetary systems (ii) $β$ Pictoris and (iii) HR 8799 to evaluate the pre-scattering orbits of ejected particles. This method provides a transparent and computationally efficient tool for identifying orbits within a given system from which interstellar objects are efficiently ejected via a single scattering event from a massive perturber.",
      "authors": [
        "Hayden Monk",
        "Darryl Z. Seligman"
      ],
      "primary_category": "astro-ph.EP",
      "categories": [
        "astro-ph.EP",
        "astro-ph.GA"
      ],
      "published": "2026-02-04T17:53:23+00:00",
      "link": "https://arxiv.org/pdf/2602.04806v2",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.04806v1",
      "title": "A Two-Dimensional Analytic Solution for the Generation of Hyperbolic Trajectories Via A Single Close Encounter with Applications To Interstellar Objects",
      "abstract": "The discovery of interstellar interlopers such as 1I/`Oumuamua, 2I/Borisov, and 3I/ATLAS have highlighted the necessity of understanding the dynamical pathways that eject small bodies from planetary systems into hyperbolic trajectories. In this paper we examine the orbital elements of particles in the restricted three-body problem prior to and post scattering onto hyperbolic trajectories by massive perturbers. Building on previous work, we calculate closed-form -- but approximate -- analytic criteria that map pre- to post-encounter orbital elements. An application of these equations demonstrates that ejection occurs most efficiently when the orbital eccentricity of the massless test particle exceeds a minimum threshold, $e\\gtrsim0.4$. The primary driver of the final eccentricity is the component of the perturber-centric velocity projected along the direction of motion of the perturber. These analytic criteria are then benchmarked and validated against numerical simulations which demonstrate that they provide a reasonably good zeroth-order approximation for ejection behavior. However, system-specific cases will generally require numerical simulations in addition to this analytic construction. The methodology is applied to (i) the solar system and exoplanetary systems (ii) $β$ Pictoris and (iii) HR 8799 to evaluate the pre-scattering orbits of ejected particles. This method provides a transparent and computationally efficient tool for identifying orbits within a given system from which interstellar objects are efficiently ejected via a single scattering event from a massive perturber.",
      "authors": [
        "Hayden Monk",
        "Darryl Z. Seligman"
      ],
      "primary_category": "astro-ph.EP",
      "categories": [
        "astro-ph.EP",
        "astro-ph.GA"
      ],
      "published": "2026-02-04T17:53:23+00:00",
      "link": "https://arxiv.org/pdf/2602.04806v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.14105v1",
      "title": "Non-Hermitian Quantum Mechanics of Open Quantum Systems: Revisiting The One-Body Problem",
      "abstract": "We review analyses of open quantum systems. We show how non-Hermiticity arises in an open quantum system with an infinite environment, focusing on the one-body problem. One of the reasons for taking the present approach is that we can solve the problem completely, making it easier to see the structures of problems involving open quantum systems. We show that this results in the discovery of a new complete set, which is one of the main topics of the present article. Another reason for focusing on the one-body problem is that the theory permits the strong coupling between the system and the environment. In the current research landscape, it is valuable to revisit the one-body problem for open quantum systems, which can be solved accurately for arbitrary strengths of the system-environment couplings. A rigorous understanding of the problem structures in the present approach will be helpful when we tackle problems with many-body interactions. First, we consider potential scattering and directly define the resonant state as an eigenstate of the Schrödinger equation under the Siegert outgoing boundary condition. We show that the resonant eigenstate can have a complex energy eigenvalue, even though the Hamiltonian is seemingly Hermitian. Second, we introduce the Feshbach formalism, which eliminates the infinite degrees of freedom of the environment and represents its effect as a complex potential. The resulting effective Hamiltonian is explicitly non-Hermitian. By unifying these two ways of defining resonant states, we obtain a new complete set of bases for the scattering problem that contains all discrete eigenstates, including resonant states. We finally mention the non-Markovian dynamics of open quantum systems. We emphasize the time-reversal symmetry of the dynamics that continuously connects the past and the future. We can capture it using the new complete set that we develop here.",
      "authors": [
        "Naomichi Hatano",
        "Gonzalo Ordonez"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "math-ph",
        "nucl-th"
      ],
      "published": "2026-02-15T11:42:18+00:00",
      "link": "https://arxiv.org/pdf/2602.14105v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2602.16551v1",
      "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation",
      "abstract": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient \"Gatekeeper\" agent for relevance filtering and a high-capability \"Analyst\" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\\%, establishing a highly efficient \"Human-in-the-loop\" workflow that reduces manual data curation time by approximately 90\\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the \"Digital Material Twin\" of built heritage.",
      "authors": [
        "Rui Hu",
        "Yue Wu",
        "Tianhao Su",
        "Yin Wang",
        "Shunbo Hu",
        "Jizhong Huang"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-18T15:53:15+00:00",
      "link": "https://arxiv.org/pdf/2602.16551v1",
      "tags": [
        "keyword:SR-RL"
      ]
    },
    {
      "id": "2601.11414v1",
      "title": "New Adaptive Mechanism for Large Neighborhood Search using Dual Actor-Critic",
      "abstract": "Adaptive Large Neighborhood Search (ALNS) is a widely used heuristic method for solving combinatorial optimization problems. ALNS explores the solution space by iteratively using destroy and repair operators with probabilities, which are adjusted by an adaptive mechanism to find optimal solutions. However, the classic ALNS adaptive mechanism does not consider the interaction between destroy and repair operators when selecting them. To overcome this limitation, this study proposes a novel adaptive mechanism. This mechanism enhances the adaptability of the algorithm through a Dual Actor-Critic (DAC) model, which fully considers the fact that the quality of new solutions is jointly determined by the destroy and repair operators. It effectively utilizes the interaction between these operators during the weight adjustment process, greatly improving the adaptability of the ALNS algorithm. In this mechanism, the destroy and repair processes are modeled as independent Markov Decision Processes to guide the selection of operators more accurately. Furthermore, we use Graph Neural Networks to extract key features from problem instances and perform effective aggregation and normalization to enhance the algorithm's transferability to different sizes and characteristics of problems. Through a series of experiments, we demonstrate that the proposed DAC-ALNS algorithm significantly improves solution efficiency and exhibits excellent transferability.",
      "authors": [
        "Shaohua Yu",
        "Wenhao Mao",
        "Zigao Wu",
        "Jakob Puchinger"
      ],
      "primary_category": "cs.GT",
      "categories": [
        "cs.GT",
        "cs.LG"
      ],
      "published": "2026-01-16T16:33:52+00:00",
      "link": "https://arxiv.org/pdf/2601.11414v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.08253v1",
      "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design",
      "abstract": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.",
      "authors": [
        "Baoyun Zhao",
        "He Wang",
        "Liang Zeng"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-09T04:13:35+00:00",
      "link": "https://arxiv.org/pdf/2602.08253v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.11010v1",
      "title": "The Dynamic Team Orienteering Problem in Spatial Crowdsourcing: A Scenario Sampling Approach",
      "abstract": "In services such as retail audits and urban infrastructure monitoring, a platform dispatches rewarded, location-based micro-tasks to mobile workers traveling along personal origin-destination (OD) trips under hard time budgets. As requests with time constraints arrive online over a finite horizon, the platform must decide which requests to accept and how to route workers to maximize collected profit. We model this setting as the Dynamic Team Orienteering Problem in Spatial Crowdsourcing (DTOP-SC). To solve this problem, we propose a scenario-sampling rolling-horizon framework that mitigates myopic bias by augmenting each planning epoch with sampled virtual tasks. At each epoch, the augmented task set defines a deterministic static subproblem solved via an adaptive large neighborhood search (ALNS). We also formulate a mixed-integer programming model to provide offline reference solutions. Computational experiments are conducted on synthetic DTOP-SC instances generated from real-world road-map coordinates and on a dynamic team orienteering (DTOP) benchmark. On the map-based instances, the proposed policy exhibits stable gaps with respect to time-limited MIP solutions across the tested scales, while maintaining smooth computational scalability as the problem size increases. On the DTOP benchmark, the policy achieves an average decision time of 0.14s per instance, with 192-198s reported for multiple plan approach as an indicative reference, while maintaining competitive profit.",
      "authors": [
        "Zhibin Wu",
        "Songhao Shen",
        "Yufeng Zhou",
        "Qin Lei"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-16T05:47:04+00:00",
      "link": "https://arxiv.org/pdf/2601.11010v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.22052v2",
      "title": "Learning to Dial-a-Ride: A Deep Graph Reinforcement Learning Approach to the Electric Dial-a-Ride Problem",
      "abstract": "Urban mobility systems are transitioning toward electric, on-demand services, creating operational challenges for fleet management under energy and service-quality constraints. The Electric Dial-a-Ride Problem (E-DARP) extends the classical dial-a-ride problem by incorporating limited battery capacity and nonlinear charging dynamics, increasing computational complexity and limiting the scalability of exact methods for real-time use. This paper proposes a deep reinforcement learning approach based on an edge-centric graph neural network encoder and an attention-driven route construction policy. By operating directly on edge attributes such as travel time and energy consumption, the method captures non-Euclidean, asymmetric, and energy-dependent routing costs in real road networks. The learned policy jointly optimizes routing, charging, and service quality without relying on Euclidean assumptions or handcrafted heuristics. The approach is evaluated on two case studies using ride-sharing data from San Francisco. On benchmark instances, the method achieves solutions within 0.4% of best-known results while reducing computation times by orders of magnitude. A second case study considers large-scale instances with up to 250 request pairs, realistic energy models, and nonlinear charging. On these instances, the learned policy outperforms Adaptive Large Neighborhood Search (ALNS) by 9.5% in solution quality while achieving 100% service completion, with inference times under 10 seconds compared to hours for the metaheuristic. Finally, sensitivity analyses quantify the impact of battery capacity, fleet size, ride-sharing capacity, and reward weights, while robustness experiments show that deterministically trained policies generalize effectively under stochastic conditions.",
      "authors": [
        "Sten Elling Tingstad Jacobsen",
        "Attila Lischka",
        "Balázs Kulcsár",
        "Anders Lindman"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-01-29T17:51:04+00:00",
      "link": "https://arxiv.org/pdf/2601.22052v2",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.12055v1",
      "title": "Multi UAVs Preflight Planning in a Shared and Dynamic Airspace",
      "abstract": "Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with Incremental and Iterative Conflict Resolution. Our framework first generates an initial solution by prioritizing missions based on urgency. Secondly, it computes roundtrip trajectories using SFIPP-ST, a novel 4D single-agent planner (Safe Flight Interval Path Planning with Soft and Temporal Constraints). SFIPP-ST handles heterogeneous UAVs, strictly enforces temporal NFZs, and models inter-agent conflicts as soft constraints. Subsequently, an iterative Large Neighborhood Search, guided by a geometric conflict graph, efficiently resolves any residual conflicts. A completeness-preserving directional pruning technique further accelerates the 3D search. On benchmarks with temporal NFZs, DTAPP-IICR achieves near-100% success with fleets of up to 1,000 UAVs and gains up to 50% runtime reduction from pruning, outperforming batch Enhanced Conflict-Based Search in the UTM context. Scaling successfully in realistic city-scale operations where other priority-based methods fail even at moderate deployments, DTAPP-IICR is positioned as a practical and scalable solution for preflight planning in dense, dynamic urban airspace.",
      "authors": [
        "Amath Sow",
        "Mauricio Rodriguez Cesen",
        "Fabiola Martins Campos de Oliveira",
        "Mariusz Wzorek",
        "Daniel de Leng",
        "Mattias Tiger",
        "Fredrik Heintz",
        "Christian Esteve Rothenberg"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "published": "2026-02-12T15:18:46+00:00",
      "link": "https://arxiv.org/pdf/2602.12055v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.17899v2",
      "title": "Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization",
      "abstract": "Neighborhood search operators are critical to the performance of Multi-Objective Evolutionary Algorithms (MOEAs) and rely heavily on expert design. Although recent LLM-based Automated Heuristic Design (AHD) methods have made notable progress, they primarily optimize individual heuristics or components independently, lacking explicit exploration and exploitation of dynamic coupling relationships between operators. In this paper, multi-operator optimization in MOEAs is formulated as a Markov decision process, enabling the improvement of interdependent operators through sequential decision-making. To address this, we propose the Evolution of Operator Combination (E2OC) framework for MOEAs, which achieves the co-evolution of design strategies and executable codes. E2OC employs Monte Carlo Tree Search to progressively search combinations of operator design strategies and adopts an operator rotation mechanism to identify effective operator configurations while supporting the integration of mainstream AHD methods as the underlying designer. Experimental results across AHD tasks with varying objectives and problem scales show that E2OC consistently outperforms state-of-the-art AHD and other multi-heuristic co-design frameworks, demonstrating strong generalization and sustained optimization capability.",
      "authors": [
        "Junhao Qiu",
        "Xin Chen",
        "Liang Ge",
        "Liyong Lin",
        "Zhichao Lu",
        "Qingfu Zhang"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-25T16:31:07+00:00",
      "link": "https://arxiv.org/pdf/2601.17899v2",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.05358v1",
      "title": "Bayesian Neighborhood Adaptation for Graph Neural Networks",
      "abstract": "The neighborhood scope (i.e., number of hops) where graph neural networks (GNNs) aggregate information to characterize a node's statistical property is critical to GNNs' performance. Two-stage approaches, training and validating GNNs for every pre-specified neighborhood scope to search for the best setting, is a time-consuming task and tends to be biased due to the search space design. How to adaptively determine proper neighborhood scopes for the aggregation process for both homophilic and heterophilic graphs remains largely unexplored. We thus propose to model the GNNs' message-passing behavior on a graph as a stochastic process by treating the number of hops as a beta process. This Bayesian framework allows us to infer the most plausible neighborhood scope for message aggregation simultaneously with the optimization of GNN parameters. Our theoretical analysis shows that the scope inference improves the expressivity of a GNN. Experiments on benchmark homophilic and heterophilic datasets show that the proposed method is compatible with state-of-the-art GNN variants, achieving competitive or superior performance on the node classification task, and providing well-calibrated predictions.",
      "authors": [
        "Paribesh Regmi",
        "Rui Li",
        "Kishan K C"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T06:29:38+00:00",
      "link": "https://arxiv.org/pdf/2602.05358v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.05358v2",
      "title": "Bayesian Neighborhood Adaptation for Graph Neural Networks",
      "abstract": "The neighborhood scope (i.e., number of hops) where graph neural networks (GNNs) aggregate information to characterize a node's statistical property is critical to GNNs' performance. Two-stage approaches, training and validating GNNs for every pre-specified neighborhood scope to search for the best setting, is a time-consuming task and tends to be biased due to the search space design. How to adaptively determine proper neighborhood scopes for the aggregation process for both homophilic and heterophilic graphs remains largely unexplored. We thus propose to model the GNNs' message-passing behavior on a graph as a stochastic process by treating the number of hops as a beta process. This Bayesian framework allows us to infer the most plausible neighborhood scope for message aggregation simultaneously with the optimization of GNN parameters. Our theoretical analysis shows that the scope inference improves the expressivity of a GNN. Experiments on benchmark homophilic and heterophilic datasets show that the proposed method is compatible with state-of-the-art GNN variants, achieving competitive or superior performance on the node classification task, and providing well-calibrated predictions. Implementation is available at : https://github.com/paribeshregmi/BNA-GNN",
      "authors": [
        "Paribesh Regmi",
        "Rui Li",
        "Kishan KC"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T06:29:38+00:00",
      "link": "https://arxiv.org/pdf/2602.05358v2",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.08616v1",
      "title": "Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces",
      "abstract": "Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.",
      "authors": [
        "Heiko Hoppe",
        "Fabian Akkerman",
        "Wouter van Heeswijk",
        "Maximilian Schiffer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09T13:05:07+00:00",
      "link": "https://arxiv.org/pdf/2602.08616v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.11047v1",
      "title": "CoG: Controllable Graph Reasoning via Relational Blueprints and Failure-Aware Refinement over Knowledge Graphs",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities but often grapple with reliability challenges like hallucinations. While Knowledge Graphs (KGs) offer explicit grounding, existing paradigms of KG-augmented LLMs typically exhibit cognitive rigidity--applying homogeneous search strategies that render them vulnerable to instability under neighborhood noise and structural misalignment leading to reasoning stagnation. To address these challenges, we propose CoG, a training-free framework inspired by Dual-Process Theory that mimics the interplay between intuition and deliberation. First, functioning as the fast, intuitive process, the Relational Blueprint Guidance module leverages relational blueprints as interpretable soft structural constraints to rapidly stabilize the search direction against noise. Second, functioning as the prudent, analytical process, the Failure-Aware Refinement module intervenes upon encountering reasoning impasses. It triggers evidence-conditioned reflection and executes controlled backtracking to overcome reasoning stagnation. Experimental results on three benchmarks demonstrate that CoG significantly outperforms state-of-the-art approaches in both accuracy and efficiency.",
      "authors": [
        "Yuanxiang Liu",
        "Songze Li",
        "Xiaoke Guo",
        "Zhaoyan Gong",
        "Qifei Zhang",
        "Huajun Chen",
        "Wen Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-01-16T07:27:40+00:00",
      "link": "https://arxiv.org/pdf/2601.11047v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.17495v1",
      "title": "PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems",
      "abstract": "In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.",
      "authors": [
        "Ruiyu Zhang",
        "Lin Nie",
        "Wai-Fung Lam",
        "Qihao Wang",
        "Xin Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-01-24T15:46:02+00:00",
      "link": "https://arxiv.org/pdf/2601.17495v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.13969v1",
      "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval",
      "abstract": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.",
      "authors": [
        "Joaquín Polonuer",
        "Lucas Vittor",
        "Iñaki Arango",
        "Ayush Noori",
        "David A. Clifton",
        "Luciano Del Corro",
        "Marinka Zitnik"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2026-01-20T13:46:37+00:00",
      "link": "https://arxiv.org/pdf/2601.13969v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.08621v1",
      "title": "GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning",
      "abstract": "Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.",
      "authors": [
        "Jiajin Liu",
        "Yuanfu Sun",
        "Dongzhe Fan",
        "Qiaoyu Tan"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-13T15:00:57+00:00",
      "link": "https://arxiv.org/pdf/2601.08621v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.12151v1",
      "title": "Significant impact of Al1-xGaxN interlayer on GaN/AlN thermal boundary conductance",
      "abstract": "AlN-GaN heterostructures are central to high-power and high-frequency electronics, including RF devices, power converters, and AI accelerators. An intermediate Al1-xGaxN (AlGaN) layer is often present, either unintentionally during growth or intentionally to induce a 2D electron gas, yet its impact on the interfacial thermal boundary conductance (TBC) remains unknown due to the lack of reliable measurement or modeling methods. Here, we report a first principles-based evaluation of the TBCs of AlN-AlGaN, AlGaN-GaN, and AlN-AlGaN-GaN interfaces over the full alloy range. This is realized by the development of accurate deep learning interatomic potentials based on first-principles simulations. Contrary to other material systems where mixed interlayers enhance thermal coupling, we find that an AlGaN interlayer markedly degrades TBC between GaN and AlN, explaining the observation in experiments. Finally, we show that if the Al composition is sigmoidally transitioned from 0 to 1 across the AlN-GaN interface, it can remarkably increase the TBC, compared to an abrupt or a linear transition. This work is expected to shed light on an accurate thermal analysis and electro-thermal co-design of future AlGaN-based devices.",
      "authors": [
        "Khalid Zobaid Adnan",
        "Hao Zhou",
        "Tianli Feng"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-01-17T19:51:35+00:00",
      "link": "https://arxiv.org/pdf/2601.12151v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.17214v1",
      "title": "Polarization Switching of Piezoelectric Films due to Proximity of Ferroelectric Nanoclusters",
      "abstract": "Using Landau-Ginzburg-Devonshire thermodynamical approach and finite element modelling, we studied the influence of nanoclusters shape on the polarization switching and domain nucleation emerging in otherwise non-switchable piezoelectric films due to the proximity of ferroelectric nanoclusters. The boundary of the ferroelectric nanocluster embedded in the piezoelectric film is a compositionally graded layer. We analyzed the conditions, which allow switching the electric polarization of the piezoelectric AlN film at coercive field significantly lower than the electric breakdown field due to the proximity of ferroelectric Al1-xScxN clusters. Due to proximity effect, the spontaneous polarization switches in all elements of the nanopatterned film, and corresponding coercive fields can be reduced significantly in the presence of spike-like Al1-xScxN clusters. We also explored the underlaying physical mechanisms of the proximity effects in the piezoelectric films with ferroelectric nanoclusters. The internal field, which is depolarizing inside the piezoelectric film (due to the larger spontaneous polarization of AlN) and polarizing in the ferroelectric cluster (due to the smaller spontaneous polarization of Al1-xScxN), lowers the potential barrier in the clusters and facilitates the instant growth of nanodomains (emerging in the clusters) through the piezoelectric film. Since considered nanostructured materials can be created by implantation of Sc ions into AlN films, obtained theoretical results can be useful for creation of nanopatterned ferroelectrics by chemical engineering, with exciting prospects for previously unrealizable ferroelectric memory technologies.",
      "authors": [
        "Anna N. Morozovska",
        "Eugene A. Eliseev",
        "Sergei V. Kalin",
        "Long-Qing Chen",
        "Dean R. Evans",
        "Venkatraman Gopalan"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "physics.app-ph"
      ],
      "published": "2026-01-23T22:53:10+00:00",
      "link": "https://arxiv.org/pdf/2601.17214v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.10495v1",
      "title": "Growth and Morphology of InN Nanowires on Si<111> and Si<100> at Back-End-Of-Line Compatible Temperatures",
      "abstract": "InN nanowires were grown on Si<111> and Si<100> substrates by plasma-assisted molecular beam epitaxy using a thin AlN buffer layer at temperatures compatible with the thermal budget limitation imposed by Back-End-Of-Line processing. Reflection high-energy electron diffraction reveals different nucleation behaviors on the two substrate orientations, with higher structural disorder in the case of Si<100>. However, vertically aligned nanowires with hexagonal cross section and N polarity are obtained on both substrates. A statistical analysis of nanowire morphology as a function of growth temperature indicates similar trends in diameter, density, and length on Si<111> and Si<100>, which are explained by adatom kinetics during growth. Nanowires on Si<100> exhibit improved uniformity and reduced tapering, attributed to the different nanowire nucleation due to microstructural properties of the AlN buffer layer. The results demonstrate the feasibility of growing high-quality InN nanowires on Si<100>, supporting their potential for monolithic integration of nanowire-based photodetectors on silicon.",
      "authors": [
        "Andrea Orlando-cunnac",
        "Arthur Arnaud",
        "Martien Den Hertog",
        "Ettore Coccato",
        "Vincent Calvo",
        "Jonathan Steckel",
        "Eva Monroy"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-01-15T15:13:56+00:00",
      "link": "https://arxiv.org/pdf/2601.10495v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.12956v1",
      "title": "Molecular Beam Epitaxy of Al$\\mathrm{_{1-x}}$Sc$\\mathrm{_{x}}$N Nanowires: Towards Group-III Nitride Piezoelectric Nanogenerators with Enhanced Response",
      "abstract": "We study the molecular beam epitaxy of self-assembled Al$\\mathrm{_{1-x}}$Sc$\\mathrm{_{x}}$N nanowires on conductive TiN layers and demonstrate their application in piezoelectric nanogenerators. Wurtzite Al$\\mathrm{_{1-x}}$Sc$\\mathrm{_{x}}$N nanowires with uniform Sc incorporation are grown across a wide composition range (0<x<0.35). At substrate temperatures below 700 $^\\circ{}$C, these nanowires exhibit an inversely tapered morphology, whereas higher temperatures favor the nucleation of additional branches due to a phase separation of Al$\\mathrm{_{1-x}}$Sc$\\mathrm{_{x}}$N into wurtzite AlN and rock-salt ScN. Phase-pure Al$\\mathrm{_{1-x}}$Sc$\\mathrm{_{x}}$N nanowires are integrated into vertical nanogenerators, where the metallic TiN substrate serves as bottom electrode. The fabricated polymer-nanowire composite devices achieve effective piezoelectric charge coefficients of up to 8.5 pC N$^{-1}$ at x=0.32, thus exceeding the piezoelectric response of bulk AlN by nearly a factor of two. Although the charge response remains lower compared to Al$\\mathrm{_{1-x}}$Sc$\\mathrm{_{x}}$N thin films, the reduced effective dielectric permittivity of the nanowire-polymer composites compensates the reduction in piezoelectric charge coefficient, eventually yielding a higher voltage response and comparable energy harvesting efficiency. Finally, effective medium modeling reveals that the device architecture is the primary factor limiting performance, providing general design principles for highly efficient nanowire-based piezoelectric energy harvesters.",
      "authors": [
        "Adriano Notarangelo",
        "Rudeesun Songmuang",
        "Mostafa Saleh",
        "Nattawadi Buatip",
        "Ileana Florea",
        "Philippe Vennéguès",
        "Aidan F. Campbell",
        "Hans Tornatzky",
        "Jonas Lähnemann",
        "Thomas Auzelle",
        "Lutz Geelhaar",
        "Oliver Brandt",
        "Philipp M. John"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.mes-hall",
        "physics.app-ph"
      ],
      "published": "2026-02-13T14:21:36+00:00",
      "link": "https://arxiv.org/pdf/2602.12956v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.13827v1",
      "title": "Ion Implantation Enhanced Nucleation Facilitates Heat Transport across Atomically-Sharp Semiconductor Interfaces",
      "abstract": "Overheating is a critical bottleneck limiting the performance and reliability of next-generation high-power and high-frequency electronics. Interfacial thermal resistance constitutes a significant portion of the total thermal resistance. In this study, we report an ultrahigh thermal boundary conductance (TBC) of approximately 800 MW/m2-K at the atomically-sharp AlN-SiC interface, achieved through an ion implantation-enhanced nucleation epitaxy technique. This value is among the highest TBC values reported for semiconductor interfaces, confirmed by structural characterizations which show an ultrahigh-quality interface. Atomistic Green Function calculations reveal that elastic phonon transmission dominates the interface, with nearly half of the acoustic modes (0-15 THz) exhibiting near-unity transmission due to the atomically sharp structure. Furthermore, using high-energy-resolution electron energy loss spectroscopy, we probe vibrational properties with nanometer spatial resolution and identify unique interfacial phonon modes connecting the mismatched phonon spectra, confirmed by molecular dynamics simulations. The ultrahigh TBC is attributed to both the high elastic phonon transmission due to the high quality interfaces and the inelastic phonon scattering channel due to interfacial phonon modes. These findings not only advance the fundamental understanding of interfacial thermal transport but also provide a pathway for effective thermal management in emerging electronic devices.",
      "authors": [
        "Jinwen Liu",
        "Zifeng Huang",
        "Lina Yang",
        "Yachao Zhang",
        "Xingqiang Zhang",
        "Kun Zhang",
        "Xufei Guo",
        "Yuxiang Wang",
        "Hong Zhou",
        "Jincheng Zhang",
        "Wei Wang",
        "Yue Hao",
        "Zhe Cheng"
      ],
      "primary_category": "cond-mat.mes-hall",
      "categories": [
        "cond-mat.mes-hall",
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-02-14T15:41:43+00:00",
      "link": "https://arxiv.org/pdf/2602.13827v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.19201v1",
      "title": "Electrically pumped AlGaN edge-emitting UV-B laser diodes grown by molecular beam epitaxy",
      "abstract": "Mid and deep ultraviolet (UV) laser diodes remain among the least explored devices in semiconductor optoelectronics, despite their importance for spectroscopy, biochemical sensing, disinfection, and emerging quantum photonics. Here, we demonstrate an electrically pumped AlGaN-based laser diode operating in the UV-B band (280-315 nm). The device is grown by molecular beam epitaxy (MBE) on single-crystal AlN substrate and fabricated in a ridge-waveguide geometry. The laser diode operates at 298.5 nm and exhibits a relatively low threshold current density of 3.4 kA/cm$^2$. Clear nonlinear light-current characteristics and pronounced spectral narrowing with a full-width-at-half-maximum (FWHM) of 0.2 nm are measured above threshold.",
      "authors": [
        "Huabin Yu",
        "Shubham Mondal",
        "Rui Shen",
        "Md Tanvir Hasan",
        "David He",
        "Jiangnan Liu",
        "Samuel Yang",
        "Minming He",
        "Omar Alkhazragi",
        "Danhao Wang",
        "Mackillo Kira",
        "Parag Deotare",
        "Di Liang",
        "Zetian Mi"
      ],
      "primary_category": "physics.optics",
      "categories": [
        "physics.optics",
        "physics.app-ph"
      ],
      "published": "2026-01-27T05:04:08+00:00",
      "link": "https://arxiv.org/pdf/2601.19201v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.20691v1",
      "title": "Impact of O concentration on the thermal stability and decomposition mechanism of (Cr,Al)N compared to (Ti,Al)N thin films",
      "abstract": "The composition-dependent thermal stability of (Cr$_{0.47 \\mp 0.03}$Al$_{0.53 \\mp 0.03}$)$_{z}$(O$_{y}$N$_{1-y}$)$_{1-z}$ thin films with O concentrations of y = 0, 0.15, and 0.40 is investigated up to 1200 °C and then compared to (Ti$_{0.56}$Al$_{0.44}$)$_{z}$(O$_{y}$N$_{1-y}$)$_{1-z}$. X-ray diffraction reveals a thermal stability limit of 1150 °C independent of the O concentration, as witnessed by the formation of decomposition products, namely h-Cr$_{2}$N for (Cr$_{0.50}$Al$_{0.50}$)$_{0.49}$N$_{0.51}$ and c-Cr for both (Cr$_{0.48}$Al$_{0.52}$)$_{0.48}$(O$_{0.15}$N$_{0.85}$)$_{0.52}$ and (Cr$_{0.44}$Al$_{0.56}$)$_{0.46}$(O$_{0.40}$N$_{0.60}$)$_{0.54}$. Based on TEM and ERDA data, the thermal stability limit is extended to 1100 - 1150 °C. DFT calculations indicate that bond breaking limits the thermal stability. In (Cr,Al)N, N has the lowest activation energy for migration. Furthermore, the O vacancy formation energy is highest in (Cr,Al)(O,N). It has to be overcome to enable diffusion on the non-metal sublattice, which is necessary for forming decomposition products like w-AlN or c-Cr. However, once Cr-N bonds break, decomposition into h-Cr$_{2}$N and subsequent c-Cr together with N$_{2}$ is triggered. This results in N evaporation, generating sufficient non-metal vacancies that greatly enhance diffusion and render the extensive vacancy formation energies for non-metals irrelevant. This reduction of the activation energy for mass transport on the non-metal sublattice to the migration barrier causes the similar thermal stability in (Cr$_{0.47 \\mp 0.03}$Al$_{0.53 \\mp 0.03}$)$_{z}$(O$_{y}$N$_{1-y}$)$_{1-z}$. In contrast, Al bonds break first without creating non-metal vacancies in (Ti,Al)(O,N). Thus, the high O vacancy formation energy in (Ti,Al)(O,N) significantly increases the thermal stability compared to (Ti,Al)N as well as the here investigated films.",
      "authors": [
        "Pauline Kümmerl",
        "Ganesh Kumar Nayak",
        "Felix Leinenbach",
        "Zsolt Czigány",
        "Daniel Primetzhofer",
        "Szilárd Kolozsvári",
        "Peter Polcik",
        "Marcus Hans",
        "Jochen M. Schneider"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-01-28T15:21:05+00:00",
      "link": "https://arxiv.org/pdf/2601.20691v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.05675v1",
      "title": "Variable Search Stepsize for Randomized Local Search in Multi-Objective Combinatorial Optimization",
      "abstract": "Over the past two decades, research in evolutionary multi-objective optimization has predominantly focused on continuous domains, with comparatively limited attention given to multi-objective combinatorial optimization problems (MOCOPs). Combinatorial problems differ significantly from continuous ones in terms of problem structure and landscape. Recent studies have shown that on MOCOPs multi-objective evolutionary algorithms (MOEAs) can even be outperformed by simple randomised local search. Starting with a randomly sampled solution in search space, randomised local search iteratively draws a random solution (from an archive) to perform local variation within its neighbourhood. However, in most existing methods, the local variation relies on a fixed neighbourhood, which limits exploration and makes the search easy to get trapped in local optima. In this paper, we present a simple yet effective local search method, called variable stepsize randomized local search (VS-RLS), which adjusts the stepsize during the search. VS-RLS transitions gradually from a broad, exploratory search in the early phases to a more focused, fine-grained search as the search progresses. We demonstrate the effectiveness and generalizability of VS-RLS through extensive evaluations against local search and MOEAs methods on diverse MOCOPs.",
      "authors": [
        "Xuepeng Ren",
        "Maocai Wang",
        "Guangming Dai",
        "Zimin Liang",
        "Qianrong Liu",
        "Shengxiang Yang",
        "Miqing Li"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-05T13:59:05+00:00",
      "link": "https://arxiv.org/pdf/2602.05675v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.01475v1",
      "title": "Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models",
      "abstract": "Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.",
      "authors": [
        "Brij Malhotra",
        "Shivvrat Arya",
        "Tahrima Rahman",
        "Vibhav Giridhar Gogate"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-01T22:43:28+00:00",
      "link": "https://arxiv.org/pdf/2602.01475v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.06318v1",
      "title": "Random is Faster than Systematic in Multi-Objective Local Search",
      "abstract": "Local search is a fundamental method in operations research and combinatorial optimisation. It has been widely applied to a variety of challenging problems, including multi-objective optimisation where multiple, often conflicting, objectives need to be simultaneously considered. In multi-objective local search algorithms, a common practice is to maintain an archive of all non-dominated solutions found so far, from which the algorithm iteratively samples a solution to explore its neighbourhood. A central issue in this process is how to explore the neighbourhood of a selected solution. In general, there are two main approaches: 1) systematic exploration and 2) random sampling. The former systematically explores the solution's neighbours until a stopping condition is met -- for example, when the neighbourhood is exhausted (i.e., the best improvement strategy) or once a better solution is found (i.e., first improvement). In contrast, the latter randomly selects and evaluates only one neighbour of the solution. One may think systematic exploration may be more efficient, as it prevents from revisiting the same neighbours multiple times. In this paper, however, we show that this may not be the case. We first empirically demonstrate that the random sampling method is consistently faster than the systematic exploration method across a range of multi-objective problems. We then give an intuitive explanation for this phenomenon using toy examples, showing that the superior performance of the random sampling method relies on the distribution of ``good neighbours''. Next, we show that the number of such neighbours follows a certain probability distribution during the search. Lastly, building on this distribution, we provide a theoretical insight for why random sampling is more efficient than systematic exploration, regardless of whether the best improvement or first improvement strategy is used.",
      "authors": [
        "Zimin Liang",
        "Miqing Li"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-09T21:27:30+00:00",
      "link": "https://arxiv.org/pdf/2601.06318v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.11883v1",
      "title": "Approximation Algorithm for Constrained $k$-Center Clustering: A Local Search Approach",
      "abstract": "Clustering is a long-standing research problem and a fundamental tool in AI and data analysis. The traditional k-center problem, a fundamental theoretical challenge in clustering, has a best possible approximation ratio of 2, and any improvement to a ratio of 2 - ε would imply P = NP. In this work, we study the constrained k-center clustering problem, where instance-level cannot-link (CL) and must-link (ML) constraints are incorporated as background knowledge. Although general CL constraints significantly increase the hardness of approximation, previous work has shown that disjoint CL sets permit constant-factor approximations. However, whether local search can achieve such a guarantee in this setting remains an open question. To this end, we propose a novel local search framework based on a transformation to a dominating matching set problem, achieving the best possible approximation ratio of 2. The experimental results on both real-world and synthetic datasets demonstrate that our algorithm outperforms baselines in solution quality.",
      "authors": [
        "Chaoqi Jia",
        "Longkun Guo",
        "Kewen Liao",
        "Zhigang Lu",
        "Chao Chen",
        "Jason Xue"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-17T02:39:41+00:00",
      "link": "https://arxiv.org/pdf/2601.11883v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.16156v1",
      "title": "All ascents exponential from valued constraint graphs of pathwidth three",
      "abstract": "Many combinatorial optimization problems can be formulated as finding as assignment that maximized some pseudo-Boolean function (that we call the fitness function). Strict local search starts with some assignment and follows some update rule to proceed to an adjacent assignment of strictly higher fitness. This means that strict local search algorithms follow ascents in the fitness landscape of the pseudo-Boolean function. The complexity of the pseudo-Boolean function (and the fitness landscapes that it represents) can be parameterized by properties of the valued constraint satisfaction problem (VCSP) that encodes the pseudo-Boolean function. We focus on properties of the constraint graphs of the VCSP, with the intuition that spare graphs are less complex than dense ones. Specifically, we argue that pathwidth is the natural sparsity parameter for understanding limits on the power of strict local search. We show that prior constructions of sparse VCSPs where all ascents are exponentially long had pathwidth greater than or equal to four. We improve this this with our controlled doubling construction: a valued constraint satisfaction problem of pathwidth three where all ascents are exponentially long from a designated initial assignment. From this, we conclude that all strict local search algorithms can be forced to take an exponential number of steps even on simple valued constraint graphs of pathwidth three.",
      "authors": [
        "Artem Kaznatcheev",
        "Willemijn Volgering"
      ],
      "primary_category": "cs.DM",
      "categories": [
        "cs.DM",
        "cs.DS"
      ],
      "published": "2026-01-22T17:57:54+00:00",
      "link": "https://arxiv.org/pdf/2601.16156v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.14212v1",
      "title": "Generalization and Completeness of Stochastic Local Search Algorithms",
      "abstract": "We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are presented. Then, we use our model to prove the Turing-completeness of SLS algorithms in general. The proof uses our framework to construct a GA able to simulate any Turing machine. This Turing-completeness implies that determining any non-trivial property concerning the relationship between the inputs and the computed outputs is undecidable for GA and, by extension, for the general set of SLS methods (although not necessarily for each particular method). Similar proofs are more informally presented for PSO and ACO.",
      "authors": [
        "Daniel Loscos",
        "Narciso Marti-Oliet",
        "Ismael Rodriguez"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.CL"
      ],
      "published": "2026-01-20T18:17:45+00:00",
      "link": "https://arxiv.org/pdf/2601.14212v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.11841v1",
      "title": "Analysis of a Random Local Search Algorithm for Dominating Set",
      "abstract": "Dominating Set is a well-known combinatorial optimization problem which finds application in computational biology or mobile communication. Because of its $\\mathrm{NP}$-hardness, one often turns to heuristics for good solutions. Many such heuristics have been empirically tested and perform rather well. However, it is not well understood why their results are so good or even what guarantees they can offer regarding their runtime or the quality of their results. For this, a strong theoretical foundation has to be established. We contribute to this by rigorously analyzing a Random Local Search (RLS) algorithm that aims to find a minimum dominating set on a graph. We consider its performance on cycle graphs with $n$ vertices. We prove an upper bound for the expected runtime until an optimum is found of $\\mathcal{O}\\left(n^4\\log^2(n)\\right)$. In doing so, we introduce several models to represent dominating sets on cycles that help us understand how RLS explores the search space to find an optimum. For our proof we use techniques which are already quite popular for the analysis of randomized algorithms. We further apply a special method to analyze a reversible Markov Chain, which arises as a result of our modeling. This method has not yet found wide application in this kind of runtime analysis.",
      "authors": [
        "Hendrik Higl"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "math.CO",
        "math.PR"
      ],
      "published": "2026-01-17T00:17:14+00:00",
      "link": "https://arxiv.org/pdf/2601.11841v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.04745v1",
      "title": "Impact of diversity on bounded archives for multi-objective local search",
      "abstract": "This work tackles two critical challenges related to the development of metaheuristics for Multi-Objective Optimization Problems (MOOPs): the exponential growth of non-dominated solutions and the tendency of metaheuristics to disproportionately concentrate their search on a subset of the Pareto Front. To counteract the first, bounded archives are employed as a strategic mechanism for effectively managing the increasing number of non-dominated solutions. Addressing the second challenge involves an in-depth exploration of solution diversity algorithms found in existing literature. Upon recognizing that current approaches predominantly center on diversity within the objective space, this research introduces innovative methods specifically designed to enhance diversity in the solution space. Results demonstrate the efficacy of the Hamming Distance Archiving Algorithm, one of the newly proposed algorithms for multi-objective local search, surpassing the performance of the Adaptive Grid Archiving and the Hypervolume Archiving, both drawn from the literature. This outcome suggests a promising avenue for enhancing the overall efficiency of metaheuristics employed for solving MOOPs.",
      "authors": [
        "Amadeu A. Coco",
        "Cyprien Borée",
        "Julien Baste",
        "Laetitia Jourdan",
        "Lucien Mousin"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-04T16:47:14+00:00",
      "link": "https://arxiv.org/pdf/2602.04745v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.12005v1",
      "title": "The challenge of scale in molecular adaptation: Local searches in astronomical genotype networks",
      "abstract": "The exploration of vast genotype spaces poses fundamental challenges for evolving populations. As the number of genotypes encoding viable phenotypes grows exponentially with genome length, populations can only explore a tiny fraction of these immense spaces, a fact consistently supported by empirical and theoretical evidence. Paradoxically, local, mutation-driven searches near abundant sequences allow populations to generate phenotypic improvements and functional innovations despite this immense search space. In this contribution, we integrate insights from viral evolution with theoretical expectations derived from genotype-phenotype maps to re-examine how high-dimensional sequence spaces shape evolutionary dynamics. In resolving the paradox, abundant phenotypes play a crucial role because their combinatorial weight biases evolutionary trajectories. We discuss how this bias, together with limited accessibility of fitness peaks, modifies traditional metaphors -- such as fitness landscapes -- and challenges standard notions of evolutionary optimality. Our results underscore that adaptation is predominantly local yet remarkably efficient, providing a unifying perspective on the coexistence of robustness, innovation, and constrained exploration in molecular evolution.",
      "authors": [
        "Susanna Manrubia",
        "Luis F. Seoane",
        "José A. Cuesta"
      ],
      "primary_category": "q-bio.PE",
      "categories": [
        "q-bio.PE"
      ],
      "published": "2026-01-17T10:54:20+00:00",
      "link": "https://arxiv.org/pdf/2601.12005v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.13266v2",
      "title": "The Query Complexity of Local Search in Rounds on General Graphs",
      "abstract": "We analyze the query complexity of finding a local minimum in $t$ rounds on general graphs. More precisely, given a graph $G = (V,E)$ and oracle access to an unknown function $f : V \\to \\mathbb{R}$, the goal is to find a local minimum--a vertex $v$ such that $f(v) \\leq f(u)$ for all $(u,v) \\in E$--using at most $t$ rounds of interaction with the oracle. The query complexity is well understood on grids, but much less is known beyond. This abstract problem captures many optimization tasks, such as finding a local minimum of a loss function during neural network training.   For each graph with $n$ vertices, we prove a deterministic upper bound of $O(t n^{1/t} (sΔ)^{1-1/t})$, where $s$ is the separation number and $Δ$ is the maximum degree of the graph. We complement this result with a randomized lower bound of $Ω(t n^{1/t}-t)$ that holds for any connected graph. We also find that parallel steepest descent with a warm start provides improved bounds for graphs with high separation number and bounded degree.   To obtain our results, we utilized an advanced version of Gemini at various stages of our research. We discuss our experience in a methodology section.",
      "authors": [
        "Simina Brânzei",
        "Ioannis Panageas",
        "Dimitris Paparas"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC",
        "cs.DS"
      ],
      "published": "2026-01-19T18:06:35+00:00",
      "link": "https://arxiv.org/pdf/2601.13266v2",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.12465v1",
      "title": "Probabilistic Design of Parametrized Quantum Circuits through Local Gate Modifications",
      "abstract": "Within quantum machine learning, parametrized quantum circuits provide flexible quantum models, but their performance is often highly task-dependent, making manual circuit design challenging. Alternatively, quantum architecture search algorithms have been proposed to automate the discovery of task-specific parametrized quantum circuits using systematic frameworks. In this work, we propose an evolution-inspired heuristic quantum architecture search algorithm, which we refer to as the local quantum architecture search. The goal of the local quantum architecture search algorithm is to optimize parametrized quantum circuit architectures through a local, probabilistic search over a fixed set of gate-level actions applied to existing circuits. We evaluate the local quantum architecture search algorithm on two synthetic function-fitting regression tasks and two quantum chemistry regression datasets, including the BSE49 dataset of bond separation energies for first- and second-row elements and a dataset of water conformers generated using the data-driven coupled-cluster approach. Using state-vector simulation, our results highlight the applicability of local quantum architecture search algorithm for identifying competitive circuit architectures with desirable performance metrics. Lastly, we analyze the properties of the discovered circuits and demonstrate the deployment of the best-performing model on state-of-the-art quantum hardware.",
      "authors": [
        "Grier M. Jones",
        "Aviraj Newatia",
        "Alexander Lao",
        "Aditya K. Rao",
        "Viki Kumar Prasad",
        "Hans-Arno Jacobsen"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "published": "2026-02-12T22:47:03+00:00",
      "link": "https://arxiv.org/pdf/2602.12465v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.13046v1",
      "title": "Classification of Local Optimization Problems in Directed Cycles",
      "abstract": "We present a complete classification of the distributed computational complexity of local optimization problems in directed cycles for both the deterministic and the randomized LOCAL model. We show that for any local optimization problem $Π$ (that can be of the form min-sum, max-sum, min-max, or max-min, for any local cost or utility function over some finite alphabet), and for any \\emph{constant} approximation ratio $α$, the task of finding an $α$-approximation of $Π$ in directed cycles has one of the following complexities:   1. $O(1)$ rounds in deterministic LOCAL, $O(1)$ rounds in randomized LOCAL,   2. $Θ(\\log^* n)$ rounds in deterministic LOCAL, $O(1)$ rounds in randomized LOCAL,   3. $Θ(\\log^* n)$ rounds in deterministic LOCAL, $Θ(\\log^* n)$ rounds in randomized LOCAL,   4. $Θ(n)$ rounds in deterministic LOCAL, $Θ(n)$ rounds in randomized LOCAL.   Moreover, for any given $Π$ and $α$, we can determine the complexity class automatically, with an efficient (centralized, sequential) meta-algorithm, and we can also efficiently synthesize an asymptotically optimal distributed algorithm.   Before this work, similar results were only known for local search problems (e.g., locally checkable labeling problems). The family of local optimization problems is a strict generalization of local search problems, and it contains numerous commonly studied distributed tasks, such as the problems of finding approximations of the maximum independent set, minimum vertex cover, minimum dominating set, and minimum vertex coloring.",
      "authors": [
        "Thomas Boudier",
        "Fabian Kuhn",
        "Augusto Modanese",
        "Ronja Stimpert",
        "Jukka Suomela"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.CC",
        "cs.FL"
      ],
      "published": "2026-02-13T16:03:14+00:00",
      "link": "https://arxiv.org/pdf/2602.13046v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.11144v3",
      "title": "Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration",
      "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between the comprehensiveness of global search and the efficiency of local search. Existing methods are often challenged by navigating large-scale hierarchical graphs, optimizing retrieval paths, and balancing exploration-exploitation dynamics, frequently lacking robust multi-stage re-ranking. To overcome these deficits, we propose Deep GraphRAG, a framework designed for a balanced approach to hierarchical retrieval and adaptive integration. It introduces a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. This strategy employs a three-stage process: (1) inter-community filtering, which prunes the search space using local context; (2) community-level refinement, which prioritizes relevant subgraphs via entity-interaction analysis; and (3) entity-level fine-grained search within target communities. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO). This novel reinforcement learning approach dynamically adjusts reward weights to balance three key objectives: relevance, faithfulness, and conciseness. This training enables compact models (1.5B) to approach the performance of large models (70B) in the integration task. Evaluations on Natural Questions and HotpotQA demonstrate that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.",
      "authors": [
        "Yuejie Li",
        "Ke Yang",
        "Tao Wang",
        "Bolin Chen",
        "Bowen Li",
        "Chengjun Mao"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-01-16T10:02:31+00:00",
      "link": "https://arxiv.org/pdf/2601.11144v3",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.08952v1",
      "title": "Clique-Based Deletion-Correcting Codes via Penalty-Guided Clique Search",
      "abstract": "We study the construction of $d$-deletion-correcting binary codes by formulating the problem as a Maximum Clique Problem (MCP). In this formulation, vertices represent candidate codewords and edges connect pairs whose longest common subsequence (LCS) distance guarantees correction of up to $d$ deletions. A valid codebook corresponds to a clique in the resulting graph, and finding the largest codebook is equivalent to identifying a maximum clique. While MCP-based formulations for deletion-correcting codes have previously been explored, we demonstrate that applying Penalty-Guided Clique Search (PGCS), a lightweight stochastic clique-search heuristic inspired by Dynamic Local Search (DLS), consistently yields larger codebooks than existing graph-based heuristics, including minimum-degree and coloring methods, for block lengths $n = 8,9,\\dots,14$ and deletion parameters $d = 1,2,3$. In several finite-length regimes, the resulting codebooks match known optimal sizes and outperform classical constructions such as Helberg codes. For decoding under segmented reception, where codeword boundaries are known, we propose an optimized LCS-based decoder that exploits symbol-count filtering and early termination to substantially reduce the number of LCS evaluations while preserving exact decoding guarantees. These optimizations lead to significantly lower average-case decoding complexity than the baseline $O(|C| n^2)$ approach.",
      "authors": [
        "Aniruddh Pandav",
        "Rajshekhar V Bhat"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-02-09T17:47:11+00:00",
      "link": "https://arxiv.org/pdf/2602.08952v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.22075v1",
      "title": "Lens-descriptor guided evolutionary algorithm for optimization of complex optical systems with glass choice",
      "abstract": "Designing high-performance optical lenses entails exploring a high-dimensional, tightly constrained space of surface curvatures, glass choices, element thicknesses, and spacings. In practice, standard optimizers (e.g., gradient-based local search and evolutionary strategies) often converge to a single local optimum, overlooking many comparably good alternatives that matter for downstream engineering decisions. We propose the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage framework for multimodal lens optimization. LDG-EA first partitions the design space into behavior descriptors defined by curvature-sign patterns and material indices, then learns a probabilistic model over descriptors to allocate evaluations toward promising regions. Within each descriptor, LDG-EA applies the Hill-Valley Evolutionary Algorithm with covariance-matrix self-adaptation to recover multiple distinct local minima, optionally followed by gradient-based refinement. On a 24-variable (18 continuous and 6 integer), six-element Double-Gauss topology, LDG-EA generates on average around 14500 candidate minima spanning 636 unique descriptors, an order of magnitude more than a CMA-ES baseline, while keeping wall-clock time at one hour scale. Although the best LDG-EA design is slightly worse than a fine-tuned reference lens, it remains in the same performance range. Overall, the proposed LDG-EA produces a diverse set of solutions while maintaining competitive quality within practical computational budgets and wall-clock time.",
      "authors": [
        "Kirill Antonov",
        "Teus Tukker",
        "Tiago Botari",
        "Thomas H. W. Bäck",
        "Anna V. Kononova",
        "Niki van Stein"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-29T18:13:24+00:00",
      "link": "https://arxiv.org/pdf/2601.22075v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.04248v1",
      "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
      "abstract": "Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.",
      "authors": [
        "Hao Lu",
        "Haoyuan Huang",
        "Yulin Zhou",
        "Chen Li",
        "Ningxin Zhu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T06:14:55+00:00",
      "link": "https://arxiv.org/pdf/2602.04248v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.11389v1",
      "title": "Hyperparameter Optimization of Constraint Programming Solvers",
      "abstract": "The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.   We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.   Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.",
      "authors": [
        "Hedieh Haddad",
        "Thibault Falque",
        "Pierre Talbot",
        "Pascal Bouvry"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-16T16:02:36+00:00",
      "link": "https://arxiv.org/pdf/2601.11389v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.19041v1",
      "title": "HEATACO: Heatmap-Guided Ant Colony Decoding for Large-Scale Travelling Salesman Problems",
      "abstract": "Heatmap-based non-autoregressive solvers for large-scale Travelling Salesman Problems output dense edge-probability scores, yet final performance largely hinges on the decoder that must satisfy degree-2 constraints and form a single Hamiltonian tour. Greedy commitment can cascade into irreparable mistakes at large $N$, whereas MCTS-guided local search is accurate but compute-heavy and highly engineered. We instead treat the heatmap as a soft edge prior and cast decoding as probabilistic tour construction under feasibility constraints, where the key is to correct local mis-rankings via inexpensive global coordination. Based on this view, we introduce HeatACO, a plug-and-play Max-Min Ant System decoder whose transition policy is softly biased by the heatmap while pheromone updates provide lightweight, instance-specific feedback to resolve global conflicts; optional 2-opt/3-opt post-processing further improves tour quality. On TSP500/1K/10K, using heatmaps produced by four pretrained predictors, HeatACO+2opt achieves gaps down to 0.11%/0.23%/1.15% with seconds-to-minutes CPU decoding for fixed heatmaps, offering a better quality--time trade-off than greedy decoding and published MCTS-based decoders. Finally, we find the gains track heatmap reliability: under distribution shift, miscalibration and confidence collapse bound decoding improvements, suggesting heatmap generalisation is a primary lever for further progress.",
      "authors": [
        "Bo-Cheng Lin",
        "Yi Mei",
        "Mengjie Zhang"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "published": "2026-01-26T23:51:19+00:00",
      "link": "https://arxiv.org/pdf/2601.19041v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.13532v1",
      "title": "Fast Swap-Based Element Selection for Multiplication-Free Dimension Reduction",
      "abstract": "In this paper, we propose a fast algorithm for element selection, a multiplication-free form of dimension reduction that produces a dimension-reduced vector by simply selecting a subset of elements from the input. Dimension reduction is a fundamental technique for reducing unnecessary model parameters, mitigating overfitting, and accelerating training and inference. A standard approach is principal component analysis (PCA), but PCA relies on matrix multiplications; on resource-constrained systems, the multiplication count itself can become a bottleneck. Element selection eliminates this cost because the reduction consists only of selecting elements, and thus the key challenge is to determine which elements should be retained. We evaluate a candidate subset through the minimum mean-squared error of linear regression that predicts a target vector from the selected elements, where the target may be, for example, a one-hot label vector in classification. When an explicit target is unavailable, the input itself can be used as the target, yielding a reconstruction-based criterion. The resulting optimization is combinatorial, and exhaustive search is impractical. To address this, we derive an efficient formula for the objective change caused by swapping a selected and an unselected element, using the matrix inversion lemma, and we perform a swap-based local search that repeatedly applies objective-decreasing swaps until no further improvement is possible. Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed method.",
      "authors": [
        "Nobutaka Ono"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.AS",
        "eess.IV",
        "eess.SP"
      ],
      "published": "2026-02-14T00:11:43+00:00",
      "link": "https://arxiv.org/pdf/2602.13532v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.12040v1",
      "title": "Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty",
      "abstract": "The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.",
      "authors": [
        "Murilo da Luz",
        "Bruno Brandão",
        "Luana Martins",
        "Gustavo Oliveira",
        "Bryan de Oliveira",
        "Luckeciano Melo",
        "Telma Soares"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-17T13:00:17+00:00",
      "link": "https://arxiv.org/pdf/2601.12040v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.18005v1",
      "title": "Flow-based Extremal Mathematical Structure Discovery",
      "abstract": "The discovery of extremal structures in mathematics requires navigating vast and nonconvex landscapes where analytical methods offer little guidance and brute-force search becomes intractable. We introduce FlowBoost, a closed-loop generative framework that learns to discover rare and extremal geometric structures by combining three components: (i) a geometry-aware conditional flow-matching model that learns to sample high-quality configurations, (ii) reward-guided policy optimization with action exploration that directly optimizes the generation process toward the objective while maintaining diversity, and (iii) stochastic local search for both training-data generation and final refinement. Unlike prior open-loop approaches, such as PatternBoost that retrains on filtered discrete samples, or AlphaEvolve which relies on frozen Large Language Models (LLMs) as evolutionary mutation operators, FlowBoost enforces geometric feasibility during sampling, and propagates reward signal directly into the generative model, closing the optimization loop and requiring much smaller training sets and shorter training times, and reducing the required outer-loop iterations by orders of magnitude, while eliminating dependence on LLMs. We demonstrate the framework on four geometric optimization problems: sphere packing in hypercubes, circle packing maximizing sum of radii, the Heilbronn triangle problem, and star discrepancy minimization. In several cases, FlowBoost discovers configurations that match or exceed the best known results. For circle packings, we improve the best known lower bounds, surpassing the LLM-based system AlphaEvolve while using substantially fewer computational resources.",
      "authors": [
        "Gergely Bérczi",
        "Baran Hashemi",
        "Jonas Klüver"
      ],
      "primary_category": "math.CO",
      "categories": [
        "math.CO",
        "cs.LG"
      ],
      "published": "2026-01-25T21:41:47+00:00",
      "link": "https://arxiv.org/pdf/2601.18005v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.09424v1",
      "title": "Reward-Guided Discrete Diffusion via Clean-Sample Markov Chain for Molecule and Biological Sequence Design",
      "abstract": "Discrete diffusion models have recently emerged as a powerful class of generative models for chemistry and biology data. In these fields, the goal is to generate various samples with high rewards (e.g., drug-likeness in molecules), making reward-based guidance crucial. Most existing methods are based on guiding the diffusion model using intermediate rewards but tend to underperform since intermediate rewards are noisy due to the non-smooth nature of reward functions used in scientific domains. To address this, we propose Clean-Sample Markov Chain (CSMC) Sampler, a method that performs effective test-time reward-guided sampling for discrete diffusion models, enabling local search without relying on intermediate rewards. CSMC constructs a Markov chain of clean samples using the Metropolis-Hastings algorithm such that its stationary distribution is the target distribution. We design a proposal distribution by sequentially applying the forward and backward diffusion processes, making the acceptance probability tractable. Experiments on molecule and biological sequence generation with various reward functions demonstrate that our method consistently outperforms prior approaches that rely on intermediate rewards.",
      "authors": [
        "Prin Phunyaphibarn",
        "Minhyuk Sung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "published": "2026-02-10T05:39:48+00:00",
      "link": "https://arxiv.org/pdf/2602.09424v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.08473v1",
      "title": "Submodular Maximization over a Matroid $k$-Intersection: Multiplicative Improvement over Greedy",
      "abstract": "We study the problem of maximizing a non-negative monotone submodular objective $f$ subject to the intersection of $k$ arbitrary matroid constraints. The natural greedy algorithm guarantees $(k+1)$-approximation for this problem, and the state-of-the-art algorithm only improves this approximation ratio to $k$. We give a $\\frac{2k\\ln2}{1+\\ln2}+O(\\sqrt{k})<0.819k+O(\\sqrt{k})$ approximation for this problem. Our result is the first multiplicative improvement over the approximation ratio of the greedy algorithm for general $k$. We further show that our algorithm can be used to obtain roughly the same approximation ratio also for the more general problem in which the objective is not guaranteed to be monotone (the sublinear term in the approximation ratio becomes $O(k^{2/3})$ rather than $O(\\sqrt{k})$ in this case).   All of our results hold also when the $k$-matroid intersection constraint is replaced with a more general matroid $k$-parity constraint. Furthermore, unlike the case in many of the previous works, our algorithms run in time that is independent of $k$ and polynomial in the size of the ground set. Our algorithms are based on a hybrid greedy local search approach recently introduced by Singer and Thiery (STOC 2025) for the weighted matroid $k$-intersection problem, which is a special case of the problem we consider. Leveraging their approach in the submodular setting requires several non-trivial insights and algorithmic modifications since the marginals of a submodular function $f$, which correspond to the weights in the weighted case, are not independent of the algorithm's internal randomness. In the special weighted case studied by Singer and Thiery, our algorithms reduce to a variant of their algorithm with an improved approximation ratio of $k\\ln2+1-\\ln2<0.694k+0.307$, compared to an approximation ratio of $\\frac{k+1}{2\\ln2}\\approx0.722k+0.722$ guaranteed by Singer and Thiery.",
      "authors": [
        "Moran Feldman",
        "Justin Ward"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "cs.DM"
      ],
      "published": "2026-02-09T10:19:45+00:00",
      "link": "https://arxiv.org/pdf/2602.08473v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.13494v1",
      "title": "Quantum Speedups for Group Relaxations of Integer Linear Programs",
      "abstract": "Integer Linear Programs (ILPs) are a flexible and ubiquitous model for discrete optimization problems. Solving ILPs is \\textsf{NP-Hard} yet of great practical importance. Super-quadratic quantum speedups for ILPs have been difficult to obtain because classical algorithms for many-constraint ILPs are global and exhaustive, whereas quantum frameworks that offer super-quadratic speedup exploit local structure of the objective and feasible set. We address this via quantum algorithms for Gomory's group relaxation. The group relaxation of an ILP is obtained by dropping nonnegativity on variables that are positive in the optimal solution of the linear programming (LP) relaxation, while retaining integrality of the decision variables. We present a competitive feasibility-preserving classical local-search algorithm for the group relaxation, and a corresponding quantum algorithm that, under reasonable technical conditions, achieves a super-quadratic speedup. When the group relaxation satisfies a nondegeneracy condition analogous to, but stronger than, LP non-degeneracy, our approach yields the optimal solution to the original ILP. Otherwise, the group relaxation tightens bounds on the optimal objective value of the ILP, and can improve downstream branch-and-cut by reducing the integrality gap; we numerically observe this on several practically relevant ILPs. To achieve these results, we derive efficiently constructible constraint-preserving mixers for the group relaxation with favorable spectral properties, which are of independent interest.",
      "authors": [
        "Brandon Augustino",
        "Dylan Herman",
        "Guneykan Ozgul",
        "Jacob Watkins",
        "Atithi Acharya",
        "Enrico Fontana",
        "Junhyung Lyle Kim",
        "Shouvanik Chakrabarti"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.DS",
        "math.OC"
      ],
      "published": "2026-02-13T21:58:59+00:00",
      "link": "https://arxiv.org/pdf/2602.13494v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.16473v1",
      "title": "Synthesis and Verification of Transformer Programs",
      "abstract": "C-RASP is a simple programming language that was recently shown to capture concepts expressible by transformers. In this paper, we develop new algorithmic techniques for automatically verifying C-RASPs. To this end, we establish a connection to the verification of synchronous dataflow programs in Lustre, which enables us to exploit state-of-the-art model checkers utilizing highly optimized SMT-solvers. Our second contribution addresses learning a C-RASP program in the first place. To this end, we provide a new algorithm for learning a C-RASP from examples using local search. We demonstrate efficacy of our implementation for benchmarks of C-RASPs in the literature, in particular in connection to the following applications: (1) transformer program optimization, and (2) constrained learning of transformer programs (based on a partial specification).",
      "authors": [
        "Hongjian Jiang",
        "Matthew Hague",
        "Philipp Rümmer",
        "Anthony Widjaja Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.FL",
        "cs.LO"
      ],
      "published": "2026-02-18T14:04:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16473v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.10159v1",
      "title": "Beyond Closed-Pool Video Retrieval: A Benchmark and Agent Framework for Real-World Video Search and Moment Localization",
      "abstract": "Traditional video retrieval benchmarks focus on matching precise descriptions to closed video pools, failing to reflect real-world searches characterized by fuzzy, multi-dimensional memories on the open web. We present \\textbf{RVMS-Bench}, a comprehensive system for evaluating real-world video memory search. It consists of \\textbf{1,440 samples} spanning \\textbf{20 diverse categories} and \\textbf{four duration groups}, sourced from \\textbf{real-world open-web videos}. RVMS-Bench utilizes a hierarchical description framework encompassing \\textbf{Global Impression, Key Moment, Temporal Context, and Auditory Memory} to mimic realistic multi-dimensional search cues, with all samples strictly verified via a human-in-the-loop protocol. We further propose \\textbf{RACLO}, an agentic framework that employs abductive reasoning to simulate the human ``Recall-Search-Verify'' cognitive process, effectively addressing the challenge of searching for videos via fuzzy memories in the real world. Experiments reveal that existing MLLMs still demonstrate insufficient capabilities in real-world Video Retrieval and Moment Localization based on fuzzy memories. We believe this work will facilitate the advancement of video retrieval robustness in real-world unstructured scenarios.",
      "authors": [
        "Tao Yu",
        "Yujia Yang",
        "Haopeng Jin",
        "Junhao Gong",
        "Xinlong Chen",
        "Yuxuan Zhou",
        "Shanbin Zhang",
        "Jiabing Yang",
        "Xinming Wang",
        "Hongzhu Yi",
        "Ping Nie",
        "Kai Zou",
        "Zhang Zhang",
        "Yan Huang",
        "Liang Wang",
        "Yeshani",
        "Ruiwen Tao",
        "Jin Ma",
        "Haijin Liang",
        "Jinwen Luo"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-10T03:50:59+00:00",
      "link": "https://arxiv.org/pdf/2602.10159v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.14162v1",
      "title": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering",
      "abstract": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.",
      "authors": [
        "Tao Xu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.IR"
      ],
      "published": "2026-02-15T14:23:50+00:00",
      "link": "https://arxiv.org/pdf/2602.14162v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.14985v1",
      "title": "Real-time Range-Angle Estimation and Tag Localization for Multi-static Backscatter Systems",
      "abstract": "Multi-static backscatter networks (BNs) are strong candidates for joint communication and localization in the ambient IoT paradigm for 6G. Enabling real-time localization in large-scale multi-static deployments with thousands of devices require highly efficient algorithms for estimating key parameters such as range and angle of arrival (AoA), and for fusing these parameters into location estimates. We propose two low-complexity algorithms, Joint Range-Angle Clustering (JRAC) and Stage-wise Range-Angle Estimation (SRAE). Both deliver range and angle estimation accuracy comparable to FFT- and subspace-based baselines while significantly reducing the computation. We then introduce two real-time localization algorithms that fuse the estimated ranges and AoAs: a maximum-likelihood (ML) method solved via gradient search and an iterative re-weighted least squares (IRLS) method. Both achieve localization accuracy comparable to ML-based brute force search albeit with far lower complexity. Experiments on a real-world large-scale multi-static testbed with 4 illuminators, 1 multi-antenna receiver, and 100 tags show that JRAC and SRAE reduce runtime by up to 40X and IRLS achieves up to 500X reduction over ML-based brute force search without degrading localization accuracy. The proposed methods achieve 3 m median localization error across all 100 tags in a sub-6GHz band with 40 MHz bandwidth. These results demonstrate that multi-static range-angle estimation and localization algorithms can make real-time, scalable backscatter localization practical for next-generation ambient IoT networks.",
      "authors": [
        "Tara Esmaeilbeig",
        "Kartik Patel",
        "Traian E. Abrudan",
        "John Kimionis",
        "Eleftherios Kampianakis",
        "Michael S. Eggleston"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-02-16T18:09:47+00:00",
      "link": "https://arxiv.org/pdf/2602.14985v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.08501v1",
      "title": "Multipoint Code-Weight Sphere Decoding: Parallel Near-ML Decoding for Short-Blocklength Codes",
      "abstract": "Ultra-reliable low-latency communications (URLLC) operate with short packets, where finite-blocklength effects make near-maximum-likelihood (near-ML) decoding desirable but often too costly. This paper proposes a two-stage near-ML decoding framework that applies to any linear block code. In the first stage, we run a low-complexity decoder to produce a candidate codeword and a cyclic redundancy check. When this stage succeeds, we terminate immediately. When it fails, we invoke a second-stage decoder, termed multipoint code-weight sphere decoding (MP-WSD). The central idea behind {MP-WSD} is to concentrate the ML search where it matters. We pre-compute a set of low-weight codewords and use them to generate structured local perturbations of the current estimate. Starting from the first-stage output, MP-WSD iteratively explores a small Euclidean sphere of candidate codewords formed by adding selected low-weight codewords, tightening the search region as better candidates are found. This design keeps the average complexity low: at high signal-to-noise ratio, the first stage succeeds with high probability and the second stage is rarely activated; when it is activated, the search remains localized. Simulation results show that the proposed decoder attains near-ML performance for short-blocklength, low-rate codes while maintaining low decoding latency.",
      "authors": [
        "Yubeen Jo",
        "Geon Choi",
        "Yongjune Kim",
        "Namyoon Lee"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "published": "2026-02-09T10:53:48+00:00",
      "link": "https://arxiv.org/pdf/2602.08501v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.06566v2",
      "title": "SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs",
      "abstract": "Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses \"thinking with images\" by 4.6 points on a challenging OOD task despite requiring a 200$\\times$ lower token budget.",
      "authors": [
        "Niccolo Avogaro",
        "Nayanika Debnath",
        "Li Mi",
        "Thomas Frick",
        "Junling Wang",
        "Zexue He",
        "Hang Hua",
        "Konrad Schindler",
        "Mattia Rigotti"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-06T10:05:25+00:00",
      "link": "https://arxiv.org/pdf/2602.06566v2",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.07621v1",
      "title": "Searching point patterns in point clouds describing local topography",
      "abstract": "We address the problem of comparing and aligning spatial point configurations in $\\mathbb{R}^3$ arising from structured geometric patterns. Each pattern is decomposed into arms along which we define a normalized finite-difference operator measuring local variations of the height component with respect to the planar geometry of the pattern. This quantity provides a parametrization-independent local descriptor that complements global similarity measures. In particular, it integrates naturally with Wasserstein-type distances for comparing point distributions and with Procrustes analysis for rigid alignment of geometric structures.",
      "authors": [
        "Ewa Bednarczuk",
        "Rafał Bieńkowski",
        "Robert Kłopotek",
        "Jan Kryński",
        "Krzysztof Leśsniewski",
        "Krzysztof Rutkowski",
        "Małgorzata Szelachowska"
      ],
      "primary_category": "cs.CG",
      "categories": [
        "cs.CG",
        "math.OC"
      ],
      "published": "2026-01-12T15:05:47+00:00",
      "link": "https://arxiv.org/pdf/2601.07621v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.23023v1",
      "title": "The Where and How of Touch: A Review of Tactile Localization Research",
      "abstract": "Tactile localization is the seemingly simple ability to 'tell' where a touch has occurred. However, how this ability is assessed, and what conclusions are drawn from experiments, depends on the theoretical ideas that inspire the research. Here, we review both theoretical frameworks and methodological approaches based on a systematic web-based literature search on tactile localization. After presenting current theories of tactile localization, we discuss task characteristics that differentiate current methodology for tactile localization into at least 8 distinct types of experimental tasks. We describe these tasks, discuss their, often implicit, underlying assumptions and cognitive requirements, and relate them to the theoretical approaches. We then compare, in an exemplary manner, the tactile localization results reported by a subset of studies and demonstrate how some methods are associated with specific biases, illustrating that the choice of experimental method significantly affects the conclusions drawn from the results. Our review suggests that the field currently lacks a clear concept of the specific processes induced by the various experimental tasks and, thus, calls for concerted efforts to clarify and unify currently diverse, fragmented, and partly inconsistent theoretical underpinnings of tactile spatial processing, flanked by dedicated data sharing to allow across-study analysis.",
      "authors": [
        "Xaver Fuchs",
        "Jason A. M. Khoury",
        "Sergiu Tcaci Popescu",
        "Tobias Heed",
        "Matej Hoffmann"
      ],
      "primary_category": "q-bio.NC",
      "categories": [
        "q-bio.NC"
      ],
      "published": "2026-01-30T14:32:03+00:00",
      "link": "https://arxiv.org/pdf/2601.23023v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.02623v1",
      "title": "Learning Consistent Causal Abstraction Networks",
      "abstract": "Causal artificial intelligence aims to enhance explainability, trustworthiness, and robustness in AI by leveraging structural causal models (SCMs). In this pursuit, recent advances formalize network sheaves and cosheaves of causal knowledge. Pushing in the same direction, we tackle the learning of consistent causal abstraction network (CAN), a sheaf-theoretic framework where (i) SCMs are Gaussian, (ii) restriction maps are transposes of constructive linear causal abstractions (CAs) adhering to the semantic embedding principle, and (iii) edge stalks correspond--up to permutation--to the node stalks of more detailed SCMs. Our problem formulation separates into edge-specific local Riemannian problems and avoids nonconvex objectives. We propose an efficient search procedure, solving the local problems with SPECTRAL, our iterative method with closed-form updates and suitable for positive definite and semidefinite covariance matrices. Experiments on synthetic data show competitive performance in the CA learning task, and successful recovery of diverse CAN structures.",
      "authors": [
        "Gabriele D'Acunto",
        "Paolo Di Lorenzo",
        "Sergio Barbarossa"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "published": "2026-02-02T16:16:29+00:00",
      "link": "https://arxiv.org/pdf/2602.02623v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.15921v1",
      "title": "Latent Objective Induction and Diversity-Constrained Selection: Algorithms for Multi-Locale Retrieval Pipelines",
      "abstract": "We present three algorithms with formal correctness guarantees and complexity bounds for the problem of selecting a diverse, multi-locale set of sources from ranked search results. First, we formulate weighted locale allocation as a constrained integer partition problem and give an $O(n \\log n)$ algorithm that simultaneously satisfies minimum-representation, budget-exhaustion, and proportionality-bound constraints; we prove all three hold with a tight deviation bound of $< 1$. Second, we define a cascaded country-code inference function as a deterministic priority chain over heterogeneous signals (TLD structure, model-inferred metadata, language fallback) and prove it satisfies both determinism and graceful degradation. Third, we introduce a $κ$-domain diversity constraint for source selection and give an $O(|K| \\cdot R)$ algorithm that maintains the invariant via hash-map lookup, eliminating the aggregator monopolization pathology present in URL-level deduplication. We further formalize Latent Objective Induction (LOI), an environment-shaping operator over prompt spaces that steers downstream model behavior without restricting the feasible output set, and prove its convergence under mild assumptions. Applied to a multi-locale retrieval pipeline, these algorithms yield 62% improvement in first-party source ratio and 89% reduction in same-domain duplication across 120 multilingual queries.",
      "authors": [
        "Faruk Alpay",
        "Levent Sarioglu"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "cs.IR"
      ],
      "published": "2026-02-17T12:25:22+00:00",
      "link": "https://arxiv.org/pdf/2602.15921v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.06269v1",
      "title": "PurSAMERE: Reliable Adversarial Purification via Sharpness-Aware Minimization of Expected Reconstruction Error",
      "abstract": "We propose a novel deterministic purification method to improve adversarial robustness by mapping a potentially adversarial sample toward a nearby sample that lies close to a mode of the data distribution, where classifiers are more reliable. We design the method to be deterministic to ensure reliable test accuracy and to prevent the degradation of effective robustness observed in stochastic purification approaches when the adversary has full knowledge of the system and its randomness. We employ a score model trained by minimizing the expected reconstruction error of noise-corrupted data, thereby learning the structural characteristics of the input data distribution. Given a potentially adversarial input, the method searches within its local neighborhood for a purified sample that minimizes the expected reconstruction error under noise corruption and then feeds this purified sample to the classifier. During purification, sharpness-aware minimization is used to guide the purified samples toward flat regions of the expected reconstruction error landscape, thereby enhancing robustness. We further show that, as the noise level decreases, minimizing the expected reconstruction error biases the purified sample toward local maximizers of the Gaussian-smoothed density; under additional local assumptions on the score model, we prove recovery of a local maximizer in the small-noise limit. Experimental results demonstrate significant gains in adversarial robustness over state-of-the-art methods under strong deterministic white-box attacks.",
      "authors": [
        "Vinh Hoang",
        "Sebastian Krumscheid",
        "Holger Rauhut",
        "Raúl Tempone"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC",
        "math.PR"
      ],
      "published": "2026-02-06T00:06:30+00:00",
      "link": "https://arxiv.org/pdf/2602.06269v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.16819v1",
      "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
      "abstract": "When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.",
      "authors": [
        "Yiqing Xie",
        "Emmy Liu",
        "Gaokai Zhang",
        "Nachiket Kotalwar",
        "Shubham Gandhi",
        "Sathwik Acharya",
        "Xingyao Wang",
        "Carolyn Rose",
        "Graham Neubig",
        "Daniel Fried"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-18T19:30:55+00:00",
      "link": "https://arxiv.org/pdf/2602.16819v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.08084v1",
      "title": "REAMP: A Stochastic Resonance Approach for Multi-Change Point Detection in High-Dimensional Data",
      "abstract": "Detecting multiple structural breaks in high-dimensional data remains a challenge, particularly when changes occur in higher-order moments or within complex manifold structures. In this paper, we propose REAMP (Resonance-Enhanced Analysis of Multi-change Points), a novel framework that integrates optimal transport theory with the physical principles of stochastic resonance. By utilizing a two-stage dimension reduction via the Earth Movers Distance (EMD) and Shortest Hamiltonian Paths (SHP), we map high-dimensional observations onto a graph-based count statistic. To overcome the locality constraints of traditional search algorithms, we implement a stochastic resonance system that utilizes randomized Beta-density priors to vibrate the objective function. This process allows multiple change points to resonate as global minima across iterative simulations, generating a candidate point cloud. A double-sharpening procedure is then applied to these candidates to pinpoint precise change point locations. We establish the asymptotic consistency of the resonance estimator and demonstrate through simulations that REAMP outperforms state-of-the-art methods, especially in scenarios involving simultaneous mean and variance shifts. The practical utility of the method is further validated through an application to time-lapse embryo monitoring, where REAMP provides both accurate detection and intuitive visualization of cell division stages.",
      "authors": [
        "Xiaoping Shi",
        "Baisuo Jin",
        "Xianhui Liu",
        "Qiong Li"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-01-12T23:57:21+00:00",
      "link": "https://arxiv.org/pdf/2601.08084v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.02486v1",
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "abstract": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
      "authors": [
        "Jialiang Zhu",
        "Gongrui Zhang",
        "Xiaolong Ma",
        "Lin Xu",
        "Miaosen Zhang",
        "Ruiqi Yang",
        "Song Wang",
        "Kai Qiu",
        "Zhirong Wu",
        "Qi Dai",
        "Ruichun Ma",
        "Bei Liu",
        "Yifan Yang",
        "Chong Luo",
        "Zhengyuan Yang",
        "Linjie Li",
        "Lijuan Wang",
        "Weizhu Chen",
        "Xin Geng",
        "Baining Guo"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-02T18:58:07+00:00",
      "link": "https://arxiv.org/pdf/2602.02486v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.12667v1",
      "title": "Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration",
      "abstract": "It is foreseeable that the number of spacecraft will increase exponentially, ushering in an era dominated by satellite mega-constellations (SMC). This necessitates a focus on energy in space: spacecraft power systems (SPS), especially their health management (HM), given their role in power supply and high failure rates. Providing health management for dozens of SPS and for thousands of SPS represents two fundamentally different paradigms. Therefore, to adapt the health management in the SMC era, this work proposes a principle of aligning underlying capabilities (AUC principle) and develops SpaceHMchat, an open-source Human-AI collaboration (HAIC) framework for all-in-loop health management (AIL HM). SpaceHMchat serves across the entire loop of work condition recognition, anomaly detection, fault localization, and maintenance decision making, achieving goals such as conversational task completion, adaptive human-in-the-loop learning, personnel structure optimization, knowledge sharing, efficiency enhancement, as well as transparent reasoning and improved interpretability. Meanwhile, to validate this exploration, a hardware-realistic fault injection experimental platform is established, and its simulation model is built and open-sourced, both fully replicating the real SPS. The corresponding experimental results demonstrate that SpaceHMchat achieves excellent performance across 23 quantitative metrics, such as 100% conclusion accuracy in logical reasoning of work condition recognition, over 99% success rate in anomaly detection tool invocation, over 90% precision in fault localization, and knowledge base search time under 3 minutes in maintenance decision-making. Another contribution of this work is the release of the first-ever AIL HM dataset of SPS. This dataset contains four sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and over 700,000 timestamps.",
      "authors": [
        "Yi Di",
        "Zhibin Zhao",
        "Fujin Wang",
        "Xue Liu",
        "Jiafeng Tang",
        "Jiaxin Ren",
        "Zhi Zhai",
        "Xuefeng Chen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-19T02:28:27+00:00",
      "link": "https://arxiv.org/pdf/2601.12667v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.21481v1",
      "title": "Compressed Sensing-Driven Near-Field Localization Exploiting Array of Subarrays",
      "abstract": "Near-field localization for ISAC requires large-aperture arrays, making fully-digital implementations prohibitively complex and costly. While sparse subarray architectures can reduce cost, they introduce severe estimation ambiguity from grating lobes. To address both issues, we propose SHARE (Sparse Hierarchical Angle-Range Estimation), a novel two-stage sparse recovery algorithm. SHARE operates in two stages. It first performs coarse, unambiguous angle estimation using individual subarrays to resolve the grating lobe ambiguity. It then leverages the full sparse aperture to perform a localized joint angle-range search. This hierarchical approach avoids an exhaustive and computationally intensive two-dimensional grid search while preserving the high resolution of the large aperture. Simulation results show that SHARE significantly outperforms conventional one-shot sparse recovery methods, such as Orthogonal Matching Pursuit (OMP), in both localization accuracy and robustness. Furthermore, we show that SHARE's overall localization accuracy is comparable to or even surpasses that of the fully-digital 2D-MUSIC algorithm, despite MUSIC having access to the complete, uncompressed data from every antenna element. SHARE therefore provides a practical path for high-resolution near-field ISAC systems.",
      "authors": [
        "Sai Pavan Deram",
        "Jacopo Pegoraro",
        "Javier Lorca Hernando",
        "Jesus O. Lacruz",
        "Joerg Widmer"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP",
        "cs.ET"
      ],
      "published": "2026-01-29T10:00:27+00:00",
      "link": "https://arxiv.org/pdf/2601.21481v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.22369v1",
      "title": "Learning Provably Correct Distributed Protocols Without Human Knowledge",
      "abstract": "Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods.",
      "authors": [
        "Yujie Hui",
        "Xiaoyi Lu",
        "Andrew Perrault",
        "Yang Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-01-29T22:24:07+00:00",
      "link": "https://arxiv.org/pdf/2601.22369v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.00667v1",
      "title": "zkCraft: Prompt-Guided LLM as a Zero-Shot Mutation Pattern Oracle for TCCT-Powered ZK Fuzzing",
      "abstract": "Zero-knowledge circuits enable privacy-preserving and scalable systems but are difficult to implement correctly due to the tight coupling between witness computation and circuit constraints. We present zkCraft, a practical framework that combines deterministic, R1CS-aware localization with proof-bearing search to detect semantic inconsistencies. zkCraft encodes candidate constraint edits into a single Row-Vortex polynomial and replaces repeated solver queries with a Violation IOP that certifies the existence of edits together with a succinct proof. Deterministic LLM-driven mutation templates bias exploration toward edge cases while preserving auditable algebraic verification. Evaluation on real Circom code shows that proof-bearing localization detects diverse under- and over-constrained faults with low false positives and reduces costly solver interaction. Our approach bridges formal verification and automated debugging, offering a scalable path for robust ZK circuit development.",
      "authors": [
        "Rong Fu",
        "Jia Yee Tan",
        "Wenxin Zhang",
        "Youjin Wang",
        "Ziyu Kong",
        "Zeli Su",
        "Zhaolu Kang",
        "Shuning Zhang",
        "Xianda Li",
        "Kun Liu",
        "Simon Fong"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "published": "2026-01-31T11:31:00+00:00",
      "link": "https://arxiv.org/pdf/2602.00667v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.23232v2",
      "title": "ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search",
      "abstract": "In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.",
      "authors": [
        "Tao Yu",
        "Haopeng Jin",
        "Hao Wang",
        "Shenghua Chai",
        "Yujia Yang",
        "Junhao Gong",
        "Jiaming Guo",
        "Minghui Zhang",
        "Xinlong Chen",
        "Zhenghao Zhang",
        "Yuxuan Zhou",
        "Yufei Xiong",
        "Shanbin Zhang",
        "Jiabing Yang",
        "Hongzhu Yi",
        "Xinming Wang",
        "Cheng Zhong",
        "Xiao Ma",
        "Zhang Zhang",
        "Yan Huang",
        "Liang Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-30T18:01:17+00:00",
      "link": "https://arxiv.org/pdf/2601.23232v2",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.08495v1",
      "title": "Radar Operating Metrics and Network Throughput for Integrated Sensing and Communications in Millimeter-wave Urban Environments",
      "abstract": "Millimeter wave integrated sensing and communication (ISAC) systems are being researched for next-generation intelligent transportation systems. Here, radar and communication functionalities share a common spectrum and hardware resources in a time-multiplexed manner. The objective of the radar is to first scan the angular search space and detect and localize mobile users/targets in the presence of discrete clutter scatterers. Subsequently, this information is used to direct highly directional beams toward these mobile users for communication service. The choice of radar parameters such as the radar duty cycle and the corresponding beamwidth are critical for realizing high communication throughput. In this work, we use the stochastic geometry-based mathematical framework to analyze the radar operating metrics as a function of diverse radar, target, and clutter parameters and subsequently use these results to study the network throughput of the ISAC system. The results are validated through Monte Carlo simulations.",
      "authors": [
        "Akanksha Sneh",
        "Shobha Sundar Ram"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-02-09T10:46:45+00:00",
      "link": "https://arxiv.org/pdf/2602.08495v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.05371v1",
      "title": "Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting",
      "abstract": "Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries, but learning high-quality oblique splits is NP-hard, and practical methods still rely on slow search or theory-free heuristics. We present the Hinge Regression Tree (HRT), which reframes each split as a non-linear least-squares problem over two linear predictors whose max/min envelope induces ReLU-like expressive power. The resulting alternating fitting procedure is exactly equivalent to a damped Newton (Gauss-Newton) method within fixed partitions. We analyze this node-level optimization and, for a backtracking line-search variant, prove that the local objective decreases monotonically and converges; in practice, both fixed and adaptive damping yield fast, stable convergence and can be combined with optional ridge regularization. We further prove that HRT's model class is a universal approximator with an explicit $O(δ^2)$ approximation rate, and show on synthetic and real-world benchmarks that it matches or outperforms single-tree baselines with more compact structures.",
      "authors": [
        "Hongyi Li",
        "Han Lin",
        "Jun Xu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T06:49:01+00:00",
      "link": "https://arxiv.org/pdf/2602.05371v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2602.02999v1",
      "title": "ResQ: Realistic Performance-Aware Query Generation",
      "abstract": "Database research and development rely heavily on realistic user workloads for benchmarking, instance optimization, migration testing, and database tuning. However, acquiring real-world SQL queries is notoriously challenging due to strict privacy regulations. While cloud database vendors have begun releasing anonymized performance traces to the research community, these traces typi- cally provide only high-level execution statistics without the origi- nal query text or data, which is insufficient for scenarios that require actual execution. Existing tools fail to capture fine-grained perfor- mance patterns or generate runnable workloads that reproduce these public traces with both high fidelity and efficiency. To bridge this gap, we propose ResQ, a fine-grained workload synthesis sys- tem designed to generate executable SQL workloads that faithfully match the per-query execution targets and operator distributions of production traces. ResQ constructs execution-aware query graphs, instantiates them into SQL via Bayesian Optimization-driven pred- icate search, and explicitly models workload repetition through reuse at both exact-query and parameterized-template levels. To ensure practical scalability, ResQ combines search-space bounding with lightweight local cost models to accelerate optimization. Ex- periments on public cloud traces (Snowset, Redset) and a newly released industrial trace (Bendset) demonstrate that ResQ signif- icantly outperforms state-of-the-art baselines, achieving 96.71% token savings and a 86.97% reduction in runtime, while lowering maximum Q-error by 14.8x on CPU time and 997.7x on scanned bytes, and closely matching operator composition.",
      "authors": [
        "Zhengle Wang",
        "Yanfei Zhang",
        "Chunwei Liu"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-03T02:15:40+00:00",
      "link": "https://arxiv.org/pdf/2602.02999v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    },
    {
      "id": "2601.07465v1",
      "title": "Exoplanet transit search at the detection limit: detection and false alarm vetting pipeline",
      "abstract": "One of the primary mission goals of the Kepler space telescope was to detect Earth-like terrestrial planets in the habitable zone around Sun-like stars. These planets are at the detection limit, where the Kepler detection and vetting pipeline produced unreliable planet candidates. We present a novel pipeline that improves the removal of localized defects prior to the planet search, improves vetting at the level of individual transits and introduces a Bayes factor test statistic and an algorithm for extracting multiple candidates from a single detection run. We show with injections in the Kepler data that the introduced novelties improve pipeline's completeness at a fixed false alarm rate. We apply the pipeline to the stars with previously identified planet candidates and show that our pipeline successfully recovers the previously confirmed candidates, but flags a considerable portion of unconfirmed candidates as likely false alarms, especially in the long period, low signal-to-noise ratio regime. In particular, several known Earth-like candidates in the habitable zone, such as KOI 8063.01, 8107.01 and 8242.01, are identified as false alarms, which could have a significant impact on the estimates of $η_{\\oplus}$, i.e., the occurrence of Earth-like planets in the habitable zone.",
      "authors": [
        "Jakob Robnik",
        "Uroš Seljak",
        "Jon M. Jenkins",
        "Steve Bryson"
      ],
      "primary_category": "astro-ph.EP",
      "categories": [
        "astro-ph.EP"
      ],
      "published": "2026-01-12T12:21:18+00:00",
      "link": "https://arxiv.org/pdf/2601.07465v1",
      "tags": [
        "keyword:SR-LNS"
      ]
    }
  ],
  "queries": [
    {
      "type": "keyword",
      "tag": "SR",
      "paper_tag": "keyword:SR",
      "query_text": "Symbolic Regression",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2601.14693v1": {
          "score": 0.730669,
          "rank": 1
        },
        "2602.08270v1": {
          "score": 0.724617,
          "rank": 2
        },
        "2602.08885v3": {
          "score": 0.663462,
          "rank": 3
        },
        "2602.15169v1": {
          "score": 0.645186,
          "rank": 4
        },
        "2601.20637v1": {
          "score": 0.644797,
          "rank": 5
        },
        "2602.13021v2": {
          "score": 0.597039,
          "rank": 6
        },
        "2602.03506v1": {
          "score": 0.590301,
          "rank": 7
        },
        "2602.00031v1": {
          "score": 0.584878,
          "rank": 8
        },
        "2601.07727v1": {
          "score": 0.584766,
          "rank": 9
        },
        "2602.17082v1": {
          "score": 0.584233,
          "rank": 10
        },
        "2602.01510v1": {
          "score": 0.5,
          "rank": 11
        },
        "2601.22328v1": {
          "score": 0.449541,
          "rank": 12
        },
        "2601.21789v1": {
          "score": 0.449153,
          "rank": 13
        },
        "2602.02311v1": {
          "score": 0.445205,
          "rank": 14
        },
        "2601.19477v1": {
          "score": 0.4451,
          "rank": 15
        },
        "2601.16852v1": {
          "score": 0.310345,
          "rank": 16
        },
        "2602.12259v1": {
          "score": 0.293785,
          "rank": 17
        },
        "2602.08733v1": {
          "score": 0.287749,
          "rank": 18
        },
        "2602.07834v1": {
          "score": 0.28769,
          "rank": 19
        },
        "2602.12870v1": {
          "score": 0.285714,
          "rank": 20
        },
        "2601.14288v1": {
          "score": 0.285714,
          "rank": 21
        },
        "2602.16166v1": {
          "score": 0.285714,
          "rank": 22
        },
        "2602.01149v1": {
          "score": 0.285714,
          "rank": 23
        },
        "2602.02886v1": {
          "score": 0.285714,
          "rank": 24
        },
        "2602.10576v1": {
          "score": 0.285714,
          "rank": 25
        },
        "2602.12109v1": {
          "score": 0.285714,
          "rank": 26
        },
        "2602.07651v1": {
          "score": 0.285714,
          "rank": 27
        },
        "2601.05894v1": {
          "score": 0.285714,
          "rank": 28
        },
        "2601.10379v1": {
          "score": 0.174528,
          "rank": 29
        },
        "2601.12979v2": {
          "score": 0.00379507,
          "rank": 30
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR",
      "paper_tag": "keyword:SR",
      "query_text": "Genetic Programming",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2602.07310v1": {
          "score": 0.590028,
          "rank": 1
        },
        "2601.14485v1": {
          "score": 0.585799,
          "rank": 2
        },
        "2602.15070v1": {
          "score": 0.584927,
          "rank": 3
        },
        "2601.15717v1": {
          "score": 0.478168,
          "rank": 4
        },
        "2602.07659v1": {
          "score": 0.40678,
          "rank": 5
        },
        "2602.01510v1": {
          "score": 0.375951,
          "rank": 6
        },
        "2602.09772v1": {
          "score": 0.289474,
          "rank": 7
        },
        "2601.15738v1": {
          "score": 0.285714,
          "rank": 8
        },
        "2602.08885v3": {
          "score": 0.285714,
          "rank": 9
        },
        "2602.10891v1": {
          "score": 0.285714,
          "rank": 10
        },
        "2602.03840v1": {
          "score": 0.285714,
          "rank": 11
        },
        "2602.04529v1": {
          "score": 0.285714,
          "rank": 12
        },
        "2602.00843v1": {
          "score": 0.285714,
          "rank": 13
        },
        "2602.00755v1": {
          "score": 0.285714,
          "rank": 14
        },
        "2602.17082v1": {
          "score": 0.285714,
          "rank": 15
        },
        "2601.08657v1": {
          "score": 0.285714,
          "rank": 16
        },
        "2602.13864v1": {
          "score": 0.285714,
          "rank": 17
        },
        "2601.10740v1": {
          "score": 0.285714,
          "rank": 18
        },
        "2602.13410v1": {
          "score": 0.117647,
          "rank": 19
        },
        "2601.14480v1": {
          "score": 0.0298507,
          "rank": 20
        },
        "2602.04901v1": {
          "score": 0.0163934,
          "rank": 21
        },
        "2601.12274v1": {
          "score": 0.00943396,
          "rank": 22
        },
        "2602.11487v1": {
          "score": 0.00684932,
          "rank": 23
        },
        "2601.08884v1": {
          "score": 0.00651466,
          "rank": 24
        },
        "2601.06820v1": {
          "score": 0.00457666,
          "rank": 25
        },
        "2602.01209v1": {
          "score": 0.00414938,
          "rank": 26
        },
        "2601.20693v1": {
          "score": 0.00372439,
          "rank": 27
        },
        "2602.00429v1": {
          "score": 0.00335008,
          "rank": 28
        },
        "2601.13407v2": {
          "score": 0.00265957,
          "rank": 29
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR",
      "paper_tag": "keyword:SR",
      "query_text": "Reinforcement Learning",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2602.02710v1": {
          "score": 0.784784,
          "rank": 1
        },
        "2602.13949v1": {
          "score": 0.767112,
          "rank": 2
        },
        "2601.07948v1": {
          "score": 0.746171,
          "rank": 3
        },
        "2602.16543v1": {
          "score": 0.745917,
          "rank": 4
        },
        "2602.07719v1": {
          "score": 0.739079,
          "rank": 5
        },
        "2601.18419v1": {
          "score": 0.738642,
          "rank": 6
        },
        "2601.18953v1": {
          "score": 0.730083,
          "rank": 7
        },
        "2602.16475v1": {
          "score": 0.72973,
          "rank": 8
        },
        "2601.07821v1": {
          "score": 0.728482,
          "rank": 9
        },
        "2602.06960v2": {
          "score": 0.726441,
          "rank": 10
        },
        "2602.01260v1": {
          "score": 0.717198,
          "rank": 11
        },
        "2602.12099v1": {
          "score": 0.714282,
          "rank": 12
        },
        "2602.08244v1": {
          "score": 0.708743,
          "rank": 13
        },
        "2601.12886v1": {
          "score": 0.705323,
          "rank": 14
        },
        "2601.23058v1": {
          "score": 0.703798,
          "rank": 15
        },
        "2601.21912v1": {
          "score": 0.698134,
          "rank": 16
        },
        "2601.18533v1": {
          "score": 0.697391,
          "rank": 17
        },
        "2601.21312v1": {
          "score": 0.695845,
          "rank": 18
        },
        "2601.20802v1": {
          "score": 0.695225,
          "rank": 19
        },
        "2601.22149v1": {
          "score": 0.69231,
          "rank": 20
        },
        "2602.10894v1": {
          "score": 0.692035,
          "rank": 21
        },
        "2601.20688v1": {
          "score": 0.691931,
          "rank": 22
        },
        "2601.13284v1": {
          "score": 0.691482,
          "rank": 23
        },
        "2601.05578v1": {
          "score": 0.690668,
          "rank": 24
        },
        "2602.12146v1": {
          "score": 0.690629,
          "rank": 25
        },
        "2601.17275v1": {
          "score": 0.690057,
          "rank": 26
        },
        "2602.14697v1": {
          "score": 0.682619,
          "rank": 27
        },
        "2602.06603v2": {
          "score": 0.676525,
          "rank": 28
        },
        "2602.14338v1": {
          "score": 0.674419,
          "rank": 29
        },
        "2602.06440v1": {
          "score": 0.669959,
          "rank": 30
        },
        "2602.03048v2": {
          "score": 0.669291,
          "rank": 31
        },
        "2602.03048v3": {
          "score": 0.669291,
          "rank": 32
        },
        "2602.00400v1": {
          "score": 0.666463,
          "rank": 33
        },
        "2602.14468v1": {
          "score": 0.666349,
          "rank": 34
        },
        "2602.11455v1": {
          "score": 0.665684,
          "rank": 35
        },
        "2602.04879v1": {
          "score": 0.663462,
          "rank": 36
        },
        "2602.17038v1": {
          "score": 0.663462,
          "rank": 37
        },
        "2602.02192v3": {
          "score": 0.663462,
          "rank": 38
        },
        "2602.02192v2": {
          "score": 0.663462,
          "rank": 39
        },
        "2601.18150v1": {
          "score": 0.663462,
          "rank": 40
        },
        "2602.03352v1": {
          "score": 0.663462,
          "rank": 41
        },
        "2602.00759v1": {
          "score": 0.663462,
          "rank": 42
        },
        "2602.06107v1": {
          "score": 0.663462,
          "rank": 43
        },
        "2602.15245v1": {
          "score": 0.663462,
          "rank": 44
        },
        "2602.10019v1": {
          "score": 0.663462,
          "rank": 45
        },
        "2602.09207v1": {
          "score": 0.663462,
          "rank": 46
        },
        "2602.01388v2": {
          "score": 0.661265,
          "rank": 47
        },
        "2602.13953v1": {
          "score": 0.66108,
          "rank": 48
        },
        "2602.10539v1": {
          "score": 0.659367,
          "rank": 49
        },
        "2602.01156v1": {
          "score": 0.657319,
          "rank": 50
        }
      }
    },
    {
      "type": "intent_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "Symbolic regression methods and applications",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2602.08270v1": {
          "score": 0.0265938,
          "rank": 1
        },
        "2601.05894v1": {
          "score": 0.0101523,
          "rank": 2
        }
      }
    },
    {
      "type": "intent_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "Recent advances and state-of-the-art methods in symbolic regression",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {}
    },
    {
      "type": "intent_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "Deep learning for symbolic regression",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2601.05894v1": {
          "score": 0.0122324,
          "rank": 1
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR-RL",
      "paper_tag": "keyword:SR-RL",
      "query_text": "symbolic regression",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2601.14693v1": {
          "score": 0.730669,
          "rank": 1
        },
        "2602.08270v1": {
          "score": 0.724617,
          "rank": 2
        },
        "2602.08885v3": {
          "score": 0.663462,
          "rank": 3
        },
        "2602.15169v1": {
          "score": 0.645186,
          "rank": 4
        },
        "2601.20637v1": {
          "score": 0.644797,
          "rank": 5
        },
        "2602.13021v2": {
          "score": 0.597039,
          "rank": 6
        },
        "2602.03506v1": {
          "score": 0.590301,
          "rank": 7
        },
        "2602.00031v1": {
          "score": 0.584878,
          "rank": 8
        },
        "2601.07727v1": {
          "score": 0.584766,
          "rank": 9
        },
        "2602.17082v1": {
          "score": 0.584233,
          "rank": 10
        },
        "2602.01510v1": {
          "score": 0.5,
          "rank": 11
        },
        "2601.22328v1": {
          "score": 0.449541,
          "rank": 12
        },
        "2601.21789v1": {
          "score": 0.449153,
          "rank": 13
        },
        "2602.02311v1": {
          "score": 0.445205,
          "rank": 14
        },
        "2601.19477v1": {
          "score": 0.4451,
          "rank": 15
        },
        "2601.16852v1": {
          "score": 0.310345,
          "rank": 16
        },
        "2602.12259v1": {
          "score": 0.293785,
          "rank": 17
        },
        "2602.08733v1": {
          "score": 0.287749,
          "rank": 18
        },
        "2602.07834v1": {
          "score": 0.28769,
          "rank": 19
        },
        "2602.12870v1": {
          "score": 0.285714,
          "rank": 20
        },
        "2601.14288v1": {
          "score": 0.285714,
          "rank": 21
        },
        "2602.16166v1": {
          "score": 0.285714,
          "rank": 22
        },
        "2602.01149v1": {
          "score": 0.285714,
          "rank": 23
        },
        "2602.02886v1": {
          "score": 0.285714,
          "rank": 24
        },
        "2602.10576v1": {
          "score": 0.285714,
          "rank": 25
        },
        "2602.12109v1": {
          "score": 0.285714,
          "rank": 26
        },
        "2602.07651v1": {
          "score": 0.285714,
          "rank": 27
        },
        "2601.05894v1": {
          "score": 0.285714,
          "rank": 28
        },
        "2601.10379v1": {
          "score": 0.174528,
          "rank": 29
        },
        "2601.12979v2": {
          "score": 0.00379507,
          "rank": 30
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR-RL",
      "paper_tag": "keyword:SR-RL",
      "query_text": "reinforcement learning",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {}
    },
    {
      "type": "keyword",
      "tag": "SR-RL",
      "paper_tag": "keyword:SR-RL",
      "query_text": "genetic programming",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {}
    },
    {
      "type": "keyword",
      "tag": "SR-RL",
      "paper_tag": "keyword:SR-RL",
      "query_text": "Transformer",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2602.08267v1": {
          "score": 0.821429,
          "rank": 1
        },
        "2602.08857v1": {
          "score": 0.807692,
          "rank": 2
        },
        "2602.06300v1": {
          "score": 0.791667,
          "rank": 3
        },
        "2602.08695v1": {
          "score": 0.791667,
          "rank": 4
        },
        "2602.12681v1": {
          "score": 0.791667,
          "rank": 5
        },
        "2601.16450v1": {
          "score": 0.791667,
          "rank": 6
        },
        "2601.10519v1": {
          "score": 0.782609,
          "rank": 7
        },
        "2602.07070v1": {
          "score": 0.782609,
          "rank": 8
        },
        "2602.14318v1": {
          "score": 0.782609,
          "rank": 9
        },
        "2601.13224v1": {
          "score": 0.772727,
          "rank": 10
        },
        "2602.14803v3": {
          "score": 0.772727,
          "rank": 11
        },
        "2601.10876v1": {
          "score": 0.772727,
          "rank": 12
        },
        "2601.05618v1": {
          "score": 0.772727,
          "rank": 13
        },
        "2601.18274v1": {
          "score": 0.772727,
          "rank": 14
        },
        "2602.16608v1": {
          "score": 0.772727,
          "rank": 15
        },
        "2602.16914v1": {
          "score": 0.772727,
          "rank": 16
        },
        "2601.07930v1": {
          "score": 0.772727,
          "rank": 17
        },
        "2601.12571v1": {
          "score": 0.772727,
          "rank": 18
        },
        "2602.08920v1": {
          "score": 0.772727,
          "rank": 19
        },
        "2601.10434v1": {
          "score": 0.772727,
          "rank": 20
        },
        "2602.16264v1": {
          "score": 0.761905,
          "rank": 21
        },
        "2601.15509v1": {
          "score": 0.761905,
          "rank": 22
        },
        "2601.09467v1": {
          "score": 0.761905,
          "rank": 23
        },
        "2602.07677v1": {
          "score": 0.75,
          "rank": 24
        },
        "2601.21069v2": {
          "score": 0.75,
          "rank": 25
        },
        "2602.00856v1": {
          "score": 0.75,
          "rank": 26
        },
        "2601.14875v1": {
          "score": 0.75,
          "rank": 27
        },
        "2602.06597v1": {
          "score": 0.75,
          "rank": 28
        },
        "2602.13188v1": {
          "score": 0.75,
          "rank": 29
        },
        "2602.17307v1": {
          "score": 0.75,
          "rank": 30
        },
        "2601.05770v1": {
          "score": 0.75,
          "rank": 31
        },
        "2601.18385v1": {
          "score": 0.75,
          "rank": 32
        },
        "2602.12515v1": {
          "score": 0.75,
          "rank": 33
        },
        "2601.21942v1": {
          "score": 0.75,
          "rank": 34
        },
        "2601.20854v1": {
          "score": 0.75,
          "rank": 35
        },
        "2602.05896v1": {
          "score": 0.75,
          "rank": 36
        },
        "2602.05523v1": {
          "score": 0.75,
          "rank": 37
        },
        "2601.22081v1": {
          "score": 0.75,
          "rank": 38
        },
        "2601.17724v1": {
          "score": 0.75,
          "rank": 39
        },
        "2601.20796v1": {
          "score": 0.75,
          "rank": 40
        },
        "2602.06246v1": {
          "score": 0.75,
          "rank": 41
        },
        "2602.11145v1": {
          "score": 0.75,
          "rank": 42
        },
        "2602.13067v1": {
          "score": 0.75,
          "rank": 43
        },
        "2601.20116v1": {
          "score": 0.736842,
          "rank": 44
        },
        "2601.14522v1": {
          "score": 0.722222,
          "rank": 45
        },
        "2601.06365v1": {
          "score": 0.722222,
          "rank": 46
        },
        "2602.12480v1": {
          "score": 0.722222,
          "rank": 47
        },
        "2601.15348v1": {
          "score": 0.722222,
          "rank": 48
        },
        "2601.11237v1": {
          "score": 0.722222,
          "rank": 49
        },
        "2601.15158v3": {
          "score": 0.722222,
          "rank": 50
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR-RL",
      "paper_tag": "keyword:SR-RL",
      "query_text": "equation discovery",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2602.12259v1": {
          "score": 0.590336,
          "rank": 1
        },
        "2602.13513v2": {
          "score": 0.589443,
          "rank": 2
        },
        "2602.13021v2": {
          "score": 0.521168,
          "rank": 3
        },
        "2602.10576v1": {
          "score": 0.510402,
          "rank": 4
        },
        "2602.07970v1": {
          "score": 0.285714,
          "rank": 5
        },
        "2601.22328v1": {
          "score": 0.198473,
          "rank": 6
        },
        "2602.04114v1": {
          "score": 0.170785,
          "rank": 7
        },
        "2601.20637v1": {
          "score": 0.0783199,
          "rank": 8
        },
        "2601.14779v2": {
          "score": 0.0625,
          "rank": 9
        },
        "2602.04907v1": {
          "score": 0.0450584,
          "rank": 10
        },
        "2601.05632v1": {
          "score": 0.0278592,
          "rank": 11
        },
        "2602.04498v1": {
          "score": 0.0278452,
          "rank": 12
        },
        "2602.04498v2": {
          "score": 0.0277778,
          "rank": 13
        },
        "2602.07733v1": {
          "score": 0.0277778,
          "rank": 14
        },
        "2601.19223v1": {
          "score": 0.0264901,
          "rank": 15
        },
        "2602.11849v1": {
          "score": 0.022935,
          "rank": 16
        },
        "2602.10632v1": {
          "score": 0.0151515,
          "rank": 17
        },
        "2601.19838v1": {
          "score": 0.0101523,
          "rank": 18
        },
        "2601.14961v1": {
          "score": 0.0101523,
          "rank": 19
        },
        "2601.19091v1": {
          "score": 0.00951985,
          "rank": 20
        },
        "2601.10038v1": {
          "score": 0.00921659,
          "rank": 21
        },
        "2602.09093v1": {
          "score": 0.00921659,
          "rank": 22
        },
        "2601.12320v1": {
          "score": 0.00862069,
          "rank": 23
        },
        "2602.07939v1": {
          "score": 0.00809717,
          "rank": 24
        },
        "2602.08269v1": {
          "score": 0.00641026,
          "rank": 25
        },
        "2602.17493v1": {
          "score": 0.0052356,
          "rank": 26
        },
        "2602.04806v2": {
          "score": 0.00473934,
          "rank": 27
        },
        "2602.04806v1": {
          "score": 0.00473934,
          "rank": 28
        },
        "2601.16852v1": {
          "score": 0.00398406,
          "rank": 29
        },
        "2602.14105v1": {
          "score": 0.0034965,
          "rank": 30
        },
        "2602.16551v1": {
          "score": 0.00335008,
          "rank": 31
        }
      }
    },
    {
      "type": "intent_query",
      "tag": "SR-RL",
      "paper_tag": "query:SR-RL",
      "query_text": "state-of-the-art reinforcement learning methods for symbolic regression",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2601.14693v1": {
          "score": 0.00651353,
          "rank": 1
        }
      }
    },
    {
      "type": "intent_query",
      "tag": "SR-RL",
      "paper_tag": "query:SR-RL",
      "query_text": "how to use policy gradient to optimize mathematical expressions",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {}
    },
    {
      "type": "keyword",
      "tag": "SR-LNS",
      "paper_tag": "keyword:SR-LNS",
      "query_text": "symbolic regression",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2601.14693v1": {
          "score": 0.730669,
          "rank": 1
        },
        "2602.08270v1": {
          "score": 0.724617,
          "rank": 2
        },
        "2602.08885v3": {
          "score": 0.663462,
          "rank": 3
        },
        "2602.15169v1": {
          "score": 0.645186,
          "rank": 4
        },
        "2601.20637v1": {
          "score": 0.644797,
          "rank": 5
        },
        "2602.13021v2": {
          "score": 0.597039,
          "rank": 6
        },
        "2602.03506v1": {
          "score": 0.590301,
          "rank": 7
        },
        "2602.00031v1": {
          "score": 0.584878,
          "rank": 8
        },
        "2601.07727v1": {
          "score": 0.584766,
          "rank": 9
        },
        "2602.17082v1": {
          "score": 0.584233,
          "rank": 10
        },
        "2602.01510v1": {
          "score": 0.5,
          "rank": 11
        },
        "2601.22328v1": {
          "score": 0.449541,
          "rank": 12
        },
        "2601.21789v1": {
          "score": 0.449153,
          "rank": 13
        },
        "2602.02311v1": {
          "score": 0.445205,
          "rank": 14
        },
        "2601.19477v1": {
          "score": 0.4451,
          "rank": 15
        },
        "2601.16852v1": {
          "score": 0.310345,
          "rank": 16
        },
        "2602.12259v1": {
          "score": 0.293785,
          "rank": 17
        },
        "2602.08733v1": {
          "score": 0.287749,
          "rank": 18
        },
        "2602.07834v1": {
          "score": 0.28769,
          "rank": 19
        },
        "2602.12870v1": {
          "score": 0.285714,
          "rank": 20
        },
        "2601.14288v1": {
          "score": 0.285714,
          "rank": 21
        },
        "2602.16166v1": {
          "score": 0.285714,
          "rank": 22
        },
        "2602.01149v1": {
          "score": 0.285714,
          "rank": 23
        },
        "2602.02886v1": {
          "score": 0.285714,
          "rank": 24
        },
        "2602.10576v1": {
          "score": 0.285714,
          "rank": 25
        },
        "2602.12109v1": {
          "score": 0.285714,
          "rank": 26
        },
        "2602.07651v1": {
          "score": 0.285714,
          "rank": 27
        },
        "2601.05894v1": {
          "score": 0.285714,
          "rank": 28
        },
        "2601.10379v1": {
          "score": 0.174528,
          "rank": 29
        },
        "2601.12979v2": {
          "score": 0.00379507,
          "rank": 30
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR-LNS",
      "paper_tag": "keyword:SR-LNS",
      "query_text": "large neighborhood search",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2601.11414v1": {
          "score": 0.61039,
          "rank": 1
        },
        "2602.08253v1": {
          "score": 0.597086,
          "rank": 2
        },
        "2601.11010v1": {
          "score": 0.285714,
          "rank": 3
        },
        "2601.22052v2": {
          "score": 0.285714,
          "rank": 4
        },
        "2602.12055v1": {
          "score": 0.285714,
          "rank": 5
        },
        "2601.17899v2": {
          "score": 0.0526316,
          "rank": 6
        },
        "2602.05358v1": {
          "score": 0.0196078,
          "rank": 7
        },
        "2602.05358v2": {
          "score": 0.0196078,
          "rank": 8
        },
        "2602.08616v1": {
          "score": 0.00943396,
          "rank": 9
        },
        "2601.11047v1": {
          "score": 0.00881057,
          "rank": 10
        },
        "2601.17495v1": {
          "score": 0.00722022,
          "rank": 11
        },
        "2601.07948v1": {
          "score": 0.00684932,
          "rank": 12
        },
        "2601.13969v1": {
          "score": 0.00651466,
          "rank": 13
        },
        "2601.08621v1": {
          "score": 0.00218103,
          "rank": 14
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR-LNS",
      "paper_tag": "keyword:SR-LNS",
      "query_text": "ALNS",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2601.12151v1": {
          "score": 0.666667,
          "rank": 1
        },
        "2601.11414v1": {
          "score": 0.666667,
          "rank": 2
        },
        "2601.17214v1": {
          "score": 0.545455,
          "rank": 3
        },
        "2601.10495v1": {
          "score": 0.444444,
          "rank": 4
        },
        "2602.12956v1": {
          "score": 0.444444,
          "rank": 5
        },
        "2602.13827v1": {
          "score": 0.285714,
          "rank": 6
        },
        "2601.22052v2": {
          "score": 0.285714,
          "rank": 7
        },
        "2601.19201v1": {
          "score": 0.285714,
          "rank": 8
        },
        "2601.20691v1": {
          "score": 0.285714,
          "rank": 9
        },
        "2601.11010v1": {
          "score": 0.285714,
          "rank": 10
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR-LNS",
      "paper_tag": "keyword:SR-LNS",
      "query_text": "local search",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {
        "2602.05675v1": {
          "score": 0.783397,
          "rank": 1
        },
        "2602.01475v1": {
          "score": 0.726883,
          "rank": 2
        },
        "2601.06318v1": {
          "score": 0.704246,
          "rank": 3
        },
        "2601.07948v1": {
          "score": 0.647276,
          "rank": 4
        },
        "2601.11883v1": {
          "score": 0.646054,
          "rank": 5
        },
        "2601.16156v1": {
          "score": 0.619491,
          "rank": 6
        },
        "2601.14212v1": {
          "score": 0.602273,
          "rank": 7
        },
        "2601.11841v1": {
          "score": 0.584364,
          "rank": 8
        },
        "2602.04745v1": {
          "score": 0.584055,
          "rank": 9
        },
        "2601.12005v1": {
          "score": 0.527416,
          "rank": 10
        },
        "2601.13266v2": {
          "score": 0.51,
          "rank": 11
        },
        "2602.12465v1": {
          "score": 0.486799,
          "rank": 12
        },
        "2602.13046v1": {
          "score": 0.482759,
          "rank": 13
        },
        "2602.01209v1": {
          "score": 0.447853,
          "rank": 14
        },
        "2601.11144v3": {
          "score": 0.393675,
          "rank": 15
        },
        "2602.08952v1": {
          "score": 0.324324,
          "rank": 16
        },
        "2602.08253v1": {
          "score": 0.310077,
          "rank": 17
        },
        "2601.22075v1": {
          "score": 0.307692,
          "rank": 18
        },
        "2602.04248v1": {
          "score": 0.302326,
          "rank": 19
        },
        "2601.11389v1": {
          "score": 0.299065,
          "rank": 20
        },
        "2601.19041v1": {
          "score": 0.291339,
          "rank": 21
        },
        "2602.13532v1": {
          "score": 0.291339,
          "rank": 22
        },
        "2601.12040v1": {
          "score": 0.289252,
          "rank": 23
        },
        "2601.18005v1": {
          "score": 0.288747,
          "rank": 24
        },
        "2602.09424v1": {
          "score": 0.285714,
          "rank": 25
        },
        "2602.08473v1": {
          "score": 0.285714,
          "rank": 26
        },
        "2602.13494v1": {
          "score": 0.285714,
          "rank": 27
        },
        "2602.16473v1": {
          "score": 0.285714,
          "rank": 28
        },
        "2602.10159v1": {
          "score": 0.273636,
          "rank": 29
        },
        "2602.14162v1": {
          "score": 0.211252,
          "rank": 30
        },
        "2602.14985v1": {
          "score": 0.1875,
          "rank": 31
        },
        "2602.08501v1": {
          "score": 0.187307,
          "rank": 32
        },
        "2602.06566v2": {
          "score": 0.171053,
          "rank": 33
        },
        "2601.07621v1": {
          "score": 0.125,
          "rank": 34
        },
        "2601.23023v1": {
          "score": 0.123596,
          "rank": 35
        },
        "2602.02623v1": {
          "score": 0.12,
          "rank": 36
        },
        "2602.15921v1": {
          "score": 0.117647,
          "rank": 37
        },
        "2602.06269v1": {
          "score": 0.117647,
          "rank": 38
        },
        "2602.16819v1": {
          "score": 0.117647,
          "rank": 39
        },
        "2601.08084v1": {
          "score": 0.0909091,
          "rank": 40
        },
        "2602.02486v1": {
          "score": 0.0909091,
          "rank": 41
        },
        "2601.12667v1": {
          "score": 0.0909091,
          "rank": 42
        },
        "2601.21481v1": {
          "score": 0.0846682,
          "rank": 43
        },
        "2601.22369v1": {
          "score": 0.0837696,
          "rank": 44
        },
        "2602.00667v1": {
          "score": 0.0794857,
          "rank": 45
        },
        "2601.23232v2": {
          "score": 0.0768241,
          "rank": 46
        },
        "2602.08495v1": {
          "score": 0.0740741,
          "rank": 47
        },
        "2602.05371v1": {
          "score": 0.0740741,
          "rank": 48
        },
        "2602.02999v1": {
          "score": 0.0740741,
          "rank": 49
        },
        "2601.07465v1": {
          "score": 0.0699365,
          "rank": 50
        }
      }
    },
    {
      "type": "intent_query",
      "tag": "SR-LNS",
      "paper_tag": "query:SR-LNS",
      "query_text": "adaptive large neighborhood search for mathematical expression discovery",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {}
    },
    {
      "type": "intent_query",
      "tag": "SR-LNS",
      "paper_tag": "query:SR-LNS",
      "query_text": "hybrid genetic programming and large neighborhood search for regression",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {}
    },
    {
      "type": "intent_query",
      "tag": "SR-LNS",
      "paper_tag": "query:SR-LNS",
      "query_text": "state-of-the-art symbolic regression using metaheuristic search",
      "logic_cn": "",
      "boolean_expr": "",
      "bm25_mode": "supabase",
      "sim_scores": {}
    }
  ]
}