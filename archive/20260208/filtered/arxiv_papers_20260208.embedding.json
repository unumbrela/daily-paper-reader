{
  "top_k": 100,
  "generated_at": "2026-02-08T14:59:22.163296+00:00",
  "papers": [
    {
      "id": "2602.06043v1",
      "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
      "abstract": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.",
      "authors": [
        "Prakhar Kaushik",
        "Ankit Vaidya",
        "Shravan Chaudhari",
        "Rama Chellappa",
        "Alan Yuille"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-05T18:59:58+00:00",
      "link": "https://arxiv.org/pdf/2602.06043v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.06042v1",
      "title": "Pseudo-Invertible Neural Networks",
      "abstract": "The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or \"Back-Projection\", $x' = x + A^\\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, \"degradation\" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.",
      "authors": [
        "Yamit Ehrlich",
        "Nimrod Berman",
        "Assaf Shocher"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2026-02-05T18:59:58+00:00",
      "link": "https://arxiv.org/pdf/2602.06042v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.06039v1",
      "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
      "abstract": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.",
      "authors": [
        "Yuxing Lu",
        "Yucheng Hu",
        "Xukai Zhao",
        "Jiuxin Cao"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T18:59:51+00:00",
      "link": "https://arxiv.org/pdf/2602.06039v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.06040v1",
      "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as \"visual thoughts\" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.",
      "authors": [
        "Jintao Tong",
        "Shilin Yan",
        "Hongwei Xue",
        "Xiaojun Tang",
        "Kunyu Shi",
        "Guannan Zhang",
        "Ruixuan Li",
        "Yixiong Zou"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T18:59:51+00:00",
      "link": "https://arxiv.org/pdf/2602.06040v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.06034v1",
      "title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval",
      "abstract": "Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.",
      "authors": [
        "Dongyang Chen",
        "Chaoyang Wang",
        "Dezhao SU",
        "Xi Xiao",
        "Zeyu Zhang",
        "Jing Xiong",
        "Qing Li",
        "Yuzhang Shang",
        "Shichao Ka"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T18:59:21+00:00",
      "link": "https://arxiv.org/pdf/2602.06034v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.06030v1",
      "title": "PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling",
      "abstract": "Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.",
      "authors": [
        "Kavana Venkatesh",
        "Yinhan He",
        "Jundong Li",
        "Jiaming Cui"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.LG"
      ],
      "published": "2026-02-05T18:59:01+00:00",
      "link": "https://arxiv.org/pdf/2602.06030v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.06031v1",
      "title": "AP-OOD: Attention Pooling for Out-of-Distribution Detection",
      "abstract": "Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.",
      "authors": [
        "Claus Hofmann",
        "Christian Huber",
        "Bernhard Lehner",
        "Daniel Klotz",
        "Sepp Hochreiter",
        "Werner Zellinger"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T18:59:01+00:00",
      "link": "https://arxiv.org/pdf/2602.06031v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.06029v1",
      "title": "Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference",
      "abstract": "Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.",
      "authors": [
        "Yingke Li",
        "Anjali Parashar",
        "Enlu Zhou",
        "Chuchu Fan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T18:58:32+00:00",
      "link": "https://arxiv.org/pdf/2602.06029v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.06025v1",
      "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
      "abstract": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
      "authors": [
        "Haozhen Zhang",
        "Haodong Yue",
        "Tao Feng",
        "Quanyu Long",
        "Jianzhu Bao",
        "Bowen Jin",
        "Weizhi Zhang",
        "Xiao Li",
        "Jiaxuan You",
        "Chengwei Qin",
        "Wenya Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-05T18:57:09+00:00",
      "link": "https://arxiv.org/pdf/2602.06025v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.06019v1",
      "title": "Multi-Token Prediction via Self-Distillation",
      "abstract": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\\times$ faster on average at $<5\\%$ drop in accuracy relative to single token decoding performance.",
      "authors": [
        "John Kirchenbauer",
        "Abhimanyu Hans",
        "Brian Bartoldson",
        "Micah Goldblum",
        "Ashwinee Panda",
        "Tom Goldstein"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-05T18:54:48+00:00",
      "link": "https://arxiv.org/pdf/2602.06019v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.06014v1",
      "title": "Optimism Stabilizes Thompson Sampling for Adaptive Inference",
      "abstract": "Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \\emph{optimism} as a key mechanism for restoring \\emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \\citep{halder2025stable} is stable for any $K \\ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \\citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.",
      "authors": [
        "Shunxing Yan",
        "Han Zhong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "math.ST",
        "stat.ML"
      ],
      "published": "2026-02-05T18:52:54+00:00",
      "link": "https://arxiv.org/pdf/2602.06014v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.06005v1",
      "title": "Supply vs. Demand in Community-Based Fact-Checking on Social Media",
      "abstract": "Fact-checking ecosystems on social media depend on the interplay between what users want checked and what contributors are willing to supply. Prior research has largely examined these forces in isolation, yet it remains unclear to what extent supply meets demand. We address this gap with an empirical analysis of a unique dataset of 1.1 million fact-checks and fact-checking requests from X's Community Notes platform between June 2024 and May 2025. We find that requests disproportionately target highly visible posts - those with more views and engagement and authored by influential accounts - whereas fact-checks are distributed more broadly across languages, sentiments, and topics. Using a quasi-experimental survival analysis, we further estimate the effect of displaying requests on subsequent note creation. Results show that requests significantly accelerate contributions from Top Writers. Altogether, our findings highlight a gap between the content that attracts requests for fact-checking and the content that ultimately receives fact-checks, while showing that user requests can steer contributors toward greater alignment. These insights carry important implications for platform governance and future research on online misinformation.",
      "authors": [
        "Moritz Pilarski",
        "Nicolas Pröllochs"
      ],
      "primary_category": "cs.SI",
      "categories": [
        "cs.SI"
      ],
      "published": "2026-02-05T18:49:14+00:00",
      "link": "https://arxiv.org/pdf/2602.06005v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.06000v1",
      "title": "Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods",
      "abstract": "Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.",
      "authors": [
        "Ali Shendabadi",
        "Parnia Izadirad",
        "Mostafa Salehi",
        "Mahmoud Bijankhan"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-05T18:46:28+00:00",
      "link": "https://arxiv.org/pdf/2602.06000v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05999v1",
      "title": "On Computation and Reinforcement Learning",
      "abstract": "How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.",
      "authors": [
        "Raj Ghugare",
        "Michał Bortkiewicz",
        "Alicja Ziarko",
        "Benjamin Eysenbach"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T18:45:57+00:00",
      "link": "https://arxiv.org/pdf/2602.05999v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05996v1",
      "title": "Orthogonal Self-Attention",
      "abstract": "Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.",
      "authors": [
        "Leo Zhang",
        "James Martens"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-05T18:42:57+00:00",
      "link": "https://arxiv.org/pdf/2602.05996v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05993v1",
      "title": "Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps",
      "abstract": "Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose \"Diamond Maps\", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.",
      "authors": [
        "Peter Holderrieth",
        "Douglas Chen",
        "Luca Eyring",
        "Ishin Shah",
        "Giri Anantharaman",
        "Yutong He",
        "Zeynep Akata",
        "Tommi Jaakkola",
        "Nicholas Matthew Boffi",
        "Max Simchowitz"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T18:42:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05993v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05988v1",
      "title": "Layer-wise LoRA fine-tuning: a similarity metric approach",
      "abstract": "Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA",
      "authors": [
        "Keith Ando Ogawa",
        "Bruno Lopes Yamamoto",
        "Lucas Lauton de Alcantara",
        "Lucas Pellicer",
        "Rosimeire Pereira Costa",
        "Edson Bollis",
        "Anna Helena Reali Costa",
        "Artur Jordao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T18:38:53+00:00",
      "link": "https://arxiv.org/pdf/2602.05988v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05977v1",
      "title": "Clifford Kolmogorov-Arnold Networks",
      "abstract": "We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.",
      "authors": [
        "Matthias Wolff",
        "Francesco Alesiani",
        "Christof Duhme",
        "Xiaoyi Jiang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T18:25:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05977v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05976v1",
      "title": "The Signed Wasserstein Barycenter Problem",
      "abstract": "Barycenter problems encode important geometric information about a metric space. While these problems are typically studied with positive weight coefficients associated to each distance term, more general signed Wasserstein barycenter problems have recently drawn a great deal of interest. These mixed sign problems have appeared in statistical inference setting as a way to generalize least squares regression to measure valued outputs and have appeared in numerical methods to improve the accuracy of Wasserstein gradient flow solvers. Unfortunately, the presence of negatively weighted distance terms destroys the Euclidean convexity of the unsigned problem, resulting in a much more challenging optimization task. The main focus of this work is to study properties of the signed barycenter problem for a general transport cost with a focus on establishing uniqueness of solutions. In particular, when there is only one positive weight, we extend the uniqueness result of Tornabene et al. (2025) to any cost satisfying a certain convexity property. In the case of arbitrary weights, we introduce the dual problem in terms of Kantorovich potentials and provide a sufficient condition for a stationary solution of the dual problem to induce an optimal signed barycenter.",
      "authors": [
        "Matt Jacobs",
        "Bohan Zhou"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "math.PR"
      ],
      "published": "2026-02-05T18:25:27+00:00",
      "link": "https://arxiv.org/pdf/2602.05976v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05975v1",
      "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
      "abstract": "Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.",
      "authors": [
        "Tiansheng Hu",
        "Yilun Zhao",
        "Canyu Zhang",
        "Arman Cohan",
        "Chen Zhao"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "published": "2026-02-05T18:25:24+00:00",
      "link": "https://arxiv.org/pdf/2602.05975v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05974v1",
      "title": "Normalization of ReLU Dual for Cut Generation in Stochastic Mixed-Integer Programs",
      "abstract": "We study the Rectified Linear Unit (ReLU) dual, an existing dual formulation for stochastic programs that reformulates non-anticipativity constraints using ReLU functions to generate tight, non-convex, and mixed-integer representable cuts. While this dual reformulation guarantees convergence with mixed-integer state variables, it admits multiple optimal solutions that can yield weak cuts. To address this issue, we propose normalizing the dual in the extended space to identify solutions that yield stronger cuts. We prove that the resulting normalized cuts are tight and Pareto-optimal in the original state space. We further compare normalization with existing regularization-based approaches for handling dual degeneracy and explain why normalization offers key advantages. In particular, we show that normalization can recover any cut obtained via regularization, whereas the converse does not hold. Computational experiments demonstrate that the proposed approach outperforms existing methods by consistently yielding stronger cuts and reducing solution times on harder instances.",
      "authors": [
        "Akul Bansal",
        "Simge Küçükyavuz"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "eess.SY"
      ],
      "published": "2026-02-05T18:25:08+00:00",
      "link": "https://arxiv.org/pdf/2602.05974v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05971v1",
      "title": "Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space",
      "abstract": "Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.",
      "authors": [
        "Felipe D. Toro-Hernández",
        "Jesuino Vieira Filho",
        "Rodrigo M. Cabral-Carvalho"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG",
        "q-bio.NC"
      ],
      "published": "2026-02-05T18:23:04+00:00",
      "link": "https://arxiv.org/pdf/2602.05971v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05951v1",
      "title": "Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching",
      "abstract": "Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.",
      "authors": [
        "Junwan Kim",
        "Jiho Park",
        "Seonghu Jeon",
        "Seungryong Kim"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-05T18:08:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05951v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05950v1",
      "title": "Breaking Symmetry Bottlenecks in GNN Readouts",
      "abstract": "Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.",
      "authors": [
        "Mouad Talhi",
        "Arne Wolf",
        "Anthea Monod"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T18:08:13+00:00",
      "link": "https://arxiv.org/pdf/2602.05950v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05945v1",
      "title": "AgenticTagger: Structured Item Representation for Recommendation with LLM Agents",
      "abstract": "High-quality representations are a core requirement for effective recommendation. In this work, we study the problem of LLM-based descriptor generation, i.e., keyphrase-like natural language item representation generation frameworks with minimal constraints on downstream applications. We propose AgenticTagger, a framework that queries LLMs for representing items with sequences of text descriptors. However, open-ended generation provides little control over the generation space, leading to high cardinality, low-performance descriptors that renders downstream modeling challenging. To this end, AgenticTagger features two core stages: (1) a vocabulary building stage where a set of hierarchical, low-cardinality, and high-quality descriptors is identified, and (2) a vocabulary assignment stage where LLMs assign in-vocabulary descriptors to items. To effectively and efficiently ground vocabulary in the item corpus of interest, we design a multi-agent reflection mechanism where an architect LLM iteratively refines the vocabulary guided by parallelized feedback from annotator LLMs that validates the vocabulary against item data. Experiments on public and private data show AgenticTagger brings consistent improvements across diverse recommendation scenarios, including generative and term-based retrieval, ranking, and controllability-oriented, critique-based recommendation.",
      "authors": [
        "Zhouhang Xie",
        "Bo Peng",
        "Zhankui He",
        "Ziqi Chen",
        "Alice Han",
        "Isabella Ye",
        "Benjamin Coleman",
        "Noveen Sachdeva",
        "Fernando Pereira",
        "Julian McAuley",
        "Wang-Cheng Kang",
        "Derek Zhiyuan Cheng",
        "Beidou Wang",
        "Randolph Brown"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-05T18:01:37+00:00",
      "link": "https://arxiv.org/pdf/2602.05945v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05944v1",
      "title": "\"Detective Work We Shouldn't Have to Do\": Practitioner Challenges in Regulatory-Aligned Data Quality in Machine Learning Systems",
      "abstract": "Ensuring data quality in machine learning (ML) systems has become increasingly complex as regulatory requirements expand. In the European Union (EU), frameworks such as the General Data Protection Regulation (GDPR) and the Artificial Intelligence Act (AI Act) articulate data quality requirements that closely parallel technical concerns in ML practice, while also extending to legal obligations related to accountability, risk management, and human rights protection. This paper presents a qualitative interview study with EU-based data practitioners working on ML systems in regulated contexts. Through semi-structured interviews, we investigate how practitioners interpret regulatory-aligned data quality, the challenges they encounter, and the supports they identify as necessary. Our findings reveal persistent gaps between legal principles and engineering workflows, fragmentation across data pipelines, limitations of existing tools, unclear responsibility boundaries between technical and legal teams, and a tendency toward reactive, audit-driven quality practices. We also identify practitioners' needs for compliance-aware tooling, clearer governance structures, and cultural shifts toward proactive data governance.",
      "authors": [
        "Yichun Wang",
        "Kristina Irion",
        "Paul Groth",
        "Hazar Harmouch"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-05T17:59:44+00:00",
      "link": "https://arxiv.org/pdf/2602.05944v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05940v1",
      "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
      "abstract": "Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.",
      "authors": [
        "Junxiao Liu",
        "Zhijun Wang",
        "Yixiao Li",
        "Zhejian Lai",
        "Liqian Huang",
        "Xin Huang",
        "Xue Han",
        "Junlan Feng",
        "Shujian Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T17:55:09+00:00",
      "link": "https://arxiv.org/pdf/2602.05940v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05936v1",
      "title": "Dimensionality Reduction on Riemannian Manifolds in Data Analysis",
      "abstract": "In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.",
      "authors": [
        "Alaa El Ichi",
        "Khalide Jbilou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T17:46:58+00:00",
      "link": "https://arxiv.org/pdf/2602.05936v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05932v1",
      "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions",
      "abstract": "Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.",
      "authors": [
        "Léo Labat",
        "Etienne Ollion",
        "François Yvon"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T17:44:06+00:00",
      "link": "https://arxiv.org/pdf/2602.05932v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05930v1",
      "title": "Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025",
      "abstract": "Large language models (LLMs) are increasingly used in academic writing workflows, yet they frequently hallucinate by generating citations to sources that do not exist. This study analyzes 100 AI-generated hallucinated citations that appeared in papers accepted by the 2025 Conference on Neural Information Processing Systems (NeurIPS), one of the world's most prestigious AI conferences. Despite review by 3-5 expert researchers per paper, these fabricated citations evaded detection, appearing in 53 published papers (approx. 1% of all accepted papers). We develop a five-category taxonomy that classifies hallucinations by their failure mode: Total Fabrication (66%), Partial Attribute Corruption (27%), Identifier Hijacking (4%), Placeholder Hallucination (2%), and Semantic Hallucination (1%). Our analysis reveals a critical finding: every hallucination (100%) exhibited compound failure modes. The distribution of secondary characteristics was dominated by Semantic Hallucination (63%) and Identifier Hijacking (29%), which often appeared alongside Total Fabrication to create a veneer of plausibility and false verifiability. These compound structures exploit multiple verification heuristics simultaneously, explaining why peer review fails to detect them. The distribution exhibits a bimodal pattern: 92% of contaminated papers contain 1-2 hallucinations (minimal AI use) while 8% contain 4-13 hallucinations (heavy reliance). These findings demonstrate that current peer review processes do not include effective citation verification and that the problem extends beyond NeurIPS to other major conferences, government reports, and professional consulting. We propose mandatory automated citation verification at submission as an implementable solution to prevent fabricated citations from becoming normalized in scientific literature.",
      "authors": [
        "Samar Ansari"
      ],
      "primary_category": "cs.DL",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "published": "2026-02-05T17:43:35+00:00",
      "link": "https://arxiv.org/pdf/2602.05930v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05929v1",
      "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
      "abstract": "Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.",
      "authors": [
        "Jian Chen",
        "Zhuoran Wang",
        "Jiayu Qin",
        "Ming Li",
        "Meng Wang",
        "Changyou Chen",
        "Yin Chen",
        "Qizhen Weng",
        "Yirui Liu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T17:41:57+00:00",
      "link": "https://arxiv.org/pdf/2602.05929v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05926v1",
      "title": "Poisson Log-Normal Process for Count Data Prediction",
      "abstract": "Modeling count data is important in physics and other scientific disciplines, where measurements often involve discrete, non-negative quantities such as photon or neutrino detection events. Traditional parametric approaches can be trained to generate integer-count predictions but may struggle with capturing complex, non-linear dependencies often observed in the data. Gaussian process (GP) regression provides a robust non-parametric alternative to modeling continuous data; however, it cannot generate integer outputs. We propose the Poisson Log-Normal (PoLoN) process, a framework that employs GP to model Poisson log-rates. As in GP regression, our approach relies on the correlations between data points captured via GP kernel structure rather than explicit functional parameterizations. We demonstrate that the PoLoN predictive distribution is Poisson-LogNormal and provide an algorithm for optimizing kernel hyperparameters. Furthermore, we adapt the PoLoN approach to the problem of detecting weak localized signals superimposed on a smoothly varying background - a task of considerable interest in many areas of science and engineering. Our framework allows us to predict the strength, location and width of the detected signals. We evaluate PoLoN's performance using both synthetic and real-world datasets, including the open dataset from CERN which was used to detect the Higgs boson at the Large Hadron Collider. Our results indicate that the PoLoN process can be used as a non-parametric alternative for analyzing, predicting, and extracting signals from integer-valued data.",
      "authors": [
        "Anushka Saha",
        "Abhijith Gandrakota",
        "Alexandre V. Morozov"
      ],
      "primary_category": "physics.data-an",
      "categories": [
        "physics.data-an"
      ],
      "published": "2026-02-05T17:37:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05926v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05902v1",
      "title": "Regularized Calibration with Successive Rounding for Post-Training Quantization",
      "abstract": "Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.",
      "authors": [
        "Seohyeon Cha",
        "Huancheng Chen",
        "Dongjun Kim",
        "Haoran Zhang",
        "Kevin Chan",
        "Gustavo de Veciana",
        "Haris Vikalo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T17:18:02+00:00",
      "link": "https://arxiv.org/pdf/2602.05902v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05893v1",
      "title": "Objective-Function Free Multi-Objective Optimization: Rate of Convergence and Performance of an Adagrad-like algorithm",
      "abstract": "We propose an Adagrad-like algorithm for multi-objective unconstrained optimization that relies on the computation of a common descent direction only. Unlike classical local algorithms for multi-objective optimization, our approach does not rely on the dominance property to accept new iterates, which allows for a flexible and function-free optimization framework. New points are obtained using an adaptive stepsize that does not require neither knowledge of Lipschitz constants nor the use of line search procedures. The rate of convergence is analyzed and is shown to be $\\mathcal{O}(1 / \\sqrt{ k+1})$ with respect to the norm of the common descent direction. The method is extensively validated on a broad class of unconstrained multi-objective problems and simple multi-task learning instances, and compared against a first-order line search algorithm. Additionally, we present a preliminary study of the behavior under noisy multi-objective settings, highlighting the robustness of the method.",
      "authors": [
        "Marianna De Santis",
        "Gabriele Eichfelder",
        "Margherita Porcelli"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-05T17:12:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05893v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05892v1",
      "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
      "abstract": "LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.",
      "authors": [
        "Han Li",
        "Letian Zhu",
        "Bohan Zhang",
        "Rili Feng",
        "Jiaming Wang",
        "Yue Pan",
        "Earl T. Barr",
        "Sarro Federica",
        "Zhaoyang Chu",
        "He Ye"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T17:10:26+00:00",
      "link": "https://arxiv.org/pdf/2602.05892v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05890v1",
      "title": "DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training",
      "abstract": "Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.",
      "authors": [
        "Dingwei Zhu",
        "Zhiheng Xi",
        "Shihan Dou",
        "Jiahan Li",
        "Chenhao Huang",
        "Junjie Ye",
        "Sixian Li",
        "Mingxu Chai",
        "Yuhui Wang",
        "Yajie Yang",
        "Ming Zhang",
        "Jiazheng Zhang",
        "Shichun Liu",
        "Caishuang Huang",
        "Yunke Zhang",
        "Yuran Wang",
        "Tao Gui",
        "Xipeng Qiu",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-05T17:07:42+00:00",
      "link": "https://arxiv.org/pdf/2602.05890v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05887v1",
      "title": "Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting",
      "abstract": "Low-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. Recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. Motivated by this observation, we propose a Simulated Oracle Direction (SOD) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. In essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. To the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. Numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. We believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.",
      "authors": [
        "Tianqi Shen",
        "Jinji Yang",
        "Junze He",
        "Kunhan Gao",
        "Ziye Ma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-05T17:05:02+00:00",
      "link": "https://arxiv.org/pdf/2602.05887v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05885v1",
      "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
      "abstract": "High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.",
      "authors": [
        "Wei Liu",
        "Jiawei Xu",
        "Yingru Li",
        "Longtao Zheng",
        "Tianjian Li",
        "Qian Liu",
        "Junxian He"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-05T17:01:09+00:00",
      "link": "https://arxiv.org/pdf/2602.05885v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05883v1",
      "title": "A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges",
      "abstract": "Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.",
      "authors": [
        "Philippe J. Giabbanelli"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T17:00:07+00:00",
      "link": "https://arxiv.org/pdf/2602.05883v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05881v1",
      "title": "Predicting galaxy bias using machine learning",
      "abstract": "Understanding how galaxies trace the underlying matter density field is essential for characterizing the influence of the large-scale structure on galaxy formation, being therefore a key ingredient in observational cosmology. This connection, commonly described through the galaxy bias, $b$, can be studied effectively using machine learning (ML) techniques, which offer strong predictive capabilities and can capture non-linear relationships. We aim to incorporate the linear bias parameter assigned to individual galaxies into a ML framework, quantify its dependence on various halo and environmental properties, and evaluate whether different algorithms can accurately predict this parameter and reproduce the scatter in several bias relations. We use data from the IllustrisTNG300 simulation, including the distance to different cosmic-web structures computed with DisPerSE. These data are complemented with an object-by-object estimator of the large-scale linear bias ($b_i$), providing the individual contribution of each galaxy to the bias of the entire population. Our ML framework uses three models to predict $b_i$: a Random Forest Regressor, a Neural Network and a probabilistic method (Normalizing Flows). We recover the full hierarchy of galaxy bias dependencies, showing that the most informative features are the overdensities, particularly $δ_8$, followed by the distances to cosmic-web structures and selected internal halo properties, most notably $z_{1/2}$. We also demonstrate that Normalizing Flows clearly outperform deterministic methods in predicting galaxy bias, including its joint distributions with galaxy properties, owing to their ability to capture the intrinsic variance associated with the stochastic nature of the matter-halo-galaxy connection. Our ML framework provides a foundation for future efforts to measure individual bias with upcoming spectroscopic surveys.",
      "authors": [
        "Catalina Riveros-Jara",
        "Antonio D. Montero-Dorta",
        "Natália V. N. Rodrigues",
        "Pía Amigo",
        "Natalí S. M. de Santi",
        "Andrés Balaguera-Antolínez",
        "Raul Abramo",
        "Neill Guzmán",
        "M. Celeste Artale"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO",
        "astro-ph.GA"
      ],
      "published": "2026-02-05T16:57:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05881v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05879v1",
      "title": "EuroLLM-22B: Technical Report",
      "abstract": "This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.",
      "authors": [
        "Miguel Moura Ramos",
        "Duarte M. Alves",
        "Hippolyte Gisserot-Boukhlef",
        "João Alves",
        "Pedro Henrique Martins",
        "Patrick Fernandes",
        "José Pombal",
        "Nuno M. Guerreiro",
        "Ricardo Rei",
        "Nicolas Boizard",
        "Amin Farajian",
        "Mateusz Klimaszewski",
        "José G. C. de Souza",
        "Barry Haddow",
        "François Yvon",
        "Pierre Colombo",
        "Alexandra Birch",
        "André F. T. Martins"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-05T16:53:47+00:00",
      "link": "https://arxiv.org/pdf/2602.05879v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05873v1",
      "title": "Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks",
      "abstract": "Bayesian (deep) neural networks (BNN) are often more attractive than the mainstream point-estimate vanilla deep learning in various aspects including uncertainty quantification, robustness to noise, resistance to overfitting, and more. The variational inference (VI) is one of the most widely adopted approximate inference methods. Whereas the ELBO-based variational free energy method is a dominant choice in the literature, in this paper we introduce a score-based alternative for BNN variational inference. Although there have been quite a few score-based variational inference methods proposed in the community, most are not adequate for large-scale BNNs for various computational and technical reasons. We propose a novel scalable VI method where the learning objective combines the score matching loss and the proximal penalty term in iterations, which helps our method avoid the reparametrized sampling, and allows for noisy unbiased mini-batch scores through stochastic gradients. This in turn makes our method scalable to large-scale neural networks including Vision Transformers, and allows for richer variational density families. On several benchmarks including visual recognition and time-series forecasting with large-scale deep networks, we empirically show the effectiveness of our approach.",
      "authors": [
        "Minyoung Kim"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T16:51:07+00:00",
      "link": "https://arxiv.org/pdf/2602.05873v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05869v1",
      "title": "Wedge Sampling: Efficient Tensor Completion with Nearly-Linear Sample Complexity",
      "abstract": "We introduce Wedge Sampling, a new non-adaptive sampling scheme for low-rank tensor completion. We study recovery of an order-$k$ low-rank tensor of dimension $n \\times \\cdots \\times n$ from a subset of its entries. Unlike the standard uniform entry model (i.e., i.i.d. samples from $[n]^k$), wedge sampling allocates observations to structured length-two patterns (wedges) in an associated bipartite sampling graph. By directly promoting these length-two connections, the sampling design strengthens the spectral signal that underlies efficient initialization, in regimes where uniform sampling is too sparse to generate enough informative correlations.   Our main result shows that this change in sampling paradigm enables polynomial-time algorithms to achieve both weak and exact recovery with nearly linear sample complexity in $n$. The approach is also plug-and-play: wedge-sampling-based spectral initialization can be combined with existing refinement procedures (e.g., spectral or gradient-based methods) using only an additional $\\tilde{O}(n)$ uniformly sampled entries, substantially improving over the $\\tilde{O}(n^{k/2})$ sample complexity typically required under uniform entry sampling for efficient methods. Overall, our results suggest that the statistical-to-computational gap highlighted in Barak and Moitra (2022) is, to a large extent, a consequence of the uniform entry sampling model for tensor completion, and that alternative non-adaptive measurement designs that guarantee a strong initialization can overcome this barrier.",
      "authors": [
        "Hengrui Luo",
        "Anna Ma",
        "Ludovic Stephan",
        "Yizhe Zhu"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.NA",
        "math.PR",
        "math.ST"
      ],
      "published": "2026-02-05T16:47:13+00:00",
      "link": "https://arxiv.org/pdf/2602.05869v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05867v1",
      "title": "The Case of the Mysterious Citations",
      "abstract": "Mysterious citations are routinely appearing in peer-reviewed publications throughout the scientific community. In this paper, we developed an automated pipeline and examine the proceedings of four major high-performance computing conferences, comparing the accuracy of citations between the 2021 and 2025 proceedings. While none of the 2021 papers contained mysterious citations, every 2025 proceeding did, impacting 2-6\\% of published papers. In addition, we observe a sharp rise in paper title and authorship errors, motivating the need for stronger citation-verification practice. No author within our dataset acknowledged using AI to generate citations even though all four conference policies required it, indicating current policies are insufficient.",
      "authors": [
        "Amanda Bienz",
        "Carl Pearson",
        "Simon Garcia de Gonzalo"
      ],
      "primary_category": "cs.DL",
      "categories": [
        "cs.DL"
      ],
      "published": "2026-02-05T16:46:27+00:00",
      "link": "https://arxiv.org/pdf/2602.05867v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05865v1",
      "title": "AMDAT: An Open-Source Molecular Dynamics Analysis Toolkit for Supercooled Liquids, Glass-Forming Materials, and Complex Fluids",
      "abstract": "AMDAT (Amorphous Molecular Dynamics Analysis Toolkit) is an open-source C++ toolkit for post-processing molecular dynamics trajectories, focused on high-performance static and dynamic analyses of amorphous, glassy, and polymer materials, including supercooled liquids and complex fluids. In this paper, we describe AMDAT's design for efficient long-timescale analysis via in-memory trajectory handling and exponential time sampling, and we demonstrate representative workflows for widely used observables such as radial distribution functions, structure factors, intermediate scattering functions, and neighbor correlations.",
      "authors": [
        "Pierre Kawak",
        "William F. Drayer",
        "David S. Simmons"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.soft",
        "cond-mat.stat-mech"
      ],
      "published": "2026-02-05T16:45:04+00:00",
      "link": "https://arxiv.org/pdf/2602.05865v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05863v1",
      "title": "Constrained Group Relative Policy Optimization",
      "abstract": "While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.",
      "authors": [
        "Roger Girgis",
        "Rodrigue de Schaetzen",
        "Luke Rowe",
        "Azalée Robitaille",
        "Christopher Pal",
        "Liam Paull"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.RO"
      ],
      "published": "2026-02-05T16:44:23+00:00",
      "link": "https://arxiv.org/pdf/2602.05863v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05859v1",
      "title": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders",
      "abstract": "Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.",
      "authors": [
        "Xu Wang",
        "Bingqing Jiang",
        "Yu Wan",
        "Baosong Yang",
        "Lingpeng Kong",
        "Difan Zou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-05T16:41:25+00:00",
      "link": "https://arxiv.org/pdf/2602.05859v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05857v1",
      "title": "BABE: Biology Arena BEnchmark",
      "abstract": "The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.",
      "authors": [
        "Junting Zhou",
        "Jin Chen",
        "Linfeng Hao",
        "Denghui Cao",
        "Zheyu Wang",
        "Qiguang Chen",
        "Chaoyou Fu",
        "Jiaze Chen",
        "Yuchen Wu",
        "Ge Zhang",
        "Mingxuan Wang",
        "Wenhao Huang",
        "Tong Yang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T16:39:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05857v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05856v1",
      "title": "\"It Talks Like a Patient, But Feels Different\": Co-Designing AI Standardized Patients with Medical Learners",
      "abstract": "Standardized patients (SPs) play a central role in clinical communication training but are costly, difficult to scale, and inconsistent. Large language model (LLM) based AI standardized patients (AI-SPs) promise flexible, on-demand practice, yet learners often report that they talk like a patient but feel different. We interviewed 12 clinical-year medical students and conducted three co-design workshops to examine how learners experience constraints of SP encounters and what they expect from AI-SPs. We identified six learner-centered needs, translated them into AI-SP design requirements, and synthesized a conceptual workflow. Our findings position AI-SPs as tools for deliberate practice and show that instructional usability, rather than conversational realism alone, drives learner trust, engagement, and educational value.",
      "authors": [
        "Zhiqi Gao",
        "Guo Zhu",
        "Huarui Luo",
        "Dongyijie Primo Pan",
        "Haoming Tang",
        "Bingquan Zhang",
        "Jiahuan Pei",
        "Jie Li",
        "Benyou Wang"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-05T16:38:51+00:00",
      "link": "https://arxiv.org/pdf/2602.05856v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05853v1",
      "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference",
      "abstract": "The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \\underline{r}ound-\\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$τ$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.",
      "authors": [
        "Siran Liu",
        "Guoxia Wang",
        "Sa Wang",
        "Jinle Zeng",
        "HaoYang Xie",
        "Siyu Lou",
        "JiaBin Yang",
        "DianHai Yu",
        "Haifeng Wang",
        "Chao Yang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T16:37:41+00:00",
      "link": "https://arxiv.org/pdf/2602.05853v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05852v1",
      "title": "Exact Recovery in the Data Block Model",
      "abstract": "Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. While classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. In this work, we study exact recovery in the Data Block Model (DBM), an SBM augmented with node-associated data, as formalized by Asadi, Abbe, and Verdú (2017). We introduce the Chernoff--TV divergence and use it to characterize a sharp exact recovery threshold for the DBM. We further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. Finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.",
      "authors": [
        "Amir R. Asadi",
        "Akbar Davoodi",
        "Ramin Javadi",
        "Farzad Parvaresh"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT",
        "stat.ML"
      ],
      "published": "2026-02-05T16:36:57+00:00",
      "link": "https://arxiv.org/pdf/2602.05852v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05849v1",
      "title": "Visualizing the loss landscapes of physics-informed neural networks",
      "abstract": "Training a neural network requires navigating a high-dimensional, non-convex loss surface to find parameters that minimize this loss. In many ways, it is surprising that optimizers such as stochastic gradient descent and ADAM can reliably locate minima which perform well on both the training and test data. To understand the success of training, a \"loss landscape\" community has emerged to study the geometry of the loss function and the dynamics of optimization, often using visualization techniques. However, these loss landscape studies have mostly been limited to machine learning for image classification. In the newer field of physics-informed machine learning, little work has been conducted to visualize the landscapes of losses defined not by regression to large data sets, but by differential operators acting on state fields discretized by neural networks. In this work, we provide a comprehensive review of the loss landscape literature, as well as a discussion of the few existing physics-informed works which investigate the loss landscape. We then use a number of the techniques we survey to empirically investigate the landscapes defined by the Deep Ritz and squared residual forms of the physics loss function. We find that the loss landscapes of physics-informed neural networks have many of the same properties as the data-driven classification problems studied in the literature. Unexpectedly, we find that the two formulations of the physics loss often give rise to similar landscapes, which appear smooth, well-conditioned, and convex in the vicinity of the solution. The purpose of this work is to introduce the loss landscape perspective to the scientific machine learning community, compare the Deep Ritz and the strong form losses, and to challenge prevailing intuitions about the complexity of the loss landscapes of physics-informed networks.",
      "authors": [
        "Conor Rowan",
        "Finn Murphy-Blanchard"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T16:35:51+00:00",
      "link": "https://arxiv.org/pdf/2602.05849v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05848v1",
      "title": "DARWIN: Dynamic Agentically Rewriting Self-Improving Network",
      "abstract": "DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.",
      "authors": [
        "Henry Jiang"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-05T16:35:46+00:00",
      "link": "https://arxiv.org/pdf/2602.05848v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05846v1",
      "title": "Optimal scaling laws in learning hierarchical multi-index models",
      "abstract": "In this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a genuinely representation-limited regime. We derive exact information-theoretic scaling laws for subspace recovery and prediction error, revealing how the hierarchical features of the target are sequentially learned through a cascade of phase transitions. We further show that these optimal rates are achieved by a simple, target-agnostic spectral estimator, which can be interpreted as the small learning-rate limit of gradient descent on the first-layer weights. Once an adapted representation is identified, the readout can be learned statistically optimally, using an efficient procedure. As a consequence, we provide a unified and rigorous explanation of scaling laws, plateau phenomena, and spectral structure in shallow neural networks trained on such hierarchical targets.",
      "authors": [
        "Leonardo Defilippis",
        "Florent Krzakala",
        "Bruno Loureiro",
        "Antoine Maillard"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-05T16:33:51+00:00",
      "link": "https://arxiv.org/pdf/2602.05846v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05845v1",
      "title": "Self-Supervised Learning with a Multi-Task Latent Space Objective",
      "abstract": "Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.",
      "authors": [
        "Pierre-François De Plaen",
        "Abhishek Jha",
        "Luc Van Gool",
        "Tinne Tuytelaars",
        "Marc Proesmans"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T16:33:30+00:00",
      "link": "https://arxiv.org/pdf/2602.05845v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05830v1",
      "title": "Learning Compact Boolean Networks",
      "abstract": "Floating-point neural networks dominate modern machine learning but incur substantial inference cost, motivating interest in Boolean networks for resource-constrained settings. However, learning compact and accurate Boolean networks is challenging due to their combinatorial nature. In this work, we address this challenge from three different angles: learned connections, compact convolutions and adaptive discretization. First, we propose a novel strategy to learn efficient connections with no additional parameters and negligible computational overhead. Second, we introduce a novel convolutional Boolean architecture that exploits the locality with reduced number of Boolean operations than existing methods. Third, we propose an adaptive discretization strategy to reduce the accuracy drop when converting a continuous-valued network into a Boolean one. Extensive results on standard vision benchmarks demonstrate that the Pareto front of accuracy vs. computation of our method significantly outperforms prior state-of-the-art, achieving better accuracy with up to 37x fewer Boolean operations.",
      "authors": [
        "Shengpu Wang",
        "Yuhao Mao",
        "Yani Zhang",
        "Martin Vechev"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-05T16:19:59+00:00",
      "link": "https://arxiv.org/pdf/2602.05830v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05818v1",
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.",
      "authors": [
        "Zihao Jiang",
        "Miao Peng",
        "Zhenyan Shan",
        "Wenjie Xu",
        "Ben Liu",
        "Gong Chen",
        "Ziqi Gao",
        "Min Peng"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "published": "2026-02-05T16:08:36+00:00",
      "link": "https://arxiv.org/pdf/2602.05818v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05816v1",
      "title": "Bayesian imaging inverse problem with scattering transform",
      "abstract": "Bayesian imaging inverse problems in astrophysics and cosmology remain challenging, particularly in low-data regimes, due to complex forward operators and the frequent lack of well-motivated priors for non-Gaussian signals. In this paper, we introduce a Bayesian approach that addresses these difficulties by relying on a low-dimensional representation of physical fields built from Scattering Transform statistics. This representation enables inference to be performed in a compact model space, where we recover a posterior distribution over signal models that are consistent with the observed data. We propose an iterative adaptive algorithm to efficiently approximate this posterior distribution. We apply our method to a large-scale structure column density field from the Quijote simulations, using a realistic instrumental forward operator. We demonstrate both accurate statistical inference and deterministic signal reconstruction from a single contaminated image, without relying on any external prior distribution for the field of interest. These results demonstrate that Scattering Transform statistics provide an effective representation for solving complex imaging inverse problems in challenging low-data regimes. Our approach opens the way to new applications for non-Gaussian astrophysical and cosmological signals for which little or no prior modeling is available.",
      "authors": [
        "Sébastien Pierre",
        "Erwan Allys",
        "Pablo Richard",
        "Roman Soletskyi",
        "Alexandros Tsouros"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM"
      ],
      "published": "2026-02-05T16:07:04+00:00",
      "link": "https://arxiv.org/pdf/2602.05816v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05805v1",
      "title": "NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking",
      "abstract": "Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via \"Effective-vs-Redundant\" neuron transfer.",
      "authors": [
        "Kang Chen",
        "Zhuoka Feng",
        "Sihan Zhao",
        "Kai Xiong",
        "Junjie Nian",
        "Yaoning Wang",
        "Changyi Xiao",
        "Yixin Cao"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T15:59:12+00:00",
      "link": "https://arxiv.org/pdf/2602.05805v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05798v1",
      "title": "Learning False Discovery Rate Control via Model-Based Neural Networks",
      "abstract": "Controlling the false discovery rate (FDR) in high-dimensional variable selection requires balancing rigorous error control with statistical power. Existing methods with provable guarantees are often overly conservative, creating a persistent gap between the realized false discovery proportion (FDP) and the target FDR level. We introduce a learning-augmented enhancement of the T-Rex Selector framework that narrows this gap. Our approach replaces the analytical FDP estimator with a neural network trained solely on diverse synthetic datasets, enabling a substantially tighter and more accurate approximation of the FDP. This refinement allows the procedure to operate much closer to the desired FDR level, thereby increasing discovery power while maintaining effective approximate control. Through extensive simulations and a challenging synthetic genome-wide association study (GWAS), we demonstrate that our method achieves superior detection of true variables compared to existing approaches.",
      "authors": [
        "Arnau Vilella",
        "Jasin Machkour",
        "Michael Muma",
        "Daniel P. Palomar"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "cs.LG",
        "eess.SP",
        "stat.ML"
      ],
      "published": "2026-02-05T15:53:11+00:00",
      "link": "https://arxiv.org/pdf/2602.05798v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05797v1",
      "title": "Classification Under Local Differential Privacy with Model Reversal and Model Averaging",
      "abstract": "Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.",
      "authors": [
        "Caihong Qin",
        "Yang Bai"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "published": "2026-02-05T15:52:34+00:00",
      "link": "https://arxiv.org/pdf/2602.05797v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05794v1",
      "title": "FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem",
      "abstract": "We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.",
      "authors": [
        "Aboli Kathar",
        "Aman Kumar",
        "Anusha Kamath",
        "Araveeti Srujan",
        "Ashish Sharma",
        "Chandra Bhushan",
        "Dilip Asbe",
        "Divya Sorate",
        "Duddu Prasanth Kumar",
        "Evan Acharya",
        "Harsh Sharma",
        "Hrithik Kadam",
        "Kanishk Singla",
        "Keyur Doshi",
        "Kiran Praveen",
        "Kolisetty Krishna SK",
        "Krishanu Adhikary",
        "Lokesh MPT",
        "Mayurdeep Sonowal",
        "Nadeem Shaikh",
        "Navya Prakash",
        "Nimit Kothari",
        "Nitin Kukreja",
        "Prashant Devadiga",
        "Rakesh Paul",
        "Ratanjeet Pratap Chauhan",
        "Raunak Kalani",
        "Raviraj Joshi",
        "Shamanth MH",
        "Shantanu Pandey",
        "Shubham Soni",
        "Siddharth Dixit",
        "Smriti Jopat",
        "Sunil Patel",
        "Suraj Singh",
        "Suvradip Paul",
        "Tulasi Pilla",
        "Utkarsh Vaidya",
        "Vineeth Nambiar",
        "Vishal Kanvaty",
        "Yatharth Dedhia"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-05T15:48:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05794v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05787v1",
      "title": "Bagging-Based Model Merging for Robust General Text Embeddings",
      "abstract": "General-purpose text embedding models underpin a wide range of NLP and information retrieval applications, and are typically trained on large-scale multi-task corpora to encourage broad generalization. However, it remains unclear how different multi-task training strategies compare in practice, and how to efficiently adapt embedding models as new domains and data types continually emerge. In this work, we present a systematic study of multi-task training for text embeddings from two perspectives: data scheduling and model merging. We compare batch-level shuffling, sequential training variants, two-stage training, and multiple merging granularities, and find that simple batch-level shuffling consistently yields the strongest overall performance, suggesting that task conflicts are limited and training datasets are largely complementary. Despite its effectiveness, batch-level shuffling exhibits two practical limitations: suboptimal out-of-domain (OOD) generalization and poor suitability for incremental learning due to expensive full retraining. To address these issues, we propose Bagging-based rObust mOdel Merging (\\modelname), which trains multiple embedding models on sampled subsets and merges them into a single model, improving robustness while retaining single-model inference efficiency. Moreover, \\modelname naturally supports efficient incremental updates by training lightweight update models on new data with a small historical subset and merging them into the existing model. Experiments across diverse embedding benchmarks demonstrate that \\modelname consistently improves both in-domain and OOD performance over full-corpus batch-level shuffling, while substantially reducing training cost in incremental learning settings.",
      "authors": [
        "Hengran Zhang",
        "Keping Bi",
        "Jiafeng Guo",
        "Jiaming Zhang",
        "Wenbo Yang",
        "Daiting Shi",
        "Xueqi Cheng"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-05T15:45:08+00:00",
      "link": "https://arxiv.org/pdf/2602.05787v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05786v1",
      "title": "Selecting Hyperparameters for Tree-Boosting",
      "abstract": "Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using $59$ regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than $100$ is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks.",
      "authors": [
        "Floris Jan Koster",
        "Fabio Sigrist"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "published": "2026-02-05T15:44:42+00:00",
      "link": "https://arxiv.org/pdf/2602.05786v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05779v1",
      "title": "How Controlling the Variance can Improve Training Stability of Sparsely Activated DNNs and CNNs",
      "abstract": "The intermediate layers of deep networks can be characterised as a Gaussian process, in particular the Edge-of-Chaos (EoC) initialisation strategy prescribes the limiting covariance matrix of the Gaussian process. Here we show that the under-utilised chosen variance of the Gaussian process is important in the training of deep networks with sparsity inducing activation, such as a shifted and clipped ReLU, $\\text{CReLU}_{τ,m}(x)=\\min(\\max(x-τ,0),m)$. Specifically, initialisations leading to larger fixed Gaussian process variances, allow for improved expressivity with activation sparsity as large as 90% in DNNs and CNNs, and generally improve the stability of the training process. Enabling full, or near full, accuracy at such high levels of sparsity in the hidden layers suggests a promising mechanism to reduce the energy consumption of machine learning models involving fully connected layers.",
      "authors": [
        "Emily Dent",
        "Jared Tanner"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "published": "2026-02-05T15:38:37+00:00",
      "link": "https://arxiv.org/pdf/2602.05779v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05778v1",
      "title": "Copula-based models for spatially dependent cylindrical data",
      "abstract": "Cylindrical data frequently arise across various scientific disciplines, including meteorology (e.g., wind direction and speed), oceanography (e.g., marine current direction and speed or wave heights), ecology (e.g., telemetry), and medicine (e.g., seasonality and intensity in disease onset). Such data often occur as spatially correlated series of intensities and angles, thereby representing dependent bivariate response vectors of linear and circular components. To accommodate both the circular-linear dependence and spatial autocorrelation, while remaining flexible in marginal specifications, copula-based models for cylindrical data have been developed in the literature. However, existing approaches typically treat the copula parameters as constants unrelated to covariates, and regression specifications for marginal distributions are frequently restricted to linear predictors, thereby ignoring spatial correlation. In this work, we propose a structured additive conditional copula regression model for cylindrical data. The circular component is modeled using a wrapped Gaussian process, and the linear component follows a distributional regression model. Both components allow for the inclusion of linear covariate effects. Furthermore, by leveraging the empirical equivalence between Gaussian random fields (GRFs) and Gaussian Markov random fields, our approach avoids the computational burden typically associated with GRFs, while simultaneously allowing for non-stationarity in the covariance structure. Posterior estimation is performed via Markov chain Monte Carlo simulation. We evaluate the proposed model in a simulation study and subsequently in an analysis of wind directions and speed in Germany.",
      "authors": [
        "Francesca Labanca",
        "Anna Gottard",
        "Nadja Klein"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-05T15:38:32+00:00",
      "link": "https://arxiv.org/pdf/2602.05778v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05774v1",
      "title": "Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance",
      "abstract": "Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.",
      "authors": [
        "Xiandong Zou",
        "Jianshu Li",
        "Jing Huang",
        "Pan Zhou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T15:36:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05774v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05769v1",
      "title": "Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors",
      "abstract": "LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.",
      "authors": [
        "Adnan Al Ali",
        "Jindřich Helcl",
        "Jindřich Libovický"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T15:31:24+00:00",
      "link": "https://arxiv.org/pdf/2602.05769v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05762v1",
      "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?",
      "abstract": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.",
      "authors": [
        "Andrei Kozyrev",
        "Nikita Khramov",
        "Denis Lochmelis",
        "Valerio Morelli",
        "Gleb Solovev",
        "Anton Podkopaev"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "cs.SE"
      ],
      "published": "2026-02-05T15:28:26+00:00",
      "link": "https://arxiv.org/pdf/2602.05762v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05744v1",
      "title": "Generalized Pinsker Inequality for Bregman Divergences of Negative Tsallis Entropies",
      "abstract": "The Pinsker inequality lower bounds the Kullback--Leibler divergence $D_{\\textrm{KL}}$ in terms of total variation and provides a canonical way to convert $D_{\\textrm{KL}}$ control into $\\lVert \\cdot \\rVert_1$-control. Motivated by applications to probabilistic prediction with Tsallis losses and online learning, we establish a generalized Pinsker inequality for the Bregman divergences $D_α$ generated by the negative $α$-Tsallis entropies -- also known as $β$-divergences. Specifically, for any $p$, $q$ in the relative interior of the probability simplex $Δ^K$, we prove the sharp bound \\[   D_α(p\\Vert q) \\ge \\frac{C_{α,K}}{2}\\cdot \\|p-q\\|_1^2, \\] and we determine the optimal constant $C_{α,K}$ explicitly for every choice of $(α,K)$.",
      "authors": [
        "Guglielmo Beretta",
        "Tommaso Cesari",
        "Roberto Colomboni"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-02-05T15:10:32+00:00",
      "link": "https://arxiv.org/pdf/2602.05744v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05742v1",
      "title": "Fast Rates for Nonstationary Weighted Risk Minimization",
      "abstract": "Weighted empirical risk minimization is a common approach to prediction under distribution drift. This article studies its out-of-sample prediction error under nonstationarity. We provide a general decomposition of the excess risk into a learning term and an error term associated with distribution drift, and prove oracle inequalities for the learning error under mixing conditions. The learning bound holds uniformly over arbitrary weight classes and accounts for the effective sample size induced by the weight vector, the complexity of the weight and hypothesis classes, and potential data dependence. We illustrate the applicability and sharpness of our results in (auto-) regression problems with linear models, basis approximations, and neural networks, recovering minimax-optimal rates (up to logarithmic factors) when specialized to unweighted and stationary settings.",
      "authors": [
        "Tobias Brock",
        "Thomas Nagler"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-05T15:10:07+00:00",
      "link": "https://arxiv.org/pdf/2602.05742v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05739v1",
      "title": "A Bayesian Optimization-Based AutoML Framework for Non-Intrusive Load Monitoring",
      "abstract": "Non-Intrusive Load Monitoring (NILM), commonly known as energy disaggregation, aims to estimate the power consumption of individual appliances by analyzing a home's total electricity usage. This method provides a cost-effective alternative to installing dedicated smart meters for each appliance. In this paper, we introduce a novel framework that incorporates Automated Machine Learning (AutoML) into the NILM domain, utilizing Bayesian Optimization for automated model selection and hyperparameter tuning. This framework empowers domain practitioners to effectively apply machine learning techniques without requiring advanced expertise in data science or machine learning. To support further research and industry adoption, we present AutoML4NILM, a flexible and extensible open-source toolkit designed to streamline the deployment of AutoML solutions for energy disaggregation. Currently, this framework supports 11 algorithms, each with different hyperparameters; however, its flexible design allows for the extension of both the algorithms and their hyperparameters.",
      "authors": [
        "Nazanin Siavash",
        "Armin Moin"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-05T15:05:24+00:00",
      "link": "https://arxiv.org/pdf/2602.05739v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05737v1",
      "title": "Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing",
      "abstract": "In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.",
      "authors": [
        "Luca Ciampi",
        "Ludovico Iannello",
        "Fabrizio Tonelli",
        "Gabriele Lagani",
        "Angelo Di Garbo",
        "Federico Cremisi",
        "Giuseppe Amato"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.NE"
      ],
      "published": "2026-02-05T15:02:07+00:00",
      "link": "https://arxiv.org/pdf/2602.05737v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05735v1",
      "title": "CSRv2: Unlocking Ultra-Sparse Embeddings",
      "abstract": "In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.",
      "authors": [
        "Lixuan Guo",
        "Yifei Wang",
        "Tiansheng Wen",
        "Yifan Wang",
        "Aosong Feng",
        "Bo Chen",
        "Stefanie Jegelka",
        "Chenyu You"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "cs.IT"
      ],
      "published": "2026-02-05T14:59:51+00:00",
      "link": "https://arxiv.org/pdf/2602.05735v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05734v1",
      "title": "Evaluating the impact of word embeddings on similarity scoring in practical information retrieval",
      "abstract": "Search behaviour is characterised using synonymy and polysemy as users often want to search information based on meaning. Semantic representation strategies represent a move towards richer associative connections that can adequately capture this complex usage of language. Vector Space Modelling (VSM) and neural word embeddings play a crucial role in modern machine learning and Natural Language Processing (NLP) pipelines. Embeddings use distributional semantics to represent words, sentences, paragraphs or entire documents as vectors in high dimensional spaces. This can be leveraged by Information Retrieval (IR) systems to exploit the semantic relatedness between queries and answers.   This paper evaluates an alternative approach to measuring query statement similarity that moves away from the common similarity measure of centroids of neural word embeddings. Motivated by the Word Movers Distance (WMD) model, similarity is evaluated using the distance between individual words of queries and statements. Results from ranked query and response statements demonstrate significant gains in accuracy using the combined approach of similarity ranking through WMD with the word embedding techniques. The top performing WMD + GloVe combination outperforms all other state-of-the-art retrieval models including Doc2Vec and the baseline LSA model. Along with the significant gains in performance of similarity ranking through WMD, we conclude that the use of pre-trained word embeddings, trained on vast amounts of data, result in domain agnostic language processing solutions that are portable to diverse business use-cases.",
      "authors": [
        "Niall McCarroll",
        "Kevin Curran",
        "Eugene McNamee",
        "Angela Clist",
        "Andrew Brammer"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-05T14:57:38+00:00",
      "link": "https://arxiv.org/pdf/2602.05734v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05729v1",
      "title": "Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification",
      "abstract": "Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.",
      "authors": [
        "Lexiang Hu",
        "Youze Xue",
        "Dian Li",
        "Gang Liu",
        "Zhouchen Lin"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-05T14:52:35+00:00",
      "link": "https://arxiv.org/pdf/2602.05729v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05728v1",
      "title": "CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering",
      "abstract": "Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.   In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.   Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.",
      "authors": [
        "Hao Yang",
        "Zhiyu Yang",
        "Xupeng Zhang",
        "Wei Wei",
        "Yunjie Zhang",
        "Lin Yang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-05T14:52:06+00:00",
      "link": "https://arxiv.org/pdf/2602.05728v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05725v1",
      "title": "Muon in Associative Memory Learning: Training Dynamics and Scaling Laws",
      "abstract": "Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.",
      "authors": [
        "Binghui Li",
        "Kaifei Wang",
        "Han Zhong",
        "Pinyan Lu",
        "Liwei Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-05T14:49:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05725v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05723v1",
      "title": "Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification",
      "abstract": "In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.",
      "authors": [
        "Taoye Yin",
        "Haoyuan Hu",
        "Yaxin Fan",
        "Xinhao Chen",
        "Xinya Wu",
        "Kai Deng",
        "Kezun Zhang",
        "Feng Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T14:49:05+00:00",
      "link": "https://arxiv.org/pdf/2602.05723v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05713v1",
      "title": "Projected Boosting with Fairness Constraints: Quantifying the Cost of Fair Training Distributions",
      "abstract": "Boosting algorithms enjoy strong theoretical guarantees: when weak learners maintain positive edge, AdaBoost achieves geometric decrease of exponential loss. We study how to incorporate group fairness constraints into boosting while preserving analyzable training dynamics. Our approach, FairBoost, projects the ensemble-induced exponential-weights distribution onto a convex set of distributions satisfying fairness constraints (as a reweighting surrogate), then trains weak learners on this fair distribution. The key theoretical insight is that projecting the training distribution reduces the effective edge of weak learners by a quantity controlled by the KL-divergence of the projection. We prove an exponential-loss bound where the convergence rate depends on weak learner edge minus a \"fairness cost\" term $δ_t = \\sqrt{\\mathrm{KL}(w^t \\| q^t)/2}$. This directly quantifies the accuracy-fairness tradeoff in boosting dynamics. Experiments on standard benchmarks validate the theoretical predictions and demonstrate competitive fairness-accuracy tradeoffs with stable training curves.",
      "authors": [
        "Amir Asiaee",
        "Kaveh Aryan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T14:38:32+00:00",
      "link": "https://arxiv.org/pdf/2602.05713v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05712v1",
      "title": "Towards Green AI: Decoding the Energy of LLM Inference in Software Development",
      "abstract": "Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.",
      "authors": [
        "Lola Solovyeva",
        "Fernando Castor"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-02-05T14:38:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05712v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05709v1",
      "title": "Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions",
      "abstract": "Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.",
      "authors": [
        "Yihao Ouyang",
        "Shiwei Li",
        "Haozhao Wang",
        "Xiandi Luo",
        "Zhuoqi Hu",
        "Yuetong Song",
        "Qiyu Qin",
        "Yichen Li",
        "Ruixuan Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T14:36:44+00:00",
      "link": "https://arxiv.org/pdf/2602.05709v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05708v1",
      "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
      "abstract": "Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.",
      "authors": [
        "Chuangtao Ma",
        "Zeyu Zhang",
        "Arijit Khan",
        "Sebastian Schelter",
        "Paul Groth"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.CL"
      ],
      "published": "2026-02-05T14:33:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05708v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05704v1",
      "title": "Limitations of SGD for Multi-Index Models Beyond Statistical Queries",
      "abstract": "Understanding the limitations of gradient methods, and stochastic gradient descent (SGD) in particular, is a central challenge in learning theory. To that end, a commonly used tool is the Statistical Queries (SQ) framework, which studies performance limits of algorithms based on noisy interaction with the data. However, it is known that the formal connection between the SQ framework and SGD is tenuous: Existing results typically rely on adversarial or specially-structured gradient noise that does not reflect the noise in standard SGD, and (as we point out here) can sometimes lead to incorrect predictions. Moreover, many analyses of SGD for challenging problems rely on non-trivial algorithmic modifications, such as restricting the SGD trajectory to the sphere or using very small learning rates. To address these shortcomings, we develop a new, non-SQ framework to study the limitations of standard vanilla SGD, for single-index and multi-index models (namely, when the target function depends on a low-dimensional projection of the inputs). Our results apply to a broad class of settings and architectures, including (potentially deep) neural networks.",
      "authors": [
        "Daniel Barzilai",
        "Ohad Shamir"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-05T14:29:10+00:00",
      "link": "https://arxiv.org/pdf/2602.05704v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05702v1",
      "title": "Broken neural scaling laws in materials science",
      "abstract": "In materials science, data are scarce and expensive to generate, whether computationally or experimentally. Therefore, it is crucial to identify how model performance scales with dataset size and model capacity to distinguish between data- and model-limited regimes. Neural scaling laws provide a framework for quantifying this behavior and guide the design of materials datasets and machine learning architectures. Here, we investigate neural scaling laws for a paradigmatic materials science task: predicting the dielectric function of metals, a high-dimensional response that governs how solids interact with light. Using over 200,000 dielectric functions from high-throughput ab initio calculations, we study two multi-objective graph neural networks trained to predict the frequency-dependent complex interband dielectric function and the Drude frequency. We observe broken neural scaling laws with respect to dataset size, whereas scaling with the number of model parameters follows a simple power law that rapidly saturates.",
      "authors": [
        "Max Großmann",
        "Malte Grunert",
        "Erich Runge"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG"
      ],
      "published": "2026-02-05T14:27:08+00:00",
      "link": "https://arxiv.org/pdf/2602.05702v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05695v1",
      "title": "Determining Energy Efficiency Sweet Spots in Production LLM Inference",
      "abstract": "Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"Sweet Spots\" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.",
      "authors": [
        "Hiari Pizzini Cavagna",
        "Andrea Proia",
        "Giacomo Madella",
        "Giovanni B. Esposito",
        "Francesco Antici",
        "Daniele Cesarini",
        "Zeynep Kiziltan",
        "Andrea Bartolini"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.PF"
      ],
      "published": "2026-02-05T14:21:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05695v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05694v1",
      "title": "Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation",
      "abstract": "Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.",
      "authors": [
        "Shuting Jiang",
        "Ran Song",
        "Yuxin Huang",
        "Yan Xiang",
        "Yantuan Xian",
        "Shengxiang Gao",
        "Zhengtao Yu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T14:20:59+00:00",
      "link": "https://arxiv.org/pdf/2602.05694v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05693v1",
      "title": "FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning",
      "abstract": "Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.",
      "authors": [
        "Arno Geimer",
        "Beltran Fiz Pontiveros",
        "Radu State"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published": "2026-02-05T14:19:21+00:00",
      "link": "https://arxiv.org/pdf/2602.05693v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05692v1",
      "title": "MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations",
      "abstract": "Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.",
      "authors": [
        "Congbo Ma",
        "Yichun Zhang",
        "Yousef Al-Jazzazi",
        "Ahamed Foisal",
        "Laasya Sharma",
        "Yousra Sadqi",
        "Khaled Saleh",
        "Jihad Mallat",
        "Farah E. Shamout"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T14:18:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05692v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05688v1",
      "title": "Mining Generalizable Activation Functions",
      "abstract": "The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.",
      "authors": [
        "Alex Vitvitskyi",
        "Michael Boratko",
        "Matej Grcic",
        "Razvan Pascanu",
        "Deep Shah",
        "Petar Veličković"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T14:13:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05688v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05687v1",
      "title": "Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction",
      "abstract": "Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data literacy of healthcare professionals (HCPs). We explore how large language models (LLMs) can support sensemaking of patient-generated health data (PGHD) with automated summaries and natural language data exploration. Using cardiovascular disease (CVD) risk reduction as a use case, 16 HCPs reviewed multimodal PGHD in a mixed-methods study with a prototype that integrated common charts, LLM-generated summaries, and a conversational interface. Findings show that AI summaries provided quick overviews that anchored exploration, while conversational interaction supported flexible analysis and bridged data-literacy gaps. However, HCPs raised concerns about transparency, privacy, and overreliance. We contribute empirical insights and sociotechnical design implications for integrating AI-driven summarization and conversation into clinical workflows to support PGHD sensemaking.",
      "authors": [
        "Pavithren V S Pakianathan",
        "Rania Islambouli",
        "Diogo Branco",
        "Albrecht Schmidt",
        "Tiago Guerreiro",
        "Jan David Smeddinck"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2026-02-05T14:11:34+00:00",
      "link": "https://arxiv.org/pdf/2602.05687v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05684v1",
      "title": "Characterizations of the Aubin property of the KKT-mapping in composite optimization by SC derivatives and quadratic bundles",
      "abstract": "For general set-valued mappings, the Aubin property is ultimately tied to limiting coderivatives by the Mordukhovich criterion. Likewise, the existence of single-valued Lipschitzian localizations is related to strict graphical derivatives. In this paper we will show that for the special case of the KKT-mapping from composite optimization, the Aubin property and the existence of single-valued Lipschitzian localizations can be characterized by SC derivatives and quadratic bundles, respectively, which are easier accessible than limiting coderivatives and strict graphical derivatives.",
      "authors": [
        "Helmut Gfrerer",
        "Jiri V. Outrata"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-05T14:09:32+00:00",
      "link": "https://arxiv.org/pdf/2602.05684v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05675v1",
      "title": "Variable Search Stepsize for Randomized Local Search in Multi-Objective Combinatorial Optimization",
      "abstract": "Over the past two decades, research in evolutionary multi-objective optimization has predominantly focused on continuous domains, with comparatively limited attention given to multi-objective combinatorial optimization problems (MOCOPs). Combinatorial problems differ significantly from continuous ones in terms of problem structure and landscape. Recent studies have shown that on MOCOPs multi-objective evolutionary algorithms (MOEAs) can even be outperformed by simple randomised local search. Starting with a randomly sampled solution in search space, randomised local search iteratively draws a random solution (from an archive) to perform local variation within its neighbourhood. However, in most existing methods, the local variation relies on a fixed neighbourhood, which limits exploration and makes the search easy to get trapped in local optima. In this paper, we present a simple yet effective local search method, called variable stepsize randomized local search (VS-RLS), which adjusts the stepsize during the search. VS-RLS transitions gradually from a broad, exploratory search in the early phases to a more focused, fine-grained search as the search progresses. We demonstrate the effectiveness and generalizability of VS-RLS through extensive evaluations against local search and MOEAs methods on diverse MOCOPs.",
      "authors": [
        "Xuepeng Ren",
        "Maocai Wang",
        "Guangming Dai",
        "Zimin Liang",
        "Qianrong Liu",
        "Shengxiang Yang",
        "Miqing Li"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-05T13:59:05+00:00",
      "link": "https://arxiv.org/pdf/2602.05675v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05674v1",
      "title": "Fast Private Adaptive Query Answering for Large Data Domains",
      "abstract": "Privately releasing marginals of a tabular dataset is a foundational problem in differential privacy. However, state-of-the-art mechanisms suffer from a computational bottleneck when marginal estimates are reconstructed from noisy measurements. Recently, residual queries were introduced and shown to lead to highly efficient reconstruction in the batch query answering setting. We introduce new techniques to integrate residual queries into state-of-the-art adaptive mechanisms such as AIM. Our contributions include a novel conceptual framework for residual queries using multi-dimensional arrays, lazy updating strategies, and adaptive optimization of the per-round privacy budget allocation. Together these contributions reduce error, improve speed, and simplify residual query operations. We integrate these innovations into a new mechanism (AIM+GReM), which improves AIM by using fast residual-based reconstruction instead of a graphical model approach. Our mechanism is orders of magnitude faster than the original framework and demonstrates competitive error and greatly improved scalability.",
      "authors": [
        "Miguel Fuentes",
        "Brett Mullins",
        "Yingtai Xiao",
        "Daniel Kifer",
        "Cameron Musco",
        "Daniel Sheldon"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.CR"
      ],
      "published": "2026-02-05T13:57:56+00:00",
      "link": "https://arxiv.org/pdf/2602.05674v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05668v1",
      "title": "Stable but Wrong: When More Data Degrades Scientific Conclusions",
      "abstract": "Modern science increasingly relies on ever-growing observational datasets and automated inference pipelines, under the implicit belief that accumulating more data makes scientific conclusions more reliable. Here we show that this belief can fail in a fundamental and irreversible way. We identify a structural regime in which standard inference procedures converge smoothly, remain well calibrated, and pass conventional diagnostic checks, yet systematically converge to incorrect conclusions. This failure arises when the reliability of observations degrades in a manner that is intrinsically unobservable to the inference process itself. Using minimal synthetic experiments, we demonstrate that in this regime additional data do not correct error but instead amplify it, while residual-based and goodness-of-fit diagnostics remain misleadingly normal. These results reveal an intrinsic limit of data-driven science: stability, convergence, and confidence are not sufficient indicators of epistemic validity. We argue that inference cannot be treated as an unconditional consequence of data availability, but must instead be governed by explicit constraints on the integrity of the observational process.",
      "authors": [
        "Zhipeng Zhang",
        "Kai Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T13:51:47+00:00",
      "link": "https://arxiv.org/pdf/2602.05668v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05657v1",
      "title": "Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization",
      "abstract": "The study of tail behaviour of SGD-induced processes has been attracting a lot of interest, due to offering strong guarantees with respect to individual runs of an algorithm. While many works provide high-probability guarantees, quantifying the error rate for a fixed probability threshold, there is a lack of work directly studying the probability of failure, i.e., quantifying the tail decay rate for a fixed error threshold. Moreover, existing results are of finite-time nature, limiting their ability to capture the true long-term tail decay which is more informative for modern learning models, typically trained for millions of iterations. Our work closes these gaps, by studying the long-term tail decay of SGD-based methods through the lens of large deviations theory, establishing several strong results in the process. First, we provide an upper bound on the tails of the gradient norm-squared of the best iterate produced by (vanilla) SGD, for non-convex costs and bounded noise, with long-term decay at rate $e^{-t/\\log(t)}$. Next, we relax the noise assumption by considering clipped SGD (c-SGD) under heavy-tailed noise with bounded moment of order $p \\in (1,2]$, showing an upper bound with long-term decay at rate $e^{-t^{β_p}/\\log(t)}$, where $β_p = \\frac{4(p-1)}{3p-2}$ for $p \\in (1,2)$ and $e^{-t/\\log^2(t)}$ for $p = 2$. Finally, we provide lower bounds on the tail decay, at rate $e^{-t}$, showing that our rates for both SGD and c-SGD are tight, up to poly-logarithmic factors. Notably, our results demonstrate an order of magnitude faster long-term tail decay compared to existing work based on finite-time bounds, which show rates $e^{-\\sqrt{t}}$ and $e^{-t^{β_p/2}}$, $p \\in (1,2]$, for SGD and c-SGD, respectively. As such, we uncover regimes where the tails decay much faster than previously known, providing stronger long-term guarantees for individual runs.",
      "authors": [
        "Aleksandar Armacki",
        "Dragana Bajović",
        "Dušan Jakovetić",
        "Soummya Kar",
        "Ali H. Sayed"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-05T13:41:13+00:00",
      "link": "https://arxiv.org/pdf/2602.05657v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05651v1",
      "title": "One Size Does NOT Fit All: On the Importance of Physical Representations for Datalog Evaluation",
      "abstract": "Datalog is an increasingly popular recursive query language that is declarative by design, meaning its programs must be translated by an engine into the actual physical execution plan. When generating this plan, a central decision is how to physically represent all involved relations, an aspect in which existing Datalog engines are surprisingly restrictive and often resort to one-size-fits-all solutions. The reason for this is that the typical execution plan of a Datalog program not only performs a single type of operation against the physical representations, but a mixture of operations, such as insertions, lookups, and containment-checks. Further, the relevance of each operation type highly depends on the workload characteristics, which range from familiar properties such as the size, multiplicity, and arity of the individual relations to very specific Datalog properties, such as the \"interweaving\" of rules when relations occur multiple times, and in particular the recursiveness of the query which might generate new tuples on the fly during evaluation. This indicates that a variety of physical representations, each with its own strengths and weaknesses, is required to meet the specific needs of different workload situations. To evaluate this, we conduct an in-depth experimental study of the interplay between potentially suitable physical representations and seven dimensions of workload characteristics that vary across actual Datalog programs, revealing which properties actually matter. Based on these insights, we design an automatic selection mechanism that utilizes a set of decision trees to identify suitable physical representations for a given workload.",
      "authors": [
        "Nick Rassau",
        "Felix Schuhknecht"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-05T13:35:25+00:00",
      "link": "https://arxiv.org/pdf/2602.05651v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05649v1",
      "title": "End-to-End Compression for Tabular Foundation Models",
      "abstract": "The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.",
      "authors": [
        "Guri Zabërgja",
        "Rafiq Kamel",
        "Arlind Kadra",
        "Christian M. M. Frey",
        "Josif Grabocka"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T13:33:58+00:00",
      "link": "https://arxiv.org/pdf/2602.05649v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05648v1",
      "title": "Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew",
      "abstract": "We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.",
      "authors": [
        "Giuseppe Samo",
        "Paola Merlo"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T13:31:21+00:00",
      "link": "https://arxiv.org/pdf/2602.05648v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05639v1",
      "title": "Joint Embedding Variational Bayes",
      "abstract": "We introduce Variational Joint Embedding (VJE), a framework that synthesizes joint embedding and variational inference to enable self-supervised learning of probabilistic representations in a reconstruction-free, non-contrastive setting. Compared to energy-based predictive objectives that optimize pointwise discrepancies, VJE maximizes a symmetric conditional evidence lower bound (ELBO) for a latent-variable model defined directly on encoder embeddings. We instantiate the conditional likelihood with a heavy-tailed Student-$t$ model using a polar decomposition that explicitly decouples directional and radial factors to prevent norm-induced instabilities during training. VJE employs an amortized inference network to parameterize a diagonal Gaussian variational posterior whose feature-wise variances are shared with the likelihood scale to capture anisotropic uncertainty without auxiliary projection heads. Across ImageNet-1K, CIFAR-10/100, and STL-10, VJE achieves performance comparable to standard non-contrastive baselines under linear and k-NN evaluation. We further validate these probabilistic semantics through one-class CIFAR-10 anomaly detection, where likelihood-based scoring under the proposed model outperforms comparable self-supervised baselines.",
      "authors": [
        "Amin Oji",
        "Paul Fieguth"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-05T13:18:53+00:00",
      "link": "https://arxiv.org/pdf/2602.05639v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05635v1",
      "title": "Structural Disentanglement in Bilinear MLPs via Architectural Inductive Bias",
      "abstract": "Selective unlearning and long-horizon extrapolation remain fragile in modern neural networks, even when tasks have underlying algebraic structure. In this work, we argue that these failures arise not solely from optimization or unlearning algorithms, but from how models structure their internal representations during training. We explore if having explicit multiplicative interactions as an architectural inductive bias helps in structural disentanglement, through Bilinear MLPs. We show analytically that bilinear parameterizations possess a `non-mixing' property under gradient flow conditions, where functional components separate into orthogonal subspace representations. This provides a mathematical foundation for surgical model modification. We validate this hypothesis through a series of controlled experiments spanning modular arithmetic, cyclic reasoning, Lie group dynamics, and targeted unlearning benchmarks. Unlike pointwise nonlinear networks, multiplicative architectures are able to recover true operators aligned with the underlying algebraic structure. Our results suggest that model editability and generalization are constrained by representational structure, and that architectural inductive bias plays a central role in enabling reliable unlearning.",
      "authors": [
        "Ojasva Nema",
        "Kaustubh Sharma",
        "Aditya Chauhan",
        "Parikshit Pareek"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T13:14:01+00:00",
      "link": "https://arxiv.org/pdf/2602.05635v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05633v1",
      "title": "CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models",
      "abstract": "Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.",
      "authors": [
        "Rui Jia",
        "Ruiyi Lan",
        "Fengrui Liu",
        "Zhongxiang Dai",
        "Bo Jiang",
        "Jing Shao",
        "Jingyuan Chen",
        "Guandong Xu",
        "Fei Wu",
        "Min Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T13:13:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05633v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05630v1",
      "title": "Rewards as Labels: Revisiting RLVR from a Classification Perspective",
      "abstract": "Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.",
      "authors": [
        "Zepeng Zhai",
        "Meilin Chen",
        "Jiaxuan Zhao",
        "Junlang Qian",
        "Lei Shen",
        "Yuan Lu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-05T13:11:36+00:00",
      "link": "https://arxiv.org/pdf/2602.05630v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05625v1",
      "title": "Reactive Knowledge Representation and Asynchronous Reasoning",
      "abstract": "Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.",
      "authors": [
        "Simon Kohaut",
        "Benedict Flade",
        "Julian Eggert",
        "Kristian Kersting",
        "Devendra Singh Dhami"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T13:02:01+00:00",
      "link": "https://arxiv.org/pdf/2602.05625v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05622v1",
      "title": "Nonsmooth Optimization with Zeroth Order Comparison Feedback",
      "abstract": "We study unconstrained optimization problems of nonsmooth, nonconvex Lipschitz functions, using only noisy pairwise comparisons governed by a known link function. Our goal is to compute a $(δ,\\varepsilon)$-Goldstein stationary point. We combine randomized smoothing with a novel unbiased reduction from comparisons to local value differences. By leveraging a Russian-roulette truncation on the Bernoulli-product expansion of the inverse link, we construct an exactly unbiased estimator for directional differences. This estimator has finite expected cost and variance scaling quadratically with the function gap, $\\mathcal{O}(B^2)$, under mild conditions. Plugging this into the smoothed gradient identity enables a standard nonconvex SGD analysis, yielding explicit comparison-complexity bounds for common symmetric links such as logistic, probit, and cauchit.",
      "authors": [
        "Taha El Bakkali",
        "El Mahdi Chayti",
        "Omar Saadi"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-05T12:58:11+00:00",
      "link": "https://arxiv.org/pdf/2602.05622v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05616v1",
      "title": "Path-Guided Flow Matching for Dataset Distillation",
      "abstract": "Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \\emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\\times$ more efficient than the diffusion-based counterparts with 78\\% mode coverage.",
      "authors": [
        "Xuhui Li",
        "Zhengquan Luo",
        "Xiwei Liu",
        "Yongqiang Yu",
        "Zhiqiang Xu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T12:52:32+00:00",
      "link": "https://arxiv.org/pdf/2602.05616v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05610v1",
      "title": "MESS: Multi-Epoch Spectroscopic Solver for Detecting Double-Lined Systems",
      "abstract": "We present MESS, a fully automated algorithm for identifying and characterizing double-lined spectroscopic binaries (SB2) in large databases of multi-epoch spectra. MESS extends the two-dimensional TODCOR approach to a global multi-epoch formalism, deriving the radial velocities (RVs) of both components at each epoch while optimizing the templates jointly across all observations. Template optimization searches a continuous synthetic-spectra manifold spanning an eight-dimensional parameter space: effective temperature, surface gravity, and rotational broadening for each star, together with a common metallicity and the flux ratio. Single-lined spectroscopic binaries (SB1) and single stars (S1) are handled within the same framework by fitting one optimized template, with either epoch-dependent RVs (SB1) or a single shared RV (S1). Model selection among S1/SB1/SB2 uses the Bayesian information criterion with an effective sample size that accounts for intra-spectrum correlations, and is complemented by the Wilson relation between the two RVs to infer the mass ratio and systemic velocity without a full orbital solution. We validate MESS on 1500 simulated LAMOST MRS systems (SNR=50), with primary RV semi-amplitudes predominantly below the instrumental resolution, achieving an overall classification accuracy of ~95%. We also derive full orbital solutions for two SB2 systems detected in our LAMOST analysis, including a faint-secondary case with flux ratio ~0.1, and present example outputs for one SB1 and three constant-velocity stars. A companion paper will report the survey-wide application to LAMOST DR11 and the resulting SB1/SB2 catalogs.",
      "authors": [
        "Gil Nachmani",
        "Simchon Faigler",
        "Tsevi Mazeh"
      ],
      "primary_category": "astro-ph.SR",
      "categories": [
        "astro-ph.SR",
        "astro-ph.IM"
      ],
      "published": "2026-02-05T12:48:17+00:00",
      "link": "https://arxiv.org/pdf/2602.05610v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05605v1",
      "title": "Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers",
      "abstract": "Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.",
      "authors": [
        "Jiaji Zhang",
        "Hailiang Zhao",
        "Guoxuan Zhu",
        "Ruichao Sun",
        "Jiaju Wu",
        "Xinkui Zhao",
        "Hanlin Tang",
        "Weiyi Lu",
        "Kan Liu",
        "Tao Lan",
        "Lin Qu",
        "Shuiguang Deng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-05T12:42:22+00:00",
      "link": "https://arxiv.org/pdf/2602.05605v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05602v1",
      "title": "Multi-instance robust fitting for non-classical geometric models",
      "abstract": "Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.",
      "authors": [
        "Zongliang Zhang",
        "Shuxiang Li",
        "Xingwang Huang",
        "Zongyue Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T12:38:38+00:00",
      "link": "https://arxiv.org/pdf/2602.05602v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05600v1",
      "title": "On the Superlinear Relationship between SGD Noise Covariance and Loss Landscape Curvature",
      "abstract": "Stochastic Gradient Descent (SGD) introduces anisotropic noise that is correlated with the local curvature of the loss landscape, thereby biasing optimization toward flat minima. Prior work often assumes an equivalence between the Fisher Information Matrix and the Hessian for negative log-likelihood losses, leading to the claim that the SGD noise covariance $\\mathbf{C}$ is proportional to the Hessian $\\mathbf{H}$. We show that this assumption holds only under restrictive conditions that are typically violated in deep neural networks. Using the recently discovered Activity--Weight Duality, we find a more general relationship agnostic to the specific loss formulation, showing that $\\mathbf{C} \\propto \\mathbb{E}_p[\\mathbf{h}_p^2]$, where $\\mathbf{h}_p$ denotes the per-sample Hessian with $\\mathbf{H} = \\mathbb{E}_p[\\mathbf{h}_p]$. As a consequence, $\\mathbf{C}$ and $\\mathbf{H}$ commute approximately rather than coincide exactly, and their diagonal elements follow an approximate power-law relation $C_{ii} \\propto H_{ii}^γ$ with a theoretically bounded exponent $1 \\leq γ\\leq 2$, determined by per-sample Hessian spectra. Experiments across datasets, architectures, and loss functions validate these bounds, providing a unified characterization of the noise-curvature relationship in deep learning.",
      "authors": [
        "Yikuan Zhang",
        "Ning Yang",
        "Yuhai Tu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T12:35:13+00:00",
      "link": "https://arxiv.org/pdf/2602.05600v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05599v1",
      "title": "BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages",
      "abstract": "Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.",
      "authors": [
        "Subhadip Maji",
        "Arnab Bhattacharya"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-05T12:33:30+00:00",
      "link": "https://arxiv.org/pdf/2602.05599v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05598v1",
      "title": "CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion",
      "abstract": "Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.",
      "authors": [
        "Aon Safdar",
        "Mohamed Saadeldin"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-05T12:33:09+00:00",
      "link": "https://arxiv.org/pdf/2602.05598v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05591v1",
      "title": "Efficient Algorithms for Robust Markov Decision Processes with $s$-Rectangular Ambiguity Sets",
      "abstract": "Robust Markov decision processes (MDPs) have attracted significant interest due to their ability to protect MDPs from poor out-of-sample performance in the presence of ambiguity. In contrast to classical MDPs, which account for stochasticity by modeling the dynamics through a stochastic process with a known transition kernel, a robust MDP additionally accounts for ambiguity by optimizing against the most adverse transition kernel from an ambiguity set constructed via historical data. In this paper, we develop a unified solution framework for a broad class of robust MDPs with $s$-rectangular ambiguity sets, where the most adverse transition probabilities are considered independently for each state. Using our algorithms, we show that $s$-rectangular robust MDPs with $1$- and $2$-norm as well as $φ$-divergence ambiguity sets can be solved several orders of magnitude faster than with state-of-the-art commercial solvers, and often only a logarithmic factor slower than classical MDPs. We demonstrate the favorable scaling properties of our algorithms on a range of synthetically generated as well as standard benchmark instances.",
      "authors": [
        "Chin Pang Ho",
        "Marek Petrik",
        "Wolfram Wiesemann"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "published": "2026-02-05T12:20:26+00:00",
      "link": "https://arxiv.org/pdf/2602.05591v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05589v1",
      "title": "Taylor-Accelerated Neural Network Interpolation Operators on Irregular Grids with Higher Order Approximation",
      "abstract": "In this paper, a new class of \\emph{Taylor-accelerated neural network interpolation operators} is introduced on quasi-uniform irregular grids. These operators improve existing neural network interpolation operators by incorporating Taylor polynomials at the sampling nodes, thereby exploiting the higher smoothness of the target function. The proposed operators are shown to be well defined, uniformly bounded, and to satisfy an exact interpolation property at the grid points. In addition, polynomial reproduction up to a prescribed degree is established. Direct approximation estimates are derived in terms of higher-order moduli of smoothness, yielding enhanced convergence rates for sufficiently smooth functions. Numerical experiments are presented to support the theoretical analysis and to demonstrate the significant accuracy improvement achieved through the Taylor-accelerated construction. In particular, higher-order convergence on irregular grids is obtained, and the proposed approach outperforms existing neural network interpolation operators on irregular partitions, including Lagrange-based schemes.",
      "authors": [
        "Sachin Saini"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA",
        "math.FA"
      ],
      "published": "2026-02-05T12:17:28+00:00",
      "link": "https://arxiv.org/pdf/2602.05589v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05579v1",
      "title": "Physics-Aware Tensor Reconstruction for Radio Maps in Pixel-Based Fluid Antenna Systems",
      "abstract": "The deployment of pixel-based antennas and fluid antenna systems (FAS) is hindered by prohibitive channel state information (CSI) acquisition overhead. While radio maps enable proactive mode selection, reconstructing high-fidelity maps from sparse measurements is challenging. Existing physics-agnostic or data-driven methods often fail to recover fine-grained shadowing details under extreme sparsity. We propose a Physics-Regularized Low-Rank Tensor Completion (PR-LRTC) framework for radio map reconstruction. By modeling the signal field as a three-way tensor, we integrate environmental low-rankness with deterministic antenna physics. Specifically, we leverage Effective Aerial Degrees-of-Freedom (EADoF) theory to derive a differential gain topology map as a physical prior for regularization. The resulting optimization problem is solved via an efficient Alternating Direction Method of Multipliers (ADMM)-based algorithm. Simulations show that PR-LRTC achieves a 4 dB gain over baselines at a 10% sampling ratio. It effectively preserves sharp shadowing edges, providing a robust, physics-compliant solution for low-overhead beam management.",
      "authors": [
        "Mu Jia",
        "Hao Sun",
        "Junting Chen",
        "Pooi-Yuen Kam"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-02-05T12:03:52+00:00",
      "link": "https://arxiv.org/pdf/2602.05579v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05571v1",
      "title": "EdgeMask-DG*: Learning Domain-Invariant Graph Structures via Adversarial Edge Masking",
      "abstract": "Structural shifts pose a significant challenge for graph neural networks, as graph topology acts as a covariate that can vary across domains. Existing domain generalization methods rely on fixed structural augmentations or training on globally perturbed graphs, mechanisms that do not pinpoint which specific edges encode domain-invariant information. We argue that domain-invariant structural information is not rigidly tied to a single topology but resides in the consensus across multiple graph structures derived from topology and feature similarity. To capture this, we first propose EdgeMask-DG, a novel min-max algorithm where an edge masker learns to find worst-case continuous masks subject to a sparsity constraint, compelling a task GNN to perform effectively under these adversarial structural perturbations. Building upon this, we introduce EdgeMask-DG*, an extension that applies this adversarial masking principle to an enriched graph. This enriched graph combines the original topology with feature-derived edges, allowing the model to discover invariances even when the original topology is noisy or domain-specific. EdgeMask-DG* is the first to systematically combine adaptive adversarial topology search with feature-enriched graphs. We provide a formal justification for our approach from a robust optimization perspective. We demonstrate that EdgeMask-DG* achieves new state-of-the-art performance on diverse graph domain generalization benchmarks, including citation networks, social networks, and temporal graphs. Notably, on the Cora OOD benchmark, EdgeMask-DG* lifts the worst-case domain accuracy to 78.0\\%, a +3.8 pp improvement over the prior state of the art (74.2\\%). The source code for our experiments can be found here: https://anonymous.4open.science/r/TMLR-EAEF/",
      "authors": [
        "Rishabh Bhattacharya",
        "Naresh Manwani"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T11:52:06+00:00",
      "link": "https://arxiv.org/pdf/2602.05571v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05567v1",
      "title": "MAGPrompt: Message-Adaptive Graph Prompt Tuning for Graph Neural Networks",
      "abstract": "Pre-trained graph neural networks (GNNs) transfer well, but adapting them to downstream tasks remains challenging due to mismatches between pre-training objectives and task requirements. Graph prompt tuning offers a parameter-efficient alternative to fine-tuning, yet most methods only modify inputs or representations and leave message passing unchanged, limiting their ability to adapt neighborhood interactions. We propose message-adaptive graph prompt tuning, which injects learnable prompts into the message passing step to reweight incoming neighbor messages and add task-specific prompt vectors during message aggregation, while keeping the backbone GNN frozen. The approach is compatible with common GNN backbones and pre-training strategies, and applicable across downstream settings. Experiments on diverse node- and graph-level datasets show consistent gains over prior graph prompting methods in few-shot settings, while achieving performance competitive with fine-tuning in full-shot regimes.",
      "authors": [
        "Long D. Nguyen",
        "Binh P. Nguyen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T11:39:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05567v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05559v1",
      "title": "Piecewise Deterministic Markov Processes for Bayesian Inference of PDE Coefficients",
      "abstract": "We develop a general framework for piecewise deterministic Markov process (PDMP) samplers that enables efficient Bayesian inference in non-linear inverse problems with expensive likelihoods. The key ingredient is a surrogate-assisted thinning scheme in which a surrogate model provides a proposal event rate and a robust correction mechanism enforces an upper bound on the true rate by dynamically adjusting an additive offset whenever violations are detected. This construction is agnostic to the choice of surrogate and PDMP, and we demonstrate it for the Zig-Zag sampler and the Bouncy particle sampler with constant, Laplace, and Gaussian process (GP) surrogates, including gradient-informed and adaptively refined GP variants. As a representative application, we consider Bayesian inference of a spatially varying Young's modulus in a one-dimensional linear elasticity problem. Across dimensions, PDMP samplers equipped with GP-based surrogates achieve substantially higher accuracy and effective sample size per forward model evaluation than Random Walk Metropolis algorithm and the No-U-Turn sampler. The Bouncy particle sampler exhibits the most favorable overall efficiency and scaling, illustrating the potential of the proposed PDMP framework beyond this particular setting.",
      "authors": [
        "Leon Riccius",
        "Iuri B. C. M. Rocha",
        "Joris Bierkens",
        "Hanne Kekkonen",
        "Frans P. van der Meer"
      ],
      "primary_category": "stat.CO",
      "categories": [
        "stat.CO",
        "math.ST",
        "stat.AP",
        "stat.ML"
      ],
      "published": "2026-02-05T11:29:53+00:00",
      "link": "https://arxiv.org/pdf/2602.05559v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05549v1",
      "title": "Logical Guidance for the Exact Composition of Diffusion Models",
      "abstract": "We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.   We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.   First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.   Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.   In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.   Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.   Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.   We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.",
      "authors": [
        "Francesco Alesiani",
        "Jonathan Warrell",
        "Tanja Bien",
        "Henrik Christiansen",
        "Matheus Ferraz",
        "Mathias Niepert"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T11:10:06+00:00",
      "link": "https://arxiv.org/pdf/2602.05549v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05539v1",
      "title": "Steering Large Reasoning Models towards Concise Reasoning via Flow Matching",
      "abstract": "Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.",
      "authors": [
        "Yawei Li",
        "Benjamin Bergner",
        "Yinghan Zhao",
        "Vihang Prakash Patil",
        "Bei Chen",
        "Cheng Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-05T10:56:13+00:00",
      "link": "https://arxiv.org/pdf/2602.05539v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05533v1",
      "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach",
      "abstract": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.",
      "authors": [
        "Zhengyi Guo",
        "Wenpin Tang",
        "Renyuan Xu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T10:46:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05533v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05531v1",
      "title": "Solving Stochastic Variational Inequalities without the Bounded Variance Assumption",
      "abstract": "We analyze algorithms for solving stochastic variational inequalities (VI) without the bounded variance or bounded domain assumptions, where our main focus is min-max optimization with possibly unbounded constraint sets. We focus on two classes of problems: monotone VIs; and structured nonmonotone VIs that admit a solution to the weak Minty VI. The latter assumption allows us to solve structured nonconvex-nonconcave min-max problems. For both classes of VIs, to make the expected residual norm less than $\\varepsilon$, we show an oracle complexity of $\\widetilde{O}(\\varepsilon^{-4})$, which is the best-known for constrained VIs. In our setting, this complexity had been obtained with the bounded variance assumption in the literature, which is not even satisfied for bilinear min-max problems with an unbounded domain. We obtain this complexity for stochastic oracles whose variance can grow as fast as the squared norm of the optimization variable.",
      "authors": [
        "Ahmet Alacaoglu",
        "Jun-Hyun Kim"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-05T10:44:04+00:00",
      "link": "https://arxiv.org/pdf/2602.05531v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05523v1",
      "title": "Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations",
      "abstract": "Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.",
      "authors": [
        "Shahin Honarvar",
        "Amber Gorzynski",
        "James Lee-Jones",
        "Harry Coppock",
        "Marek Rei",
        "Joseph Ryan",
        "Alastair F. Donaldson"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-02-05T10:30:57+00:00",
      "link": "https://arxiv.org/pdf/2602.05523v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05522v1",
      "title": "Mapper-GIN: Lightweight Structural Graph Abstraction for Corrupted 3D Point Cloud Classification",
      "abstract": "Robust 3D point cloud classification is often pursued by scaling up backbones or relying on specialized data augmentation. We instead ask whether structural abstraction alone can improve robustness, and study a simple topology-inspired decomposition based on the Mapper algorithm. We propose Mapper-GIN, a lightweight pipeline that partitions a point cloud into overlapping regions using Mapper (PCA lens, cubical cover, and followed by density-based clustering), constructs a region graph from their overlaps, and performs graph classification with a Graph Isomorphism Network. On the corruption benchmark ModelNet40-C, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters. In contrast to prior approaches that require heavier architectures or additional mechanisms to gain robustness, Mapper-GIN attains strong corruption robustness through simple region-level graph abstraction and GIN message passing. Overall, our results suggest that region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition.",
      "authors": [
        "Jeongbin You",
        "Donggun Kim",
        "Sejun Park",
        "Seungsang Oh"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "math.GT"
      ],
      "published": "2026-02-05T10:30:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05522v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05519v1",
      "title": "Wikipedia and Grokipedia: A Comparison of Human and Generative Encyclopedias",
      "abstract": "We present a comparative analysis of Wikipedia and Grokipedia to examine how generative mediation alters content selection, textual rewriting, narrative structure, and evaluative framing in encyclopedic content. We model page inclusion in Grokipedia as a function of Wikipedia page popularity, density of reference, and recent editorial activity. Inclusion is non-uniform: pages with higher visibility and greater editorial conflict in Wikipedia are more likely to appear in Grokipedia. For included pages, we distinguish between verbatim reproduction and generative rewriting. Rewriting is more frequent for pages with higher reference density and recent controversy, while highly popular pages are more often reproduced without modification. We compare editing activity across the two platforms and estimate page complexity using a fitness-complexity framework to assess whether generative mediation alters patterns of editorial participation. To assess narrative organization, we construct actor-relation networks from article texts using abstract meaning representation. Across multiple topical domains, including U.S. politics, geopolitics, and conspiracy-related narratives, narrative structure remains largely consistent between the two sources. Analysis of lead sections shows broadly correlated framing, with localized shifts in laudatory and conflict-oriented language for some topics in Grokipedia. Overall, generative systems preserve the main structural organization of encyclopedic content, while affecting how content is selected, rewritten, and framed.",
      "authors": [
        "Ortal Hadad",
        "Edoardo Loru",
        "Jacopo Nudo",
        "Anita Bonetti",
        "Matteo Cinelli",
        "Walter Quattrociocchi"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.SI",
        "physics.soc-ph"
      ],
      "published": "2026-02-05T10:24:21+00:00",
      "link": "https://arxiv.org/pdf/2602.05519v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05515v1",
      "title": "A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma",
      "abstract": "Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.",
      "authors": [
        "Ajo Babu George",
        "Anna Mariam John",
        "Athul Anoop",
        "Balu Bhasuran"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-05T10:15:34+00:00",
      "link": "https://arxiv.org/pdf/2602.05515v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05512v1",
      "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
      "abstract": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
      "authors": [
        "Larissa Pusch",
        "Alexandre Courtiol",
        "Tim Conrad"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-05T10:10:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05512v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05506v1",
      "title": "Relying on LLMs: Student Practices and Instructor Norms are Changing in Computer Science Education",
      "abstract": "Prior research has raised concerns about students' over-reliance on large language models (LLMs) in higher education. This paper examines how Computer Science students and instructors engage with LLMs across five scenarios: \"Writing\", \"Quiz\", \"Programming\", \"Project-based learning\", and \"Information retrieval\". Through user studies with 16 students and 6 instructors, we identify 7 key intents, including increasingly complex student practices. Findings reveal varying levels of conflict between student practices and instructor norms, ranging from clear conflict in \"Writing-generation\" and \"(Programming) quiz-solving\", through partial conflict in \"Programming project-implementation\" and \"Project-based learning\", to broad agreement in \"Writing-revision & ideation\", \"(Programming) quiz-correction\" and \"Info-query & summary\". We document instructors are shifting from prohibiting to recognizing students' use of LLMs for high-quality work, integrating usage records into assessment grading. Finally, we propose LLM design guidelines: deploying default guardrails with game-like and empathetic interaction to prevent students from \"deserting\" LLMs, especially for \"Writing-generation\", while utilizing comprehension checks in low-conflict intents to promote learning.",
      "authors": [
        "Xinrui Lin",
        "Heyan Huang",
        "Shumin Shi",
        "John Vines"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-05T10:06:53+00:00",
      "link": "https://arxiv.org/pdf/2602.05506v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05504v1",
      "title": "Continuized Nesterov Momentum Achieves the $O(\\varepsilon^{-7/4})$ Complexity without Additional Mechanisms",
      "abstract": "For first-order optimization of non-convex functions with Lipschitz continuous gradient and Hessian, the best known complexity for reaching an $\\varepsilon$-approximation of a stationary point is $O(\\varepsilon^{-7/4})$. Existing algorithms achieving this bound are based on momentum, but are always complemented with safeguard mechanisms, such as restarts or negative-curvature exploitation steps. Whether such mechanisms are fundamentally necessary has remained an open question. Leveraging the continuized method, we show that a Nesterov momentum algorithm with stochastic parameters alone achieves the same complexity in expectation. This result holds up to a multiplicative stochastic factor with unit expectation and a restriction to a subset of the realizations, both of which are independent of the objective function. We empirically verify that these constitute mild limitations.",
      "authors": [
        "Julien Hermant",
        "Jean-François Aujol",
        "Charles Dossal",
        "Lorick Huang",
        "Aude Rondepierre"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-05T10:05:47+00:00",
      "link": "https://arxiv.org/pdf/2602.05504v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05494v1",
      "title": "A Unified Framework for Rethinking Policy Divergence Measures in GRPO",
      "abstract": "Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.",
      "authors": [
        "Qingyuan Wu",
        "Yuhui Wang",
        "Simon Sinong Zhan",
        "Yanning Dai",
        "Shilong Deng",
        "Sarra Habchi",
        "Qi Zhu",
        "Matthias Gallé",
        "Chao Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T09:56:16+00:00",
      "link": "https://arxiv.org/pdf/2602.05494v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05493v1",
      "title": "LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation",
      "abstract": "Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.",
      "authors": [
        "Bingru Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-02-05T09:55:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05493v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05489v1",
      "title": "Convergence Rate of the Last Iterate of Stochastic Proximal Algorithms",
      "abstract": "We analyze two classical algorithms for solving additively composite convex optimization problems where the objective is the sum of a smooth term and a nonsmooth regularizer: proximal stochastic gradient method for a single regularizer; and the randomized incremental proximal method, which uses the proximal operator of a randomly selected function when the regularizer is given as the sum of many nonsmooth functions. We focus on relaxing the bounded variance assumption that is common, yet stringent, for getting last iterate convergence rates. We prove the $\\widetilde{O}(1/\\sqrt{T})$ rate of convergence for the last iterate of both algorithms under componentwise convexity and smoothness, which is optimal up to log terms. Our results apply directly to graph-guided regularizers that arise in multi-task and federated learning, where the regularizer decomposes as a sum over edges of a collaboration graph.",
      "authors": [
        "Kevin Kurian Thomas Vaidyan",
        "Michael P. Friedlander",
        "Ahmet Alacaoglu"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-05T09:50:06+00:00",
      "link": "https://arxiv.org/pdf/2602.05489v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05483v1",
      "title": "Toward Operationalizing Rasmussen: Drift Observability on the Simplex for Evolving Systems",
      "abstract": "Monitoring drift into failure is hindered by Euclidean anomaly detection that can conflate safe operational trade-offs with risk accumulation in signals expressed as shares, and by architectural churn that makes fixed schemas (and learned models) stale before rare boundary events occur. Rasmussen's dynamic safety model motivates drift under competing pressures, but operationalizing it for software is difficult because many high-value operational signals (effort, remaining margin, incident impact) are compositional and their parts evolve. We propose a vision for drift observability on the simplex: model drift and boundary proximity in Aitchison geometry to obtain coordinate-invariant direction and distance-to-safety in interpretable balance coordinates. To remain comparable under churn, a monitor would continuously refresh its part inventory and policy-defined boundaries from engineering artifacts and apply lineage-aware aggregation. We outline early-warning diagnostics and falsifiable hypotheses for future evaluation.",
      "authors": [
        "Anatoly A. Krasnovsky"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.CY",
        "stat.AP"
      ],
      "published": "2026-02-05T09:41:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05483v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05472v1",
      "title": "ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation",
      "abstract": "The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \\textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \\textbf{costly} to scale, \\textbf{brittle} across domains, and \\textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \\textbf{ALIVE} (\\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \\emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.",
      "authors": [
        "Yiwen Duan",
        "Jing Ye",
        "Xinpei Zhao"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T09:20:23+00:00",
      "link": "https://arxiv.org/pdf/2602.05472v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05471v1",
      "title": "Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision",
      "abstract": "Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.",
      "authors": [
        "Md. Mithun Hossaina",
        "Mashary N. Alrasheedy",
        "Nirban Bhowmick",
        "Shamim Forhad",
        "Md. Shakil Hossain",
        "Sudipto Chaki",
        "Md Shafiqul Islam"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T09:17:25+00:00",
      "link": "https://arxiv.org/pdf/2602.05471v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05470v1",
      "title": "Branch-and-Bound Tensor Networks for Exact Ground-State Characterization",
      "abstract": "Characterizing the ground-state properties of disordered systems, such as spin glasses and combinatorial optimization problems, is fundamental to science and engineering. However, computing exact ground states and counting their degeneracies are generally NP-hard and #P-hard problems, respectively, posing a formidable challenge for exact algorithms. Recently, Tensor Networks methods, which utilize high-dimensional linear algebra and achieve massive hardware parallelization, have emerged as a rapidly developing paradigm for efficiently solving these tasks. Despite their success, these methods are fundamentally constrained by the exponential growth of space complexity, which severely limits their scalability. To address this bottleneck, we introduce the Branch-and-Bound Tensor Network (BBTN) method, which seamlessly integrates the adaptive search framework of branch-and-bound with the efficient contraction of tropical tensor networks, significantly extending the reach of exact algorithms. We show that BBTN significantly surpasses existing state-of-the-art solvers, setting new benchmarks for exact computation. It pushes the boundaries of tractability to previously unreachable scales, enabling exact ground-state counting for $\\pm J$ spin glasses up to $64 \\times 64$ and solving Maximum Independent Set problems on King's subgraphs up to $100 \\times 100$. For hard instances, BBTN dramatically reduces the computational cost of standard Tropical Tensor Networks, compressing years of runtime into minutes. Furthermore, it outperforms leading integer-programming solvers by over 30$\\times$, establishing a versatile and scalable framework for solving hard problems in statistical physics and combinatorial optimization.",
      "authors": [
        "Yijia Wang",
        "Xuanzhao Gao",
        "Pan Zhang",
        "Feng Pan",
        "Jinguo Liu"
      ],
      "primary_category": "cond-mat.stat-mech",
      "categories": [
        "cond-mat.stat-mech",
        "cond-mat.dis-nn",
        "physics.comp-ph"
      ],
      "published": "2026-02-05T09:17:23+00:00",
      "link": "https://arxiv.org/pdf/2602.05470v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05464v1",
      "title": "Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning",
      "abstract": "Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.",
      "authors": [
        "Jiaquan Wang",
        "Yan Lyu",
        "Chen Li",
        "Yuheng Jia"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-05T09:14:44+00:00",
      "link": "https://arxiv.org/pdf/2602.05464v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05463v1",
      "title": "Thermodynamic Limits of Physical Intelligence",
      "abstract": "Modern AI systems achieve remarkable capabilities at the cost of substantial energy consumption. To connect intelligence to physical efficiency, we propose two complementary bits-per-joule metrics under explicit accounting conventions: (1) Thermodynamic Epiplexity per Joule -- bits of structural information about a theoretical environment-instance variable newly encoded in an agent's internal state per unit measured energy within a stated boundary -- and (2) Empowerment per Joule -- the embodied sensorimotor channel capacity (control information) per expected energetic cost over a fixed horizon. These provide two axes of physical intelligence: recognition (model-building) vs.control (action influence). Drawing on stochastic thermodynamics, we show how a Landauer-scale closed-cycle benchmark for epiplexity acquisition follows as a corollary of a standard thermodynamic-learning inequality under explicit subsystem assumptions, and we clarify how Landauer-scaled costs act as closed-cycle benchmarks under explicit reset/reuse and boundary-closure assumptions; conversely, we give a simple decoupling construction showing that without such assumptions -- and without charging for externally prepared low-entropy resources (e.g.fresh memory) crossing the boundary -- information gain and in-boundary dissipation need not be tightly linked. For empirical settings where the latent structure variable is unavailable, we align the operational notion of epiplexity with compute-bounded MDL epiplexity and recommend reporting MDL-epiplexity / compression-gain surrogates as companions. Finally, we propose a unified efficiency framework that reports both metrics together with a minimal checklist of boundary/energy accounting, coarse-graining/noise, horizon/reset, and cost conventions to reduce ambiguity and support consistent bits-per-joule comparisons, and we sketch connections to energy-adjusted scaling analyses.",
      "authors": [
        "Koichi Takahashi",
        "Yusuke Hayashi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "published": "2026-02-05T09:12:43+00:00",
      "link": "https://arxiv.org/pdf/2602.05463v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05459v1",
      "title": "When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL",
      "abstract": "Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\\approx$ 20\\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.",
      "authors": [
        "Jan Malte Töpperwien",
        "Aditya Mohan",
        "Marius Lindauer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T09:08:17+00:00",
      "link": "https://arxiv.org/pdf/2602.05459v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05454v1",
      "title": "Attention Retention for Continual Learning with Vision Transformers",
      "abstract": "Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.",
      "authors": [
        "Yue Lu",
        "Xiangyu Zhou",
        "Shizhou Zhang",
        "Yinghui Xing",
        "Guoqiang Liang",
        "Wencong Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-05T08:55:58+00:00",
      "link": "https://arxiv.org/pdf/2602.05454v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05452v1",
      "title": "DistillER: Knowledge Distillation in Entity Resolution with Large Language Models",
      "abstract": "Recent advances in Entity Resolution (ER) have leveraged Large Language Models (LLMs), achieving strong performance but at the cost of substantial computational resources or high financial overhead. Existing LLM-based ER approaches operate either in unsupervised settings and rely on very large and costly models, or in supervised settings and require ground-truth annotations, leaving a critical gap between time efficiency and effectiveness. To make LLM-powered ER more practical, we investigate Knowledge Distillation (KD) as a means to transfer knowledge from large, effective models (Teachers) to smaller, more efficient models (Students) without requiring gold labels. We introduce DistillER, the first framework that systematically bridges this gap across three dimensions: (i) Data Selection, where we study strategies for identifying informative subsets of data; (ii) Knowledge Elicitation, where we compare single- and multi-teacher settings across LLMs and smaller language models (SLMs); and (iii) Distillation Algorithms, where we evaluate supervised fine-tuning and reinforcement learning approaches. Our experiments reveal that supervised fine-tuning of Students on noisy labels generated by LLM Teachers consistently outperforms alternative KD strategies, while also enabling high-quality explanation generation. Finally, we benchmark DistillER against established supervised and unsupervised ER methods based on LLMs and SLMs, demonstrating significant improvements in both effectiveness and efficiency.",
      "authors": [
        "Alexandros Zeakis",
        "George Papadakis",
        "Dimitrios Skoutas",
        "Manolis Koubarakis"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-05T08:51:06+00:00",
      "link": "https://arxiv.org/pdf/2602.05452v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05448v1",
      "title": "BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs",
      "abstract": "Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\\times$ fewer than pairwise methods at near-identical quality.",
      "authors": [
        "Sheshansh Agrawal",
        "Thien Hang Nguyen",
        "Douwe Kiela"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T08:41:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05448v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05447v1",
      "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale",
      "abstract": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.   Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.   These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.",
      "authors": [
        "Damon McMillan"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-05T08:39:05+00:00",
      "link": "https://arxiv.org/pdf/2602.05447v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05445v1",
      "title": "Forward Index Compression for Learned Sparse Retrieval",
      "abstract": "Text retrieval using learned sparse representations of queries and documents has, over the years, evolved into a highly effective approach to search. It is thanks to recent advances in approximate nearest neighbor search-with the emergence of highly efficient algorithms such as the inverted index-based Seismic and the graph-based Hnsw-that retrieval with sparse representations became viable in practice. In this work, we scrutinize the efficiency of sparse retrieval algorithms and focus particularly on the size of a data structure that is common to all algorithmic flavors and that constitutes a substantial fraction of the overall index size: the forward index. In particular, we seek compression techniques to reduce the storage footprint of the forward index without compromising search quality or inner product computation latency. In our examination with various integer compression techniques, we report that StreamVByte achieves the best trade-off between memory footprint, retrieval accuracy, and latency. We then improve StreamVByte by introducing DotVByte, a new algorithm tailored to inner product computation. Experiments on MsMarco show that our improvements lead to significant space savings while maintaining retrieval efficiency.",
      "authors": [
        "Sebastian Bruch",
        "Martino Fontana",
        "Franco Maria Nardini",
        "Cosimo Rulli",
        "Rossano Venturini"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-05T08:35:17+00:00",
      "link": "https://arxiv.org/pdf/2602.05445v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05444v1",
      "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs",
      "abstract": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \\textbf{C}ausal \\textbf{F}ront-Door \\textbf{A}djustment \\textbf{A}ttack ({\\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.",
      "authors": [
        "Yao Zhou",
        "Zeen Song",
        "Wenwen Qiang",
        "Fengge Wu",
        "Shuyi Zhou",
        "Changwen Zheng",
        "Hui Xiong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T08:34:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05444v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05426v1",
      "title": "Multi-AD: Cross-Domain Unsupervised Anomaly Detection for Medical and Industrial Applications",
      "abstract": "Traditional deep learning models often lack annotated data, especially in cross-domain applications such as anomaly detection, which is critical for early disease diagnosis in medicine and defect detection in industry. To address this challenge, we propose Multi-AD, a convolutional neural network (CNN) model for robust unsupervised anomaly detection across medical and industrial images. Our approach employs the squeeze-and-excitation (SE) block to enhance feature extraction via channel-wise attention, enabling the model to focus on the most relevant features and detect subtle anomalies. Knowledge distillation (KD) transfers informative features from the teacher to the student model, enabling effective learning of the differences between normal and anomalous data. Then, the discriminator network further enhances the model's capacity to distinguish between normal and anomalous data. At the inference stage, by integrating multi-scale features, the student model can detect anomalies of varying sizes. The teacher-student (T-S) architecture ensures consistent representation of high-dimensional features while adapting them to enhance anomaly detection. Multi-AD was evaluated on several medical datasets, including brain MRI, liver CT, and retina OCT, as well as industrial datasets, such as MVTec AD, demonstrating strong generalization across multiple domains. Experimental results demonstrated that our approach consistently outperformed state-of-the-art models, achieving the best average AUROC for both image-level (81.4% for medical and 99.6% for industrial) and pixel-level (97.0% for medical and 98.4% for industrial) tasks, making it effective for real-world applications.",
      "authors": [
        "Wahyu Rahmaniar",
        "Kenji Suzuki"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T08:17:42+00:00",
      "link": "https://arxiv.org/pdf/2602.05426v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05424v1",
      "title": "THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs",
      "abstract": "Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.",
      "authors": [
        "Weijian Yu",
        "Yuhuan Lu",
        "Dingqi Yang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T08:15:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05424v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05419v1",
      "title": "Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation",
      "abstract": "Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.",
      "authors": [
        "Takumi Goto",
        "Yusuke Sakai",
        "Taro Watanabe"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T08:05:42+00:00",
      "link": "https://arxiv.org/pdf/2602.05419v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05417v1",
      "title": "Optimistic Bilevel Optimization with Composite Lower-Level Problem",
      "abstract": "This paper introduces a novel double regularization scheme for bilevel optimization problems whose lower-level problem is composite and convex, but not necessarily strongly convex, in the lower-level variable. The analysis focuses on the primal-dual solution mapping of the regularized lower-level problem and exploits its properties to derive an almost-everywhere formula for the gradient of the regularized hyper-objective under mild assumptions. The paper then establishes conditions under which the hyper-objective of the actual problem is well defined and shows that its gradient can be approximated by the gradient of the regularized hyper-objective. Building on these results, a gradient sampling-based algorithm computes approximately stationary points of the regularized hyper-objective, and we prove its convergence to stationary points of the actual problem. Two numerical examples from machine learning demonstrate the proposed approach.",
      "authors": [
        "Mattia Solla",
        "Johannes O. Royset"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-05T08:02:57+00:00",
      "link": "https://arxiv.org/pdf/2602.05417v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05415v1",
      "title": "VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection",
      "abstract": "Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.",
      "authors": [
        "Ningkang Peng",
        "Qianfeng Yu",
        "Yuhao Zhang",
        "Yafei Liu",
        "Xiaoqian Peng",
        "Peirong Ma",
        "Yi Chen",
        "Peiheng Li",
        "Yanhui Gu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T07:58:12+00:00",
      "link": "https://arxiv.org/pdf/2602.05415v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05413v1",
      "title": "SciDef: Automating Definition Extraction from Academic Literature with Large Language Models",
      "abstract": "Definitions are the foundation for any scientific work, but with a significant increase in publication numbers, gathering definitions relevant to any keyword has become challenging. We therefore introduce SciDef, an LLM-based pipeline for automated definition extraction. We test SciDef on DefExtra & DefSim, novel datasets of human-extracted definitions and definition-pairs' similarity, respectively. Evaluating 16 language models across prompting strategies, we demonstrate that multi-step and DSPy-optimized prompting improve extraction performance. To evaluate extraction, we test various metrics and show that an NLI-based method yields the most reliable results. We show that LLMs are largely able to extract definitions from scientific literature (86.4% of definitions from our test-set); yet future work should focus not just on finding definitions, but on identifying relevant ones, as models tend to over-generate them.   Code & datasets are available at https://github.com/Media-Bias-Group/SciDef.",
      "authors": [
        "Filip Kučera",
        "Christoph Mandl",
        "Isao Echizen",
        "Radu Timofte",
        "Timo Spinde"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "published": "2026-02-05T07:52:08+00:00",
      "link": "https://arxiv.org/pdf/2602.05413v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05410v1",
      "title": "Robust Federated Learning via Byzantine Filtering over Encrypted Updates",
      "abstract": "Federated Learning (FL) aims to train a collaborative model while preserving data privacy. However, the distributed nature of this approach still raises privacy and security issues, such as the exposure of sensitive data due to inference attacks and the influence of Byzantine behaviors on the trained model. In particular, achieving both secure aggregation and Byzantine resilience remains challenging, as existing solutions often address these aspects independently. In this work, we propose to address these challenges through a novel approach that combines homomorphic encryption for privacy-preserving aggregation with property-inference-inspired meta-classifiers for Byzantine filtering. First, following the property-inference attacks blueprint, we train a set of filtering meta-classifiers on labeled shadow updates, reproducing a diverse ensemble of Byzantine misbehaviors in FL, including backdoor, gradient-inversion, label-flipping and shuffling attacks. The outputs of these meta-classifiers are then used to cancel the Byzantine encrypted updates by reweighting. Second, we propose an automated method for selecting the optimal kernel and the dimensionality hyperparameters with respect to homomorphic inference, aggregation constraints and efficiency over the CKKS cryptosystem. Finally, we demonstrate through extensive experiments the effectiveness of our approach against Byzantine participants on the FEMNIST, CIFAR10, GTSRB, and acsincome benchmarks. More precisely, our SVM filtering achieves accuracies between $90$% and $94$% for identifying Byzantine updates at the cost of marginal losses in model utility and encrypted inference runtimes ranging from $6$ to $24$ seconds and from $9$ to $26$ seconds for an overall aggregation.",
      "authors": [
        "Adda Akram Bendoukha",
        "Aymen Boudguiga",
        "Nesrine Kaaniche",
        "Renaud Sirdey",
        "Didem Demirag",
        "Sébastien Gambs"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T07:46:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05410v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05408v1",
      "title": "Rich-Media Re-Ranker: A User Satisfaction-Driven LLM Re-ranking Framework for Rich-Media Search",
      "abstract": "Re-ranking plays a crucial role in modern information search systems by refining the ranking of initial search results to better satisfy user information needs. However, existing methods show two notable limitations in improving user search satisfaction: inadequate modeling of multifaceted user intents and neglect of rich side information such as visual perception signals. To address these challenges, we propose the Rich-Media Re-Ranker framework, which aims to enhance user search satisfaction through multi-dimensional and fine-grained modeling. Our approach begins with a Query Planner that analyzes the sequence of query refinements within a session to capture genuine search intents, decomposing the query into clear and complementary sub-queries to enable broader coverage of users' potential intents. Subsequently, moving beyond primary text content, we integrate richer side information of candidate results, including signals modeling visual content generated by the VLM-based evaluator. These comprehensive signals are then processed alongside carefully designed re-ranking principle that considers multiple facets, including content relevance and quality, information gain, information novelty, and the visual presentation of cover images. Then, the LLM-based re-ranker performs the holistic evaluation based on these principles and integrated signals. To enhance the scenario adaptability of the VLM-based evaluator and the LLM-based re-ranker, we further enhance their capabilities through multi-task reinforcement learning. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines. Notably, the proposed framework has been deployed in a large-scale industrial search system, yielding substantial improvements in online user engagement rates and satisfaction metrics.",
      "authors": [
        "Zihao Guo",
        "Ligang Zhou",
        "Zeyang Tang",
        "Feicheng Li",
        "Ying Nie",
        "Zhiming Peng",
        "Qingyun Sun",
        "Jianxin Li"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-05T07:45:42+00:00",
      "link": "https://arxiv.org/pdf/2602.05408v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05406v1",
      "title": "Enabling Automatic Disordered Speech Recognition: An Impaired Speech Dataset in the Akan Language",
      "abstract": "The lack of impaired speech data hinders advancements in the development of inclusive speech technologies, particularly in low-resource languages such as Akan. To address this gap, this study presents a curated corpus of speech samples from native Akan speakers with speech impairment. The dataset comprises of 50.01 hours of audio recordings cutting across four classes of impaired speech namely stammering, cerebral palsy, cleft palate, and stroke induced speech disorder. Recordings were done in controlled supervised environments were participants described pre-selected images in their own words. The resulting dataset is a collection of audio recordings, transcriptions, and associated metadata on speaker demographics, class of impairment, recording environment and device. The dataset is intended to support research in low-resource automatic disordered speech recognition systems and assistive speech technology.",
      "authors": [
        "Isaac Wiafe",
        "Akon Obu Ekpezu",
        "Sumaya Ahmed Salihs",
        "Elikem Doe Atsakpo",
        "Fiifi Baffoe Payin Winful",
        "Jamal-Deen Abdulai"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "published": "2026-02-05T07:44:13+00:00",
      "link": "https://arxiv.org/pdf/2602.05406v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05403v1",
      "title": "Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation",
      "abstract": "Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogenous levels. Moreover, penalty-based constraints adopted in existing methods struggle to deeply encode physical priors, leading to optimization pathologies and discrepancy between latent representations and physical transparency. To this end, we offer a physical view to interpret opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Building upon the Neural ODEs, we define the neural opinion dynamics to coordinate neural networks with physical priors, and further present the OPINN, a physics-informed neural framework for opinion dynamics modeling. Evaluated on real-world and synthetic datasets, OPINN achieves state-of-the-art performance in opinion evolution forecasting, offering a promising paradigm for the nexus of cyber, physical, and social systems.",
      "authors": [
        "Chenghua Gong",
        "Yihang Jiang",
        "Hao Li",
        "Rui Sun",
        "Juyuan Zhang",
        "Tianjun Gu",
        "Liming Pan",
        "Linyuan Lü"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "published": "2026-02-05T07:41:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05403v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05400v1",
      "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
      "abstract": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
      "authors": [
        "Shaobo Wang",
        "Xuan Ouyang",
        "Tianyi Xu",
        "Yuzheng Hu",
        "Jialin Liu",
        "Guo Chen",
        "Tianyu Zhang",
        "Junhao Zheng",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T07:34:23+00:00",
      "link": "https://arxiv.org/pdf/2602.05400v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05396v1",
      "title": "ELFO: A Python package for emission line fitting optimization in integral field spectroscopy data",
      "abstract": "Integral field spectroscopy (IFS) provides spatially resolved spectra, enabling detailed studies that address the physical and kinematic properties of the interstellar medium. A critical step in analyzing IFS data is the decomposition of emission lines, where different velocity components are often modeled with Gaussian profiles. However, conventional fitting methods that treat each spectrum independently often yield spatial discontinuities in the fitting results. Here, we present Emission Line Fitting Optimization (ELFO), a Python package for IFS spectral fitting. ELFO uses the results of neighboring spectra to determine multiple initial guesses and selects the result that exhibits spatial smoothness. We tested ELFO on IFS data of two quasars obtained from the Multi-Unit Spectroscopic Explorer, where it successfully corrected anomalous fits, revealed previously unresolved substructures, and made large-scale kinematic structures more evident. With minor modifications, this method can also be easily adapted to other IFS data and different emission lines.",
      "authors": [
        "Hui Guo",
        "Guilin Liu",
        "Jianghui Xu",
        "Chao Geng",
        "Zhicheng He",
        "Shiyin Shen",
        "Lei Hao"
      ],
      "primary_category": "astro-ph.GA",
      "categories": [
        "astro-ph.GA"
      ],
      "published": "2026-02-05T07:25:38+00:00",
      "link": "https://arxiv.org/pdf/2602.05396v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05395v1",
      "title": "Optimal Bayesian Stopping for Efficient Inference of Consistent LLM Answers",
      "abstract": "A simple strategy for improving LLM accuracy, especially in math and reasoning problems, is to sample multiple responses and submit the answer most consistently reached. In this paper we leverage Bayesian prior information to save on sampling costs, stopping once sufficient consistency is reached. Although the exact posterior is computationally intractable, we further introduce an efficient \"L-aggregated\" stopping policy that tracks only the L-1 most frequent answer counts. Theoretically, we prove that L=3 is all you need: this coarse approximation is sufficient to achieve asymptotic optimality, and strictly dominates prior-free baselines, while having a fast posterior computation. Empirically, this identifies the most consistent (i.e., mode) LLM answer using fewer samples, and can achieve similar answer accuracy while cutting the number of LLM calls (i.e., saving on LLM inference costs) by up to 50%.",
      "authors": [
        "Jingkai Huang",
        "Will Ma",
        "Zhengyuan Zhou"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-05T07:22:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05395v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05393v1",
      "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better",
      "abstract": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \\textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\\times$ speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\\times$ fewer parameters than the target model.",
      "authors": [
        "Ji Zhao",
        "Yufei Gu",
        "Shitong Shao",
        "Xun Zhou",
        "Liang Xiang",
        "Zeke Xie"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-05T07:19:34+00:00",
      "link": "https://arxiv.org/pdf/2602.05393v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05391v1",
      "title": "Dataset Distillation via Relative Distribution Matching and Cognitive Heritage",
      "abstract": "Dataset distillation seeks to synthesize a highly compact dataset that achieves performance comparable to the original dataset on downstream tasks. For the classification task that use pre-trained self-supervised models as backbones, previous linear gradient matching optimizes synthetic images by encouraging them to mimic the gradient updates induced by real images on the linear classifier. However, this batch-level formulation requires loading thousands of real images and applying multiple rounds of differentiable augmentations to synthetic images at each distillation step, leading to substantial computational and memory overhead. In this paper, we introduce statistical flow matching , a stable and efficient supervised learning framework that optimizes synthetic images by aligning constant statistical flows from target class centers to non-target class centers in the original data. Our approach loads raw statistics only once and performs a single augmentation pass on the synthetic data, achieving performance comparable to or better than the state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Furthermore, we propose a classifier inheritance strategy that reuses the classifier trained on the original dataset for inference, requiring only an extremely lightweight linear projector and marginal storage while achieving substantial performance gains.",
      "authors": [
        "Qianxin Xia",
        "Jiawei Du",
        "Yuhan Zhang",
        "Jielei Wang",
        "Guoming Lu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T07:18:48+00:00",
      "link": "https://arxiv.org/pdf/2602.05391v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05389v1",
      "title": "A Decomposition-based State Space Model for Multivariate Time-Series Forecasting",
      "abstract": "Multivariate time series (MTS) forecasting is crucial for decision-making in domains such as weather, energy, and finance. It remains challenging because real-world sequences intertwine slow trends, multi-rate seasonalities, and irregular residuals. Existing methods often rely on rigid, hand-crafted decompositions or generic end-to-end architectures that entangle components and underuse structure shared across variables. To address these limitations, we propose DecompSSM, an end-to-end decomposition framework using three parallel deep state space model branches to capture trend, seasonal, and residual components. The model features adaptive temporal scales via an input-dependent predictor, a refinement module for shared cross-variable context, and an auxiliary loss that enforces reconstruction and orthogonality. Across standard benchmarks (ECL, Weather, ETTm2, and PEMS04), DecompSSM outperformed strong baselines, indicating the effectiveness of combining component-wise deep state space models and global context refinement.",
      "authors": [
        "Shunya Nagashima",
        "Shuntaro Suzuki",
        "Shuitsu Koyama",
        "Shinnosuke Hirano"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T07:17:08+00:00",
      "link": "https://arxiv.org/pdf/2602.05389v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05385v1",
      "title": "IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models",
      "abstract": "Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.",
      "authors": [
        "Tao Liu",
        "Jiafan Lu",
        "Bohan Yu",
        "Pengcheng Wu",
        "Liu Haixin",
        "Guoyu Xu",
        "Li Xiangheng",
        "Lixiao Li",
        "Jiaming Hou",
        "Zhao Shijun",
        "Xinglin Lyu",
        "Kunli Zhang",
        "Yuxiang Jia",
        "Hongyin Zan"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T07:10:45+00:00",
      "link": "https://arxiv.org/pdf/2602.05385v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05384v1",
      "title": "Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting",
      "abstract": "Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.",
      "authors": [
        "Hao Feng",
        "Wei Shi",
        "Ke Zhang",
        "Xiang Fei",
        "Lei Liao",
        "Dingkang Yang",
        "Yongkun Du",
        "Xuecheng Wu",
        "Jingqun Tang",
        "Yang Liu",
        "Hong Chen",
        "Can Huang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T07:09:57+00:00",
      "link": "https://arxiv.org/pdf/2602.05384v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05381v1",
      "title": "Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation",
      "abstract": "Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.",
      "authors": [
        "Ting Fang Tan",
        "Kabilan Elangovan",
        "Andreas Pollreisz",
        "Kevin Bryan Dy",
        "Wei Yan Ng",
        "Joy Le Yi Wong",
        "Jin Liyuan",
        "Chrystie Quek Wan Ning",
        "Ashley Shuen Ying Hong",
        "Arun James Thirunavukarasu",
        "Shelley Yin-His Chang",
        "Jie Yao",
        "Dylan Hong",
        "Wang Zhaoran",
        "Amrita Gupta",
        "Daniel SW Ting"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T07:00:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05381v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05380v1",
      "title": "SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback",
      "abstract": "Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \\textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \\textbf{SAIL} (\\textbf{S}elf-\\textbf{A}mplified \\textbf{I}terative \\textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.",
      "authors": [
        "Xiaoxuan He",
        "Siming Fu",
        "Wanli Li",
        "Zhiyuan Li",
        "Dacheng Yin",
        "Kang Rong",
        "Fengyun Rao",
        "Bo Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T06:58:38+00:00",
      "link": "https://arxiv.org/pdf/2602.05380v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05375v1",
      "title": "Erase at the Core: Representation Unlearning for Machine Unlearning",
      "abstract": "Many approximate machine unlearning methods demonstrate strong logit-level forgetting -- such as near-zero accuracy on the forget set -- yet continue to preserve substantial information within their internal feature representations. We refer to this discrepancy as superficial forgetting. Recent studies indicate that most existing unlearning approaches primarily alter the final classifier, leaving intermediate representations largely unchanged and highly similar to those of the original model. To address this limitation, we introduce the Erase at the Core (EC), a framework designed to enforce forgetting throughout the entire network hierarchy. EC integrates multi-layer contrastive unlearning on the forget set with retain set preservation through deeply supervised learning. Concretely, EC attaches auxiliary modules to intermediate layers and applies both contrastive unlearning and cross-entropy losses at each supervision point, with layer-wise weighted losses. Experimental results show that EC not only achieves effective logit-level forgetting, but also substantially reduces representational similarity to the original model across intermediate layers. Furthermore, EC is model-agnostic and can be incorporated as a plug-in module into existing unlearning methods, improving representation-level forgetting while maintaining performance on the retain set.",
      "authors": [
        "Jaewon Lee",
        "Yongwoo Kim",
        "Donghyun Kim"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2026-02-05T06:54:44+00:00",
      "link": "https://arxiv.org/pdf/2602.05375v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05374v1",
      "title": "Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks",
      "abstract": "In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.",
      "authors": [
        "Chaimae Abouzahir",
        "Congbo Ma",
        "Nizar Habash",
        "Farah E. Shamout"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-05T06:52:46+00:00",
      "link": "https://arxiv.org/pdf/2602.05374v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05371v1",
      "title": "Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting",
      "abstract": "Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries, but learning high-quality oblique splits is NP-hard, and practical methods still rely on slow search or theory-free heuristics. We present the Hinge Regression Tree (HRT), which reframes each split as a non-linear least-squares problem over two linear predictors whose max/min envelope induces ReLU-like expressive power. The resulting alternating fitting procedure is exactly equivalent to a damped Newton (Gauss-Newton) method within fixed partitions. We analyze this node-level optimization and, for a backtracking line-search variant, prove that the local objective decreases monotonically and converges; in practice, both fixed and adaptive damping yield fast, stable convergence and can be combined with optional ridge regularization. We further prove that HRT's model class is a universal approximator with an explicit $O(δ^2)$ approximation rate, and show on synthetic and real-world benchmarks that it matches or outperforms single-tree baselines with more compact structures.",
      "authors": [
        "Hongyi Li",
        "Han Lin",
        "Jun Xu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T06:49:01+00:00",
      "link": "https://arxiv.org/pdf/2602.05371v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05370v1",
      "title": "PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning",
      "abstract": "Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \\ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \\textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.",
      "authors": [
        "Jun Rao",
        "Zixiong Yu",
        "Xuebo Liu",
        "Guhan Chen",
        "Jing Li",
        "Jiansheng Wei",
        "Xiaojun Meng",
        "Min Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T06:47:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05370v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05367v1",
      "title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs",
      "abstract": "Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\\times$ inference speed-up over full-precision models on an RTX 4090.",
      "authors": [
        "Youngcheon You",
        "Banseok Lee",
        "Minseop Choi",
        "Seonyoung Kim",
        "Hyochan Chong",
        "Changdong Kim",
        "Youngmin Kim",
        "Dongkyu Kim"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T06:41:11+00:00",
      "link": "https://arxiv.org/pdf/2602.05367v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05366v1",
      "title": "Multi-Field Tool Retrieval",
      "abstract": "Integrating external tools enables Large Language Models (LLMs) to interact with real-world environments and solve complex tasks. Given the growing scale of available tools, effective tool retrieval is essential to mitigate constraints of LLMs' context windows and ensure computational efficiency. Existing approaches typically treat tool retrieval as a traditional ad-hoc retrieval task, matching user queries against the entire raw tool documentation. In this paper, we identify three fundamental challenges that limit the effectiveness of this paradigm: (i) the incompleteness and structural inconsistency of tool documentation; (ii) the significant semantic and granular mismatch between user queries and technical tool documents; and, most importantly, (iii) the multi-aspect nature of tool utility, that involves distinct dimensions, such as functionality, input constraints, and output formats, varying in format and importance. To address these challenges, we introduce Multi-Field Tool Retrieval, a framework designed to align user intent with tool representations through fine-grained, multi-field modeling. Experimental results show that our framework achieves SOTA performance on five datasets and a mixed benchmark, exhibiting superior generalizability and robustness.",
      "authors": [
        "Yichen Tang",
        "Weihang Su",
        "Yiqun Liu",
        "Qingyao Ai"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "published": "2026-02-05T06:41:01+00:00",
      "link": "https://arxiv.org/pdf/2602.05366v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05358v1",
      "title": "Bayesian Neighborhood Adaptation for Graph Neural Networks",
      "abstract": "The neighborhood scope (i.e., number of hops) where graph neural networks (GNNs) aggregate information to characterize a node's statistical property is critical to GNNs' performance. Two-stage approaches, training and validating GNNs for every pre-specified neighborhood scope to search for the best setting, is a time-consuming task and tends to be biased due to the search space design. How to adaptively determine proper neighborhood scopes for the aggregation process for both homophilic and heterophilic graphs remains largely unexplored. We thus propose to model the GNNs' message-passing behavior on a graph as a stochastic process by treating the number of hops as a beta process. This Bayesian framework allows us to infer the most plausible neighborhood scope for message aggregation simultaneously with the optimization of GNN parameters. Our theoretical analysis shows that the scope inference improves the expressivity of a GNN. Experiments on benchmark homophilic and heterophilic datasets show that the proposed method is compatible with state-of-the-art GNN variants, achieving competitive or superior performance on the node classification task, and providing well-calibrated predictions.",
      "authors": [
        "Paribesh Regmi",
        "Rui Li",
        "Kishan K C"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T06:29:38+00:00",
      "link": "https://arxiv.org/pdf/2602.05358v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05357v1",
      "title": "Twice Epi-Differentiability of Spectral Functions and its applications",
      "abstract": "Second-order variational properties have been shown to play important theoretical and numerical roles for different classes of optimization problems. Among such properties, twice epi-differentiability has a special place because of its ubiquitous presence in various classes of extended-real-valued functions that are important for optimization problems. We provide a useful characterization of this property for spectral functions by demonstrating that it can be characterized via the same property of the symmetric part of the spectral representation of an eigenvalue function. Our approach allows us to bypass the rather restrictive convexity assumption, used in many recent works that targeted second-order variational properties of spectral functions. By this theoretical tool, several applications on the proto-differentiability of subgradient mappings, the directional differentiability of the proximal mapping of spectral functions are achieved. We finally use our established theory to study twice epi-differentiability of leading eigenvalue functions and practical regularization terms that have important applications in statistics and the robust PCA.",
      "authors": [
        "Chao Ding",
        "Ebrahim Sarabi",
        "Shiwei Wang"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-05T06:26:37+00:00",
      "link": "https://arxiv.org/pdf/2602.05357v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05353v1",
      "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
      "abstract": "Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.",
      "authors": [
        "Ruijie Shi",
        "Houbin Zhang",
        "Yuecheng Han",
        "Yuheng Wang",
        "Jingru Fan",
        "Runde Yang",
        "Yufan Dang",
        "Huatao Li",
        "Dewen Liu",
        "Yuan Cheng",
        "Chen Qian"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-05T06:24:15+00:00",
      "link": "https://arxiv.org/pdf/2602.05353v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05349v1",
      "title": "Learning with Adaptive Prototype Manifolds for Out-of-Distribution Detection",
      "abstract": "Out-of-distribution (OOD) detection is a critical task for the safe deployment of machine learning models in the real world. Existing prototype-based representation learning methods have demonstrated exceptional performance. Specifically, we identify two fundamental flaws that universally constrain these methods: the Static Homogeneity Assumption (fixed representational resources for all classes) and the Learning-Inference Disconnect (discarding rich prototype quality knowledge at inference). These flaws fundamentally limit the model's capacity and performance. To address these issues, we propose APEX (Adaptive Prototype for eXtensive OOD Detection), a novel OOD detection framework designed via a Two-Stage Repair process to optimize the learned feature manifold. APEX introduces two key innovations to address these respective flaws: (1) an Adaptive Prototype Manifold (APM), which leverages the Minimum Description Length (MDL) principle to automatically determine the optimal prototype complexity $K_c^*$ for each class, thereby fundamentally resolving prototype collision; and (2) a Posterior-Aware OOD Scoring (PAOS) mechanism, which quantifies prototype quality (cohesion and separation) to bridge the learning-inference disconnect. Comprehensive experiments on benchmarks such as CIFAR-100 validate the superiority of our method, where APEX achieves new state-of-the-art performance.",
      "authors": [
        "Ningkang Peng",
        "JiuTao Zhou",
        "Yuhao Zhang",
        "Xiaoqian Peng",
        "Qianfeng Yu",
        "Linjing Qian",
        "Tingyu Lu",
        "Yi Chen",
        "Yanhui Gu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T06:21:16+00:00",
      "link": "https://arxiv.org/pdf/2602.05349v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05341v1",
      "title": "Numerically Informed Convolutional Operator Network with Subproblem Decomposition for Poisson Equations",
      "abstract": "Neural operators have shown remarkable performance in approximating solutions of partial differential equations. However, their convergence behavior under grid refinement is still not well understood from the viewpoint of numerical analysis. In this work, we propose a numerically informed convolutional operator network, called NICON, that explicitly couples classical finite difference and finite element methods with operator learning through residual-based training loss functions. We introduce two types of networks, FD-CON and FE-CON, which use residual-based loss functions derived from the corresponding numerical methods. We derive error estimates for FD-CON and FE-CON using finite difference and finite element analysis. These estimates show a direct relation between the convergence behavior and the decay rate of the training loss. From these analyses, we establish training strategies that guarantee optimal convergence rates under grid refinement. Several numerical experiments are presented to validate the theoretical results and show performance on fine grids.",
      "authors": [
        "Kyoungjin Jung",
        "Jae Yong Lee",
        "Dongwook Shin"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-05T06:08:56+00:00",
      "link": "https://arxiv.org/pdf/2602.05341v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05340v1",
      "title": "Decision-Focused Sequential Experimental Design: A Directional Uncertainty-Guided Approach",
      "abstract": "We consider the sequential experimental design problem in the predict-then-optimize paradigm. In this paradigm, the outputs of the prediction model are used as coefficient vectors in a downstream linear optimization problem. Traditional sequential experimental design aims to control the input variables (features) so that the improvement in prediction accuracy from each experimental outcome (label) is maximized. However, in the predict-then-optimize setting, performance is ultimately evaluated based on the decision loss induced by the downstream optimization, rather than by prediction error. This mismatch between prediction accuracy and decision loss renders traditional decision-blind designs inefficient. To address this issue, we propose a directional-based metric to quantify predictive uncertainty. This metric does not require solving an optimization oracle and is therefore computationally tractable. We show that the resulting sequential design criterion enjoys strong consistency and convergence guarantees. Under a broad class of distributions, we demonstrate that our directional uncertainty-based design attains an earlier stopping time than decision-blind designs. This advantage is further supported by real-world experiments on an LLM job allocation problem.",
      "authors": [
        "Beichen Wan",
        "Mo Liu",
        "Paul Grigas",
        "Zuo-Jun Max Shen"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-05T06:06:07+00:00",
      "link": "https://arxiv.org/pdf/2602.05340v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05339v1",
      "title": "Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation",
      "abstract": "With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.",
      "authors": [
        "Yongwoo Kim",
        "Sungmin Cha",
        "Hyunsoo Kim",
        "Jaewon Lee",
        "Donghyun Kim"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-05T06:05:24+00:00",
      "link": "https://arxiv.org/pdf/2602.05339v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05334v1",
      "title": "NeuCLIRTech: Chinese Monolingual and Cross-Language Information Retrieval Evaluation in a Challenging Domain",
      "abstract": "Measuring advances in retrieval requires test collections with relevance judgments that can faithfully distinguish systems. This paper presents NeuCLIRTech, an evaluation collection for cross-language retrieval over technical information. The collection consists of technical documents written natively in Chinese and those same documents machine translated into English. It includes 110 queries with relevance judgments. The collection supports two retrieval scenarios: monolingual retrieval in Chinese, and cross-language retrieval with English as the query language. NeuCLIRTech combines the TREC NeuCLIR track topics of 2023 and 2024. The 110 queries with 35,962 document judgments provide strong statistical discriminatory power when trying to distinguish retrieval approaches. A fusion baseline of strong neural retrieval systems is included so that developers of reranking algorithms are not reliant on BM25 as their first stage retriever. The dataset and artifacts are released on Huggingface Datasets",
      "authors": [
        "Dawn Lawrie",
        "James Mayfield",
        "Eugene Yang",
        "Andrew Yates",
        "Sean MacAvaney",
        "Ronak Pradeep",
        "Scott Miller",
        "Paul McNamee",
        "Luca Soldaini"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-05T05:57:55+00:00",
      "link": "https://arxiv.org/pdf/2602.05334v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05333v1",
      "title": "Pool-based Active Learning as Noisy Lossy Compression: Characterizing Label Complexity via Finite Blocklength Analysis",
      "abstract": "This paper proposes an information-theoretic framework for analyzing the theoretical limits of pool-based active learning (AL), in which a subset of instances is selectively labeled. The proposed framework reformulates pool-based AL as a noisy lossy compression problem by mapping pool observations to noisy symbol observations, data selection to compression, and learning to decoding. This correspondence enables a unified information-theoretic analysis of data selection and learning in pool-based AL. Applying finite blocklength analysis of noisy lossy compression, we derive information-theoretic lower bounds on label complexity and generalization error that serve as theoretical limits for a given learning algorithm under its associated optimal data selection strategy. Specifically, our bounds include terms that reflect overfitting induced by the learning algorithm and the discrepancy between its inductive bias and the target task, and are closely related to established information-theoretic bounds and stability theory, which have not been previously applied to the analysis of pool-based AL. These properties yield a new theoretical perspective on pool-based AL.",
      "authors": [
        "Kosuke Sugiyama",
        "Masato Uchida"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "published": "2026-02-05T05:57:54+00:00",
      "link": "https://arxiv.org/pdf/2602.05333v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05323v1",
      "title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL",
      "abstract": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to \"stitch\" optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.",
      "authors": [
        "Zifan Liu",
        "Xinran Li",
        "Shibo Chen",
        "Jun Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T05:44:48+00:00",
      "link": "https://arxiv.org/pdf/2602.05323v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05315v1",
      "title": "On the Reachability Problem for One-Dimensional Thin Grammar Vector Addition Systems",
      "abstract": "Vector addition systems with states (VASS) are a classic model in concurrency theory. Grammar vector addition systems (GVAS), equivalently, pushdown VASS, extend VASS by using a context-free grammar to control addition. In this paper, our main focus is on the reachability problem for one-dimensional thin GVAS (thin 1-GVAS), a structurally restricted yet expressive subclass. By adopting the index measure for complexity, and by generalizing the decomposition technique developed in the study of VASS reachability to grammar-generated derivation trees of GVAS, an effective integer programming system is established for a thin 1-GVAS. In this way, a nondeterministic algorithm with $\\mathbf{F}_{2k}$ complexity is obtained for the reachability of thin 1-GVAS with index $k$, yielding a tighter upper bound than the previous one.",
      "authors": [
        "Chengfeng Xue",
        "Yuxi Fu"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO"
      ],
      "published": "2026-02-05T05:21:01+00:00",
      "link": "https://arxiv.org/pdf/2602.05315v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05313v1",
      "title": "Beyond overcomplication: a linear model suffices to decode hidden structure-property relationships in glasses",
      "abstract": "Establishing reliable and interpretable structure-property relationships in glasses is a longstanding challenge in condensed matter physics. While modern data-driven machine learning techniques have proven highly effective in establishing structure-property correlations, many models are criticized for lacking physical interpretability and being task-specific. In this work, we identify an approximate linear relation between structure profiles and disorder-induced responses of glass properties based on first order perturbation theory. We analytically demonstrate that this relationship holds universally across glassy systems with varying dimensions and distinct interaction types. This robust theoretical relationship motivates the adoption of linear machine learning models, which we show numerically to achieve surprisingly high predictive accuracy for structure-property mapping in a wide variety of glassy materials. We further devise regularization analysis to further enhance the interpretability of our model, bridging the gap between predictive performance and physical insight. Overall, this linear relation establishes a simple yet powerful connection between structural disorder and spectral properties in glasses, opening a new avenue for advancing their studies.",
      "authors": [
        "Chenyan Wang",
        "Mouyang Cheng",
        "Ji Chen"
      ],
      "primary_category": "cond-mat.dis-nn",
      "categories": [
        "cond-mat.dis-nn"
      ],
      "published": "2026-02-05T05:16:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05313v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05307v1",
      "title": "MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning",
      "abstract": "Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.",
      "authors": [
        "Haojin Wang",
        "Yike Wang",
        "Shangbin Feng",
        "Hannaneh Hajishirzi",
        "Yulia Tsvetkov"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T04:58:16+00:00",
      "link": "https://arxiv.org/pdf/2602.05307v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05306v1",
      "title": "Inverse Optimization Without Inverse Optimization: Direct Solution Prediction with Transformer Models",
      "abstract": "We present an end-to-end framework for generating solutions to combinatorial optimization problems with unknown components using transformer-based sequence-to-sequence neural networks. Our framework learns directly from past solutions and incorporates the known components, such as hard constraints, via a constraint reasoning module, yielding a constrained learning scheme. The trained model generates new solutions that are structurally similar to past solutions and are guaranteed to respect the known constraints. We apply our approach to three combinatorial optimization problems with unknown components: the knapsack problem with an unknown reward function, the bipartite matching problem with an unknown objective function, and the single-machine scheduling problem with release times and unknown precedence constraints, with the objective of minimizing average completion time. We demonstrate that transformer models have remarkably strong performance and often produce near-optimal solutions in a fraction of a second. They can be particularly effective in the presence of more complex underlying objective functions and unknown implicit constraints compared to an LSTM-based alternative and inverse optimization.",
      "authors": [
        "Macarena Navarro",
        "Willem-Jan van Hoeve",
        "Karan Singh"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-05T04:58:13+00:00",
      "link": "https://arxiv.org/pdf/2602.05306v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05304v1",
      "title": "A Short and Unified Convergence Analysis of the SAG, SAGA, and IAG Algorithms",
      "abstract": "Stochastic variance-reduced algorithms such as Stochastic Average Gradient (SAG) and SAGA, and their deterministic counterparts like the Incremental Aggregated Gradient (IAG) method, have been extensively studied in large-scale machine learning. Despite their popularity, existing analyses for these algorithms are disparate, relying on different proof techniques tailored to each method. Furthermore, the original proof of SAG is known to be notoriously involved, requiring computer-aided analysis. Focusing on finite-sum optimization with smooth and strongly convex objective functions, our main contribution is to develop a single unified convergence analysis that applies to all three algorithms: SAG, SAGA, and IAG. Our analysis features two key steps: (i) establishing a bound on delays due to stochastic sub-sampling using simple concentration tools, and (ii) carefully designing a novel Lyapunov function that accounts for such delays. The resulting proof is short and modular, providing the first high-probability bounds for SAG and SAGA that can be seamlessly extended to non-convex objectives and Markov sampling. As an immediate byproduct of our new analysis technique, we obtain the best known rates for the IAG algorithm, significantly improving upon prior bounds.",
      "authors": [
        "Feng Zhu",
        "Robert W. Heath",
        "Aritra Mitra"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.SY",
        "math.OC"
      ],
      "published": "2026-02-05T04:57:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05304v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05302v1",
      "title": "PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences",
      "abstract": "We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.",
      "authors": [
        "Chris Zhu",
        "Sasha Cui",
        "Will Sanok Dufallo",
        "Runzhi Jin",
        "Zhen Xu",
        "Linjun Zhang",
        "Daylian Cain"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T04:52:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05302v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05298v1",
      "title": "Logarithmic-time Schedules for Scaling Language Models with Momentum",
      "abstract": "In practice, the hyperparameters $(β_1, β_2)$ and weight-decay $λ$ in AdamW are typically kept at fixed values. Is there any reason to do otherwise? We show that for large-scale language model training, the answer is yes: by exploiting the power-law structure of language data, one can design time-varying schedules for $(β_1, β_2, λ)$ that deliver substantial performance gains.   We study logarithmic-time scheduling, in which the optimizer's gradient memory horizon grows with training time. Although naive variants of this are unstable, we show that suitable damping mechanisms restore stability while preserving the benefits of longer memory. Based on this, we present ADANA, an AdamW-like optimizer that couples log-time schedules with explicit damping to balance stability and performance. We empirically evaluate ADANA across transformer scalings (45M to 2.6B parameters), comparing against AdamW, Muon, and AdEMAMix.   When properly tuned, ADANA achieves up to 40% compute efficiency relative to a tuned AdamW, with gains that persist--and even improve--as model scale increases. We further show that similar benefits arise when applying logarithmic-time scheduling to AdEMAMix, and that logarithmic-time weight-decay alone can yield significant improvements. Finally, we present variants of ADANA that mitigate potential failure modes and improve robustness.",
      "authors": [
        "Damien Ferbach",
        "Courtney Paquette",
        "Gauthier Gidel",
        "Katie Everett",
        "Elliot Paquette"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-05T04:42:27+00:00",
      "link": "https://arxiv.org/pdf/2602.05298v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05297v1",
      "title": "Aspect-Aware MOOC Recommendation in a Heterogeneous Network",
      "abstract": "MOOC recommendation systems have received increasing attention to help learners navigate and select preferred learning content. Traditional methods such as collaborative filtering and content-based filtering suffer from data sparsity and over-specialization. To alleviate these limitations, graph-based approaches have been proposed; however, they still rely heavily on manually predefined metapaths, which often capture only superficial structural relationships and impose substantial burdens on domain experts as well as significant engineering costs. To overcome these limitations, we propose AMR (Aspect-aware MOOC Recommendation), a novel framework that models path-specific multiple aspects by embedding the semantic content of nodes within each metapath. AMR automatically discovers metapaths through bi-directional walks, derives aspect-aware path representations using a bi-LSTM-based encoder, and incorporates these representations as edge features in the learner-learner and KC-KC subgraphs to achieve fine-grained semantically informed KC recommendations. Extensive experiments on the large-scale MOOCCube and PEEK datasets show that AMR consistently outperforms state-of-the-art graph neural network baselines across key metrics such as HR@K and nDCG@K. Further analysis confirms that AMR effectively captures rich path-specific aspect information, allowing more accurate recommendations than those methods that rely solely on predefined metapaths. The code will be available upon accepted.",
      "authors": [
        "Seongyeub Chu",
        "Jongwoo Kim",
        "Mun Yong Yi"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T04:41:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05297v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05286v1",
      "title": "HealthMamba: An Uncertainty-aware Spatiotemporal Graph State Space Model for Effective and Reliable Healthcare Facility Visit Prediction",
      "abstract": "Healthcare facility visit prediction is essential for optimizing healthcare resource allocation and informing public health policy. Despite advanced machine learning methods being employed for better prediction performance, existing works usually formulate this task as a time-series forecasting problem without considering the intrinsic spatial dependencies of different types of healthcare facilities, and they also fail to provide reliable predictions under abnormal situations such as public emergencies. To advance existing research, we propose HealthMamba, an uncertainty-aware spatiotemporal framework for accurate and reliable healthcare facility visit prediction. HealthMamba comprises three key components: (i) a Unified Spatiotemporal Context Encoder that fuses heterogeneous static and dynamic information, (ii) a novel Graph State Space Model called GraphMamba for hierarchical spatiotemporal modeling, and (iii) a comprehensive uncertainty quantification module integrating three uncertainty quantification mechanisms for reliable prediction. We evaluate HealthMamba on four large-scale real-world datasets from California, New York, Texas, and Florida. Results show HealthMamba achieves around 6.0% improvement in prediction accuracy and 3.5% improvement in uncertainty quantification over state-of-the-art baselines.",
      "authors": [
        "Dahai Yu",
        "Lin Jiang",
        "Rongchao Xu",
        "Guang Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T04:14:27+00:00",
      "link": "https://arxiv.org/pdf/2602.05286v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05281v1",
      "title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.",
      "authors": [
        "Pengyi Li",
        "Elizaveta Goncharova",
        "Andrey Kuznetsov",
        "Ivan Oseledets"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-05T04:06:55+00:00",
      "link": "https://arxiv.org/pdf/2602.05281v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05275v1",
      "title": "Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.",
      "authors": [
        "Qi Li",
        "Yanzhe Zhao",
        "Yongxin Zhou",
        "Yameng Wang",
        "Yandong Yang",
        "Yuanjia Zhou",
        "Jue Wang",
        "Zuojian Wang",
        "Jinxiang Liu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T04:01:01+00:00",
      "link": "https://arxiv.org/pdf/2602.05275v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05271v1",
      "title": "Unlocking Prototype Potential: An Efficient Tuning Framework for Few-Shot Class-Incremental Learning",
      "abstract": "Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model's capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.",
      "authors": [
        "Shengqin Jiang",
        "Xiaoran Feng",
        "Yuankai Qi",
        "Haokui Zhang",
        "Renlong Hang",
        "Qingshan Liu",
        "Lina Yao",
        "Quan Z. Sheng",
        "Ming-Hsuan Yang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T03:50:53+00:00",
      "link": "https://arxiv.org/pdf/2602.05271v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05270v1",
      "title": "PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models",
      "abstract": "As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.",
      "authors": [
        "Thanh Le-Cong",
        "Bach Le",
        "Toby Murray",
        "Michael Pradel",
        "Cristian Cadar"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-05T03:48:17+00:00",
      "link": "https://arxiv.org/pdf/2602.05270v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05266v1",
      "title": "Beyond Cosine Similarity",
      "abstract": "Cosine similarity, the standard metric for measuring semantic similarity in vector spaces, is mathematically grounded in the Cauchy-Schwarz inequality, which inherently limits it to capturing linear relationships--a constraint that fails to model the complex, nonlinear structures of real-world semantic spaces. We advance this theoretical underpinning by deriving a tighter upper bound for the dot product than the classical Cauchy-Schwarz bound. This new bound leads directly to recos, a similarity metric that normalizes the dot product by the sorted vector components. recos relaxes the condition for perfect similarity from strict linear dependence to ordinal concordance, thereby capturing a broader class of relationships. Extensive experiments across 11 embedding models--spanning static, contextualized, and universal types--demonstrate that recos consistently outperforms traditional cosine similarity, achieving higher correlation with human judgments on standard Semantic Textual Similarity (STS) benchmarks. Our work establishes recos as a mathematically principled and empirically superior alternative, offering enhanced accuracy for semantic analysis in complex embedding spaces.",
      "authors": [
        "Xinbo Ai"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T03:46:21+00:00",
      "link": "https://arxiv.org/pdf/2602.05266v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05262v1",
      "title": "ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network",
      "abstract": "Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \\textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \\textbf{80.85\\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \\textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \\textbf{3.1\\%} AP on COCO object detection and \\textbf{3.6\\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.",
      "authors": [
        "Junzhou Li",
        "Manqi Zhao",
        "Yilin Gao",
        "Zhiheng Yu",
        "Yin Li",
        "Dongsheng Jiang",
        "Li Xiao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T03:43:29+00:00",
      "link": "https://arxiv.org/pdf/2602.05262v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05251v1",
      "title": "TADS: Task-Aware Data Selection for Multi-Task Multimodal Pre-Training",
      "abstract": "Large-scale multimodal pre-trained models like CLIP rely heavily on high-quality training data, yet raw web-crawled datasets are often noisy, misaligned, and redundant, leading to inefficient training and suboptimal generalization. Existing data selection methods are either heuristic-based, suffering from bias and limited diversity, or data-driven but task-agnostic, failing to optimize for multi-task scenarios. To address these gaps, we introduce TADS (Task-Aware Data Selection), a novel framework for multi-task multimodal pre-training that integrates Intrinsic Quality, Task Relevance, and Distributional Diversity into a learnable value function. TADS employs a comprehensive quality assessment system with unimodal and cross-modal operators, quantifies task relevance via interpretable similarity vectors, and optimizes diversity through cluster-based weighting. A feedback-driven meta-learning mechanism adaptively refines the selection strategy based on proxy model performance across multiple downstream tasks. Experiments on CC12M demonstrate that TADS achieves superior zero-shot performance on benchmarks like ImageNet, CIFAR-100, MS-COCO, and Flickr30K, using only 36% of the data while outperforming baselines by an average of 1.0%. This highlights that TADS significantly enhances data efficiency by curating a high-utility subset that yields a much higher performance ceiling within the same computational constraints.",
      "authors": [
        "Guanjie Cheng",
        "Boyi Li",
        "Lingyu Sun",
        "Mengying Zhu",
        "Yangyang Wu",
        "Xinkui Zhao",
        "Shuiguang Deng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T03:08:45+00:00",
      "link": "https://arxiv.org/pdf/2602.05251v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05242v1",
      "title": "EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering",
      "abstract": "Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.",
      "authors": [
        "Chenhui Mao",
        "Yuanting Lei",
        "Zhixiang Wei",
        "Ming Liang",
        "Zhixiang Wang",
        "Jingxuan Xu",
        "Dajun Chen",
        "Wei Jiang",
        "Yong Li"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-02-05T03:02:54+00:00",
      "link": "https://arxiv.org/pdf/2602.05242v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05235v1",
      "title": "FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.",
      "authors": [
        "Zhilin Liang",
        "Yuxiang Wang",
        "Zimu Zhou",
        "Hainan Zhang",
        "Boyi Liu",
        "Yongxin Tong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T02:52:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05235v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05232v1",
      "title": "Balanced Anomaly-guided Ego-graph Diffusion Model for Inductive Graph Anomaly Detection",
      "abstract": "Graph anomaly detection (GAD) is crucial in applications like fraud detection and cybersecurity. Despite recent advancements using graph neural networks (GNNs), two major challenges persist. At the model level, most methods adopt a transductive learning paradigm, which assumes static graph structures, making them unsuitable for dynamic, evolving networks. At the data level, the extreme class imbalance, where anomalous nodes are rare, leads to biased models that fail to generalize to unseen anomalies. These challenges are interdependent: static transductive frameworks limit effective data augmentation, while imbalance exacerbates model distortion in inductive learning settings. To address these challenges, we propose a novel data-centric framework that integrates dynamic graph modeling with balanced anomaly synthesis. Our framework features: (1) a discrete ego-graph diffusion model, which captures the local topology of anomalies to generate ego-graphs aligned with anomalous structural distribution, and (2) a curriculum anomaly augmentation mechanism, which dynamically adjusts synthetic data generation during training, focusing on underrepresented anomaly patterns to improve detection and generalization. Experiments on five datasets demonstrate that the effectiveness of our framework.",
      "authors": [
        "Chunyu Wei",
        "Siyuan He",
        "Yu Wang",
        "Yueguo Chen",
        "Yunhai Wang",
        "Bing Bai",
        "Yidong Zhang",
        "Yong Xie",
        "Shunming Zhang",
        "Fei Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T02:46:54+00:00",
      "link": "https://arxiv.org/pdf/2602.05232v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05230v1",
      "title": "ZeroS: Zero-Sum Linear Attention for Efficient Transformers",
      "abstract": "Linear attention methods offer Transformers $O(N)$ complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term $1/t$ and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining $O(N)$ complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks.",
      "authors": [
        "Jiecheng Lu",
        "Xu Han",
        "Yan Sun",
        "Viresh Pati",
        "Yubin Kim",
        "Siddhartha Somani",
        "Shihao Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-05T02:45:19+00:00",
      "link": "https://arxiv.org/pdf/2602.05230v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05228v1",
      "title": "Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink",
      "abstract": "Harmful fine-tuning can invalidate safety alignment of large language models, exposing significant safety risks. In this paper, we utilize the attention sink mechanism to mitigate harmful fine-tuning. Specifically, we first measure a statistic named \\emph{sink divergence} for each attention head and observe that \\emph{different attention heads exhibit two different signs of sink divergence}. To understand its safety implications, we conduct experiments and find that the number of attention heads of positive sink divergence increases along with the increase of the model's harmfulness when undergoing harmful fine-tuning. Based on this finding, we propose a separable sink divergence hypothesis -- \\emph{attention heads associating with learning harmful patterns during fine-tuning are separable by their sign of sink divergence}. Based on the hypothesis, we propose a fine-tuning-stage defense, dubbed Surgery. Surgery utilizes a regularizer for sink divergence suppression, which steers attention heads toward the negative sink divergence group, thereby reducing the model's tendency to learn and amplify harmful patterns. Extensive experiments demonstrate that Surgery improves defense performance by 5.90\\%, 11.25\\%, and 9.55\\% on the BeaverTails, HarmBench, and SorryBench benchmarks, respectively. Source code is available on https://github.com/Lslland/Surgery.",
      "authors": [
        "Guozhi Liu",
        "Weiwei Lin",
        "Tiansheng Huang",
        "Ruichao Mo",
        "Qi Mu",
        "Xiumin Wang",
        "Li Shen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T02:38:58+00:00",
      "link": "https://arxiv.org/pdf/2602.05228v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05227v1",
      "title": "Radon--Wasserstein Gradient Flows for Interacting-Particle Sampling in High Dimensions",
      "abstract": "Gradient flows of the Kullback--Leibler (KL) divergence, such as the Fokker--Planck equation and Stein Variational Gradient Descent, evolve a distribution toward a target density known only up to a normalizing constant. We introduce new gradient flows of the KL divergence with a remarkable combination of properties: they admit accurate interacting-particle approximations in high dimensions, and the per-step cost scales linearly in both the number of particles and the dimension. These gradient flows are based on new transportation-based Riemannian geometries on the space of probability measures: the Radon--Wasserstein geometry and the related Regularized Radon--Wasserstein (RRW) geometry. We define these geometries using the Radon transform so that the gradient-flow velocities depend only on one-dimensional projections. This yields interacting-particle-based algorithms whose per-step cost follows from efficient Fast Fourier Transform-based evaluation of the required 1D convolutions. We additionally provide numerical experiments that study the performance of the proposed algorithms and compare convergence behavior and quantization. Finally, we prove some theoretical results including well-posedness of the flows and long-time convergence guarantees for the RRW flow.",
      "authors": [
        "Elias Hess-Childs",
        "Dejan Slepčev",
        "Lantian Xu"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.AP",
        "math.NA",
        "stat.ME"
      ],
      "published": "2026-02-05T02:38:56+00:00",
      "link": "https://arxiv.org/pdf/2602.05227v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05225v1",
      "title": "Metric space valued Fréchet regression",
      "abstract": "We consider the problem of estimating the Fréchet and conditional Fréchet mean from data taking values in separable metric spaces. Unlike Euclidean spaces, where well-established methods are available, there is no practical estimator that works universally for all metric spaces. Therefore, we introduce a computable estimator for the Fréchet mean based on random quantization techniques and establish its universal consistency across any separable metric spaces. Additionally, we propose another estimator for the conditional Fréchet mean, leveraging data-driven partitioning and quantization, and demonstrate its universal consistency when the output space is any Banach space.",
      "authors": [
        "László Györfi",
        "Pierre Humbert",
        "Batiste Le Bars"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "stat.ML"
      ],
      "published": "2026-02-05T02:29:45+00:00",
      "link": "https://arxiv.org/pdf/2602.05225v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05219v1",
      "title": "Private Prediction via Shrinkage",
      "abstract": "We study differentially private prediction introduced by Dwork and Feldman (COLT 2018): an algorithm receives one labeled sample set $S$ and then answers a stream of unlabeled queries while the output transcript remains $(\\varepsilon,δ)$-differentially private with respect to $S$. Standard composition yields a $\\sqrt{T}$ dependence for $T$ queries.   We show that this dependence can be reduced to polylogarithmic in $T$ in streaming settings. For an oblivious online adversary and any concept class $\\mathcal{C}$, we give a private predictor that answers $T$ queries with $|S|= \\tilde{O}(VC(\\mathcal{C})^{3.5}\\log^{3.5}T)$ labeled examples. For an adaptive online adversary and halfspaces over $\\mathbb{R}^d$, we obtain $|S|=\\tilde{O}\\left(d^{5.5}\\log T\\right)$.",
      "authors": [
        "Chao Yan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T02:17:48+00:00",
      "link": "https://arxiv.org/pdf/2602.05219v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05218v1",
      "title": "Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification",
      "abstract": "Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.",
      "authors": [
        "Jiahao Nie",
        "Yun Xing",
        "Wenbin An",
        "Qingsong Zhao",
        "Jiawei Shao",
        "Yap-Peng Tan",
        "Alex C. Kot",
        "Shijian Lu",
        "Xuelong Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T02:17:38+00:00",
      "link": "https://arxiv.org/pdf/2602.05218v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05216v1",
      "title": "Semantic Search over 9 Million Mathematical Theorems",
      "abstract": "Searching for mathematical results remains difficult: most existing tools retrieve entire papers, while mathematicians and theorem-proving agents often seek a specific theorem, lemma, or proposition that answers a query. While semantic search has seen rapid progress, its behavior on large, highly technical corpora such as research-level mathematical theorems remains poorly understood. In this work, we introduce and study semantic theorem retrieval at scale over a unified corpus of $9.2$ million theorem statements extracted from arXiv and seven other sources, representing the largest publicly available corpus of human-authored, research-level theorems. We represent each theorem with a short natural-language description as a retrieval representation and systematically analyze how representation context, language model choice, embedding model, and prompting strategy affect retrieval quality. On a curated evaluation set of theorem-search queries written by professional mathematicians, our approach substantially improves both theorem-level and paper-level retrieval compared to existing baselines, demonstrating that semantic theorem search is feasible and effective at web scale. The theorem search tool is available at \\href{https://huggingface.co/spaces/uw-math-ai/theorem-search}{this link}, and the dataset is available at \\href{https://huggingface.co/datasets/uw-math-ai/TheoremSearch}{this link}.",
      "authors": [
        "Luke Alexander",
        "Eric Leonen",
        "Sophie Szeto",
        "Artemii Remizov",
        "Ignacio Tejeda",
        "Giovanni Inchiostro",
        "Vasily Ilin"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "math.HO"
      ],
      "published": "2026-02-05T02:16:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05216v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05214v1",
      "title": "Disentangled Representation Learning via Flow Matching",
      "abstract": "Disentangled representation learning aims to capture the underlying explanatory factors of observed data, enabling a principled understanding of the data-generating process. Recent advances in generative modeling have introduced new paradigms for learning such representations. However, existing diffusion-based methods encourage factor independence via inductive biases, yet frequently lack strong semantic alignment. In this work, we propose a flow matching-based framework for disentangled representation learning, which casts disentanglement as learning factor-conditioned flows in a compact latent space. To enforce explicit semantic alignment, we introduce a non-overlap (orthogonality) regularizer that suppresses cross-factor interference and reduces information leakage between factors. Extensive experiments across multiple datasets demonstrate consistent improvements over representative baselines, yielding higher disentanglement scores as well as improved controllability and sample fidelity.",
      "authors": [
        "Jinjin Chi",
        "Taoping Liu",
        "Mengtao Yin",
        "Ximing Li",
        "Yongcheng Jing",
        "Dacheng Tao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T02:14:36+00:00",
      "link": "https://arxiv.org/pdf/2602.05214v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05211v1",
      "title": "Quantifying the Knowledge Proximity Between Academic and Industry Research: An Entity and Semantic Perspective",
      "abstract": "The academia and industry are characterized by a reciprocal shaping and dynamic feedback mechanism. Despite distinct institutional logics, they have adapted closely in collaborative publishing and talent mobility, demonstrating tension between institutional divergence and intensive collaboration. Existing studies on their knowledge proximity mainly rely on macro indicators such as the number of collaborative papers or patents, lacking an analysis of knowledge units in the literature. This has led to an insufficient grasp of fine-grained knowledge proximity between industry and academia, potentially undermining collaboration frameworks and resource allocation efficiency. To remedy the limitation, this study quantifies the trajectory of academia-industry co-evolution through fine-grained entities and semantic space. In the entity measurement part, we extract fine-grained knowledge entities via pre-trained models, measure sequence overlaps using cosine similarity, and analyze topological features through complex network analysis. At the semantic level, we employ unsupervised contrastive learning to quantify convergence in semantic spaces by measuring cross-institutional textual similarities. Finally, we use citation distribution patterns to examine correlations between bidirectional knowledge flows and similarity. Analysis reveals that knowledge proximity between academia and industry rises, particularly following technological change. This provides textual evidence of bidirectional adaptation in co-evolution. Additionally, academia's knowledge dominance weakens during technological paradigm shifts. The dataset and code for this paper can be accessed at https://github.com/tinierZhao/Academic-Industrial-associations.",
      "authors": [
        "Hongye Zhao",
        "Yi Zhao",
        "Chengzhi Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.DL"
      ],
      "published": "2026-02-05T02:12:47+00:00",
      "link": "https://arxiv.org/pdf/2602.05211v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05207v1",
      "title": "ARCHI-TTS: A flow-matching-based Text-to-Speech Model with Self-supervised Semantic Aligner and Accelerated Inference",
      "abstract": "Although diffusion-based, non-autoregressive text-to-speech (TTS) systems have demonstrated impressive zero-shot synthesis capabilities, their efficacy is still hindered by two key challenges: the difficulty of text-speech alignment modeling and the high computational overhead of the iterative denoising process. To address these limitations, we propose ARCHI-TTS that features a dedicated semantic aligner to ensure robust temporal and semantic consistency between text and audio. To overcome high computational inference costs, ARCHI-TTS employs an efficient inference strategy that reuses encoder features across denoising steps, drastically accelerating synthesis without performance degradation. An auxiliary CTC loss applied to the condition encoder further enhances the semantic understanding. Experimental results demonstrate that ARCHI-TTS achieves a WER of 1.98% on LibriSpeech-PC test-clean, and 1.47%/1.42% on SeedTTS test-en/test-zh with a high inference efficiency, consistently outperforming recent state-of-the-art TTS systems.",
      "authors": [
        "Chunyat Wu",
        "Jiajun Deng",
        "Zhengxi Liu",
        "Zheqi Dai",
        "Haolin He",
        "Qiuqiang Kong"
      ],
      "primary_category": "eess.AS",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "published": "2026-02-05T02:05:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05207v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05205v1",
      "title": "Aligning Large Language Model Behavior with Human Citation Preferences",
      "abstract": "Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\\%$ relative to humans) and sentences containing personal names (by $-20.1\\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.",
      "authors": [
        "Kenichiro Ando",
        "Tatsuya Harada"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-05T02:02:43+00:00",
      "link": "https://arxiv.org/pdf/2602.05205v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05202v1",
      "title": "GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling",
      "abstract": "Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \\modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\\times$ to $65\\times$ fewer than existing VLM-based approaches.",
      "authors": [
        "Shivanshu Shekhar",
        "Uttaran Bhattacharya",
        "Raghavendra Addanki",
        "Mehrab Tanjim",
        "Somdeb Sarkhel",
        "Tong Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T01:54:01+00:00",
      "link": "https://arxiv.org/pdf/2602.05202v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05195v1",
      "title": "Traceable Cross-Source RAG for Chinese Tibetan Medicine Question Answering",
      "abstract": "Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine, encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics or clinical papers provide more authoritative evidence. We study a practical setting with three KBs (encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff $K{=}5$) covering both single-KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence coverage without relying on naive concatenation. All answers are generated by a lightweight generator, \\textsc{openPangu-Embedded-7B}. Experiments show consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and citation correctness.",
      "authors": [
        "Fengxian Chen",
        "Zhilong Tao",
        "Jiaxuan Li",
        "Yunlong Li",
        "Qingguo Zhou"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-05T01:46:09+00:00",
      "link": "https://arxiv.org/pdf/2602.05195v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05193v1",
      "title": "Polynomial-Time Solutions for Longest Common Subsequence Related Problems Between a Sequence and a Pangenome Graph",
      "abstract": "A pangenome captures the genetic diversity across multiple individuals simultaneously, providing a more comprehensive reference for genome analysis than a single linear genome, which may introduce allele bias. A widely adopted pangenome representation is a node-labeled directed graph, wherein the paths correspond to plausible genomic sequences within a species. Consequently, evaluating sequence-to-pangenome graph similarity constitutes a fundamental task in pangenome construction and analysis. This study explores the Longest Common Subsequence (LCS) problem and three of its variants involving a sequence and a pangenome graph. We present four polynomial-time reductions that transform these LCS-related problems into the longest path problem in a directed acyclic graph (DAG). These reductions demonstrate that all four problems can be solved in polynomial time, establishing their membership in the complexity class P.",
      "authors": [
        "Xingfu Li",
        "Yongping Wang"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS"
      ],
      "published": "2026-02-05T01:40:46+00:00",
      "link": "https://arxiv.org/pdf/2602.05193v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05191v1",
      "title": "Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs",
      "abstract": "As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions across heads and layers, whereas top-p sparse attention directly preserves attention mass and provides stronger accuracy guarantees. Existing top-p methods, however, fail to jointly optimize top-p accuracy, selection overhead, and sparse attention cost, which limits their overall efficiency. We present Double-P, a hierarchical sparse attention framework that optimizes all three stages. Double-P first performs coarse-grained top-p estimation at the cluster level using size-weighted centroids, then adaptively refines computation through a second top-p stage that allocates token-level attention only when needed. Across long-context benchmarks, Double-P consistently achieves near-zero accuracy drop, reducing attention computation overhead by up to 1.8x and delivers up to 1.3x end-to-end decoding speedup over state-of-the-art fixed-budget sparse attention methods.",
      "authors": [
        "Wentao Ni",
        "Kangqi Zhang",
        "Zhongming Yu",
        "Oren Nelson",
        "Mingu Lee",
        "Hong Cai",
        "Fatih Porikli",
        "Jongryool Kim",
        "Zhijian Liu",
        "Jishen Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T01:37:10+00:00",
      "link": "https://arxiv.org/pdf/2602.05191v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05187v1",
      "title": "SpectraKAN: Conditioning Spectral Operators",
      "abstract": "Spectral neural operators, particularly Fourier Neural Operators (FNO), are a powerful framework for learning solution operators of partial differential equations (PDEs) due to their efficient global mixing in the frequency domain. However, existing spectral operators rely on static Fourier kernels applied uniformly across inputs, limiting their ability to capture multi-scale, regime-dependent, and anisotropic dynamics governed by the global state of the system. We introduce SpectraKAN, a neural operator that conditions the spectral operator on the input itself, turning static spectral convolution into an input-conditioned integral operator. This is achieved by extracting a compact global representation from spatio-temporal history and using it to modulate a multi-scale Fourier trunk via single-query cross-attention, enabling the operator to adapt its behaviour while retaining the efficiency of spectral mixing. We provide theoretical justification showing that this modulation converges to a resolution-independent continuous operator under mesh refinement and KAN gives smooth, Lipschitz-controlled global modulation. Across diverse PDE benchmarks, SpectraKAN achieves state-of-the-art performance, reducing RMSE by up to 49% over strong baselines, with particularly large gains on challenging spatio-temporal prediction tasks.",
      "authors": [
        "Chun-Wun Cheng",
        "Carola-Bibiane Schönlieb",
        "Angelica I. Aviles-Rivero"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-05T01:30:25+00:00",
      "link": "https://arxiv.org/pdf/2602.05187v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05184v1",
      "title": "Towards Worst-Case Guarantees with Scale-Aware Interpretability",
      "abstract": "Neural networks organize information according to the hierarchical, multi-scale structure of natural data. Methods to interpret model internals should be similarly scale-aware, explicitly tracking how features compose across resolutions and guaranteeing bounds on the influence of fine-grained structure that is discarded as irrelevant noise. We posit that the renormalisation framework from physics can meet this need by offering technical tools that can overcome limitations of current methods. Moreover, relevant work from adjacent fields has now matured to a point where scattered research threads can be synthesized into practical, theory-informed tools. To combine these threads in an AI safety context, we propose a unifying research agenda -- \\emph{scale-aware interpretability} -- to develop formal machinery and interpretability tools that have robustness and faithfulness properties supported by statistical physics.",
      "authors": [
        "Lauren Greenspan",
        "David Berman",
        "Aryeh Brill",
        "Ro Jefferson",
        "Artemy Kolchinsky",
        "Jennifer Lin",
        "Andrew Mack",
        "Anindita Maiti",
        "Fernando E. Rosas",
        "Alexander Stapleton",
        "Lucas Teixeira",
        "Dmitry Vaintrob"
      ],
      "primary_category": "hep-th",
      "categories": [
        "hep-th",
        "cond-mat.dis-nn",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-05T01:22:31+00:00",
      "link": "https://arxiv.org/pdf/2602.05184v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05182v1",
      "title": "The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems",
      "abstract": "Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.",
      "authors": [
        "Shangbin Feng",
        "Kishan Panaganti",
        "Yulia Tsvetkov",
        "Wenhao Yu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T01:20:32+00:00",
      "link": "https://arxiv.org/pdf/2602.05182v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05179v1",
      "title": "From Sequential to Parallel: Reformulating Dynamic Programming as GPU Kernels for Large-Scale Stochastic Combinatorial Optimization",
      "abstract": "A major bottleneck in scenario-based Sample Average Approximation (SAA) for stochastic programming (SP) is the cost of solving an exact second-stage problem for every scenario, especially when each scenario contains an NP-hard combinatorial structure. This has led much of the SP literature to restrict the second stage to linear or simplified models. We develop a GPU-based framework that makes full-fidelity integer second-stage models tractable at scale. The key innovation is a set of hardware-aware, scenario-batched GPU kernels that expose parallelism across scenarios, dynamic-programming (DP) layers, and route or action options, enabling Bellman updates to be executed in a single pass over more than 1,000,000 realizations. We evaluate the approach in two representative SP settings: a vectorized split operator for stochastic vehicle routing and a DP for inventory reinsertion. Implementation scales nearly linearly in the number of scenarios and achieves a one-two to four-five orders of magnitude speedup, allowing far larger scenario sets and reliably stronger first-stage decisions. The computational leverage directly improves decision quality: much larger scenario sets and many more first-stage candidates can be evaluated within fixed time budgets, consistently yielding stronger SAA solutions. Our results show that full-fidelity integer second-stage models are tractable at scales previously considered impossible, providing a practical path to large-scale, realistic stochastic discrete optimization.",
      "authors": [
        "Jingyi Zhao",
        "Linxin Yang",
        "Haohua Zhang",
        "Tian Ding"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.DC"
      ],
      "published": "2026-02-05T01:17:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05179v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05175v1",
      "title": "ShapePuri: Shape Guided and Appearance Generalized Adversarial Purification",
      "abstract": "Deep neural networks demonstrate impressive performance in visual recognition, but they remain vulnerable to adversarial attacks that is imperceptible to the human. Although existing defense strategies such as adversarial training and purification have achieved progress, diffusion-based purification often involves high computational costs and information loss. To address these challenges, we introduce Shape Guided Purification (ShapePuri), a novel defense framework enhances robustness by aligning model representations with stable structural invariants. ShapePuri integrates two components: a Shape Encoding Module (SEM) that provides dense geometric guidance through Signed Distance Functions (SDF), and a Global Appearance Debiasing (GAD) module that mitigates appearance bias via stochastic transformations. In our experiments, ShapePuri achieves $84.06\\%$ clean accuracy and $81.64\\%$ robust accuracy under the AutoAttack protocol, representing the first defense framework to surpass the $80\\%$ threshold on this benchmark. Our approach provides a scalable and efficient adversarial defense that preserves prediction stability during inference without requiring auxiliary modules or additional computational cost.",
      "authors": [
        "Zhe Li",
        "Bernhard Kainz"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05T01:07:27+00:00",
      "link": "https://arxiv.org/pdf/2602.05175v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05174v1",
      "title": "Total Variation Rates for Riemannian Flow Matching",
      "abstract": "Riemannian flow matching (RFM) extends flow-based generative modeling to data supported on manifolds by learning a time-dependent tangent vector field whose flow-ODE transports a simple base distribution to the data law. We develop a nonasymptotic Total Variation (TV) convergence analysis for RFM samplers that use a learned vector field together with Euler discretization on manifolds. Our key technical ingredient is a differential inequality governing the evolution of TV between two manifold ODE flows, which expresses the time-derivative of TV through the divergence of the vector-field mismatch and the score of the reference flow; controlling these terms requires establishing new bounds that explicitly account for parallel transport and curvature. Under smoothness assumptions on the population flow-matching field and either uniform (compact manifolds) or mean-square (Hadamard manifolds) approximation guarantees for the learned field, we obtain explicit bounds of the form $\\mathrm{TV}\\le C_{\\mathrm{Lip}}\\,h + C_{\\varepsilon}\\,\\varepsilon$ (with an additional higher-order $\\varepsilon^2$ term on compact manifolds), cleanly separating numerical discretization and learning errors. Here, $h$ is the step-size and $\\varepsilon$ is the target accuracy. Instantiations yield \\emph{explicit} polynomial iteration complexities on the hypersphere $S^d$, and on the SPD$(n)$ manifolds under mild moment conditions.",
      "authors": [
        "Yunrui Guan",
        "Krishnakumar Balasubramanian",
        "Shiqian Ma"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-05T01:06:53+00:00",
      "link": "https://arxiv.org/pdf/2602.05174v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05172v1",
      "title": "Finite-Particle Rates for Regularized Stein Variational Gradient Descent",
      "abstract": "We derive finite-particle rates for the regularized Stein variational gradient descent (R-SVGD) algorithm introduced by He et al. (2024) that corrects the constant-order bias of the SVGD by applying a resolvent-type preconditioner to the kernelized Wasserstein gradient. For the resulting interacting $N$-particle system, we establish explicit non-asymptotic bounds for time-averaged (annealed) empirical measures, illustrating convergence in the \\emph{true} (non-kernelized) Fisher information and, under a $\\mathrm{W}_1\\mathrm{I}$ condition on the target, corresponding $\\mathrm{W}_1$ convergence for a large class of smooth kernels. Our analysis covers both continuous- and discrete-time dynamics and yields principled tuning rules for the regularization parameter, step size, and averaging horizon that quantify the trade-off between approximating the Wasserstein gradient flow and controlling finite-particle estimation error.",
      "authors": [
        "Ye He",
        "Krishnakumar Balasubramanian",
        "Sayan Banerjee",
        "Promit Ghosal"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-05T01:00:00+00:00",
      "link": "https://arxiv.org/pdf/2602.05172v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05167v1",
      "title": "Path Sampling for Rare Events Boosted by Machine Learning",
      "abstract": "The study by Jung et al. (Jung H, Covino R, Arjun A, et al., Nat Comput Sci. 3:334-345 (2023)) introduced Artificial Intelligence for Molecular Mechanism Discovery (AIMMD), a novel sampling algorithm that integrates machine learning to enhance the efficiency of transition path sampling (TPS). By enabling on-the-fly estimation of the committor probability and simultaneously deriving a human-interpretable reaction coordinate, AIMMD offers a robust framework for elucidating the mechanistic pathways of complex molecular processes. This commentary provides a discussion and critical analysis of the core AIMMD framework, explores its recent extensions, and offers an assessment of the method's potential impact and limitations.",
      "authors": [
        "Porhouy Minh",
        "Sapna Sarupria"
      ],
      "primary_category": "physics.comp-ph",
      "categories": [
        "physics.comp-ph",
        "cond-mat.soft",
        "cond-mat.stat-mech",
        "cs.LG",
        "physics.chem-ph"
      ],
      "published": "2026-02-05T00:34:02+00:00",
      "link": "https://arxiv.org/pdf/2602.05167v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05162v1",
      "title": "SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition",
      "abstract": "Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.",
      "authors": [
        "Anay Majee",
        "Rishabh Iyer"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-05T00:28:46+00:00",
      "link": "https://arxiv.org/pdf/2602.05162v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05152v1",
      "title": "RAG without Forgetting: Continual Query-Infused Key Memory",
      "abstract": "Retrieval-augmented generation (RAG) systems commonly improve robustness via query-time adaptations such as query expansion and iterative retrieval. While effective, these approaches are inherently stateless: adaptations are recomputed for each query and discarded thereafter, precluding cumulative learning and repeatedly incurring inference-time cost. Index-side approaches like key expansion introduce persistence but rely on offline preprocessing or heuristic updates that are weakly aligned with downstream task utility, leading to semantic drift and noise accumulation. We propose Evolving Retrieval Memory (ERM), a training-free framework that transforms transient query-time gains into persistent retrieval improvements. ERM updates the retrieval index through correctness-gated feedback, selectively attributes atomic expansion signals to the document keys they benefit, and progressively evolves keys via stable, norm-bounded updates. We show that query and key expansion are theoretically equivalent under standard similarity functions and prove convergence of ERM's selective updates, amortizing optimal query expansion into a stable index with zero inference-time overhead. Experiments on BEIR and BRIGHT across 13 domains demonstrate consistent gains in retrieval and generation, particularly on reasoning-intensive tasks, at native retrieval speed.",
      "authors": [
        "Yuntong Hu",
        "Sha Li",
        "Naren Ramakrishnan",
        "Liang Zhao"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-05T00:12:45+00:00",
      "link": "https://arxiv.org/pdf/2602.05152v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05150v1",
      "title": "GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek",
      "abstract": "Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.",
      "authors": [
        "Yang Zhang",
        "Mersin Konomi",
        "Christos Xypolopoulos",
        "Konstantinos Divriotis",
        "Konstantinos Skianis",
        "Giannis Nikolentzos",
        "Giorgos Stamou",
        "Guokan Shang",
        "Michalis Vazirgiannis"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-05T00:12:18+00:00",
      "link": "https://arxiv.org/pdf/2602.05150v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05148v1",
      "title": "CoSA: Compressed Sensing-Based Adaptation of Large Language Models",
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.",
      "authors": [
        "Songtao Wei",
        "Yi Li",
        "Bohan Zhang",
        "Zhichun Guo",
        "Ying Huang",
        "Yuede Ji",
        "Miao Yin",
        "Guanpeng Li",
        "Bingzhe Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T00:11:43+00:00",
      "link": "https://arxiv.org/pdf/2602.05148v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05146v1",
      "title": "Cross-talk based multi-task learning for fault classification of physically coupled machine system",
      "abstract": "Machine systems inherently generate signals in which fault conditions and various physical variables are physically coupled. Although many existing fault classification studies rely solely on direct fault labels, the aforementioned signals naturally embed additional information shaped by other physically coupled information. Herein, we leverage this coupling through a multi-task learning (MTL) framework that jointly learns fault conditions and the related physical variables. Among MTL architectures, crosstalk structures have distinct advantages because they allow for controlled information exchange between tasks through the cross-talk layer while preventing negative transfer, in contrast to shared trunk architectures that often mix incompatible features. We build on our previously introduced residual neural dimension reductor model, and extend its application to two benchmarks where physical coupling is prominent. The first benchmark is a drone fault dataset, in which machine type and maneuvering direction significantly alter the frequency components of measured signals even under the same nominal condition. By learning fault classification together with these physical attributes, the cross-talk architecture can better classify faults. The second benchmark dataset is the motor compound fault dataset. In this system, each fault component, inner race fault, outer race fault, misalignment, and unbalance is coupled to the other. For motor compound fault, we also test classification performance when we use single-channel data or multi-channel data as input to the classifier. Across both benchmarks, our residual neural dimension reductor, consistently outperformed single-task models, multi-class models that merge all label combinations, and shared trunk multi-task models.",
      "authors": [
        "Wonjun Yi",
        "Rismaya Kumar Mishra",
        "Yong-Hwa Park"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T00:10:16+00:00",
      "link": "https://arxiv.org/pdf/2602.05146v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05145v1",
      "title": "TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference",
      "abstract": "Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptation directly into high-performance LLM inference systems. TIDE reuses target model hidden states generated during inference as training signals, enabling zero-overhead draft adaptation without reloading the target model, and employs adaptive runtime control to activate speculation and training only when beneficial. TIDE exploits heterogeneous clusters by mapping decoupled inference and training to appropriate GPU classes. Across diverse real-world workloads, TIDE achieves up to 1.15x throughput improvement over static speculative decoding while reducing draft training time by 1.67x compared to approaches that recompute training signals.",
      "authors": [
        "Jiyoung Park",
        "Hankyu Jang",
        "Changseok Song",
        "Wookeun Jung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05T00:06:12+00:00",
      "link": "https://arxiv.org/pdf/2602.05145v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05144v1",
      "title": "Fairness Under Group-Conditional Prior Probability Shift: Invariance, Drift, and Target-Aware Post-Processing",
      "abstract": "Machine learning systems are often trained and evaluated for fairness on historical data, yet deployed in environments where conditions have shifted. A particularly common form of shift occurs when the prevalence of positive outcomes changes differently across demographic groups--for example, when disease rates rise faster in one population than another, or when economic conditions affect loan default rates unequally. We study group-conditional prior probability shift (GPPS), where the label prevalence $P(Y=1\\mid A=a)$ may change between training and deployment while the feature-generation process $P(X\\mid Y,A)$ remains stable. Our analysis yields three main contributions. First, we prove a fundamental dichotomy: fairness criteria based on error rates (equalized odds) are structurally invariant under GPPS, while acceptance-rate criteria (demographic parity) can drift--and we prove this drift is unavoidable for non-trivial classifiers (shift-robust impossibility). Second, we show that target-domain risk and fairness metrics are identifiable without target labels: the invariance of ROC quantities under GPPS enables consistent estimation from source labels and unlabeled target data alone, with finite-sample guarantees. Third, we propose TAP-GPPS, a label-free post-processing algorithm that estimates prevalences from unlabeled data, corrects posteriors, and selects thresholds to satisfy demographic parity in the target domain. Experiments validate our theoretical predictions and demonstrate that TAP-GPPS achieves target fairness with minimal utility loss.",
      "authors": [
        "Amir Asiaee",
        "Kaveh Aryan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05T00:01:02+00:00",
      "link": "https://arxiv.org/pdf/2602.05144v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05143v1",
      "title": "HugRAG: Hierarchical Causal Knowledge Graph Design for RAG",
      "abstract": "Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.",
      "authors": [
        "Nengbo Wang",
        "Tuo Liang",
        "Vikash Singh",
        "Chaoda Song",
        "Van Yang",
        "Yu Yin",
        "Jing Ma",
        "Jagdip Singh",
        "Vipin Chaudhary"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2026-02-04T23:59:02+00:00",
      "link": "https://arxiv.org/pdf/2602.05143v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05140v1",
      "title": "Is Innovation Becoming Less Disruptive? An Inventory of the Literature",
      "abstract": "A growing literature has examined whether innovation is becoming less disruptive, spanning diverse domains and data sources and using a range of methodologies. This paper provides an inventory of 105 studies exploring this question. The evidence is largely consistent in direction. Studies spanning scientific papers, patents, products, legal cases, music, and visual art consistently report evidence of a decline. This pattern holds not only for citation-based measures, but also for text-based approaches, firm displacement rates, product similarity networks, and audio and visual embeddings. The literature has also identified notable exceptions, including rebounds in specific domains and predictable variation across field lifecycles. We catalog each study's data, methods, and findings to provide a resource for researchers and policymakers seeking to understand the current state of the evidence.",
      "authors": [
        "Xiangting Wu",
        "Linhui Wu",
        "Michael Park",
        "Erin Leahey",
        "Russell J. Funk"
      ],
      "primary_category": "physics.soc-ph",
      "categories": [
        "physics.soc-ph"
      ],
      "published": "2026-02-04T23:53:09+00:00",
      "link": "https://arxiv.org/pdf/2602.05140v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05139v1",
      "title": "Adaptive Exploration for Latent-State Bandits",
      "abstract": "The multi-armed bandit problem is a core framework for sequential decision-making under uncertainty, but classical algorithms often fail in environments with hidden, time-varying states that confound reward estimation and optimal action selection. We address key challenges arising from unobserved confounders, such as biased reward estimates and limited state information, by introducing a family of state-model-free bandit algorithms that leverage lagged contextual features and coordinated probing strategies. These implicitly track latent states and disambiguate state-dependent reward patterns. Our methods and their adaptive variants can learn optimal policies without explicit state modeling, combining computational efficiency with robust adaptation to non-stationary rewards. Empirical results across diverse settings demonstrate superior performance over classical approaches, and we provide practical recommendations for algorithm selection in real-world applications.",
      "authors": [
        "Jikai Jin",
        "Kenneth Hung",
        "Sanath Kumar Krishnamurthy",
        "Baoyi Shi",
        "Congshan Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T23:49:39+00:00",
      "link": "https://arxiv.org/pdf/2602.05139v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05136v1",
      "title": "Decoupled Orthogonal Dynamics: Regularization for Deep Network Optimizers",
      "abstract": "Is the standard weight decay in AdamW truly optimal? Although AdamW decouples weight decay from adaptive gradient scaling, a fundamental conflict remains: the Radial Tug-of-War. In deep learning, gradients tend to increase parameter norms to expand effective capacity while steering directions to learn features, whereas weight decay indiscriminately suppresses norm growth. This push--pull interaction induces radial oscillations, injecting noise into Adam's second-moment estimates and potentially degrading delicate tangential feature learning. We argue that magnitude and direction play distinct roles and should be decoupled in optimizer dynamics. We propose Orthogonal Dynamics Decoupling and instantiate it as AdamO: an SGD-style update handles the one-dimensional norm control, while Adam's adaptive preconditioning is confined to the tangential subspace. AdamO further incorporates curvature-adaptive radial step sizing and architecture-aware rules and projections for scale-invariant layers and low-dimensional parameters. Experiments on vision and language tasks show that AdamO improves generalization and stability over AdamW without introducing additional complex constraints.",
      "authors": [
        "Hao Chen",
        "Jinghui Yuan",
        "Hanmin Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T23:41:51+00:00",
      "link": "https://arxiv.org/pdf/2602.05136v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05134v1",
      "title": "SemPipes -- Optimizable Semantic Data Operators for Tabular Machine Learning Pipelines",
      "abstract": "Real-world machine learning on tabular data relies on complex data preparation pipelines for prediction, data integration, augmentation, and debugging. Designing these pipelines requires substantial domain expertise and engineering effort, motivating the question of how large language models (LLMs) can support tabular ML through code synthesis. We introduce SemPipes, a novel declarative programming model that integrates LLM-powered semantic data operators into tabular ML pipelines. Semantic operators specify data transformations in natural language while delegating execution to a runtime system. During training, SemPipes synthesizes custom operator implementations based on data characteristics, operator instructions, and pipeline context. This design enables the automatic optimization of data operations in a pipeline via LLM-based code synthesis guided by evolutionary search. We evaluate SemPipes across diverse tabular ML tasks and show that semantic operators substantially improve end-to-end predictive performance for both expert-designed and agent-generated pipelines, while reducing pipeline complexity. We implement SemPipes in Python and release it at https://github.com/deem-data/sempipes/tree/v1.",
      "authors": [
        "Olga Ovcharenko",
        "Matthias Boehm",
        "Sebastian Schelter"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DB"
      ],
      "published": "2026-02-04T23:36:29+00:00",
      "link": "https://arxiv.org/pdf/2602.05134v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05131v1",
      "title": "Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025",
      "abstract": "Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.",
      "authors": [
        "Irene Bonati",
        "Silvina Caino-Lores",
        "Tainã Coleman",
        "Sagar Dolas",
        "Sandro Fiore",
        "Venkatesh Kannan",
        "Marco Verdicchio",
        "Sean R. Wilkinson",
        "Rafael Ferreira da Silva"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-04T23:32:58+00:00",
      "link": "https://arxiv.org/pdf/2602.05131v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05122v1",
      "title": "TestMigrationsInPy: A Dataset of Test Migrations from Unittest to Pytest",
      "abstract": "Unittest and pytest are the most popular testing frameworks in Python. Overall, pytest provides some advantages, including simpler assertion, reuse of fixtures, and interoperability. Due to such benefits, multiple projects in the Python ecosystem have migrated from unittest to pytest. To facilitate the migration, pytest can also run unittest tests, thus, the migration can happen gradually over time. However, the migration can be time-consuming and take a long time to conclude. In this context, projects would benefit from automated solutions to support the migration process. In this paper, we propose TestMigrationsInPy, a dataset of test migrations from unittest to pytest. TestMigrationsInPy contains 923 real-world migrations performed by developers. Future research proposing novel solutions to migrate frameworks in Python can rely on TestMigrationsInPy as a ground truth. Moreover, as TestMigrationsInPy includes information about the migration type (e.g., changes in assertions or fixtures), our dataset enables novel solutions to be verified effectively, for instance, from simpler assertion migrations to more complex fixture migrations. TestMigrationsInPy is publicly available at: https://github.com/altinoalvesjunior/TestMigrationsInPy.",
      "authors": [
        "Altino Alves",
        "Andre Hora"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-04T23:14:17+00:00",
      "link": "https://arxiv.org/pdf/2602.05122v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05120v1",
      "title": "Certifiable Boolean Reasoning Is Universal",
      "abstract": "The proliferation of agentic systems has thrust the reasoning capabilities of AI into the forefront of contemporary machine learning. While it is known that there \\emph{exist} neural networks which can reason through any Boolean task $f:\\{0,1\\}^B\\to\\{0,1\\}$, in the sense that they emulate Boolean circuits with fan-in $2$ and fan-out $1$ gates, trained models have been repeatedly demonstrated to fall short of these theoretical ideals. This raises the question: \\textit{Can one exhibit a deep learning model which \\textbf{certifiably} always reasons and can \\textbf{universally} reason through any Boolean task?} Moreover, such a model should ideally require few parameters to solve simple Boolean tasks.   We answer this question affirmatively by exhibiting a deep learning architecture which parameterizes distributions over Boolean circuits with the guarantee that, for every parameter configuration, a sample is almost surely a valid Boolean circuit (and hence admits an intrinsic circuit-level certificate). We then prove a universality theorem: for any Boolean $f:\\{0,1\\}^B\\to\\{0,1\\}$, there exists a parameter configuration under which the sampled circuit computes $f$ with arbitrarily high probability. When $f$ is an $\\mathcal{O}(\\log B)$-junta, the required number of parameters scales linearly with the input dimension $B$.   Empirically, on a controlled truth-table completion benchmark aligned with our setting, the proposed architecture trains reliably and achieves high exact-match accuracy while preserving the predicted structure: every internal unit is Boolean-valued on $\\{0,1\\}^B$. Matched MLP baselines reach comparable accuracy, but only about $10\\%$ of hidden units admit a Boolean representation; i.e.\\ are two-valued over the Boolean cube.",
      "authors": [
        "Wenhao Li",
        "Anastasis Kratsios",
        "Hrad Ghoukasian",
        "Dennis Zvigelsky"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC",
        "cs.LG"
      ],
      "published": "2026-02-04T23:09:49+00:00",
      "link": "https://arxiv.org/pdf/2602.05120v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05119v1",
      "title": "Unbiased Single-Queried Gradient for Combinatorial Objective",
      "abstract": "In a probabilistic reformulation of a combinatorial problem, we often face an optimization over a hypercube, which corresponds to the Bernoulli probability parameter for each binary variable in the primal problem. The combinatorial nature suggests that an exact gradient computation requires multiple queries. We propose a stochastic gradient that is unbiased and requires only a single query of the combinatorial function. This method encompasses a well-established REINFORCE (through an importance sampling), as well as including a class of new stochastic gradients.",
      "authors": [
        "Thanawat Sornwanee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-04T23:08:07+00:00",
      "link": "https://arxiv.org/pdf/2602.05119v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05114v1",
      "title": "Scalable Generation and Validation of Isomorphic Physics Problems with GenAI",
      "abstract": "Traditional synchronous STEM assessments face growing challenges including accessibility barriers, security concerns from resource-sharing platforms, and limited comparability across institutions. We present a framework for generating and evaluating large-scale isomorphic physics problem banks using Generative AI to enable asynchronous, multi-attempt assessments. Isomorphic problems test identical concepts through varied surface features and contexts, providing richer variation than conventional parameterized questions while maintaining consistent difficulty. Our generation framework employs prompt chaining and tool use to achieve precise control over structural variations (numeric values, spatial relations) alongside diverse contextual variations. For pre-deployment validation, we evaluate generated items using 17 open-source language models (LMs) (0.6B-32B) and compare against actual student performance (N>200) across three midterm exams. Results show that 73% of deployed banks achieve statistically homogeneous difficulty, and LMs pattern correlate strongly with student performance (Pearson's $ρ$ up to 0.594). Additionally, LMs successfully identify problematic variants, such as ambiguous problem texts. Model scale also proves critical for effective validation, where extremely small (<4B) and large (>14B) models exhibit floor and ceiling effects respectively, making mid-sized models optimal for detecting difficulty outliers.",
      "authors": [
        "Naiming Liu",
        "Leo Murch",
        "Spencer Moore",
        "Tong Wan",
        "Shashank Sonkar",
        "Richard Baraniuk",
        "Zhongzhou Chen"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-04T23:01:20+00:00",
      "link": "https://arxiv.org/pdf/2602.05114v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05111v1",
      "title": "Metacognitive Demands and Strategies While Using Off-The-Shelf AI Conversational Agents for Health Information",
      "abstract": "As Artificial Intelligence (AI) conversational agents become widespread, people are increasingly using them for health information seeking. The use of off-the-shelf conversational agents for health information seeking could place high metacognitive demands (the need for extensive monitoring and control of one's own thought process) on individuals, which could compromise their experience of seeking health information. However, currently, the specific demands that arise while using conversational agents for health information seeking, and the strategies people use to cope with those demands, remain unknown. To address these gaps, we conducted a think-aloud study with 15 participants as they sought health information using our off-the-shelf AI conversational agent. We identified the metacognitive demands such systems impose, the strategies people adopt in response, and propose considerations for designing beyond off-the-shelf interfaces to reduce these demands and support better user experiences and affordances in health information seeking.",
      "authors": [
        "Shri Harini Ramesh",
        "Foroozan Daneshzand",
        "Babak Rashidi",
        "Shriti Raj",
        "Hariharan Subramonyam",
        "Fateme Rajabiyazdi"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-04T22:56:42+00:00",
      "link": "https://arxiv.org/pdf/2602.05111v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05107v1",
      "title": "Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text",
      "abstract": "Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.",
      "authors": [
        "Ahmed Ruby",
        "Christian Hardmeier",
        "Sara Stymne"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T22:50:27+00:00",
      "link": "https://arxiv.org/pdf/2602.05107v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05106v1",
      "title": "Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models",
      "abstract": "Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.",
      "authors": [
        "Michael Browder",
        "Kevin Duh",
        "J. David Harris",
        "Vince Lyzinski",
        "Paul McNamee",
        "Youngser Park",
        "Carey E. Priebe",
        "Peter Viechnicki"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-04T22:41:39+00:00",
      "link": "https://arxiv.org/pdf/2602.05106v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05103v1",
      "title": "Learning Nonlinear Continuous-Time Systems for Formal Uncertainty Propagation and Probabilistic Evaluation",
      "abstract": "Nonlinear ordinary differential equations (ODEs) are powerful tools for modeling real-world dynamical systems. However, propagating initial state uncertainty through nonlinear dynamics, especially when the ODE is unknown and learned from data, remains a major challenge. This paper introduces a novel continuum dynamics perspective for model learning that enables formal uncertainty propagation by constructing Taylor series approximations of probabilistic events. We establish sufficient conditions for the soundness of the approach and prove its asymptotic convergence. Empirical results demonstrate the framework's effectiveness, particularly when predicting rare events.",
      "authors": [
        "Peter Amorese",
        "Morteza Lahijanian"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "math.DS"
      ],
      "published": "2026-02-04T22:37:48+00:00",
      "link": "https://arxiv.org/pdf/2602.05103v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05087v1",
      "title": "Autodiscover: A reinforcement learning recommendation system for the cold-start imbalance challenge in active learning, powered by graph-aware thompson sampling",
      "abstract": "Systematic literature reviews (SLRs) are fundamental to evidence-based research, but manual screening is an increasing bottleneck as scientific output grows. Screening features low prevalence of relevant studies and scarce, costly expert decisions. Traditional active learning (AL) systems help, yet typically rely on fixed query strategies for selecting the next unlabeled documents. These static strategies do not adapt over time and ignore the relational structure of scientific literature networks. This thesis introduces AutoDiscover, a framework that reframes AL as an online decision-making problem driven by an adaptive agent. Literature is modeled as a heterogeneous graph capturing relationships among documents, authors, and metadata. A Heterogeneous Graph Attention Network (HAN) learns node representations, which a Discounted Thompson Sampling (DTS) agent uses to dynamically manage a portfolio of query strategies. With real-time human-in-the-loop labels, the agent balances exploration and exploitation under non-stationary review dynamics, where strategy utility changes over time. On the 26-dataset SYNERGY benchmark, AutoDiscover achieves higher screening efficiency than static AL baselines. Crucially, the agent mitigates cold start by bootstrapping discovery from minimal initial labels where static approaches fail. We also introduce TS-Insight, an open-source visual analytics dashboard to interpret, verify, and diagnose the agent's decisions. Together, these contributions accelerate SLR screening under scarce expert labels and low prevalence of relevant studies.",
      "authors": [
        "Parsa Vares"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.IR",
        "cs.SI"
      ],
      "published": "2026-02-04T22:16:50+00:00",
      "link": "https://arxiv.org/pdf/2602.05087v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05085v1",
      "title": "Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories",
      "abstract": "In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.",
      "authors": [
        "Sidi Lu",
        "Zhenwen Liang",
        "Dongyang Ma",
        "Yan Wang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T22:09:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05085v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05084v1",
      "title": "Individual Fairness In Strategic Classification",
      "abstract": "Strategic classification, where individuals modify their features to influence machine learning (ML) decisions, presents critical fairness challenges. While group fairness in this setting has been widely studied, individual fairness remains underexplored. We analyze threshold-based classifiers and prove that deterministic thresholds violate individual fairness. Then, we investigate the possibility of using a randomized classifier to achieve individual fairness. We introduce conditions under which a randomized classifier ensures individual fairness and leverage these conditions to find an optimal and individually fair randomized classifier through a linear programming problem. Additionally, we demonstrate that our approach can be extended to group fairness notions. Experiments on real-world datasets confirm that our method effectively mitigates unfairness and improves the fairness-accuracy trade-off.",
      "authors": [
        "Zhiqun Zuo",
        "Mohammad Mahdi Khalili"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-04T22:09:18+00:00",
      "link": "https://arxiv.org/pdf/2602.05084v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05082v1",
      "title": "Reliable Explanations or Random Noise? A Reliability Metric for XAI",
      "abstract": "In recent years, explaining decisions made by complex machine learning models has become essential in high-stakes domains such as energy systems, healthcare, finance, and autonomous systems. However, the reliability of these explanations, namely, whether they remain stable and consistent under realistic, non-adversarial changes, remains largely unmeasured. Widely used methods such as SHAP and Integrated Gradients (IG) are well-motivated by axiomatic notions of attribution, yet their explanations can vary substantially even under system-level conditions, including small input perturbations, correlated representations, and minor model updates. Such variability undermines explanation reliability, as reliable explanations should remain consistent across equivalent input representations and small, performance-preserving model changes. We introduce the Explanation Reliability Index (ERI), a family of metrics that quantifies explanation stability under four reliability axioms: robustness to small input perturbations, consistency under feature redundancy, smoothness across model evolution, and resilience to mild distributional shifts. For each axiom, we derive formal guarantees, including Lipschitz-type bounds and temporal stability results. We further propose ERI-T, a dedicated measure of temporal reliability for sequential models, and introduce ERI-Bench, a benchmark designed to systematically stress-test explanation reliability across synthetic and real-world datasets. Experimental results reveal widespread reliability failures in popular explanation methods, showing that explanations can be unstable under realistic deployment conditions. By exposing and quantifying these instabilities, ERI enables principled assessment of explanation reliability and supports more trustworthy explainable AI (XAI) systems.",
      "authors": [
        "Poushali Sengupta",
        "Sabita Maharjan",
        "Frank Eliassen",
        "Shashi Raj Pandey",
        "Yan Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-04T22:04:07+00:00",
      "link": "https://arxiv.org/pdf/2602.05082v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05068v1",
      "title": "E-Globe: Scalable $ε$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching",
      "abstract": "Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound (BaB) framework that efficiently tightens both upper and lower bounds until an $ε-$global optimum is reached or early stop is triggered. The key is an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph, so any feasible solution yields a valid counterexample and enables rapid pruning of unsafe subproblems. We further accelerate verification with (i) warm-started NLP solves requiring minimal constraint-matrix updates and (ii) pattern-aligned strong branching that prioritizes splits most effective at tightening relaxations. We also provide conditions under which NLP-CC upper bounds are tight. Experiments on MNIST and CIFAR-10 show markedly tighter upper bounds than PGD across perturbation radii spanning up to three orders of magnitude, fast per-node solves in practice, and substantial end-to-end speedups over MIP-based verification, amplified by warm-starting, GPU batching, and pattern-aligned branching.",
      "authors": [
        "Wenting Li",
        "Saif R. Kazi",
        "Russell Bent",
        "Duo Zhou",
        "Huan Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-04T21:42:29+00:00",
      "link": "https://arxiv.org/pdf/2602.05068v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05065v1",
      "title": "Does SGD Seek Flatness or Sharpness? An Exactly Solvable Model",
      "abstract": "A large body of theory and empirical work hypothesizes a connection between the flatness of a neural network's loss landscape during training and its performance. However, there have been conceptually opposite pieces of evidence regarding when SGD prefers flatter or sharper solutions during training. In this work, we partially but causally clarify the flatness-seeking behavior of SGD by identifying and exactly solving an analytically solvable model that exhibits both flattening and sharpening behavior during training. In this model, the SGD training has no \\textit{a priori} preference for flatness, but only a preference for minimal gradient fluctuations. This leads to the insight that, at least within this model, it is data distribution that uniquely determines the sharpness at convergence, and that a flat minimum is preferred if and only if the noise in the labels is isotropic across all output dimensions. When the noise in the labels is anisotropic, the model instead prefers sharpness and can converge to an arbitrarily sharp solution, depending on the imbalance in the noise in the labels spectrum. We reproduce this key insight in controlled settings with different model architectures such as MLP, RNN, and transformers.",
      "authors": [
        "Yizhou Xu",
        "Pierfrancesco Beneventano",
        "Isaac Chuang",
        "Liu Ziyin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-04T21:36:31+00:00",
      "link": "https://arxiv.org/pdf/2602.05065v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05062v1",
      "title": "Scaling Laws for Embedding Dimension in Information Retrieval",
      "abstract": "Dense retrieval, which encodes queries and documents into a single dense vector, has become the dominant neural retrieval approach due to its simplicity and compatibility with fast approximate nearest neighbor algorithms. As the tasks dense retrieval performs grow in complexity, the fundamental limitations of the underlying data structure and similarity metric -- namely vectors and inner-products -- become more apparent. Prior recent work has shown theoretical limitations inherent to single vectors and inner-products that are generally tied to the embedding dimension. Given the importance of embedding dimension for retrieval capacity, understanding how dense retrieval performance changes as embedding dimension is scaled is fundamental to building next generation retrieval models that balance effectiveness and efficiency. In this work, we conduct a comprehensive analysis of the relationship between embedding dimension and retrieval performance. Our experiments include two model families and a range of model sizes from each to construct a detailed picture of embedding scaling behavior. We find that the scaling behavior fits a power law, allowing us to derive scaling laws for performance given only embedding dimension, as well as a joint law accounting for embedding dimension and model size. Our analysis shows that for evaluation tasks aligned with the training task, performance continues to improve as embedding size increases, though with diminishing returns. For evaluation data that is less aligned with the training task, we find that performance is less predictable, with performance degrading with larger embedding dimensions for certain tasks. We hope our work provides additional insight into the limitations of embeddings and their behavior as well as offers a practical guide for selecting model and embedding dimension to achieve optimal performance with reduced storage and compute costs.",
      "authors": [
        "Julian Killingback",
        "Mahta Rafiee",
        "Madine Manas",
        "Hamed Zamani"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "published": "2026-02-04T21:27:12+00:00",
      "link": "https://arxiv.org/pdf/2602.05062v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05059v1",
      "title": "Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education",
      "abstract": "Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.   The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.   These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.",
      "authors": [
        "Adithya Kulkarni",
        "Mohna Chakraborty",
        "Jay Bagga"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-04T21:20:25+00:00",
      "link": "https://arxiv.org/pdf/2602.05059v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05056v1",
      "title": "VEXA: Evidence-Grounded and Persona-Adaptive Explanations for Scam Risk Sensemaking",
      "abstract": "Online scams across email, short message services, and social media increasingly challenge everyday risk assessment, particularly as generative AI enables more fluent and context-aware deception. Although transformer-based detectors achieve strong predictive performance, their explanations are often opaque to non-experts or misaligned with model decisions. We propose VEXA, an evidence-grounded and persona-adaptive framework for generating learner-facing scam explanations by integrating GradientSHAP-based attribution with theory-informed vulnerability personas. Evaluation across multi-channel datasets shows that grounding explanations in detector-derived evidence improves semantic reliability without increasing linguistic complexity, while persona conditioning introduces interpretable stylistic variation without disrupting evidential alignment. These results reveal a key design insight: evidential grounding governs semantic correctness, whereas persona-based adaptation operates at the level of presentation under constraints of faithfulness. Together, VEXA demonstrates the feasibility of persona-adaptive, evidence-grounded explanations and provides design guidance for trustworthy, learner-facing security explanations in non-formal contexts.",
      "authors": [
        "Heajun An",
        "Connor Ng",
        "Sandesh Sharma Dulal",
        "Junghwan Kim",
        "Jin-Hee Cho"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-04T21:16:24+00:00",
      "link": "https://arxiv.org/pdf/2602.05056v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05052v1",
      "title": "Learning, Solving and Optimizing PDEs with TensorGalerkin: an efficient high-performance Galerkin assembly algorithm",
      "abstract": "We present a unified algorithmic framework for the numerical solution, constrained optimization, and physics-informed learning of PDEs with a variational structure. Our framework is based on a Galerkin discretization of the underlying variational forms, and its high efficiency stems from a novel highly-optimized and GPU-compliant TensorGalerkin framework for linear system assembly (stiffness matrices and load vectors). TensorGalerkin operates by tensorizing element-wise operations within a Python-level Map stage and then performs global reduction with a sparse matrix multiplication that performs message passing on the mesh-induced sparsity graph. It can be seamlessly employed downstream as i) a highly-efficient numerical PDEs solver, ii) an end-to-end differentiable framework for PDE-constrained optimization, and iii) a physics-informed operator learning algorithm for PDEs. With multiple benchmarks, including 2D and 3D elliptic, parabolic, and hyperbolic PDEs on unstructured meshes, we demonstrate that the proposed framework provides significant computational efficiency and accuracy gains over a variety of baselines in all the targeted downstream applications.",
      "authors": [
        "Shizheng Wen",
        "Mingyuan Chi",
        "Tianwei Yu",
        "Ben Moseley",
        "Mike Yan Michelis",
        "Pu Ren",
        "Hao Sun",
        "Siddhartha Mishra"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T21:04:09+00:00",
      "link": "https://arxiv.org/pdf/2602.05052v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.05048v1",
      "title": "MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation",
      "abstract": "Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.",
      "authors": [
        "Zeyu Fang",
        "Tian Lan",
        "Mahdi Imani"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "published": "2026-02-04T20:58:53+00:00",
      "link": "https://arxiv.org/pdf/2602.05048v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05036v1",
      "title": "Feedback Control for Multi-Objective Graph Self-Supervision",
      "abstract": "Can multi-task self-supervised learning on graphs be coordinated without the usual tug-of-war between objectives? Graph self-supervised learning (SSL) offers a growing toolbox of pretext objectives: mutual information, reconstruction, contrastive learning; yet combining them reliably remains a challenge due to objective interference and training instability. Most multi-pretext pipelines use per-update mixing, forcing every parameter update to be a compromise, leading to three failure modes: Disagreement (conflict-induced negative transfer), Drift (nonstationary objective utility), and Drought (hidden starvation of underserved objectives). We argue that coordination is fundamentally a temporal allocation problem: deciding when each objective receives optimization budget, not merely how to weigh them. We introduce ControlG, a control-theoretic framework that recasts multi-objective graph SSL as feedback-controlled temporal allocation by estimating per-objective difficulty and pairwise antagonism, planning target budgets via a Pareto-aware log-hypervolume planner, and scheduling with a Proportional-Integral-Derivative (PID) controller. Across 9 datasets, ControlG consistently outperforms state-of-the-art baselines, while producing an auditable schedule that reveals which objectives drove learning.",
      "authors": [
        "Karish Grover",
        "Theodore Vasiloudis",
        "Han Xie",
        "Sixing Lu",
        "Xiang Song",
        "Christos Faloutsos"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T20:42:10+00:00",
      "link": "https://arxiv.org/pdf/2602.05036v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05035v1",
      "title": "Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation",
      "abstract": "Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.",
      "authors": [
        "Sean Trott",
        "Pamela D. Rivière"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T20:42:08+00:00",
      "link": "https://arxiv.org/pdf/2602.05035v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05033v1",
      "title": "Causal Representation Meets Stochastic Modeling under Generic Geometry",
      "abstract": "Learning meaningful causal representations from observations has emerged as a crucial task for facilitating machine learning applications and driving scientific discoveries in fields such as climate science, biology, and physics. This process involves disentangling high-level latent variables and their causal relationships from low-level observations. Previous work in this area that achieves identifiability typically focuses on cases where the observations are either i.i.d. or follow a latent discrete-time process. Nevertheless, many real-world settings require identifying latent variables that are continuous-time stochastic processes (e.g., multivariate point processes). To this end, we develop identifiable causal representation learning for continuous-time latent stochastic point processes. We study its identifiability by analyzing the geometry of the parameter space. Furthermore, we develop MUTATE, an identifiable variational autoencoder framework with a time-adaptive transition module to infer stochastic dynamics. Across simulated and empirical studies, we find that MUTATE can effectively answer scientific questions, such as the accumulation of mutations in genomics and the mechanisms driving neuron spike triggers in response to time-varying dynamics.",
      "authors": [
        "Jiaxu Ren",
        "Yixin Wang",
        "Biwei Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.AG"
      ],
      "published": "2026-02-04T20:40:53+00:00",
      "link": "https://arxiv.org/pdf/2602.05033v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05032v1",
      "title": "Fast Compute via MC Boosting",
      "abstract": "Modern training and inference pipelines in statistical learning and deep learning repeatedly invoke linear-system solves as inner loops, yet high-accuracy deterministic solvers can be prohibitively expensive when solves must be repeated many times or when only partial information (selected components or linear functionals) is required. We position \\emph{Monte Carlo boosting} as a practical alternative in this regime, surveying random-walk estimators and sequential residual correction in a unified notation (Neumann-series representation, forward/adjoint estimators, and Halton-style sequential correction), with extensions to overdetermined/least-squares problems and connections to IRLS-style updates in data augmentation and EM/ECM algorithms. Empirically, we compare Jacobi and Gauss--Seidel iterations with plain Monte Carlo, exact sequential Monte Carlo, and a subsampled sequential variant, illustrating scaling regimes that motivate when Monte Carlo boosting can be an enabling compute primitive for modern statistical learning workflows.",
      "authors": [
        "Sarah Polson",
        "Vadim Sokolov"
      ],
      "primary_category": "stat.CO",
      "categories": [
        "stat.CO"
      ],
      "published": "2026-02-04T20:36:14+00:00",
      "link": "https://arxiv.org/pdf/2602.05032v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05026v1",
      "title": "Laws of Learning Dynamics and the Core of Learners",
      "abstract": "We formulate the fundamental laws governing learning dynamics, namely the conservation law and the decrease of total entropy. Within this framework, we introduce an entropy-based lifelong ensemble learning method. We evaluate its effectiveness by constructing an immunization mechanism to defend against transfer-based adversarial attacks on the CIFAR-10 dataset. Compared with a naive ensemble formed by simply averaging models specialized on clean and adversarial samples, the resulting logifold achieves higher accuracy in most test cases, with particularly large gains under strong perturbations.",
      "authors": [
        "Inkee Jung",
        "Siu Cheong Lau"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "published": "2026-02-04T20:27:47+00:00",
      "link": "https://arxiv.org/pdf/2602.05026v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05019v1",
      "title": "A Simple Reduction Scheme for Constrained Contextual Bandits with Adversarial Contexts via Regression",
      "abstract": "We study constrained contextual bandits (CCB) with adversarially chosen contexts, where each action yields a random reward and incurs a random cost. We adopt the standard realizability assumption: conditioned on the observed context, rewards and costs are drawn independently from fixed distributions whose expectations belong to known function classes. We consider the continuing setting, in which the algorithm operates over the entire horizon even after the budget is exhausted. In this setting, the objective is to simultaneously control regret and cumulative constraint violation. Building on the seminal SquareCB framework of Foster et al. (2018), we propose a simple and modular algorithmic scheme that leverages online regression oracles to reduce the constrained problem to a standard unconstrained contextual bandit problem with adaptively defined surrogate reward functions. In contrast to most prior work on CCB, which focuses on stochastic contexts, our reduction yields improved guarantees for the more general adversarial context setting, together with a compact and transparent analysis.",
      "authors": [
        "Dhruv Sarkar",
        "Abhishek Sinha"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T20:19:55+00:00",
      "link": "https://arxiv.org/pdf/2602.05019v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05014v1",
      "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
      "abstract": "With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.",
      "authors": [
        "Zhanli Li",
        "Huiwen Tian",
        "Lvzhou Luo",
        "Yixuan Cao",
        "Ping Luo"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-04T20:03:28+00:00",
      "link": "https://arxiv.org/pdf/2602.05014v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05012v1",
      "title": "Private PoEtry: Private In-Context Learning via Product of Experts",
      "abstract": "In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks with only a small set of examples at inference time, thereby avoiding task-specific fine-tuning. However, in-context examples may contain privacy-sensitive information that should not be revealed through model outputs. Existing differential privacy (DP) approaches to ICL are either computationally expensive or rely on heuristics with limited effectiveness, including context oversampling, synthetic data generation, or unnecessary thresholding. We reformulate private ICL through the lens of a Product-of-Experts model. This gives a theoretically grounded framework, and the algorithm can be trivially parallelized. We evaluate our method across five datasets in text classification, math, and vision-language. We find that our method improves accuracy by more than 30 percentage points on average compared to prior DP-ICL methods, while maintaining strong privacy guarantees.",
      "authors": [
        "Rob Romijnders",
        "Mohammad Mahdi Derakhshani",
        "Jonathan Petit",
        "Max Welling",
        "Christos Louizos",
        "Yuki M. Asano"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T19:56:24+00:00",
      "link": "https://arxiv.org/pdf/2602.05012v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.05006v1",
      "title": "Enhanced QKNorm normalization for neural transformers with the Lp norm",
      "abstract": "The normalization of query and key vectors is an essential part of the Transformer architecture. It ensures that learning is stable regardless of the scale of these vectors. Some normalization approaches are available. In this preliminary work, a generalization of the QKNorm normalization scheme is proposed. The approach is based on the Lp norm, allowing non-Euclidean norms to be employed. Experimental results demonstrate the suitability of the method for a simple problem.",
      "authors": [
        "Ezequiel Lopez-Rubio",
        "Javier Montes-Perez",
        "Esteban Jose Palomo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T19:45:39+00:00",
      "link": "https://arxiv.org/pdf/2602.05006v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.05000v1",
      "title": "EntRGi: Entropy Aware Reward Guidance for Diffusion Language Models",
      "abstract": "Reward guidance has been applied to great success in the test-time adaptation of continuous diffusion models; it updates each denoising step using the gradients from a downstream reward model. We study reward guidance for discrete diffusion language models, where one cannot differentiate through the natural outputs of the model because they are discrete tokens. Existing approaches either replace these discrete tokens with continuous relaxations, or employ techniques like the straight-through estimator. In this work, we show the downsides of both these methods. The former degrades gradient feedback because the reward model has never been trained with continuous inputs. The latter involves incorrect optimization because the gradient evaluated at discrete tokens is used to update continuous logits. Our key innovation is to go beyond this tradeoff by introducing a novel mechanism called EntRGi: Entropy aware Reward Guidance that dynamically regulates the gradients from the reward model. By modulating the continuous relaxation using the model's confidence, our approach substantially improves reward guidance while providing reliable inputs to the reward model. We empirically validate our approach on a 7B-parameter diffusion language model across 3 diverse reward models and 3 multi-skill benchmarks, showing consistent improvements over state-of-the-art methods.",
      "authors": [
        "Atula Tejaswi",
        "Litu Rout",
        "Constantine Caramanis",
        "Sanjay Shakkottai",
        "Sujay Sanghavi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T19:37:14+00:00",
      "link": "https://arxiv.org/pdf/2602.05000v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04982v1",
      "title": "BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations",
      "abstract": "With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.",
      "authors": [
        "Deepak Gupta",
        "Davis Bartels",
        "Dina Demner-Fuhsman"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T19:13:43+00:00",
      "link": "https://arxiv.org/pdf/2602.04982v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04972v1",
      "title": "Learning Context Matters: Measuring and Diagnosing Personalization Gaps in LLM-Based Instructional Design",
      "abstract": "The adoption of generative AI in education has accelerated dramatically in recent years, with Large Language Models (LLMs) increasingly integrated into learning environments in the hope of providing personalized support that enhances learner engagement and knowledge retention. However, truly personalized support requires access to meaningful Learning Context (LC) regarding who the learner is, what they are trying to understand, and how they are engaging with the material. In this paper, we present a framework for measuring and diagnosing how the LC influences instructional strategy selection in LLM-based tutoring systems. Using psychometrically grounded synthetic learning contexts and a pedagogically grounded decision space, we compare LLM instructional decisions in context-blind and context-aware conditions and quantify their alignment with the pedagogical judgments of subject matter experts. Our results show that, while providing the LC induces systematic, measurable changes in instructional decisions that move LLM policies closer to the subject matter expert policy, substantial misalignment remains. To diagnose this misalignment, we introduce a relevance-impact analysis that reveals which learner characteristics are attended to, ignored, or spuriously influential in LLM instructional decision-making. This analysis, conducted in collaboration with subject matter experts, demonstrates that LC materially shapes LLM instructional planning but does not reliably induce pedagogically appropriate personalization. Our results enable principled evaluation of context-aware LLM systems and provide a foundation for improving personalization through learner characteristic prioritization, pedagogical model tuning, and LC engineering.",
      "authors": [
        "Johaun Hatchett",
        "Debshila Basu Mallick",
        "Brittany C. Bradford",
        "Richard G. Baraniuk"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-04T19:02:28+00:00",
      "link": "https://arxiv.org/pdf/2602.04972v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04964v1",
      "title": "Classifying white dwarfs from multi-object spectroscopy surveys with machine learning",
      "abstract": "With tens to hundreds of spectra of white dwarfs being taken each night from multi-object spectroscopic surveys, automated spectral classification is essential as part of efficient data processing. In this study, we design a neural network to classify the spectral type of white dwarfs using a combination of spectra from the Dark Energy Spectroscopic Instrument (DESI) data release~1 and imaging from Pan-STARRS photometry. The trained network has a near 100% accuracy at identifying DA and DB white dwarf spectral types, while having an 85-95% accuracy for identifying all other primary types, including metal pollution. Distinct spectral or photometric features map into separate structures when performing a Uniform Manifold Approximation and Projection (UMAP) dimensionality reduction. Investigating further and looking at multiple epoch spectra, we performed a separate search for objects that have strongly changing spectral signatures using UMAP, discovering 3 new inhomogeneous surface composition ('double-faced') white dwarfs in the process. We lastly show how machine learning has the potential to separate single white dwarfs from double white dwarf binary star systems in a large dataset, ideal for isolating a single star population. The results from all of these techniques show a compelling use of machine learning to boost efficiency in analysing white dwarfs observed in multi-object spectroscopy surveys, at times replacing the need for human-driven spectral classifications. This demonstrates our techniques as powerful tools for batch population analyses, finding outliers as a form of rare subclass detection, and in conducting multi-epoch spectral analyses.",
      "authors": [
        "James Munday",
        "Pier-Emmanuel Tremblay",
        "Ingrid Pelisoli",
        "Thomas Killestein",
        "Julia Martikainen",
        "David Jones",
        "Antoine Bédard",
        "Paulina Sowicka"
      ],
      "primary_category": "astro-ph.SR",
      "categories": [
        "astro-ph.SR",
        "astro-ph.GA",
        "astro-ph.IM"
      ],
      "published": "2026-02-04T19:00:05+00:00",
      "link": "https://arxiv.org/pdf/2602.04964v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04884v1",
      "title": "Reinforced Attention Learning",
      "abstract": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.   We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.",
      "authors": [
        "Bangzheng Li",
        "Jianmo Ni",
        "Chen Qu",
        "Ian Miao",
        "Liu Yang",
        "Xingyu Fu",
        "Muhao Chen",
        "Derek Zhiyuan Cheng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-04T18:59:52+00:00",
      "link": "https://arxiv.org/pdf/2602.04884v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04879v1",
      "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.",
      "authors": [
        "Penghui Qi",
        "Xiangxin Zhou",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Min Lin",
        "Wee Sun Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T18:59:04+00:00",
      "link": "https://arxiv.org/pdf/2602.04879v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04872v1",
      "title": "Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning",
      "abstract": "Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood. We introduce a mathematically tractable framework for studying multi-modal learning and explore when transformer-like architectures can recover Bayes-optimal performance in-context. To model multi-modal problems, we assume the observed data arises from a latent factor model. Our first result comprises a negative take on expressibility: we prove that single-layer, linear self-attention fails to recover the Bayes-optimal predictor uniformly over the task distribution. To address this limitation, we introduce a novel, linearized cross-attention mechanism, which we study in the regime where both the number of cross-attention layers and the context length are large. We show that this cross-attention mechanism is provably Bayes optimal when optimized using gradient flow. Our results underscore the benefits of depth for in-context learning and establish the provable utility of cross-attention for multi-modal distributions.",
      "authors": [
        "Nicholas Barnfield",
        "Subhabrata Sen",
        "Pragya Sur"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-04T18:57:30+00:00",
      "link": "https://arxiv.org/pdf/2602.04872v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04864v1",
      "title": "When LLaVA Meets Objects: Token Composition for Vision-Language-Models",
      "abstract": "Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.",
      "authors": [
        "Soumya Jahagirdar",
        "Walid Bousselham",
        "Anna Kukleva",
        "Hilde Kuehne"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-04T18:50:46+00:00",
      "link": "https://arxiv.org/pdf/2602.04864v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04861v1",
      "title": "From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures",
      "abstract": "Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an \"in-the-loop\" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.",
      "authors": [
        "Ryan Liu",
        "Eric Qu",
        "Tobias Kreiman",
        "Samuel M. Blau",
        "Aditi S. Krishnapriyan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.chem-ph"
      ],
      "published": "2026-02-04T18:50:10+00:00",
      "link": "https://arxiv.org/pdf/2602.04861v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04852v1",
      "title": "The Key to State Reduction in Linear Attention: A Rank-based Perspective",
      "abstract": "Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.",
      "authors": [
        "Philipp Nazari",
        "T. Konstantin Rusch"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T18:39:38+00:00",
      "link": "https://arxiv.org/pdf/2602.04852v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04941v1",
      "title": "Improving Set Function Approximation with Quasi-Arithmetic Neural Networks",
      "abstract": "Sets represent a fundamental abstraction across many types of data. To handle the unordered nature of set-structured data, models such as DeepSets and PointNet rely on fixed, non-learnable pooling operations (e.g., sum or max) -- a design choice that can hinder the transferability of learned embeddings and limits model expressivity. More recently, learnable aggregation functions have been proposed as more expressive alternatives. In this work, we advance this line of research by introducing the Neuralized Kolmogorov Mean (NKM) -- a novel, trainable framework for learning a generalized measure of central tendency through an invertible neural function. We further propose quasi-arithmetic neural networks (QUANNs), which incorporate the NKM as a learnable aggregation function. We provide a theoretical analysis showing that, QUANNs are universal approximators for a broad class of common set-function decompositions and, thanks to their invertible neural components, learn more structured latent representations. Empirically, QUANNs outperform state-of-the-art baselines across diverse benchmarks, while learning embeddings that transfer effectively even to tasks that do not involve sets.",
      "authors": [
        "Tomas Tokar",
        "Scott Sanner"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T18:36:31+00:00",
      "link": "https://arxiv.org/pdf/2602.04941v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04843v1",
      "title": "Fluid Representations in Reasoning Models",
      "abstract": "Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.",
      "authors": [
        "Dmitrii Kharlapenko",
        "Alessandro Stolfo",
        "Arthur Conmy",
        "Mrinmaya Sachan",
        "Zhijing Jin"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-04T18:34:50+00:00",
      "link": "https://arxiv.org/pdf/2602.04843v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04841v1",
      "title": "Vivifying LIME: Visual Interactive Testbed for LIME Analysis",
      "abstract": "Explainable Artificial Intelligence (XAI) has gained importance in interpreting model predictions. Among leading techniques for XAI, Local Interpretable Model-agnostic Explanations (LIME) is most frequently utilized as it notably helps people's understanding of complex models. However, LIME's analysis is constrained to a single image at a time. Besides, it lacks interaction mechanisms for observing the LIME's results and direct manipulations of factors affecting the results. To address these issues, we introduce an interactive visualization tool, LIMEVis, which improves the analysis workflow of LIME by enabling users to explore multiple LIME results simultaneously and modify them directly. With LIMEVis, we could conveniently identify common features in images that a model seems to mainly consider for category classification. Additionally, by interactively modifying the LIME results, we could determine which segments in an image influence the model's classification.",
      "authors": [
        "Jeongmin Rhee",
        "Changhee Lee",
        "DongHwa Shin",
        "Bohyoung Kim"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-04T18:32:31+00:00",
      "link": "https://arxiv.org/pdf/2602.04841v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04837v1",
      "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
      "abstract": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.",
      "authors": [
        "Zhaotian Weng",
        "Antonis Antoniades",
        "Deepak Nathani",
        "Zhen Zhang",
        "Xiao Pu",
        "Xin Eric Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-04T18:29:36+00:00",
      "link": "https://arxiv.org/pdf/2602.04837v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04833v1",
      "title": "Continuous eigenvalues of minimal subshifts via S-adic representations",
      "abstract": "We provide characterizations of continuous eigenvalues for minimal symbolic dynamical systems described by $S$-adic structures satisfying natural mild conditions, such as recognizability and primitiveness. Under the additional assumptions of finite alphabet rank or decisiveness of the directive sequence, these characterizations are expressed in terms of associated sequences of local coboundaries. We emphasize the role of combinatorics in the study of continuous eigenvalues through the interplay between coboundaries and extension graphs, and we give several types of sufficient conditions for the nonexistence of trivial letter-coboundaries. As further results, we apply coboundaries in the context of bounded discrepancy, and in particular we obtain a simple characterization of letter-balance for primitive substitutive subshifts. Moreover, we recover a result of Tijdeman on the minimal factor complexity of transitive subshifts with rationally independent letter frequencies. Finally, we use linear-algebraic duality to refine known descriptions of the possible values of eigenvalues in terms of measures of bases.",
      "authors": [
        "Valérie Berthé",
        "Paulina Cecchi-Bernales",
        "Bastián Espinoza"
      ],
      "primary_category": "math.DS",
      "categories": [
        "math.DS"
      ],
      "published": "2026-02-04T18:24:51+00:00",
      "link": "https://arxiv.org/pdf/2602.04833v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04832v1",
      "title": "It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task",
      "abstract": "Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the learning dynamics at the level of individual neurons in single hidden layer ReLU networks. We identify three dynamical principles -- mutual alignment, unlocking and racing -- that together explain why we can often successfully reduce capacity after training through the merging of equivalent neurons or the pruning of low norm weights. We specifically explain the mechanism behind the lottery ticket conjecture, or why the specific, beneficial initial conditions of some neurons lead them to obtain higher weight norms.",
      "authors": [
        "Hannah Pinson"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "published": "2026-02-04T18:22:40+00:00",
      "link": "https://arxiv.org/pdf/2602.04832v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04823v1",
      "title": "Adaptive estimation of Sobolev-type energy functionals on the sphere",
      "abstract": "We study the estimation of quadratic Sobolev-type integral functionals of an unknown density on the unit sphere. The functional is defined through fractional powers of the Laplace--Beltrami operator and provides a global measure of smoothness and spectral energy. Our approach relies on spherical needlet frames, which yield a localized multiscale decomposition while preserving tight frame properties in the natural square-integrable function space on the sphere.   We construct unbiased estimators of suitably truncated versions of the functional and derive sharp oracle risk bounds through an explicit bias--variance analysis. When the smoothness of the density is unknown, we propose a Lepski-type data-driven selection of the resolution level. The resulting adaptive estimator achieves minimax-optimal rates over Sobolev classes, without resorting to nonlinear or sparsity-based methods.",
      "authors": [
        "Claudio Durastanti"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST"
      ],
      "published": "2026-02-04T18:12:20+00:00",
      "link": "https://arxiv.org/pdf/2602.04823v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04816v2",
      "title": "Horizon-LM: A RAM-Centric Architecture for LLM Training",
      "abstract": "The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2$\\times$ higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.",
      "authors": [
        "Zhengqing Yuan",
        "Lichao Sun",
        "Yanfang Ye"
      ],
      "primary_category": "cs.OS",
      "categories": [
        "cs.OS",
        "cs.CL",
        "cs.DC"
      ],
      "published": "2026-02-04T18:04:46+00:00",
      "link": "https://arxiv.org/pdf/2602.04816v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04812v1",
      "title": "Robust Generalizable Heterogeneous Legal Link Prediction",
      "abstract": "Recent work has applied link prediction to large heterogeneous legal citation networks \\new{with rich meta-features}. We find that this approach can be improved by including edge dropout and feature concatenation for the learning of more robust representations, which reduces error rates by up to 45%. We also propose an approach based on multilingual node features with an improved asymmetric decoder for compatibility, which allows us to generalize and extend the prediction to more, geographically and linguistically disjoint, data from New Zealand. Our adaptations also improve inductive transferability between these disjoint legal systems.",
      "authors": [
        "Lorenz Wendlinger",
        "Simon Alexander Nonn",
        "Abdullah Al Zubaer",
        "Michael Granitzer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "published": "2026-02-04T17:59:13+00:00",
      "link": "https://arxiv.org/pdf/2602.04812v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04811v1",
      "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
      "abstract": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.",
      "authors": [
        "Jiarui Yuan",
        "Tailin Jin",
        "Weize Chen",
        "Zeyuan Liu",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-04T17:58:32+00:00",
      "link": "https://arxiv.org/pdf/2602.04811v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04810v1",
      "title": "Game of Coding for Vector-Valued Computations",
      "abstract": "The game of coding is a new framework at the intersection of game theory and coding theory; designed to transcend the fundamental limitations of classical coding theory. While traditional coding theoretic schemes rely on a strict trust assumption, that honest nodes must outnumber adversarial ones to guarantee valid decoding, the game of coding leverages the economic rationality of actors to guarantee correctness and reliable decodability, even in the presence of an adversarial majority. This capability is paramount for emerging permissionless applications, particularly decentralized machine learning (DeML). However, prior investigations into the game of coding have been strictly confined to scalar computations, limiting their applicability to real world tasks where high dimensional data is the norm. In this paper, we bridge this gap by extending the framework to the general $N$-dimensional Euclidean space. We provide a rigorous problem formulation for vector valued computations and fully characterize the equilibrium strategies of the resulting high dimensional game. Our analysis demonstrates that the resilience properties established in the scalar setting are preserved in the vector regime, establishing a theoretical foundation for secure, large scale decentralized computing without honest majority assumptions.",
      "authors": [
        "Hanzaleh Akbari Nodehi",
        "Parsa Moradi",
        "Soheil Mohajer",
        "Mohammad Ali Maddah-Ali"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-02-04T17:55:29+00:00",
      "link": "https://arxiv.org/pdf/2602.04810v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04803v1",
      "title": "Safe-NEureka: a Hybrid Modular Redundant DNN Accelerator for On-board Satellite AI Processing",
      "abstract": "Low Earth Orbit (LEO) constellations are revolutionizing the space sector, with on-board Artificial Intelligence (AI) becoming pivotal for next-generation satellites. AI acceleration is essential for safety-critical functions such as autonomous Guidance, Navigation, and Control (GNC), where errors cannot be tolerated, and performance-critical processing of high-bandwidth sensor data, where occasional errors are tolerable. Consequently, AI accelerators for satellites must combine robust protection against radiation-induced faults with high throughput. This paper presents Safe-NEureka, a Hybrid Modular Redundant Deep Neural Network (DNN) accelerator for heterogeneous RISC-V systems. It operates in two modes: a redundancy mode utilizing Dual Modular Redundancy (DMR) with hardware-based recovery, and a performance mode repurposing redundant datapaths to maximize parallel throughput. Furthermore, its memory interface is protected by Error Correction Codes (ECCs), and the controller by Triple Modular Redundancy (TMR). Implementation in GlobalFoundries 12nm technology shows a 96 reduction in faulty executions in redundancy mode, with a manageable 15 area overhead. In performance mode, the architecture achieves near-baseline speeds on 3x3 dense convolutions with a 5 throughput and 11 efficiency reduction, compared to 48 and 53 in redundancy mode. This flexibility ensures high overheads are limited to critical tasks, establishing Safe-NEureka as a versatile solution for space applications.",
      "authors": [
        "Riccardo Tedeschi",
        "Luigi Ghionda",
        "Alessandro Nadalini",
        "Yvan Tortorella",
        "Arpan Suravi Prasad",
        "Luca Benini",
        "Davide Rossi",
        "Francesco Conti"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-02-04T17:49:45+00:00",
      "link": "https://arxiv.org/pdf/2602.04803v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04802v1",
      "title": "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?",
      "abstract": "Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.",
      "authors": [
        "Qing'an Liu",
        "Juntong Feng",
        "Yuhao Wang",
        "Xinzhe Han",
        "Yujie Cheng",
        "Yue Zhu",
        "Haiwen Diao",
        "Yunzhi Zhuge",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-04T17:48:55+00:00",
      "link": "https://arxiv.org/pdf/2602.04802v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04798v1",
      "title": "Score-Based Change-Point Detection and Region Localization for Spatio-Temporal Point Processes",
      "abstract": "We study sequential change-point detection for spatio-temporal point processes, where actionable detection requires not only identifying when a distributional change occurs but also localizing where it manifests in space. While classical quickest change detection methods provide strong guarantees on detection delay and false-alarm rates, existing approaches for point-process data predominantly focus on temporal changes and do not explicitly infer affected spatial regions. We propose a likelihood-free, score-based detection framework that jointly estimates the change time and the change region in continuous space-time without assuming parametric knowledge of the pre- or post-change dynamics. The method leverages a localized and conditionally weighted Hyvärinen score to quantify event-level deviations from nominal behavior and aggregates these scores using a spatio-temporal CUSUM-type statistic over a prescribed class of spatial regions. Operating sequentially, the procedure outputs both a stopping time and an estimated change region, enabling real-time detection with spatial interpretability. We establish theoretical guarantees on false-alarm control, detection delay, and spatial localization accuracy, and demonstrate the effectiveness of the proposed approach through simulations and real-world spatio-temporal event data.",
      "authors": [
        "Wenbin Zhou",
        "Liyan Xie",
        "Shixiang Zhu"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "published": "2026-02-04T17:44:41+00:00",
      "link": "https://arxiv.org/pdf/2602.04798v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04785v1",
      "title": "Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation",
      "abstract": "While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.",
      "authors": [
        "Congjing Zhang",
        "Ryan Feng Lin",
        "Ruoxuan Bao",
        "Shuai Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-04T17:34:41+00:00",
      "link": "https://arxiv.org/pdf/2602.04785v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04784v1",
      "title": "From independent patches to coordinated attention: Controlling information flow in vision transformers",
      "abstract": "We make the information transmitted by attention an explicit, measurable quantity in vision transformers. By inserting variational information bottlenecks on all attention-mediated writes to the residual stream -- without other architectural changes -- we train models with an explicit information cost and obtain a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, we characterize how classification behavior and information routing evolve across this spectrum, and provide initial insights into how global visual representations emerge from local patch processing by analyzing the first attention heads that transmit information. By biasing learning toward solutions with constrained internal communication, our approach yields models that are more tractable for mechanistic analysis and more amenable to control.",
      "authors": [
        "Kieran A. Murphy"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T17:33:24+00:00",
      "link": "https://arxiv.org/pdf/2602.04784v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04776v1",
      "title": "Speaker-Aware Simulation Improves Conversational Speech Recognition",
      "abstract": "Automatic speech recognition (ASR) for conversational speech remains challenging due to the limited availability of large-scale, well-annotated multi-speaker dialogue data and the complex temporal dynamics of natural interactions. Speaker-aware simulated conversations (SASC) offer an effective data augmentation strategy by transforming single-speaker recordings into realistic multi-speaker dialogues. However, prior work has primarily focused on English data, leaving questions about the applicability to lower-resource languages. In this paper, we adapt and implement the SASC framework for Hungarian conversational ASR. We further propose C-SASC, an extended variant that incorporates pause modeling conditioned on utterance duration, enabling a more faithful representation of local temporal dependencies observed in human conversation while retaining the simplicity and efficiency of the original approach. We generate synthetic Hungarian dialogues from the BEA-Large corpus and combine them with real conversational data for ASR training. Both SASC and C-SASC are evaluated extensively under a wide range of simulation configurations, using conversational statistics derived from CallHome, BEA-Dialogue, and GRASS corpora. Experimental results show that speaker-aware conversational simulation consistently improves recognition performance over naive concatenation-based augmentation. While the additional duration conditioning in C-SASC yields modest but systematic gains--most notably in character-level error rates--its effectiveness depends on the match between source conversational statistics and the target domain. Overall, our findings confirm the robustness of speaker-aware conversational simulation for Hungarian ASR and highlight the benefits and limitations of increasingly detailed temporal modeling in synthetic dialogue generation.",
      "authors": [
        "Máté Gedeon",
        "Péter Mihajlik"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "published": "2026-02-04T17:12:09+00:00",
      "link": "https://arxiv.org/pdf/2602.04776v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04775v1",
      "title": "Interval-Based AUC (iAUC): Extending ROC Analysis to Uncertainty-Aware Classification",
      "abstract": "In high-stakes risk prediction, quantifying uncertainty through interval-valued predictions is essential for reliable decision-making. However, standard evaluation tools like the receiver operating characteristic (ROC) curve and the area under the curve (AUC) are designed for point scores and fail to capture the impact of predictive uncertainty on ranking performance. We propose an uncertainty-aware ROC framework specifically for interval-valued predictions, introducing two new measures: $AUC_L$ and $AUC_U$. This framework enables an informative three-region decomposition of the ROC plane, partitioning pairwise rankings into correct, incorrect, and uncertain orderings. This approach naturally supports selective prediction by allowing models to abstain from ranking cases with overlapping intervals, thereby optimizing the trade-off between abstention rate and discriminative reliability. We prove that under valid class-conditional coverage, $AUC_L$ and $AUC_U$ provide formal lower and upper bounds on the theoretical optimal AUC ($AUC^*$), characterizing the physical limit of achievable discrimination. The proposed framework applies broadly to interval-valued prediction models, regardless of the interval construction method. Experiments on real-world benchmark datasets, using bootstrap-based intervals as one instantiation, validate the framework's correctness and demonstrate its practical utility for uncertainty-aware evaluation and decision-making.",
      "authors": [
        "Yuqi Li",
        "Matthew M. Engelhard"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T17:12:04+00:00",
      "link": "https://arxiv.org/pdf/2602.04775v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04774v1",
      "title": "Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model",
      "abstract": "Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule $η_T^\\star(t)$ where $t$ is the current iterate and $T$ is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay $η_T^\\star(t) \\simeq T^{-ξ} (1-t/T)^δ$ where $ξ$ and $δ$ depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in $T$) initial learning rate and annealing performed over a vanishing (in $T$) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum $β(t)$, where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates $η_T(t) \\sim T^{-ξ}$ (2) optimal power laws $η_T(t) \\sim T^{-ξ} t^{-χ}$, finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups.",
      "authors": [
        "Blake Bordelon",
        "Francesco Mori"
      ],
      "primary_category": "cond-mat.dis-nn",
      "categories": [
        "cond-mat.dis-nn",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-04T17:11:36+00:00",
      "link": "https://arxiv.org/pdf/2602.04774v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04773v1",
      "title": "Scalar machine learning of tensorial quantities -- Born effective charges from monopole models",
      "abstract": "Predicting tensorial properties with machine learning models typically requires carefully designed tensorial descriptors. In this work, we introduce an alternative strategy for learning tensorial quantities based on scalar descriptors. We apply this approach to the Born effective charge tensor, showing that scalar (monopole) kernel models can successfully capture its tensorial nature by exploiting the definition of the Born effective charge tensor as the derivative of the polarisation with respect to atomic displacements. We compare this method with tensorial (dipole) kernel models, as established in our previous work, in which the tensorial structure of the Born effective charge is encoded directly in the kernel and obtained via its derivative. Both approaches are then used for charge partitioning, enabling the separation of monopole and dipole contributions. Finally, we demonstrate the effectiveness of the framework by computing finite-temperature infrared spectra for a range of complex materials.",
      "authors": [
        "Bernhard Schmiedmayer",
        "Angela Rittsteuer",
        "Tobias Hilpert",
        "Georg Kresse"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-02-04T17:09:53+00:00",
      "link": "https://arxiv.org/pdf/2602.04773v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04768v1",
      "title": "Billion-Scale Graph Foundation Models",
      "abstract": "Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fusion (GraphBFF): the first end-to-end recipe for building billion-parameter Graph Foundation Models (GFMs) for arbitrary heterogeneous, billion-scale graphs. Central to the recipe is the GraphBFF Transformer, a flexible and scalable architecture designed for practical billion-scale GFMs. Using the GraphBFF, we present the first neural scaling laws for general graphs and show that loss decreases predictably as either model capacity or training data scales, depending on which factor is the bottleneck. The GraphBFF framework provides concrete methodologies for data batching, pretraining, and fine-tuning for building GFMs at scale. We demonstrate the effectiveness of the framework with an evaluation of a 1.4 billion-parameter GraphBFF Transformer pretrained on one billion samples. Across ten diverse, real-world downstream tasks on graphs unseen during training, spanning node- and link-level classification and regression, GraphBFF achieves remarkable zero-shot and probing performance, including in few-shot settings, with large margins of up to 31 PRAUC points. Finally, we discuss key challenges and open opportunities for making GFMs a practical and principled foundation for graph learning at industrial scale.",
      "authors": [
        "Maya Bechler-Speicher",
        "Yoel Gottlieb",
        "Andrey Isakov",
        "David Abensur",
        "Ami Tavory",
        "Daniel Haimovich",
        "Ido Guy",
        "Udi Weinsberg"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-04T17:03:51+00:00",
      "link": "https://arxiv.org/pdf/2602.04768v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04764v1",
      "title": "Beyond Many-Shot Translation: Scaling In-Context Demonstrations For Low-Resource Machine Translation",
      "abstract": "Building machine translation (MT) systems for low-resource languages is notably difficult due to the scarcity of high-quality data. Although Large Language Models (LLMs) have improved MT system performance, adapting them to lesser-represented languages remains challenging. In-context learning (ICL) may offer novel ways to adapt LLMs for low-resource MT by conditioning models on demonstration at inference time. In this study, we explore scaling low-resource machine translation ICL beyond the few-shot setting to thousands of examples with long-context models. We scale in-context token budget to 1M tokens and compare three types of training corpora used as in-context supervision: monolingual unsupervised data, instruction-style data, and parallel data (English--target and Indonesian--target). Our experiments on Javanese and Sundanese show that gains from additional context saturate quickly and can degrade near the maximum context window, with scaling behavior strongly dependent on corpus type. Notably, some forms of monolingual supervision can be competitive with parallel data, despite the latter offering additional supervision. Overall, our results characterize the effective limits and corpus-type sensitivity of long-context ICL for low-resource MT, highlighting that larger context windows do not necessarily yield proportional quality gains.",
      "authors": [
        "Luis Frentzen Salim",
        "Esteban Carlin",
        "Alexandre Morinvil",
        "Xi Ai",
        "Lun-Wei Ku"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T17:02:22+00:00",
      "link": "https://arxiv.org/pdf/2602.04764v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04761v1",
      "title": "Improved Dimension Dependence for Bandit Convex Optimization with Gradient Variations",
      "abstract": "Gradient-variation online learning has drawn increasing attention due to its deep connections to game theory, optimization, etc. It has been studied extensively in the full-information setting, but is underexplored with bandit feedback. In this work, we focus on gradient variation in Bandit Convex Optimization (BCO) with two-point feedback. By proposing a refined analysis on the non-consecutive gradient variation, a fundamental quantity in gradient variation with bandits, we improve the dimension dependence for both convex and strongly convex functions compared with the best known results (Chiang et al., 2013). Our improved analysis for the non-consecutive gradient variation also implies other favorable problem-dependent guarantees, such as gradient-variance and small-loss regrets. Beyond the two-point setup, we demonstrate the versatility of our technique by achieving the first gradient-variation bound for one-point bandit linear optimization over hyper-rectangular domains. Finally, we validate the effectiveness of our results in more challenging tasks such as dynamic/universal regret minimization and bandit games, establishing the first gradient-variation dynamic and universal regret bounds for two-point BCO and fast convergence rates in bandit games.",
      "authors": [
        "Hang Yu",
        "Yu-Hu Yan",
        "Peng Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-04T16:58:53+00:00",
      "link": "https://arxiv.org/pdf/2602.04761v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04757v1",
      "title": "A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates",
      "abstract": "Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.",
      "authors": [
        "Yuchen Ye",
        "Zixuan Qi",
        "Shixuan Li",
        "Wei Qi",
        "Yanpeng Cai",
        "Chaoxia Yuan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T16:55:43+00:00",
      "link": "https://arxiv.org/pdf/2602.04757v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04755v1",
      "title": "When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?",
      "abstract": "Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\\%$ and $5.80\\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.",
      "authors": [
        "Xinyu Zhou",
        "Chang Jin",
        "Carsten Eickhoff",
        "Zhijiang Guo",
        "Seyed Ali Bahrainian"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-04T16:54:47+00:00",
      "link": "https://arxiv.org/pdf/2602.04755v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04752v1",
      "title": "Decomposing Query-Key Feature Interactions Using Contrastive Covariances",
      "abstract": "Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our method both analytically and empirically in a simplified setting. We then apply our method to large language models to identify human-interpretable QK subspaces for categorical semantic features and binding features. Finally, we demonstrate how attention scores can be attributed to our identified features.",
      "authors": [
        "Andrew Lee",
        "Yonatan Belinkov",
        "Fernanda Viégas",
        "Martin Wattenberg"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T16:50:02+00:00",
      "link": "https://arxiv.org/pdf/2602.04752v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04750v1",
      "title": "Exploiting contextual information to improve stance detection in informal political discourse with LLMs",
      "abstract": "This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from historical posts, can improve classification accuracy. Using a real-world political forum dataset, we generate structured profiles that summarize users' ideological leaning, recurring topics, and linguistic patterns. We evaluate seven state-of-the-art LLMs across baseline and context-enriched setups through a comprehensive cross-model evaluation. Our findings show that contextual prompts significantly boost accuracy, with improvements ranging from +17.5\\% to +38.5\\%, achieving up to 74\\% accuracy that surpasses previous approaches. We also analyze how profile size and post selection strategies affect performance, showing that strategically chosen political content yields better results than larger, randomly selected contexts. These findings underscore the value of incorporating user-level context to enhance LLM performance in nuanced political classification tasks.",
      "authors": [
        "Arman Engin Sucu",
        "Yixiang Zhou",
        "Mario A. Nascimento",
        "Tony Mullen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-04T16:49:26+00:00",
      "link": "https://arxiv.org/pdf/2602.04750v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04736v1",
      "title": "Conditional Counterfactual Mean Embeddings: Doubly Robust Estimation and Learning Rates",
      "abstract": "A complete understanding of heterogeneous treatment effects involves characterizing the full conditional distribution of potential outcomes. To this end, we propose the Conditional Counterfactual Mean Embeddings (CCME), a framework that embeds conditional distributions of counterfactual outcomes into a reproducing kernel Hilbert space (RKHS). Under this framework, we develop a two-stage meta-estimator for CCME that accommodates any RKHS-valued regression in each stage. Based on this meta-estimator, we develop three practical CCME estimators: (1) Ridge Regression estimator, (2) Deep Feature estimator that parameterizes the feature map by a neural network, and (3) Neural-Kernel estimator that performs RKHS-valued regression, with the coefficients parameterized by a neural network. We provide finite-sample convergence rates for all estimators, establishing that they possess the double robustness property. Our experiments demonstrate that our estimators accurately recover distributional features including multimodal structure of conditional counterfactual distributions.",
      "authors": [
        "Thatchanon Anancharoenkij",
        "Donlapark Ponnoprat"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-04T16:40:29+00:00",
      "link": "https://arxiv.org/pdf/2602.04736v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04735v1",
      "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
      "abstract": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.",
      "authors": [
        "Mengru Wang",
        "Zhenqian Xu",
        "Junfeng Fang",
        "Yunzhi Yao",
        "Shumin Deng",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.IR"
      ],
      "published": "2026-02-04T16:37:17+00:00",
      "link": "https://arxiv.org/pdf/2602.04735v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04731v1",
      "title": "Less Finetuning, Better Retrieval: Rethinking LLM Adaptation for Biomedical Retrievers via Synthetic Data and Model Merging",
      "abstract": "Retrieval-augmented generation (RAG) has become the backbone of grounding Large Language Models (LLMs), improving knowledge updates and reducing hallucinations. Recently, LLM-based retriever models have shown state-of-the-art performance for RAG applications. However, several technical aspects remain underexplored on how to adapt general-purpose LLMs into effective domain-specific retrievers, especially in specialized domains such as biomedicine. We present Synthesize-Train-Merge (STM), a modular framework that enhances decoder-only LLMs with synthetic hard negatives, retrieval prompt optimization, and model merging. Experiments on a subset of 12 medical and general tasks from the MTEB benchmark show STM boosts task-specific experts by up to 23.5\\% (average 7.5\\%) and produces merged models that outperform both single experts and strong baselines without extensive pretraining. Our results demonstrate a scalable, efficient path for turning general LLMs into high-performing, domain-specialized retrievers, preserving general-domain capabilities while excelling on specialized tasks.",
      "authors": [
        "Sameh Khattab",
        "Jean-Philippe Corbeil",
        "Osman Alperen Koraş",
        "Amin Dada",
        "Julian Friedrich",
        "François Beaulieu",
        "Paul Vozila",
        "Jens Kleesiek"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-04T16:36:00+00:00",
      "link": "https://arxiv.org/pdf/2602.04731v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04729v1",
      "title": "\"Be My Cheese?\": Cultural Nuance Benchmarking for Machine Translation in Multilingual LLMs",
      "abstract": "We present a large-scale human evaluation benchmark for assessing cultural localisation in machine translation produced by state-of-the-art multilingual large language models (LLMs). Existing MT benchmarks emphasise token-level and grammatical accuracy, but of ten overlook pragmatic and culturally grounded competencies required for real-world localisation. Building on a pilot study of 87 translations across 20 languages, we evaluate 7 multilingual LLMs across 15 target languages with 5 native-speaker raters per language. Raters scored both full-text translations and segment-level instances of culturally nuanced language (idioms, puns, holidays, and culturally embedded concepts) on an ordinal 0-3 quality scale; segment ratings additionally included an NA option for untranslated segments.   Across full-text evaluations, mean overall quality is modest (1.68/3): GPT-5 (2.10/3), Claude Sonnet 3.7 (1.97/3), and Mistral Medium 3.1 (1.84/3) form the strongest tier with fewer catastrophic failures. Segment-level results show sharp category effects: holidays (2.20/3) and cultural concepts (2.19/3) translate substantially better than idioms (1.65/3) and puns (1.45/3), and idioms are most likely to be left untranslated. These findings demonstrate a persistent gap between grammatical adequacy and cultural resonance. To our knowledge, this is the first multilingual, human-annotated benchmark focused explicitly on cultural nuance in translation and localisation, highlighting the need for culturally informed training data, improved cross-lingual pragmatics, and evaluation paradigms that better reflect real-world communicative competence.",
      "authors": [
        "Madison Van Doren",
        "Casey Ford",
        "Jennifer Barajas",
        "Cory Holland"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T16:35:48+00:00",
      "link": "https://arxiv.org/pdf/2602.04729v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04720v1",
      "title": "On Data-Driven Unbiased Predictors using the Koopman Operator",
      "abstract": "The Koopman operator and its data-driven approximations, such as extended dynamic mode decomposition (EDMD), are widely used for analysing, modelling, and controlling nonlinear dynamical systems. However, when the true Koopman eigenfunctions cannot be identified from finite data, multi-step predictions may suffer from structural inaccuracies and systematic bias. To address this issue, we analyse the first and second moments of the multi-step prediction residual. By decomposing the residual into contributions from the one-step approximation error and the propagation of accumulated inaccuracies, we derive a closed-form expression characterising these effects. This analysis enables the development of a novel and computationally efficient algorithm that enforces unbiasedness and reduces variance in the resulting predictor. The proposed method is validated in numerical simulations, showing improved uncertainty properties compared to standard EDMD. These results lay the foundation for uncertainty-aware and unbiased Koopman-based prediction frameworks that can be extended to controlled and stochastic systems.",
      "authors": [
        "Roland Schurig",
        "Pieter van Goor",
        "Karl Worthmann",
        "Rolf Findeisen"
      ],
      "primary_category": "math.DS",
      "categories": [
        "math.DS",
        "eess.SY"
      ],
      "published": "2026-02-04T16:30:41+00:00",
      "link": "https://arxiv.org/pdf/2602.04720v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04718v1",
      "title": "Identifying Intervenable and Interpretable Features via Orthogonality Regularization",
      "abstract": "With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogonality penalty, a desirable property for interpretability. Invoking the $\\textit{Independent Causal Mechanisms}$ principle, we argue that orthogonality promotes modular representations amenable to causal intervention. We empirically show that these increasingly orthogonalized features allow for isolated interventions. Our code is available under $\\texttt{https://github.com/mrtzmllr/sae-icm}$.",
      "authors": [
        "Moritz Miller",
        "Florent Draye",
        "Bernhard Schölkopf"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04T16:29:14+00:00",
      "link": "https://arxiv.org/pdf/2602.04718v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04717v1",
      "title": "Evolutionary Mapping of Neural Networks to Spatial Accelerators",
      "abstract": "Spatial accelerators, composed of arrays of compute-memory integrated units, offer an attractive platform for deploying inference workloads with low latency and low energy consumption. However, fully exploiting their architectural advantages typically requires careful, expert-driven mapping of computational graphs to distributed processing elements. In this work, we automate this process by framing the mapping challenge as a black-box optimization problem. We introduce the first evolutionary, hardware-in-the-loop mapping framework for neuromorphic accelerators, enabling users without deep hardware knowledge to deploy workloads more efficiently. We evaluate our approach on Intel Loihi 2, a representative spatial accelerator featuring 152 cores per chip in a 2D mesh. Our method achieves up to 35% reduction in total latency compared to default heuristics on two sparse multi-layer perceptron networks. Furthermore, we demonstrate the scalability of our approach to multi-chip systems and observe an up to 40% improvement in energy efficiency, without explicitly optimizing for it.",
      "authors": [
        "Alessandro Pierro",
        "Jonathan Timcheck",
        "Jason Yik",
        "Marius Lindauer",
        "Eyke Hüllermeier",
        "Marcel Wever"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-04T16:28:08+00:00",
      "link": "https://arxiv.org/pdf/2602.04717v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04712v1",
      "title": "SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation",
      "abstract": "We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.",
      "authors": [
        "David F. Ramirez",
        "Tim Overman",
        "Kristen Jaskie",
        "Joe Marvin",
        "Andreas Spanias"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "published": "2026-02-04T16:23:16+00:00",
      "link": "https://arxiv.org/pdf/2602.04712v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04711v2",
      "title": "Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention",
      "abstract": "Retrieval Augmented Generation (RAG) is a highly effective paradigm for keeping LLM-based responses up-to-date and reducing the likelihood of hallucinations. Yet, RAG was recently shown to be quite vulnerable to corpus knowledge poisoning: an attacker injects misleading documents to the corpus to steer an LLM's output to an undesired response. We argue that the standard causal attention mechanism in LLMs enables harmful cross-document interactions, specifically in cases of attacks. Accordingly, we introduce a novel defense approach for RAG: Sparse Document Attention RAG (SDAG). This is a block-sparse attention mechanism that disallows cross-attention between retrieved documents. SDAG requires a minimal inference-time change to the attention mask; furthermore, no fine-tuning or additional architectural changes are needed. We present an empirical evaluation of LLM-based question answering (QA) with a variety of attack strategies on RAG. We show that our SDAG method substantially outperforms the standard causal attention mechanism in terms of attack success rate. We further demonstrate the clear merits of integrating SDAG with state-of-the-art RAG defense methods. Specifically, the integration results in performance that is statistically significantly better than the state-of-the-art.",
      "authors": [
        "Sagie Dekel",
        "Moshe Tennenholtz",
        "Oren Kurland"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-04T16:22:20+00:00",
      "link": "https://arxiv.org/pdf/2602.04711v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04938v1",
      "title": "Large Language Models in Software Documentation and Modeling: A Literature Review and Findings",
      "abstract": "Generative artificial intelligence attracts significant attention, especially with the introduction of large language models. Its capabilities are being exploited to solve various software engineering tasks. Thanks to their ability to understand natural language and generate natural language responses, large language models are great for processing various software documentation artifacts. At the same time, large language models excel at understanding structured languages, having the potential for working with software programs and models. We conduct a literature review on the usage of large language models for software engineering tasks related to documentation and modeling. We analyze articles from four major venues in the area, organize them per tasks they solve, and provide an overview of used prompt techniques, metrics, approaches to human-based evaluation, and major datasets.",
      "authors": [
        "Lukas Radosky",
        "Ivan Polasek"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-04T16:21:50+00:00",
      "link": "https://arxiv.org/pdf/2602.04938v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04709v1",
      "title": "Towards Understanding and Avoiding Limitations of Convolutions on Graphs",
      "abstract": "While message-passing neural networks (MPNNs) have shown promising results, their real-world impact remains limited. Although various limitations have been identified, their theoretical foundations remain poorly understood, leading to fragmented research efforts. In this thesis, we provide an in-depth theoretical analysis and identify several key properties limiting the performance of MPNNs. Building on these findings, we propose several frameworks that address these shortcomings. We identify two properties exhibited by many MPNNs: shared component amplification (SCA), where each message-passing iteration amplifies the same components across all feature channels, and component dominance (CD), where a single component gets increasingly amplified as more message-passing steps are applied. These properties lead to the observable phenomenon of rank collapse of node representations, which generalizes the established over-smoothing phenomenon. By generalizing and decomposing over-smoothing, we enable a deeper understanding of MPNNs, more targeted solutions, and more precise communication within the field. To avoid SCA, we show that utilizing multiple computational graphs or edge relations is necessary. Our multi-relational split (MRS) framework transforms any existing MPNN into one that leverages multiple edge relations. Additionally, we introduce the spectral graph convolution for multiple feature channels (MIMO-GC), which naturally uses multiple computational graphs. A localized variant, LMGC, approximates the MIMO-GC while inheriting its beneficial properties. To address CD, we demonstrate a close connection between MPNNs and the PageRank algorithm. Based on personalized PageRank, we propose a variant of MPNNs that allows for infinitely many message-passing iterations, while preserving initial node features. Collectively, these results deepen the theoretical understanding of MPNNs.",
      "authors": [
        "Andreas Roth"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T16:21:18+00:00",
      "link": "https://arxiv.org/pdf/2602.04709v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04707v1",
      "title": "Exact Volumes of Semi-Algebraic Convex Bodies",
      "abstract": "We compute the volumes of convex bodies that are given by inequalities of concave polynomials. These volumes are found to arbitrary precision thanks to the representation of periods by linear differential equations. Our approach rests on work of Lairez, Mezzarobba, and Safey El Din. We present a novel method to identify the relevant critical values. Convexity allows us to reduce the required number of creative telescoping steps by an exponential factor. We provide an implementation based on the ore_algebra package in SageMath. This is applied to a problem in geometric statistics, where the convex body is an intersection of $\\ell_p$-balls.",
      "authors": [
        "Lakshmi Ramesh",
        "Nicolas Weiss"
      ],
      "primary_category": "math.AG",
      "categories": [
        "math.AG",
        "cs.SC"
      ],
      "published": "2026-02-04T16:19:07+00:00",
      "link": "https://arxiv.org/pdf/2602.04707v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04706v1",
      "title": "LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers",
      "abstract": "Tokenization is fundamental to how language models represent and process text, yet the behavior of widely used BPE tokenizers has received far less study than model architectures and training. In this paper, we investigate intermediate merge residues in BPE vocabularies: tokens that are frequent during merge learning so that retained in the final vocabulary, but are mostly further merged and rarely emitted when tokenizing the corpus during tokenizer usage. Such low-frequency tokens not only waste vocabulary capacity but also increase vulnerability to adversarial or atypical inputs. We present a systematic empirical characterization of this phenomenon across commonly used tokenizers and introduce LiteToken, a simple method for removing residue tokens. Because the affected tokens are rarely used, pretrained models can often accommodate the modified tokenizer without additional fine-tuning. Experiments show that LiteToken reduces token fragmentation, reduces parameters, and improves robustness to noisy or misspelled inputs, while preserving overall performance.",
      "authors": [
        "Yike Sun",
        "Haotong Yang",
        "Zhouchen Lin",
        "Muhan Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T16:19:05+00:00",
      "link": "https://arxiv.org/pdf/2602.04706v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04705v1",
      "title": "ERNIE 5.0 Technical Report",
      "abstract": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.",
      "authors": [
        "Haifeng Wang",
        "Hua Wu",
        "Tian Wu",
        "Yu Sun",
        "Jing Liu",
        "Dianhai Yu",
        "Yanjun Ma",
        "Jingzhou He",
        "Zhongjun He",
        "Dou Hong",
        "Qiwen Liu",
        "Shuohuan Wang",
        "Junyuan Shang",
        "Zhenyu Zhang",
        "Yuchen Ding",
        "Jinle Zeng",
        "Jiabin Yang",
        "Liang Shen",
        "Ruibiao Chen",
        "Weichong Yin",
        "Siyu Ding",
        "Dai Dai",
        "Shikun Feng",
        "Siqi Bao",
        "Bolei He",
        "Yan Chen",
        "Zhenyu Jiao",
        "Ruiqing Zhang",
        "Zeyu Chen",
        "Qingqing Dang",
        "Kaipeng Deng",
        "Jiajun Jiang",
        "Enlei Gong",
        "Guoxia Wang",
        "Yanlin Sha",
        "Yi Liu",
        "Yehan Zheng",
        "Weijian Xu",
        "Jiaxiang Liu",
        "Zengfeng Zeng",
        "Yingqi Qu",
        "Zhongli Li",
        "Zhengkun Zhang",
        "Xiyang Wang",
        "Zixiang Xu",
        "Xinchao Xu",
        "Zhengjie Huang",
        "Dong Wang",
        "Bingjin Chen",
        "Yue Chang",
        "Xing Yuan",
        "Shiwei Huang",
        "Qiao Zhao",
        "Xinzhe Ding",
        "Shuangshuang Qiao",
        "Baoshan Yang",
        "Bihong Tang",
        "Bin Li",
        "Bingquan Wang",
        "Binhan Tang",
        "Binxiong Zheng",
        "Bo Cui",
        "Bo Ke",
        "Bo Zhang",
        "Bowen Zhang",
        "Boyan Zhang",
        "Boyang Liu",
        "Caiji Zhang",
        "Can Li",
        "Chang Xu",
        "Chao Pang",
        "Chao Zhang",
        "Chaoyi Yuan",
        "Chen Chen",
        "Cheng Cui",
        "Chenlin Yin",
        "Chun Gan",
        "Chunguang Chai",
        "Chuyu Fang",
        "Cuiyun Han",
        "Dan Zhang",
        "Danlei Feng",
        "Danxiang Zhu",
        "Dong Sun",
        "Dongbo Li",
        "Dongdong Li",
        "Dongdong Liu",
        "Dongxue Liu",
        "Fan Ding",
        "Fan Hu",
        "Fan Li",
        "Fan Mo",
        "Feisheng Wu",
        "Fengwei Liu",
        "Gangqiang Hu",
        "Gaofeng Lu",
        "Gaopeng Yong",
        "Gexiao Tian",
        "Guan Wang",
        "Guangchen Ni",
        "Guangshuo Wu",
        "Guanzhong Wang",
        "Guihua Liu",
        "Guishun Li",
        "Haibin Li",
        "Haijian Liang",
        "Haipeng Ming",
        "Haisu Wang",
        "Haiyang Lu",
        "Haiye Lin",
        "Han Zhou",
        "Hangting Lou",
        "Hanwen Du",
        "Hanzhi Zhang",
        "Hao Chen",
        "Hao Du",
        "Hao Liu",
        "Hao Zhou",
        "Haochen Jiang",
        "Haodong Tian",
        "Haoshuang Wang",
        "Haozhe Geng",
        "Heju Yin",
        "Hong Chen",
        "Hongchen Xue",
        "Hongen Liu",
        "Honggeng Zhang",
        "Hongji Xu",
        "Hongwei Chen",
        "Hongyang Zhang",
        "Hongyuan Zhang",
        "Hua Lu",
        "Huan Chen",
        "Huan Wang",
        "Huang He",
        "Hui Liu",
        "Hui Zhong",
        "Huibin Ruan",
        "Jiafeng Lu",
        "Jiage Liang",
        "Jiahao Hu",
        "Jiahao Hu",
        "Jiajie Yang",
        "Jialin Li",
        "Jian Chen",
        "Jian Wu",
        "Jianfeng Yang",
        "Jianguang Jiang",
        "Jianhua Wang",
        "Jianye Chen",
        "Jiaodi Liu",
        "Jiarui Zhou",
        "Jiawei Lv",
        "Jiaxin Zhou",
        "Jiaxuan Liu",
        "Jie Han",
        "Jie Sun",
        "Jiefan Fang",
        "Jihan Liu",
        "Jihua Liu",
        "Jing Hu",
        "Jing Qian",
        "Jing Yan",
        "Jingdong Du",
        "Jingdong Wang",
        "Jingjing Wu",
        "Jingyong Li",
        "Jinheng Wang",
        "Jinjin Li",
        "Jinliang Lu",
        "Jinlin Yu",
        "Jinnan Liu",
        "Jixiang Feng",
        "Jiyi Huang",
        "Jiyuan Zhang",
        "Jun Liang",
        "Jun Xia",
        "Jun Yu",
        "Junda Chen",
        "Junhao Feng",
        "Junhong Xiang",
        "Junliang Li",
        "Kai Liu",
        "Kailun Chen",
        "Kairan Su",
        "Kang Hu",
        "Kangkang Zhou",
        "Ke Chen",
        "Ke Wei",
        "Kui Huang",
        "Kun Wu",
        "Kunbin Chen",
        "Lei Han",
        "Lei Sun",
        "Lei Wen",
        "Linghui Meng",
        "Linhao Yu",
        "Liping Ouyang",
        "Liwen Zhang",
        "Longbin Ji",
        "Longzhi Wang",
        "Meng Sun",
        "Meng Tian",
        "Mengfei Li",
        "Mengqi Zeng",
        "Mengyu Zhang",
        "Ming Hong",
        "Mingcheng Zhou",
        "Mingming Huang",
        "Mingxin Chen",
        "Mingzhu Cai",
        "Naibin Gu",
        "Nemin Qiu",
        "Nian Wang",
        "Peng Qiu",
        "Peng Zhao",
        "Pengyu Zou",
        "Qi Wang",
        "Qi Xin",
        "Qian Wang",
        "Qiang Zhu",
        "Qianhui Luo",
        "Qianwei Yang",
        "Qianyue He",
        "Qifei Wu",
        "Qinrui Li",
        "Qiwen Bao",
        "Quan Zhang",
        "Quanxiang Liu",
        "Qunyi Xie",
        "Rongrui Zhan",
        "Rufeng Dai",
        "Rui Peng",
        "Ruian Liu",
        "Ruihao Xu",
        "Ruijie Wang",
        "Ruixi Zhang",
        "Ruixuan Liu",
        "Runsheng Shi",
        "Ruting Wang",
        "Senbo Kang",
        "Shan Lu",
        "Shaofei Yu",
        "Shaotian Gong",
        "Shenwei Hu",
        "Shifeng Zheng",
        "Shihao Guo",
        "Shilong Fan",
        "Shiqin Liu",
        "Shiwei Gu",
        "Shixi Zhang",
        "Shuai Yao",
        "Shuang Zhang",
        "Shuangqiao Liu",
        "Shuhao Liang",
        "Shuwei He",
        "Shuwen Yang",
        "Sijun He",
        "Siming Dai",
        "Siming Wu",
        "Siyi Long",
        "Songhe Deng",
        "Suhui Dong",
        "Suyin Liang",
        "Teng Hu",
        "Tianchan Xu",
        "Tianliang Lv",
        "Tianmeng Yang",
        "Tianyi Wei",
        "Tiezhu Gao",
        "Ting Sun",
        "Ting Zhang",
        "Tingdan Luo",
        "Wei He",
        "Wei Luan",
        "Wei Yin",
        "Wei Zhang",
        "Wei Zhou",
        "Weibao Gong",
        "Weibin Li",
        "Weicheng Huang",
        "Weichong Dang",
        "Weiguo Zhu",
        "Weilong Zhang",
        "Weiqi Tan",
        "Wen Huang",
        "Wenbin Chang",
        "Wenjing Du",
        "Wenlong Miao",
        "Wenpei Luo",
        "Wenquan Wu",
        "Xi Shi",
        "Xi Zhao",
        "Xiang Gao",
        "Xiangguo Zhang",
        "Xiangrui Yu",
        "Xiangsen Wang",
        "Xiangzhe Wang",
        "Xianlong Luo",
        "Xianying Ma",
        "Xiao Tan",
        "Xiaocong Lin",
        "Xiaofei Wang",
        "Xiaofeng Peng",
        "Xiaofeng Wu",
        "Xiaojian Xu",
        "Xiaolan Yuan",
        "Xiaopeng Cui",
        "Xiaotian Han",
        "Xiaoxiong Liu",
        "Xiaoxu Fei",
        "Xiaoxuan Wu",
        "Xiaoyu Wang",
        "Xiaoyu Zhang",
        "Xin Sun",
        "Xin Wang",
        "Xinhui Huang",
        "Xinming Zhu",
        "Xintong Yu",
        "Xinyi Xu",
        "Xinyu Wang",
        "Xiuxian Li",
        "XuanShi Zhu",
        "Xue Xu",
        "Xueying Lv",
        "Xuhong Li",
        "Xulong Wei",
        "Xuyi Chen",
        "Yabing Shi",
        "Yafeng Wang",
        "Yamei Li",
        "Yan Liu",
        "Yanfu Cheng",
        "Yang Gao",
        "Yang Liang",
        "Yang Wang",
        "Yang Wang",
        "Yang Yang",
        "Yanlong Liu",
        "Yannian Fu",
        "Yanpeng Wang",
        "Yanzheng Lin",
        "Yao Chen",
        "Yaozong Shen",
        "Yaqian Han",
        "Yehua Yang",
        "Yekun Chai",
        "Yesong Wang",
        "Yi Song",
        "Yichen Zhang",
        "Yifei Wang",
        "Yifeng Guo",
        "Yifeng Kou",
        "Yilong Chen",
        "Yilong Guo",
        "Yiming Wang",
        "Ying Chen",
        "Ying Wang",
        "Yingsheng Wu",
        "Yingzhan Lin",
        "Yinqi Yang",
        "Yiran Xing",
        "Yishu Lei",
        "Yixiang Tu",
        "Yiyan Chen",
        "Yong Zhang",
        "Yonghua Li",
        "Yongqiang Ma",
        "Yongxing Dai",
        "Yongyue Zhang",
        "Yu Ran",
        "Yu Sun",
        "Yu-Wen Michael Zhang",
        "Yuang Liu",
        "Yuanle Liu",
        "Yuanyuan Zhou",
        "Yubo Zhang",
        "Yuchen Han",
        "Yucheng Wang",
        "Yude Gao",
        "Yuedong Luo",
        "Yuehu Dong",
        "Yufeng Hu",
        "Yuhui Cao",
        "Yuhui Yun",
        "Yukun Chen",
        "Yukun Gao",
        "Yukun Li",
        "Yumeng Zhang",
        "Yun Fan",
        "Yun Ma",
        "Yunfei Zhang",
        "Yunshen Xie",
        "Yuping Xu",
        "Yuqin Zhang",
        "Yuqing Liu",
        "Yurui Li",
        "Yuwen Wang",
        "Yuxiang Lu",
        "Zefeng Cai",
        "Zelin Zhao",
        "Zelun Zhang",
        "Zenan Lin",
        "Zezhao Dong",
        "Zhaowu Pan",
        "Zhaoyu Liu",
        "Zhe Dong",
        "Zhe Zhang",
        "Zhen Zhang",
        "Zhengfan Wu",
        "Zhengrui Wei",
        "Zhengsheng Ning",
        "Zhenxing Li",
        "Zhenyu Li",
        "Zhenyu Qian",
        "Zhenyun Li",
        "Zhi Li",
        "Zhichao Chen",
        "Zhicheng Dong",
        "Zhida Feng",
        "Zhifan Feng",
        "Zhihao Deng",
        "Zhijin Yu",
        "Zhiyang Chen",
        "Zhonghui Zheng",
        "Zhuangzhuang Guo",
        "Zhujun Zhang",
        "Zhuo Sun",
        "Zichang Liu",
        "Zihan Lin",
        "Zihao Huang",
        "Zihe Zhu",
        "Ziheng Zhao",
        "Ziping Chen",
        "Zixuan Zhu",
        "Ziyang Xu",
        "Ziyi Liang",
        "Ziyuan Gao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T16:18:15+00:00",
      "link": "https://arxiv.org/pdf/2602.04705v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04699v1",
      "title": "Annotation Free Spacecraft Detection and Segmentation using Vision Language Models",
      "abstract": "Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.",
      "authors": [
        "Samet Hicsonmez",
        "Jose Sosa",
        "Dan Pineau",
        "Inder Pal Singh",
        "Arunkumar Rathinam",
        "Abd El Rahman Shabayek",
        "Djamila Aouada"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-04T16:07:29+00:00",
      "link": "https://arxiv.org/pdf/2602.04699v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04696v1",
      "title": "Beyond Learning on Molecules by Weakly Supervising on Molecules",
      "abstract": "Molecular representations are inherently task-dependent, yet most pre-trained molecular encoders are not. Task conditioning promises representations that reorganize based on task descriptions, but existing approaches rely on expensive labeled data. We show that weak supervision on programmatically derived molecular motifs is sufficient. Our Adaptive Chemical Embedding Model (ACE-Mol) learns from hundreds of motifs paired with natural language descriptors that are cheap to compute, trivial to scale. Conventional encoders slowly search the embedding space for task-relevant structure, whereas ACE-Mol immediately aligns its representations with the task. ACE-Mol achieves state-of-the-art performance across molecular property prediction benchmarks with interpretable, chemically meaningful representations.",
      "authors": [
        "Gordan Prastalo",
        "Kevin Maik Jablonka"
      ],
      "primary_category": "physics.chem-ph",
      "categories": [
        "physics.chem-ph",
        "cs.LG"
      ],
      "published": "2026-02-04T16:03:20+00:00",
      "link": "https://arxiv.org/pdf/2602.04696v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04695v1",
      "title": "Turbulence teaches equivariance to neural networks",
      "abstract": "We investigate how the rotational nature of turbulence affects learned mappings between quantities governed by the Navier-Stokes equations. By varying the degree of anisotropy in a turbulence dataset, we explore how statistical symmetry affects these mappings. To do this, we train super-resolution models at different wall-normal locations in a channel flow, where anisotropy varies naturally, and test their generalization. By evaluating the learned mappings on new coordinate frames and new flow conditions, we find that coordinate-frame generalization is a key part of the generalization problem. Turbulent flows naturally present a wide range of local orientations, so respecting the symmetries of the Navier-Stokes equations improves generalization to new flows. Importantly, turbulence's rotational structure can embed these symmetries into learned mappings -- an effect that strengthens with isotropy and dataset size. This is because a more isotropic dataset samples a wider range of orientations, more fully covering the rotational symmetries of the Navier-Stokes equations. The dependence on isotropy means equivariance error is also scale-dependent, consistent with Kolmogorov's hypothesis. Therefore, turbulence provides its own data augmentation (we term this implicit data augmentation). We expect this effect to apply broadly to learned mappings between tensorial flow quantities, making it relevant to most machine learning applications in turbulence.",
      "authors": [
        "Ryley McConkey",
        "Julia Balla",
        "Jeremiah Bailey",
        "Ali Backour",
        "Elyssa Hofgard",
        "Tommi Jaakkola",
        "Abigail Bodner",
        "Tess Smidt"
      ],
      "primary_category": "physics.flu-dyn",
      "categories": [
        "physics.flu-dyn",
        "physics.comp-ph"
      ],
      "published": "2026-02-04T16:00:13+00:00",
      "link": "https://arxiv.org/pdf/2602.04695v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04693v1",
      "title": "LinGO: A Linguistic Graph Optimization Framework with LLMs for Interpreting Intents of Online Uncivil Discourse",
      "abstract": "Detecting uncivil language is crucial for maintaining safe, inclusive, and democratic online spaces. Yet existing classifiers often misinterpret posts containing uncivil cues but expressing civil intents, leading to inflated estimates of harmful incivility online. We introduce LinGO, a linguistic graph optimization framework for large language models (LLMs) that leverages linguistic structures and optimization techniques to classify multi-class intents of incivility that use various direct and indirect expressions. LinGO decomposes language into multi-step linguistic components, identifies targeted steps that cause the most errors, and iteratively optimizes prompt and/or example components for targeted steps. We evaluate it using a dataset collected during the 2022 Brazilian presidential election, encompassing four forms of political incivility: Impoliteness (IMP), Hate Speech and Stereotyping (HSST), Physical Harm and Violent Political Rhetoric (PHAVPR), and Threats to Democratic Institutions and Values (THREAT). Each instance is annotated with six types of civil/uncivil intent. We benchmark LinGO using three cost-efficient LLMs: GPT-5-mini, Gemini 2.5 Flash-Lite, and Claude 3 Haiku, and four optimization techniques: TextGrad, AdalFlow, DSPy, and Retrieval-Augmented Generation (RAG). The results show that, across all models, LinGO consistently improves accuracy and weighted F1 compared with zero-shot, chain-of-thought, direct optimization, and fine-tuning baselines. RAG is the strongest optimization technique and, when paired with Gemini model, achieves the best overall performance. These findings demonstrate that incorporating multi-step linguistic components into LLM instructions and optimize targeted components can help the models explain complex semantic meanings, which can be extended to other complex semantic explanation tasks in the future.",
      "authors": [
        "Yuan Zhang",
        "Thales Bertaglia"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "published": "2026-02-04T15:56:35+00:00",
      "link": "https://arxiv.org/pdf/2602.04693v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04691v1",
      "title": "Linear Regression: Inference Based on Cluster Estimates",
      "abstract": "This article proposes a novel estimator for regression coefficients in clustered data that explicitly accounts for within-cluster dependence. We study the asymptotic properties of the proposed estimator under both finite and infinite cluster sizes. The analysis is then extended to a standard random coefficient model, where we derive asymptotic results for the average (common) parameters and develop a Wald-type test for general linear hypotheses. We also investigate the performance of the conventional pooled ordinary least squares (POLS) estimator within the random coefficients framework and show that it can be unreliable across a wide range of empirically relevant settings. Furthermore, we introduce a new test for parameter stability at a higher (superblock; Tier 2, Tier 3,...) level, assuming that parameters are stable across clusters within that level. Extensive simulation studies demonstrate the effectiveness of the proposed tests, and an empirical application illustrates their practical relevance.",
      "authors": [
        "Subhodeep Dey",
        "Gopal K. Basak",
        "Samarjit Das"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-04T15:55:58+00:00",
      "link": "https://arxiv.org/pdf/2602.04691v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04690v1",
      "title": "Multi-Source Retrieval and Reasoning for Legal Sentencing Prediction",
      "abstract": "Legal judgment prediction (LJP) aims to predict judicial outcomes from case facts and typically includes law article, charge, and sentencing prediction. While recent methods perform well on the first two subtasks, legal sentencing prediction (LSP) remains difficult due to its need for fine-grained objective knowledge and flexible subjective reasoning. To address these limitations, we propose $MSR^2$, a framework that integrates multi-source retrieval and reasoning in LLMs with reinforcement learning. $MSR^2$ enables LLMs to perform multi-source retrieval based on reasoning needs and applies a process-level reward to guide intermediate subjective reasoning steps. Experiments on two real-world datasets show that $MSR^2$ improves both accuracy and interpretability in LSP, providing a promising step toward practical legal AI. Our code is available at https://anonymous.4open.science/r/MSR2-FC3B.",
      "authors": [
        "Junjie Chen",
        "Haitao Li",
        "Qilei Zhang",
        "Zhenghua Li",
        "Ya Zhang",
        "Quan Zhou",
        "Cheng Luo",
        "Yiqun Liu",
        "Dongsheng Guo",
        "Qingyao Ai"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-04T15:55:55+00:00",
      "link": "https://arxiv.org/pdf/2602.04690v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04689v1",
      "title": "Static and auto-regressive neural emulation of phytoplankton biomass dynamics from physical predictors in the global ocean",
      "abstract": "Phytoplankton is the basis of marine food webs, driving both ecological processes and global biogeochemical cycles. Despite their ecological and climatic significance, accurately simulating phytoplankton dynamics remains a major challenge for biogeochemical numerical models due to limited parameterizations, sparse observational data, and the complexity of oceanic processes. Here, we explore how deep learning models can be used to address these limitations predicting the spatio-temporal distribution of phytoplankton biomass in the global ocean based on satellite observations and environmental conditions. First, we investigate several deep learning architectures. Among the tested models, the UNet architecture stands out for its ability to reproduce the seasonal and interannual patterns of phytoplankton biomass more accurately than other models like CNNs, ConvLSTM, and 4CastNet. When using one to two months of environmental data as input, UNet performs better, although it tends to underestimate the amplitude of low-frequency changes in phytoplankton biomass. Thus, to improve predictions over time, an auto-regressive version of UNet was also tested, where the model uses its own previous predictions to forecast future conditions. This approach works well for short-term forecasts (up to five months), though its performance decreases for longer time scales. Overall, our study shows that combining ocean physical predictors with deep learning allows for reconstruction and short-term prediction of phytoplankton dynamics. These models could become powerful tools for monitoring ocean health and supporting marine ecosystem management, especially in the context of climate change.",
      "authors": [
        "Mahima Lakra",
        "Ronan Fablet",
        "Lucas Drumetz",
        "Etienne Pauthenet",
        "Elodie Martinez"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T15:55:34+00:00",
      "link": "https://arxiv.org/pdf/2602.04689v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04682v1",
      "title": "Covariate Selection for Joint Latent Space Modeling of Sparse Network Data",
      "abstract": "Network data are increasingly common in the social sciences and infectious disease epidemiology. Analyses often link network structure to node-level covariates, but existing methods falter with sparse networks and high-dimensional node features. We propose a joint latent space modeling framework for sparse networks with high-dimensional binary node covariates that performs covariate selection while accounting for uncertainty in estimated latent positions. Building on joint latent space models that couple edges and node variables through shared latent positions, we introduce a group lasso screening step and incorporate a measurement-error-aware stabilization term to mitigate bias from using estimated latent positions as predictors. We establish prediction error rates for the covariate component both when latent positions are treated as observed and when they are estimated with bounded error; under uniform control across $q$ covariates and $n$ nodes, the rate is of order $O(\\log q / n)$ up to an additional term due to latent position estimation error. Our method addresses three challenges: (1) incorporating information from isolated nodes, which are common in sparse networks but often ignored; (2) selecting relevant covariates from high-dimensional spaces; and (3) accounting for uncertainty in estimated latent positions. Simulations show predictive performance remains stable as covariate sparsity grows, while naive approaches degrade. We illustrate how the method can support efficient study design using household social networks from 75 Indian villages, where an emulated pilot study screens a large covariate battery and substantially reduces required subsequent data collection without sacrificing network predictive accuracy.",
      "authors": [
        "Emma G Crenshaw",
        "Yuhua Zhang",
        "Jukka-Pekka Onnela"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-04T15:53:16+00:00",
      "link": "https://arxiv.org/pdf/2602.04682v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04681v1",
      "title": "HFMCA: Orthonormal Feature Learning for EEG-based Brain Decoding",
      "abstract": "Electroencephalography (EEG) analysis is critical for brain-computer interfaces and neuroscience, but the intrinsic noise and high dimensionality of EEG signals hinder effective feature learning. We propose a self-supervised framework based on the Hierarchical Functional Maximal Correlation Algorithm (HFMCA), which learns orthonormal EEG representations by enforcing feature decorrelation and reducing redundancy. This design enables robust capture of essential brain dynamics for various EEG recognition tasks. We validate HFMCA on two benchmark datasets, SEED and BCIC-2A, where pretraining with HFMCA consistently outperforms competitive self-supervised baselines, achieving notable gains in classification accuracy. Across diverse EEG tasks, our method demonstrates superior cross-subject generalization under leave-one-subject-out validation, advancing state-of-the-art by 2.71\\% on SEED emotion recognition and 2.57\\% on BCIC-2A motor imagery classification.",
      "authors": [
        "Yinghao Wang",
        "Lintao Xu",
        "Shujian Yu",
        "Enzo Tartaglione",
        "Van-Tam Nguyen"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-02-04T15:53:03+00:00",
      "link": "https://arxiv.org/pdf/2602.04681v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04677v1",
      "title": "REDistill: Robust Estimator Distillation for Balancing Robustness and Efficiency",
      "abstract": "Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student by aligning their predictive distributions. However, conventional KD formulations - typically based on Kullback-Leibler divergence - assume that the teacher provides reliable soft targets. In practice, teacher predictions are often noisy or overconfident, and existing correction-based approaches rely on ad-hoc heuristics and extensive hyper-parameter tuning, which hinders generalization. We introduce REDistill (Robust Estimator Distillation), a simple yet principled framework grounded in robust statistics. REDistill replaces the standard KD objective with a power divergence loss, a generalization of KL divergence that adaptively downweights unreliable teacher output while preserving informative logit relationships. This formulation provides a unified and interpretable treatment of teacher noise, requires only logits, integrates seamlessly into existing KD pipelines, and incurs negligible computational overhead. Extensive experiments on CIFAR-100 and ImageNet-1k demonstrate that REDistill consistently improves student accuracy in diverse teacher-student architectures. Remarkably, it achieves these gains without model-specific hyper-parameter tuning, underscoring its robustness and strong generalization to unseen teacher-student pairs.",
      "authors": [
        "Ondrej Tybl",
        "Lukas Neumann"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2026-02-04T15:50:53+00:00",
      "link": "https://arxiv.org/pdf/2602.04677v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04675v1",
      "title": "Generalized Schrödinger Bridge on Graphs",
      "abstract": "Transportation on graphs is a fundamental challenge across many domains, where decisions must respect topological and operational constraints. Despite the need for actionable policies, existing graph-transport methods lack this expressivity. They rely on restrictive assumptions, fail to generalize across sparse topologies, and scale poorly with graph size and time horizon. To address these issues, we introduce Generalized Schrödinger Bridge on Graphs (GSBoG), a novel scalable data-driven framework for learning executable controlled continuous-time Markov chain (CTMC) policies on arbitrary graphs under state cost augmented dynamics. Notably, GSBoG learns trajectory-level policies, avoiding dense global solvers and thereby enhancing scalability. This is achieved via a likelihood optimization approach, satisfying the endpoint marginals, while simultaneously optimizing intermediate behavior under state-dependent running costs. Extensive experimentation on challenging real-world graph topologies shows that GSBoG reliably learns accurate, topology-respecting policies while optimizing application-specific intermediate state costs, highlighting its broad applicability and paving new avenues for cost-aware dynamical transport on general graphs.",
      "authors": [
        "Panagiotis Theodoropoulos",
        "Juno Nam",
        "Evangelos Theodorou",
        "Jaemoo Choi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T15:48:09+00:00",
      "link": "https://arxiv.org/pdf/2602.04675v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.04936v1",
      "title": "Deterministic Retrieval at Scale: Optimal-Space LCP Indexing and 308x Energy Reduction on Modern GPUs",
      "abstract": "We study deterministic top-k retrieval under Longest Common Prefix (LCP) similarity for N sequences of length L. We prove a tight Omega(N) space lower bound (cell-probe model) and present a trie-based index using O(N*L) space with O(L+k) query time. We contrast this with pairwise materialization (Theta(N^2)), which hits a practical OOM wall at scale, while our indexed approach remains O(N) in memory. We then introduce Thermal-Aware Logic (TAL), which turns prefix structure into range-bounded scans. In hardware measurements, TAL reduces energy per query by 308x (0.0145 J vs 4.46 J) and cuts p95 latency by 329x (0.114 ms vs 37.5 ms) on a 20M-item range-scan benchmark, while sustaining near-peak utilization (~99%) under long runs. The result is a deterministic retrieval primitive with receipts in regimes where approximate methods are unacceptable.",
      "authors": [
        "Stanislav Byriukov"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "cs.IR"
      ],
      "published": "2026-02-04T15:40:42+00:00",
      "link": "https://arxiv.org/pdf/2602.04936v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04668v1",
      "title": "Estimation of reliability and accuracy of models of $\\varphi$-sub-Gaussian process using generating functions of polynomial expansions",
      "abstract": "Stochastic processes are often represented through orthonormal series expansions, a framework originating in the classical works of Loève and Karhunen and widely used for simulation and numerical approximation. While truncation error in such expansions has been extensively studied, practical models frequently involve an additional source of error arising from the approximation of coefficient functions when closed-form expressions are unavailable. The combined effect of these two errors remains insufficiently addressed in the literature. Building on the author's earlier work on reliability and accuracy estimates for $\\varphi$-sub-Gaussian processes, this paper extends the methodology to orthonormal polynomial systems that do not possess normalized generating functions in analytical form, including the Legendre, generalized Laguerre, and Gegenbauer families. New bounds are derived for models in $L_p(T)$ and $C([0,T])$ that simultaneously account for truncation and coefficient approximation. The resulting criteria provide practical guidance for selecting the number of series terms required to achieve prescribed levels of reliability and accuracy across a broader class of polynomial-based stochastic process models.",
      "authors": [
        "Oleksandr Mokliachuk"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST"
      ],
      "published": "2026-02-04T15:38:20+00:00",
      "link": "https://arxiv.org/pdf/2602.04668v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04659v1",
      "title": "Approaches to Semantic Textual Similarity in Slovak Language: From Algorithms to Transformers",
      "abstract": "Semantic textual similarity (STS) plays a crucial role in many natural language processing tasks. While extensively studied in high-resource languages, STS remains challenging for under-resourced languages such as Slovak. This paper presents a comparative evaluation of sentence-level STS methods applied to Slovak, including traditional algorithms, supervised machine learning models, and third-party deep learning tools. We trained several machine learning models using outputs from traditional algorithms as features, with feature selection and hyperparameter tuning jointly guided by artificial bee colony optimization. Finally, we evaluated several third-party tools, including fine-tuned model by CloudNLP, OpenAI's embedding models, GPT-4 model, and pretrained SlovakBERT model. Our findings highlight the trade-offs between different approaches.",
      "authors": [
        "Lukas Radosky",
        "Miroslav Blstak",
        "Matej Krajcovic",
        "Ivan Polasek"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T15:35:16+00:00",
      "link": "https://arxiv.org/pdf/2602.04659v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04651v1",
      "title": "SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF",
      "abstract": "Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE",
      "authors": [
        "Dipan Maity"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T15:26:44+00:00",
      "link": "https://arxiv.org/pdf/2602.04651v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04649v1",
      "title": "Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models",
      "abstract": "Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.",
      "authors": [
        "Binghai Wang",
        "Yantao Liu",
        "Yuxuan Liu",
        "Tianyi Tang",
        "Shenzhi Wang",
        "Chang Gao",
        "Chujie Zheng",
        "Yichang Zhang",
        "Le Yu",
        "Shixuan Liu",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang",
        "Bowen Yu",
        "Fei Huang",
        "Junyang Lin"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T15:24:52+00:00",
      "link": "https://arxiv.org/pdf/2602.04649v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04643v1",
      "title": "MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction",
      "abstract": "Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.",
      "authors": [
        "Yanan He",
        "Yunshi Wen",
        "Xin Wang",
        "Tengfei Ma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04T15:11:29+00:00",
      "link": "https://arxiv.org/pdf/2602.04643v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04634v1",
      "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
      "abstract": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
      "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Ruize Zhang",
        "Chunyang Zhu",
        "Shi Yu",
        "Weilin Liu",
        "Quanlu Zhang",
        "Wenbo Ding",
        "Chao Yu",
        "Yu Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2026-02-04T15:05:12+00:00",
      "link": "https://arxiv.org/pdf/2602.04634v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.04630v1",
      "title": "Mapping the Web of Science, a large-scale graph and text-based dataset with LLM embeddings",
      "abstract": "Large text data sets, such as publications, websites, and other text-based media, inherit two distinct types of features: (1) the text itself, its information conveyed through semantics, and (2) its relationship to other texts through links, references, or shared attributes. While the latter can be described as a graph structure and can be handled by a range of established algorithms for classification and prediction, the former has recently gained new potential through the use of LLM embedding models. Demonstrating these possibilities and their practicability, we investigate the Web of Science dataset, containing ~56 million scientific publications through the lens of our proposed embedding method, revealing a self-structured landscape of texts.",
      "authors": [
        "Tim Kunt",
        "Annika Buchholz",
        "Imene Khebouri",
        "Thorsten Koch",
        "Ida Litzel",
        "Thi Huong Vu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-04T15:02:32+00:00",
      "link": "https://arxiv.org/pdf/2602.04630v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    }
  ],
  "queries": [
    {
      "type": "keyword",
      "tag": "SR",
      "paper_tag": "keyword:SR",
      "query_text": "Symbolic regression for physics or astronomy applications",
      "sim_scores": {
        "2602.05926v1": {
          "score": 0.7771048545837402,
          "rank": 1
        },
        "2602.05371v1": {
          "score": 0.7710664868354797,
          "rank": 2
        },
        "2602.05225v1": {
          "score": 0.7694491147994995,
          "rank": 3
        },
        "2602.05184v1": {
          "score": 0.7625647783279419,
          "rank": 4
        },
        "2602.05033v1": {
          "score": 0.7610341906547546,
          "rank": 5
        },
        "2602.05704v1": {
          "score": 0.7593419551849365,
          "rank": 6
        },
        "2602.05600v1": {
          "score": 0.7591161727905273,
          "rank": 7
        },
        "2602.05032v1": {
          "score": 0.7576611042022705,
          "rank": 8
        },
        "2602.05936v1": {
          "score": 0.7569407224655151,
          "rank": 9
        },
        "2602.05172v1": {
          "score": 0.7558897733688354,
          "rank": 10
        },
        "2602.05106v1": {
          "score": 0.7558092474937439,
          "rank": 11
        },
        "2602.05134v1": {
          "score": 0.7536845207214355,
          "rank": 12
        },
        "2602.05849v1": {
          "score": 0.7507355809211731,
          "rank": 13
        },
        "2602.05114v1": {
          "score": 0.7507335543632507,
          "rank": 14
        },
        "2602.04936v1": {
          "score": 0.7505954504013062,
          "rank": 15
        },
        "2602.04941v1": {
          "score": 0.7502597570419312,
          "rank": 16
        },
        "2602.05742v1": {
          "score": 0.7497122287750244,
          "rank": 17
        },
        "2602.05639v1": {
          "score": 0.748894453048706,
          "rank": 18
        },
        "2602.04699v1": {
          "score": 0.7482600212097168,
          "rank": 19
        },
        "2602.04668v1": {
          "score": 0.7477113008499146,
          "rank": 20
        },
        "2602.05391v1": {
          "score": 0.7471721172332764,
          "rank": 21
        },
        "2602.05445v1": {
          "score": 0.7470107078552246,
          "rank": 22
        },
        "2602.04823v1": {
          "score": 0.746559202671051,
          "rank": 23
        },
        "2602.05610v1": {
          "score": 0.7461402416229248,
          "rank": 24
        },
        "2602.05816v1": {
          "score": 0.7460066080093384,
          "rank": 25
        },
        "2602.06042v1": {
          "score": 0.7454415559768677,
          "rank": 26
        },
        "2602.05709v1": {
          "score": 0.7449992299079895,
          "rank": 27
        },
        "2602.05873v1": {
          "score": 0.7433955669403076,
          "rank": 28
        },
        "2602.04718v1": {
          "score": 0.7431408166885376,
          "rank": 29
        },
        "2602.05389v1": {
          "score": 0.7431198954582214,
          "rank": 30
        },
        "2602.04643v1": {
          "score": 0.7422053217887878,
          "rank": 31
        },
        "2602.05152v1": {
          "score": 0.7419583201408386,
          "rank": 32
        },
        "2602.05786v1": {
          "score": 0.7419342994689941,
          "rank": 33
        },
        "2602.05266v1": {
          "score": 0.7416415214538574,
          "rank": 34
        },
        "2602.05778v1": {
          "score": 0.7412497401237488,
          "rank": 35
        },
        "2602.04677v1": {
          "score": 0.7412338256835938,
          "rank": 36
        },
        "2602.05396v1": {
          "score": 0.7411651611328125,
          "rank": 37
        },
        "2602.05227v1": {
          "score": 0.7411603927612305,
          "rank": 38
        },
        "2602.04695v1": {
          "score": 0.741081714630127,
          "rank": 39
        },
        "2602.05235v1": {
          "score": 0.7409858703613281,
          "rank": 40
        },
        "2602.05559v1": {
          "score": 0.7397430539131165,
          "rank": 41
        },
        "2602.05657v1": {
          "score": 0.7394828796386719,
          "rank": 42
        },
        "2602.05304v1": {
          "score": 0.7391622066497803,
          "rank": 43
        },
        "2602.05881v1": {
          "score": 0.7390532493591309,
          "rank": 44
        },
        "2602.05143v1": {
          "score": 0.7386798858642578,
          "rank": 45
        },
        "2602.05695v1": {
          "score": 0.7377709746360779,
          "rank": 46
        },
        "2602.05103v1": {
          "score": 0.7375838756561279,
          "rank": 47
        },
        "2602.04803v1": {
          "score": 0.7367931604385376,
          "rank": 48
        },
        "2602.05349v1": {
          "score": 0.7364605665206909,
          "rank": 49
        },
        "2602.05062v1": {
          "score": 0.7361557483673096,
          "rank": 50
        },
        "2602.05725v1": {
          "score": 0.7359639406204224,
          "rank": 51
        },
        "2602.05464v1": {
          "score": 0.735872209072113,
          "rank": 52
        },
        "2602.05539v1": {
          "score": 0.7355292439460754,
          "rank": 53
        },
        "2602.05708v1": {
          "score": 0.7348995208740234,
          "rank": 54
        },
        "2602.05976v1": {
          "score": 0.7347579598426819,
          "rank": 55
        },
        "2602.05145v1": {
          "score": 0.7345423698425293,
          "rank": 56
        },
        "2602.06019v1": {
          "score": 0.7345343232154846,
          "rank": 57
        },
        "2602.04630v1": {
          "score": 0.7344732284545898,
          "rank": 58
        },
        "2602.05616v1": {
          "score": 0.7344052791595459,
          "rank": 59
        },
        "2602.04785v1": {
          "score": 0.7340564727783203,
          "rank": 60
        },
        "2602.04736v1": {
          "score": 0.7338253259658813,
          "rank": 61
        },
        "2602.04707v1": {
          "score": 0.7338058948516846,
          "rank": 62
        },
        "2602.05774v1": {
          "score": 0.7336363196372986,
          "rank": 63
        },
        "2602.05385v1": {
          "score": 0.7335013151168823,
          "rank": 64
        },
        "2602.05887v1": {
          "score": 0.7334628701210022,
          "rank": 65
        },
        "2602.05400v1": {
          "score": 0.732999324798584,
          "rank": 66
        },
        "2602.05483v1": {
          "score": 0.73259437084198,
          "rank": 67
        },
        "2602.05651v1": {
          "score": 0.7323082089424133,
          "rank": 68
        },
        "2602.05068v1": {
          "score": 0.7319614887237549,
          "rank": 69
        },
        "2602.05174v1": {
          "score": 0.73137366771698,
          "rank": 70
        },
        "2602.04720v1": {
          "score": 0.7309818267822266,
          "rank": 71
        },
        "2602.04689v1": {
          "score": 0.7307165861129761,
          "rank": 72
        },
        "2602.05739v1": {
          "score": 0.7306671142578125,
          "rank": 73
        },
        "2602.04964v1": {
          "score": 0.7305736541748047,
          "rank": 74
        },
        "2602.04775v1": {
          "score": 0.7300702333450317,
          "rank": 75
        },
        "2602.05869v1": {
          "score": 0.7295278906822205,
          "rank": 76
        },
        "2602.05119v1": {
          "score": 0.7291879653930664,
          "rank": 77
        },
        "2602.05415v1": {
          "score": 0.729171633720398,
          "rank": 78
        },
        "2602.05242v1": {
          "score": 0.7290812730789185,
          "rank": 79
        },
        "2602.05602v1": {
          "score": 0.7289906144142151,
          "rank": 80
        },
        "2602.04773v1": {
          "score": 0.7288398742675781,
          "rank": 81
        },
        "2602.05712v1": {
          "score": 0.7282094359397888,
          "rank": 82
        },
        "2602.05146v1": {
          "score": 0.7282048463821411,
          "rank": 83
        },
        "2602.05410v1": {
          "score": 0.7281404733657837,
          "rank": 84
        },
        "2602.05470v1": {
          "score": 0.7281365394592285,
          "rank": 85
        },
        "2602.05635v1": {
          "score": 0.7279455661773682,
          "rank": 86
        },
        "2602.05579v1": {
          "score": 0.7278417944908142,
          "rank": 87
        },
        "2602.04798v1": {
          "score": 0.7276372909545898,
          "rank": 88
        },
        "2602.05367v1": {
          "score": 0.7274078130722046,
          "rank": 89
        },
        "2602.05218v1": {
          "score": 0.7273389101028442,
          "rank": 90
        },
        "2602.05052v1": {
          "score": 0.7273369431495667,
          "rank": 91
        },
        "2602.04768v1": {
          "score": 0.7271082401275635,
          "rank": 92
        },
        "2602.05830v1": {
          "score": 0.7269452214241028,
          "rank": 93
        },
        "2602.05298v1": {
          "score": 0.7268542051315308,
          "rank": 94
        },
        "2602.05744v1": {
          "score": 0.7267309427261353,
          "rank": 95
        },
        "2602.04691v1": {
          "score": 0.7266616225242615,
          "rank": 96
        },
        "2602.05944v1": {
          "score": 0.7265650033950806,
          "rank": 97
        },
        "2602.05929v1": {
          "score": 0.7263681888580322,
          "rank": 98
        },
        "2602.05522v1": {
          "score": 0.7263407111167908,
          "rank": 99
        },
        "2602.05452v1": {
          "score": 0.7261768579483032,
          "rank": 100
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR",
      "paper_tag": "keyword:SR",
      "query_text": "Symbolic regression and genetic programming for mathematical equation discovery",
      "sim_scores": {
        "2602.05306v1": {
          "score": 0.754052996635437,
          "rank": 1
        },
        "2602.05371v1": {
          "score": 0.7506925463676453,
          "rank": 2
        },
        "2602.04941v1": {
          "score": 0.7368060946464539,
          "rank": 3
        },
        "2602.05134v1": {
          "score": 0.7349137663841248,
          "rank": 4
        },
        "2602.05103v1": {
          "score": 0.7329062819480896,
          "rank": 5
        },
        "2602.05370v1": {
          "score": 0.7324101328849792,
          "rank": 6
        },
        "2602.05114v1": {
          "score": 0.7286731600761414,
          "rank": 7
        },
        "2602.05539v1": {
          "score": 0.7267853021621704,
          "rank": 8
        },
        "2602.05032v1": {
          "score": 0.7262247204780579,
          "rank": 9
        },
        "2602.05675v1": {
          "score": 0.7257734537124634,
          "rank": 10
        },
        "2602.04690v1": {
          "score": 0.7238675951957703,
          "rank": 11
        },
        "2602.05119v1": {
          "score": 0.7234242558479309,
          "rank": 12
        },
        "2602.05709v1": {
          "score": 0.7205869555473328,
          "rank": 13
        },
        "2602.05688v1": {
          "score": 0.719558835029602,
          "rank": 14
        },
        "2602.05704v1": {
          "score": 0.7185236215591431,
          "rank": 15
        },
        "2602.05616v1": {
          "score": 0.7184932231903076,
          "rank": 16
        },
        "2602.05893v1": {
          "score": 0.7183571457862854,
          "rank": 17
        },
        "2602.05340v1": {
          "score": 0.7180681228637695,
          "rank": 18
        },
        "2602.04761v1": {
          "score": 0.7178358435630798,
          "rank": 19
        },
        "2602.05417v1": {
          "score": 0.7172224521636963,
          "rank": 20
        },
        "2602.05033v1": {
          "score": 0.7171651124954224,
          "rank": 21
        },
        "2602.05172v1": {
          "score": 0.7161679267883301,
          "rank": 22
        },
        "2602.05068v1": {
          "score": 0.7158588767051697,
          "rank": 23
        },
        "2602.04718v1": {
          "score": 0.7154452204704285,
          "rank": 24
        },
        "2602.05887v1": {
          "score": 0.7152427434921265,
          "rank": 25
        },
        "2602.05052v1": {
          "score": 0.7148409485816956,
          "rank": 26
        },
        "2602.05271v1": {
          "score": 0.7140271663665771,
          "rank": 27
        },
        "2602.05531v1": {
          "score": 0.7136093378067017,
          "rank": 28
        },
        "2602.05333v1": {
          "score": 0.7130916714668274,
          "rank": 29
        },
        "2602.05307v1": {
          "score": 0.7127106785774231,
          "rank": 30
        },
        "2602.05391v1": {
          "score": 0.7125847339630127,
          "rank": 31
        },
        "2602.05106v1": {
          "score": 0.7125661373138428,
          "rank": 32
        },
        "2602.05762v1": {
          "score": 0.7123640179634094,
          "rank": 33
        },
        "2602.05464v1": {
          "score": 0.7123229503631592,
          "rank": 34
        },
        "2602.05635v1": {
          "score": 0.7122668623924255,
          "rank": 35
        },
        "2602.05216v1": {
          "score": 0.7116740942001343,
          "rank": 36
        },
        "2602.04810v1": {
          "score": 0.7112293243408203,
          "rank": 37
        },
        "2602.06042v1": {
          "score": 0.7111532688140869,
          "rank": 38
        },
        "2602.05225v1": {
          "score": 0.7111018896102905,
          "rank": 39
        },
        "2602.05019v1": {
          "score": 0.7110528945922852,
          "rank": 40
        },
        "2602.05605v1": {
          "score": 0.7103267312049866,
          "rank": 41
        },
        "2602.05193v1": {
          "score": 0.7099883556365967,
          "rank": 42
        },
        "2602.05622v1": {
          "score": 0.709882915019989,
          "rank": 43
        },
        "2602.05152v1": {
          "score": 0.7098290920257568,
          "rank": 44
        },
        "2602.05445v1": {
          "score": 0.7098183631896973,
          "rank": 45
        },
        "2602.05742v1": {
          "score": 0.7093823552131653,
          "rank": 46
        },
        "2602.05830v1": {
          "score": 0.7092409133911133,
          "rank": 47
        },
        "2602.04677v1": {
          "score": 0.7086817026138306,
          "rank": 48
        },
        "2602.05304v1": {
          "score": 0.7086395025253296,
          "rank": 49
        },
        "2602.05184v1": {
          "score": 0.7086281776428223,
          "rank": 50
        },
        "2602.05890v1": {
          "score": 0.7085669636726379,
          "rank": 51
        },
        "2602.04843v1": {
          "score": 0.7084039449691772,
          "rank": 52
        },
        "2602.05591v1": {
          "score": 0.7081642150878906,
          "rank": 53
        },
        "2602.05036v1": {
          "score": 0.7079483866691589,
          "rank": 54
        },
        "2602.05187v1": {
          "score": 0.7076542973518372,
          "rank": 55
        },
        "2602.04649v1": {
          "score": 0.706896185874939,
          "rank": 56
        },
        "2602.05600v1": {
          "score": 0.7066119909286499,
          "rank": 57
        },
        "2602.05353v1": {
          "score": 0.7064632177352905,
          "rank": 58
        },
        "2602.05489v1": {
          "score": 0.7063987255096436,
          "rank": 59
        },
        "2602.05504v1": {
          "score": 0.7059956192970276,
          "rank": 60
        },
        "2602.05242v1": {
          "score": 0.7058899402618408,
          "rank": 61
        },
        "2602.04675v1": {
          "score": 0.7058880925178528,
          "rank": 62
        },
        "2602.05798v1": {
          "score": 0.7051805257797241,
          "rank": 63
        },
        "2602.04768v1": {
          "score": 0.7050936818122864,
          "rank": 64
        },
        "2602.05684v1": {
          "score": 0.7045265436172485,
          "rank": 65
        },
        "2602.05385v1": {
          "score": 0.704512357711792,
          "rank": 66
        },
        "2602.05410v1": {
          "score": 0.7044429183006287,
          "rank": 67
        },
        "2602.05512v1": {
          "score": 0.7042450904846191,
          "rank": 68
        },
        "2602.05139v1": {
          "score": 0.704198956489563,
          "rank": 69
        },
        "2602.05713v1": {
          "score": 0.704102635383606,
          "rank": 70
        },
        "2602.05084v1": {
          "score": 0.7040549516677856,
          "rank": 71
        },
        "2602.05281v1": {
          "score": 0.7039093971252441,
          "rank": 72
        },
        "2602.05786v1": {
          "score": 0.703691840171814,
          "rank": 73
        },
        "2602.05048v1": {
          "score": 0.7036715149879456,
          "rank": 74
        },
        "2602.04668v1": {
          "score": 0.7035569548606873,
          "rank": 75
        },
        "2602.05341v1": {
          "score": 0.7033741474151611,
          "rank": 76
        },
        "2602.06029v1": {
          "score": 0.7032765746116638,
          "rank": 77
        },
        "2602.04785v1": {
          "score": 0.7031826376914978,
          "rank": 78
        },
        "2602.05977v1": {
          "score": 0.7025731205940247,
          "rank": 79
        },
        "2602.05549v1": {
          "score": 0.702102780342102,
          "rank": 80
        },
        "2602.05472v1": {
          "score": 0.7018181681632996,
          "rank": 81
        },
        "2602.05219v1": {
          "score": 0.7017723321914673,
          "rank": 82
        },
        "2602.05143v1": {
          "score": 0.7014992833137512,
          "rank": 83
        },
        "2602.05400v1": {
          "score": 0.7014012336730957,
          "rank": 84
        },
        "2602.05179v1": {
          "score": 0.701251745223999,
          "rank": 85
        },
        "2602.05993v1": {
          "score": 0.7010067105293274,
          "rank": 86
        },
        "2602.05059v1": {
          "score": 0.7009389996528625,
          "rank": 87
        },
        "2602.04972v1": {
          "score": 0.7009305357933044,
          "rank": 88
        },
        "2602.05848v1": {
          "score": 0.7008287906646729,
          "rank": 89
        },
        "2602.05315v1": {
          "score": 0.7007580995559692,
          "rank": 90
        },
        "2602.05744v1": {
          "score": 0.7003348469734192,
          "rank": 91
        },
        "2602.05974v1": {
          "score": 0.7003097534179688,
          "rank": 92
        },
        "2602.05712v1": {
          "score": 0.7003086805343628,
          "rank": 93
        },
        "2602.05357v1": {
          "score": 0.6999155282974243,
          "rank": 94
        },
        "2602.05523v1": {
          "score": 0.6996877193450928,
          "rank": 95
        },
        "2602.06039v1": {
          "score": 0.6993731260299683,
          "rank": 96
        },
        "2602.05846v1": {
          "score": 0.6992343664169312,
          "rank": 97
        },
        "2602.05323v1": {
          "score": 0.6992245316505432,
          "rank": 98
        },
        "2602.05936v1": {
          "score": 0.6992191076278687,
          "rank": 99
        },
        "2602.05805v1": {
          "score": 0.6991311311721802,
          "rank": 100
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR",
      "paper_tag": "keyword:SR",
      "query_text": "Deep learning and neural network architectures for symbolic regression tasks",
      "sim_scores": {
        "2602.05830v1": {
          "score": 0.7472310662269592,
          "rank": 1
        },
        "2602.04941v1": {
          "score": 0.737247884273529,
          "rank": 2
        },
        "2602.05996v1": {
          "score": 0.7359166741371155,
          "rank": 3
        },
        "2602.05391v1": {
          "score": 0.7308891415596008,
          "rank": 4
        },
        "2602.05704v1": {
          "score": 0.73080974817276,
          "rank": 5
        },
        "2602.05136v1": {
          "score": 0.7304758429527283,
          "rank": 6
        },
        "2602.05639v1": {
          "score": 0.729184091091156,
          "rank": 7
        },
        "2602.05635v1": {
          "score": 0.7291422486305237,
          "rank": 8
        },
        "2602.06042v1": {
          "score": 0.7272545099258423,
          "rank": 9
        },
        "2602.05600v1": {
          "score": 0.7272297143936157,
          "rank": 10
        },
        "2602.05068v1": {
          "score": 0.7260458469390869,
          "rank": 11
        },
        "2602.05426v1": {
          "score": 0.7259379029273987,
          "rank": 12
        },
        "2602.05725v1": {
          "score": 0.7248835563659668,
          "rank": 13
        },
        "2602.05845v1": {
          "score": 0.7247742414474487,
          "rank": 14
        },
        "2602.05032v1": {
          "score": 0.7235110402107239,
          "rank": 15
        },
        "2602.05464v1": {
          "score": 0.7221043109893799,
          "rank": 16
        },
        "2602.05306v1": {
          "score": 0.7220655679702759,
          "rank": 17
        },
        "2602.05873v1": {
          "score": 0.7220168113708496,
          "rank": 18
        },
        "2602.05735v1": {
          "score": 0.7214476466178894,
          "rank": 19
        },
        "2602.04774v1": {
          "score": 0.7211896181106567,
          "rank": 20
        },
        "2602.04736v1": {
          "score": 0.7209946513175964,
          "rank": 21
        },
        "2602.05539v1": {
          "score": 0.7208688259124756,
          "rank": 22
        },
        "2602.05859v1": {
          "score": 0.7207823395729065,
          "rank": 23
        },
        "2602.05371v1": {
          "score": 0.7207678556442261,
          "rank": 24
        },
        "2602.05999v1": {
          "score": 0.7194806337356567,
          "rank": 25
        },
        "2602.05162v1": {
          "score": 0.7191303372383118,
          "rank": 26
        },
        "2602.04718v1": {
          "score": 0.7183036804199219,
          "rank": 27
        },
        "2602.05134v1": {
          "score": 0.7180569171905518,
          "rank": 28
        },
        "2602.05779v1": {
          "score": 0.7175827026367188,
          "rank": 29
        },
        "2602.05571v1": {
          "score": 0.7173740267753601,
          "rank": 30
        },
        "2602.05367v1": {
          "score": 0.7173689007759094,
          "rank": 31
        },
        "2602.05341v1": {
          "score": 0.7149513959884644,
          "rank": 32
        },
        "2602.04864v1": {
          "score": 0.7136228084564209,
          "rank": 33
        },
        "2602.05298v1": {
          "score": 0.712702751159668,
          "rank": 34
        },
        "2602.04872v1": {
          "score": 0.7123964428901672,
          "rank": 35
        },
        "2602.05106v1": {
          "score": 0.7123491168022156,
          "rank": 36
        },
        "2602.05713v1": {
          "score": 0.7122504711151123,
          "rank": 37
        },
        "2602.05649v1": {
          "score": 0.7122296094894409,
          "rank": 38
        },
        "2602.05737v1": {
          "score": 0.7122129797935486,
          "rank": 39
        },
        "2602.05019v1": {
          "score": 0.7120343446731567,
          "rank": 40
        },
        "2602.05175v1": {
          "score": 0.7118813395500183,
          "rank": 41
        },
        "2602.06025v1": {
          "score": 0.7117869257926941,
          "rank": 42
        },
        "2602.04768v1": {
          "score": 0.7117812037467957,
          "rank": 43
        },
        "2602.06043v1": {
          "score": 0.7116279602050781,
          "rank": 44
        },
        "2602.04709v1": {
          "score": 0.7103229761123657,
          "rank": 45
        },
        "2602.05375v1": {
          "score": 0.7097774147987366,
          "rank": 46
        },
        "2602.05605v1": {
          "score": 0.7093061804771423,
          "rank": 47
        },
        "2602.05950v1": {
          "score": 0.7082587480545044,
          "rank": 48
        },
        "2602.05846v1": {
          "score": 0.7082067131996155,
          "rank": 49
        },
        "2602.05695v1": {
          "score": 0.7064787149429321,
          "rank": 50
        },
        "2602.05187v1": {
          "score": 0.706358790397644,
          "rank": 51
        },
        "2602.06019v1": {
          "score": 0.7061265707015991,
          "rank": 52
        },
        "2602.05085v1": {
          "score": 0.7057324051856995,
          "rank": 53
        },
        "2602.05202v1": {
          "score": 0.7049919366836548,
          "rank": 54
        },
        "2602.05146v1": {
          "score": 0.704973042011261,
          "rank": 55
        },
        "2602.05048v1": {
          "score": 0.7049269676208496,
          "rank": 56
        },
        "2602.05214v1": {
          "score": 0.7042933702468872,
          "rank": 57
        },
        "2602.04816v2": {
          "score": 0.7041150331497192,
          "rank": 58
        },
        "2602.04884v1": {
          "score": 0.7036244869232178,
          "rank": 59
        },
        "2602.05184v1": {
          "score": 0.7034252882003784,
          "rank": 60
        },
        "2602.04852v1": {
          "score": 0.7033438682556152,
          "rank": 61
        },
        "2602.05598v1": {
          "score": 0.7032856345176697,
          "rank": 62
        },
        "2602.05709v1": {
          "score": 0.7028144598007202,
          "rank": 63
        },
        "2602.05616v1": {
          "score": 0.7026059627532959,
          "rank": 64
        },
        "2602.05890v1": {
          "score": 0.7022842764854431,
          "rank": 65
        },
        "2602.05358v1": {
          "score": 0.7019656896591187,
          "rank": 66
        },
        "2602.05787v1": {
          "score": 0.701960563659668,
          "rank": 67
        },
        "2602.05006v1": {
          "score": 0.701633095741272,
          "rank": 68
        },
        "2602.05454v1": {
          "score": 0.7014001607894897,
          "rank": 69
        },
        "2602.05805v1": {
          "score": 0.7010674476623535,
          "rank": 70
        },
        "2602.04649v1": {
          "score": 0.70039302110672,
          "rank": 71
        },
        "2602.05172v1": {
          "score": 0.6999988555908203,
          "rank": 72
        },
        "2602.05262v1": {
          "score": 0.699941873550415,
          "rank": 73
        },
        "2602.05630v1": {
          "score": 0.6996136903762817,
          "rank": 74
        },
        "2602.04735v1": {
          "score": 0.6991308927536011,
          "rank": 75
        },
        "2602.05333v1": {
          "score": 0.6988391876220703,
          "rank": 76
        },
        "2602.05729v1": {
          "score": 0.698706865310669,
          "rank": 77
        },
        "2602.05988v1": {
          "score": 0.6987043023109436,
          "rank": 78
        },
        "2602.04705v1": {
          "score": 0.6986666917800903,
          "rank": 79
        },
        "2602.04784v1": {
          "score": 0.6986563205718994,
          "rank": 80
        },
        "2602.05472v1": {
          "score": 0.6975921392440796,
          "rank": 81
        },
        "2602.06000v1": {
          "score": 0.6970620155334473,
          "rank": 82
        },
        "2602.05230v1": {
          "score": 0.6968659162521362,
          "rank": 83
        },
        "2602.05853v1": {
          "score": 0.69679856300354,
          "rank": 84
        },
        "2602.05000v1": {
          "score": 0.6965077519416809,
          "rank": 85
        },
        "2602.06040v1": {
          "score": 0.695544958114624,
          "rank": 86
        },
        "2602.05191v1": {
          "score": 0.6955137252807617,
          "rank": 87
        },
        "2602.05887v1": {
          "score": 0.6947879791259766,
          "rank": 88
        },
        "2602.05459v1": {
          "score": 0.6946334838867188,
          "rank": 89
        },
        "2602.04690v1": {
          "score": 0.6942603588104248,
          "rank": 90
        },
        "2602.05251v1": {
          "score": 0.6940509080886841,
          "rank": 91
        },
        "2602.05742v1": {
          "score": 0.6937866806983948,
          "rank": 92
        },
        "2602.05951v1": {
          "score": 0.6936323642730713,
          "rank": 93
        },
        "2602.05774v1": {
          "score": 0.6935710310935974,
          "rank": 94
        },
        "2602.05567v1": {
          "score": 0.6933344602584839,
          "rank": 95
        },
        "2602.04681v1": {
          "score": 0.6932138204574585,
          "rank": 96
        },
        "2602.04717v1": {
          "score": 0.6932006478309631,
          "rank": 97
        },
        "2602.05929v1": {
          "score": 0.6931264400482178,
          "rank": 98
        },
        "2602.05120v1": {
          "score": 0.692918062210083,
          "rank": 99
        },
        "2602.05065v1": {
          "score": 0.6928777694702148,
          "rank": 100
        }
      }
    },
    {
      "type": "llm_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "帮我找符号回归与其他学科交叉并且有实证实验的最新论文",
      "sim_scores": {
        "2602.05334v1": {
          "score": 0.7652500867843628,
          "rank": 1
        },
        "2602.05195v1": {
          "score": 0.7431259155273438,
          "rank": 2
        },
        "2602.05692v1": {
          "score": 0.7387793064117432,
          "rank": 3
        },
        "2602.05374v1": {
          "score": 0.7345786094665527,
          "rank": 4
        },
        "2602.05940v1": {
          "score": 0.726041316986084,
          "rank": 5
        },
        "2602.05014v1": {
          "score": 0.7246172428131104,
          "rank": 6
        },
        "2602.05879v1": {
          "score": 0.7221016883850098,
          "rank": 7
        },
        "2602.04729v1": {
          "score": 0.7206251621246338,
          "rank": 8
        },
        "2602.05150v1": {
          "score": 0.719718873500824,
          "rank": 9
        },
        "2602.05152v1": {
          "score": 0.7187478542327881,
          "rank": 10
        },
        "2602.04982v1": {
          "score": 0.7180662155151367,
          "rank": 11
        },
        "2602.05932v1": {
          "score": 0.7142224311828613,
          "rank": 12
        },
        "2602.05769v1": {
          "score": 0.7139933109283447,
          "rank": 13
        },
        "2602.05512v1": {
          "score": 0.7116435766220093,
          "rank": 14
        },
        "2602.05506v1": {
          "score": 0.711442232131958,
          "rank": 15
        },
        "2602.05867v1": {
          "score": 0.7107566595077515,
          "rank": 16
        },
        "2602.05205v1": {
          "score": 0.709048330783844,
          "rank": 17
        },
        "2602.05107v1": {
          "score": 0.7086935043334961,
          "rank": 18
        },
        "2602.05408v1": {
          "score": 0.7075764536857605,
          "rank": 19
        },
        "2602.04659v1": {
          "score": 0.706778347492218,
          "rank": 20
        },
        "2602.05856v1": {
          "score": 0.7053473591804504,
          "rank": 21
        },
        "2602.05728v1": {
          "score": 0.705338716506958,
          "rank": 22
        },
        "2602.05413v1": {
          "score": 0.7052785158157349,
          "rank": 23
        },
        "2602.05493v1": {
          "score": 0.7042763233184814,
          "rank": 24
        },
        "2602.05734v1": {
          "score": 0.7019082307815552,
          "rank": 25
        },
        "2602.05599v1": {
          "score": 0.7014272212982178,
          "rank": 26
        },
        "2602.04630v1": {
          "score": 0.7011436223983765,
          "rank": 27
        },
        "2602.04693v1": {
          "score": 0.6996139287948608,
          "rank": 28
        },
        "2602.04731v1": {
          "score": 0.6994328498840332,
          "rank": 29
        },
        "2602.05235v1": {
          "score": 0.698744535446167,
          "rank": 30
        },
        "2602.05384v1": {
          "score": 0.6985952854156494,
          "rank": 31
        },
        "2602.05419v1": {
          "score": 0.6984668970108032,
          "rank": 32
        },
        "2602.05385v1": {
          "score": 0.6978954076766968,
          "rank": 33
        },
        "2602.04802v1": {
          "score": 0.6964232921600342,
          "rank": 34
        },
        "2602.05633v1": {
          "score": 0.6960315704345703,
          "rank": 35
        },
        "2602.05448v1": {
          "score": 0.6953981518745422,
          "rank": 36
        },
        "2602.05381v1": {
          "score": 0.6952778100967407,
          "rank": 37
        },
        "2602.05400v1": {
          "score": 0.6951018571853638,
          "rank": 38
        },
        "2602.04764v1": {
          "score": 0.6948854327201843,
          "rank": 39
        },
        "2602.04938v1": {
          "score": 0.6930862069129944,
          "rank": 40
        },
        "2602.06034v1": {
          "score": 0.6930626630783081,
          "rank": 41
        },
        "2602.05883v1": {
          "score": 0.6925230622291565,
          "rank": 42
        },
        "2602.05035v1": {
          "score": 0.6924813389778137,
          "rank": 43
        },
        "2602.05366v1": {
          "score": 0.6918295621871948,
          "rank": 44
        },
        "2602.05975v1": {
          "score": 0.6904263496398926,
          "rank": 45
        },
        "2602.04812v1": {
          "score": 0.6900061368942261,
          "rank": 46
        },
        "2602.05406v1": {
          "score": 0.6896212100982666,
          "rank": 47
        },
        "2602.04776v1": {
          "score": 0.6895038485527039,
          "rank": 48
        },
        "2602.04785v1": {
          "score": 0.6892781257629395,
          "rank": 49
        },
        "2602.05148v1": {
          "score": 0.689002275466919,
          "rank": 50
        },
        "2602.05143v1": {
          "score": 0.6888576745986938,
          "rank": 51
        },
        "2602.05207v1": {
          "score": 0.6887292861938477,
          "rank": 52
        },
        "2602.05519v1": {
          "score": 0.6881130933761597,
          "rank": 53
        },
        "2602.05853v1": {
          "score": 0.6880350112915039,
          "rank": 54
        },
        "2602.04711v2": {
          "score": 0.6879507303237915,
          "rank": 55
        },
        "2602.05471v1": {
          "score": 0.6877362728118896,
          "rank": 56
        },
        "2602.05191v1": {
          "score": 0.6873721480369568,
          "rank": 57
        },
        "2602.05648v1": {
          "score": 0.6872376203536987,
          "rank": 58
        },
        "2602.06031v1": {
          "score": 0.6867642402648926,
          "rank": 59
        },
        "2602.05452v1": {
          "score": 0.6854614019393921,
          "rank": 60
        },
        "2602.05902v1": {
          "score": 0.6850664615631104,
          "rank": 61
        },
        "2602.05012v1": {
          "score": 0.6849873065948486,
          "rank": 62
        },
        "2602.05674v1": {
          "score": 0.6847047209739685,
          "rank": 63
        },
        "2602.04712v1": {
          "score": 0.6846733093261719,
          "rank": 64
        },
        "2602.05694v1": {
          "score": 0.6846398711204529,
          "rank": 65
        },
        "2602.05708v1": {
          "score": 0.6843186616897583,
          "rank": 66
        },
        "2602.05134v1": {
          "score": 0.6839367747306824,
          "rank": 67
        },
        "2602.05515v1": {
          "score": 0.6836895942687988,
          "rank": 68
        },
        "2602.05216v1": {
          "score": 0.6835675239562988,
          "rank": 69
        },
        "2602.04841v1": {
          "score": 0.6826115846633911,
          "rank": 70
        },
        "2602.05393v1": {
          "score": 0.6820125579833984,
          "rank": 71
        },
        "2602.04752v1": {
          "score": 0.6818792223930359,
          "rank": 72
        },
        "2602.05302v1": {
          "score": 0.6818562746047974,
          "rank": 73
        },
        "2602.05297v1": {
          "score": 0.6818516850471497,
          "rank": 74
        },
        "2602.04757v1": {
          "score": 0.6818022131919861,
          "rank": 75
        },
        "2602.05794v1": {
          "score": 0.6816943287849426,
          "rank": 76
        },
        "2602.05687v1": {
          "score": 0.6816213130950928,
          "rank": 77
        },
        "2602.05122v1": {
          "score": 0.6816198825836182,
          "rank": 78
        },
        "2602.05087v1": {
          "score": 0.681437075138092,
          "rank": 79
        },
        "2602.05339v1": {
          "score": 0.6813256144523621,
          "rank": 80
        },
        "2602.05140v1": {
          "score": 0.680313229560852,
          "rank": 81
        },
        "2602.05944v1": {
          "score": 0.6799954771995544,
          "rank": 82
        },
        "2602.04706v1": {
          "score": 0.6797910928726196,
          "rank": 83
        },
        "2602.05447v1": {
          "score": 0.6791776418685913,
          "rank": 84
        },
        "2602.05892v1": {
          "score": 0.6789385080337524,
          "rank": 85
        },
        "2602.04750v1": {
          "score": 0.6787470579147339,
          "rank": 86
        },
        "2602.05945v1": {
          "score": 0.6784015893936157,
          "rank": 87
        },
        "2602.05131v1": {
          "score": 0.6777681112289429,
          "rank": 88
        },
        "2602.05930v1": {
          "score": 0.6776760220527649,
          "rank": 89
        },
        "2602.05275v1": {
          "score": 0.6774623990058899,
          "rank": 90
        },
        "2602.05270v1": {
          "score": 0.6769459247589111,
          "rank": 91
        },
        "2602.05445v1": {
          "score": 0.6767401695251465,
          "rank": 92
        },
        "2602.06005v1": {
          "score": 0.6765494346618652,
          "rank": 93
        },
        "2602.05266v1": {
          "score": 0.6765474081039429,
          "rank": 94
        },
        "2602.05395v1": {
          "score": 0.6763750910758972,
          "rank": 95
        },
        "2602.04755v1": {
          "score": 0.6758551597595215,
          "rank": 96
        },
        "2602.04690v1": {
          "score": 0.6755808591842651,
          "rank": 97
        },
        "2602.05286v1": {
          "score": 0.6755229234695435,
          "rank": 98
        },
        "2602.05228v1": {
          "score": 0.6753764748573303,
          "rank": 99
        },
        "2602.05111v1": {
          "score": 0.6752989292144775,
          "rank": 100
        }
      }
    },
    {
      "type": "llm_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "Recent advances and state-of-the-art methods in symbolic regression",
      "sim_scores": {
        "2602.05371v1": {
          "score": 0.7853887677192688,
          "rank": 1
        },
        "2602.05742v1": {
          "score": 0.7602043151855469,
          "rank": 2
        },
        "2602.05225v1": {
          "score": 0.7583268880844116,
          "rank": 3
        },
        "2602.04736v1": {
          "score": 0.7528586983680725,
          "rank": 4
        },
        "2602.05600v1": {
          "score": 0.7519118785858154,
          "rank": 5
        },
        "2602.05172v1": {
          "score": 0.7497613430023193,
          "rank": 6
        },
        "2602.04718v1": {
          "score": 0.7489181756973267,
          "rank": 7
        },
        "2602.04941v1": {
          "score": 0.7483577132225037,
          "rank": 8
        },
        "2602.05032v1": {
          "score": 0.7479270696640015,
          "rank": 9
        },
        "2602.05657v1": {
          "score": 0.7475008964538574,
          "rank": 10
        },
        "2602.04668v1": {
          "score": 0.744939386844635,
          "rank": 11
        },
        "2602.05639v1": {
          "score": 0.7448898553848267,
          "rank": 12
        },
        "2602.06042v1": {
          "score": 0.7423050403594971,
          "rank": 13
        },
        "2602.05033v1": {
          "score": 0.7417202591896057,
          "rank": 14
        },
        "2602.05134v1": {
          "score": 0.7414641976356506,
          "rank": 15
        },
        "2602.05464v1": {
          "score": 0.7400065660476685,
          "rank": 16
        },
        "2602.05704v1": {
          "score": 0.7390105724334717,
          "rank": 17
        },
        "2602.05306v1": {
          "score": 0.7385653257369995,
          "rank": 18
        },
        "2602.05106v1": {
          "score": 0.7382647395133972,
          "rank": 19
        },
        "2602.05744v1": {
          "score": 0.7378438711166382,
          "rank": 20
        },
        "2602.05391v1": {
          "score": 0.7370095252990723,
          "rank": 21
        },
        "2602.05887v1": {
          "score": 0.7368670701980591,
          "rank": 22
        },
        "2602.05144v1": {
          "score": 0.7364275455474854,
          "rank": 23
        },
        "2602.05068v1": {
          "score": 0.7358174324035645,
          "rank": 24
        },
        "2602.05019v1": {
          "score": 0.7352839112281799,
          "rank": 25
        },
        "2602.05591v1": {
          "score": 0.7347335815429688,
          "rank": 26
        },
        "2602.05304v1": {
          "score": 0.7345978021621704,
          "rank": 27
        },
        "2602.05333v1": {
          "score": 0.7343317270278931,
          "rank": 28
        },
        "2602.05489v1": {
          "score": 0.7334015369415283,
          "rank": 29
        },
        "2602.05635v1": {
          "score": 0.7333629727363586,
          "rank": 30
        },
        "2602.05531v1": {
          "score": 0.7329609990119934,
          "rank": 31
        },
        "2602.04643v1": {
          "score": 0.7328983545303345,
          "rank": 32
        },
        "2602.06019v1": {
          "score": 0.7328246831893921,
          "rank": 33
        },
        "2602.05622v1": {
          "score": 0.7323094606399536,
          "rank": 34
        },
        "2602.04761v1": {
          "score": 0.732262134552002,
          "rank": 35
        },
        "2602.04768v1": {
          "score": 0.7321760654449463,
          "rank": 36
        },
        "2602.05709v1": {
          "score": 0.7312747240066528,
          "rank": 37
        },
        "2602.05539v1": {
          "score": 0.7310390472412109,
          "rank": 38
        },
        "2602.05103v1": {
          "score": 0.7309253811836243,
          "rank": 39
        },
        "2602.05859v1": {
          "score": 0.7302314043045044,
          "rank": 40
        },
        "2602.05232v1": {
          "score": 0.730088472366333,
          "rank": 41
        },
        "2602.04691v1": {
          "score": 0.7298883199691772,
          "rank": 42
        },
        "2602.04677v1": {
          "score": 0.7295699119567871,
          "rank": 43
        },
        "2602.05830v1": {
          "score": 0.7294943332672119,
          "rank": 44
        },
        "2602.04682v1": {
          "score": 0.729343593120575,
          "rank": 45
        },
        "2602.04833v1": {
          "score": 0.7292566895484924,
          "rank": 46
        },
        "2602.05082v1": {
          "score": 0.7289483547210693,
          "rank": 47
        },
        "2602.04775v1": {
          "score": 0.7288909554481506,
          "rank": 48
        },
        "2602.05684v1": {
          "score": 0.7286643385887146,
          "rank": 49
        },
        "2602.05713v1": {
          "score": 0.7283035516738892,
          "rank": 50
        },
        "2602.05605v1": {
          "score": 0.7278134226799011,
          "rank": 51
        },
        "2602.05559v1": {
          "score": 0.727699875831604,
          "rank": 52
        },
        "2602.05549v1": {
          "score": 0.7276527285575867,
          "rank": 53
        },
        "2602.05266v1": {
          "score": 0.7274798154830933,
          "rank": 54
        },
        "2602.05410v1": {
          "score": 0.7271538972854614,
          "rank": 55
        },
        "2602.05000v1": {
          "score": 0.7271162867546082,
          "rank": 56
        },
        "2602.04720v1": {
          "score": 0.7269737720489502,
          "rank": 57
        },
        "2602.05993v1": {
          "score": 0.7268835306167603,
          "rank": 58
        },
        "2602.05649v1": {
          "score": 0.7268795371055603,
          "rank": 59
        },
        "2602.05725v1": {
          "score": 0.7266486883163452,
          "rank": 60
        },
        "2602.05786v1": {
          "score": 0.7264903783798218,
          "rank": 61
        },
        "2602.05184v1": {
          "score": 0.7257645726203918,
          "rank": 62
        },
        "2602.05723v1": {
          "score": 0.7253077030181885,
          "rank": 63
        },
        "2602.05616v1": {
          "score": 0.7251303195953369,
          "rank": 64
        },
        "2602.04649v1": {
          "score": 0.7243254780769348,
          "rank": 65
        },
        "2602.05389v1": {
          "score": 0.7243075370788574,
          "rank": 66
        },
        "2602.05571v1": {
          "score": 0.724077045917511,
          "rank": 67
        },
        "2602.05693v1": {
          "score": 0.7235747575759888,
          "rank": 68
        },
        "2602.06014v1": {
          "score": 0.7234401702880859,
          "rank": 69
        },
        "2602.05358v1": {
          "score": 0.7233486771583557,
          "rank": 70
        },
        "2602.05219v1": {
          "score": 0.7233443260192871,
          "rank": 71
        },
        "2602.05735v1": {
          "score": 0.7232928276062012,
          "rank": 72
        },
        "2602.05340v1": {
          "score": 0.7231564521789551,
          "rank": 73
        },
        "2602.05367v1": {
          "score": 0.7226393222808838,
          "rank": 74
        },
        "2602.05298v1": {
          "score": 0.7220882177352905,
          "rank": 75
        },
        "2602.05146v1": {
          "score": 0.7217347621917725,
          "rank": 76
        },
        "2602.05712v1": {
          "score": 0.7214260101318359,
          "rank": 77
        },
        "2602.05890v1": {
          "score": 0.721267819404602,
          "rank": 78
        },
        "2602.05214v1": {
          "score": 0.7211304903030396,
          "rank": 79
        },
        "2602.04651v1": {
          "score": 0.7210823893547058,
          "rank": 80
        },
        "2602.05162v1": {
          "score": 0.7210821509361267,
          "rank": 81
        },
        "2602.05380v1": {
          "score": 0.7209752798080444,
          "rank": 82
        },
        "2602.05445v1": {
          "score": 0.7207955121994019,
          "rank": 83
        },
        "2602.05242v1": {
          "score": 0.7206211090087891,
          "rank": 84
        },
        "2602.05533v1": {
          "score": 0.7197297811508179,
          "rank": 85
        },
        "2602.04774v1": {
          "score": 0.7192621231079102,
          "rank": 86
        },
        "2602.05349v1": {
          "score": 0.7192280292510986,
          "rank": 87
        },
        "2602.04735v1": {
          "score": 0.7191721200942993,
          "rank": 88
        },
        "2602.05950v1": {
          "score": 0.7190539836883545,
          "rank": 89
        },
        "2602.05444v1": {
          "score": 0.7190132141113281,
          "rank": 90
        },
        "2602.05846v1": {
          "score": 0.7189544439315796,
          "rank": 91
        },
        "2602.04785v1": {
          "score": 0.7189393043518066,
          "rank": 92
        },
        "2602.05974v1": {
          "score": 0.718677818775177,
          "rank": 93
        },
        "2602.05787v1": {
          "score": 0.7185325026512146,
          "rank": 94
        },
        "2602.04690v1": {
          "score": 0.7183089852333069,
          "rank": 95
        },
        "2602.04812v1": {
          "score": 0.7177916765213013,
          "rank": 96
        },
        "2602.05695v1": {
          "score": 0.7177671790122986,
          "rank": 97
        },
        "2602.05774v1": {
          "score": 0.7175672650337219,
          "rank": 98
        },
        "2602.05976v1": {
          "score": 0.7172523736953735,
          "rank": 99
        },
        "2602.05062v1": {
          "score": 0.7172064185142517,
          "rank": 100
        }
      }
    },
    {
      "type": "llm_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "sim_scores": {
        "2602.04690v1": {
          "score": 0.7585701942443848,
          "rank": 1
        },
        "2602.04812v1": {
          "score": 0.758306086063385,
          "rank": 2
        },
        "2602.05033v1": {
          "score": 0.7437887191772461,
          "rank": 3
        },
        "2602.05371v1": {
          "score": 0.7402633428573608,
          "rank": 4
        },
        "2602.05184v1": {
          "score": 0.7402275204658508,
          "rank": 5
        },
        "2602.05143v1": {
          "score": 0.7361694574356079,
          "rank": 6
        },
        "2602.04630v1": {
          "score": 0.7356798052787781,
          "rank": 7
        },
        "2602.05944v1": {
          "score": 0.7340596318244934,
          "rank": 8
        },
        "2602.05216v1": {
          "score": 0.7310450673103333,
          "rank": 9
        },
        "2602.05225v1": {
          "score": 0.7303913831710815,
          "rank": 10
        },
        "2602.05266v1": {
          "score": 0.7297818660736084,
          "rank": 11
        },
        "2602.05413v1": {
          "score": 0.7278774976730347,
          "rank": 12
        },
        "2602.05549v1": {
          "score": 0.7261802554130554,
          "rank": 13
        },
        "2602.05975v1": {
          "score": 0.7259117364883423,
          "rank": 14
        },
        "2602.05152v1": {
          "score": 0.7258940935134888,
          "rank": 15
        },
        "2602.04718v1": {
          "score": 0.7257918119430542,
          "rank": 16
        },
        "2602.05114v1": {
          "score": 0.7238703966140747,
          "rank": 17
        },
        "2602.05235v1": {
          "score": 0.7221635580062866,
          "rank": 18
        },
        "2602.05103v1": {
          "score": 0.7217786312103271,
          "rank": 19
        },
        "2602.05936v1": {
          "score": 0.7216845750808716,
          "rank": 20
        },
        "2602.05539v1": {
          "score": 0.7214316129684448,
          "rank": 21
        },
        "2602.06034v1": {
          "score": 0.7212949991226196,
          "rank": 22
        },
        "2602.05424v1": {
          "score": 0.7211207151412964,
          "rank": 23
        },
        "2602.05134v1": {
          "score": 0.7207380533218384,
          "rank": 24
        },
        "2602.05087v1": {
          "score": 0.7193443775177002,
          "rank": 25
        },
        "2602.05014v1": {
          "score": 0.7191624045372009,
          "rank": 26
        },
        "2602.05082v1": {
          "score": 0.7190119624137878,
          "rank": 27
        },
        "2602.04941v1": {
          "score": 0.718923032283783,
          "rank": 28
        },
        "2602.05032v1": {
          "score": 0.7185774445533752,
          "rank": 29
        },
        "2602.05600v1": {
          "score": 0.7172633409500122,
          "rank": 30
        },
        "2602.05452v1": {
          "score": 0.7171767354011536,
          "rank": 31
        },
        "2602.05400v1": {
          "score": 0.7167054414749146,
          "rank": 32
        },
        "2602.05370v1": {
          "score": 0.7166787385940552,
          "rank": 33
        },
        "2602.05410v1": {
          "score": 0.7165000438690186,
          "rank": 34
        },
        "2602.04677v1": {
          "score": 0.7162773609161377,
          "rank": 35
        },
        "2602.05668v1": {
          "score": 0.7162287831306458,
          "rank": 36
        },
        "2602.05219v1": {
          "score": 0.7157683372497559,
          "rank": 37
        },
        "2602.05445v1": {
          "score": 0.7155927419662476,
          "rank": 38
        },
        "2602.05708v1": {
          "score": 0.714882493019104,
          "rank": 39
        },
        "2602.04936v1": {
          "score": 0.713846743106842,
          "rank": 40
        },
        "2602.05651v1": {
          "score": 0.7135981917381287,
          "rank": 41
        },
        "2602.05971v1": {
          "score": 0.713421106338501,
          "rank": 42
        },
        "2602.04843v1": {
          "score": 0.7131638526916504,
          "rank": 43
        },
        "2602.06030v1": {
          "score": 0.7131161689758301,
          "rank": 44
        },
        "2602.05062v1": {
          "score": 0.712973952293396,
          "rank": 45
        },
        "2602.05512v1": {
          "score": 0.7129476070404053,
          "rank": 46
        },
        "2602.05798v1": {
          "score": 0.7127987742424011,
          "rank": 47
        },
        "2602.05106v1": {
          "score": 0.7126718759536743,
          "rank": 48
        },
        "2602.05639v1": {
          "score": 0.7122828960418701,
          "rank": 49
        },
        "2602.05026v1": {
          "score": 0.7121914625167847,
          "rank": 50
        },
        "2602.05926v1": {
          "score": 0.7118062376976013,
          "rank": 51
        },
        "2602.05391v1": {
          "score": 0.7112727761268616,
          "rank": 52
        },
        "2602.05463v1": {
          "score": 0.7111799716949463,
          "rank": 53
        },
        "2602.05712v1": {
          "score": 0.7108746767044067,
          "rank": 54
        },
        "2602.05464v1": {
          "score": 0.7107207775115967,
          "rank": 55
        },
        "2602.04768v1": {
          "score": 0.710493803024292,
          "rank": 56
        },
        "2602.05702v1": {
          "score": 0.7101932764053345,
          "rank": 57
        },
        "2602.04696v1": {
          "score": 0.7100987434387207,
          "rank": 58
        },
        "2602.04752v1": {
          "score": 0.7098080515861511,
          "rank": 59
        },
        "2602.05172v1": {
          "score": 0.7097033262252808,
          "rank": 60
        },
        "2602.05307v1": {
          "score": 0.7087820172309875,
          "rank": 61
        },
        "2602.05742v1": {
          "score": 0.7087311744689941,
          "rank": 62
        },
        "2602.05366v1": {
          "score": 0.7082797884941101,
          "rank": 63
        },
        "2602.04731v1": {
          "score": 0.7080749273300171,
          "rank": 64
        },
        "2602.05211v1": {
          "score": 0.7078952193260193,
          "rank": 65
        },
        "2602.04861v1": {
          "score": 0.7077098488807678,
          "rank": 66
        },
        "2602.05616v1": {
          "score": 0.7075159549713135,
          "rank": 67
        },
        "2602.05349v1": {
          "score": 0.7071582078933716,
          "rank": 68
        },
        "2602.04823v1": {
          "score": 0.706547737121582,
          "rank": 69
        },
        "2602.06029v1": {
          "score": 0.7060893177986145,
          "rank": 70
        },
        "2602.05723v1": {
          "score": 0.7060801982879639,
          "rank": 71
        },
        "2602.05313v1": {
          "score": 0.7059451341629028,
          "rank": 72
        },
        "2602.05559v1": {
          "score": 0.705707311630249,
          "rank": 73
        },
        "2602.04785v1": {
          "score": 0.7056897282600403,
          "rank": 74
        },
        "2602.05853v1": {
          "score": 0.7054183483123779,
          "rank": 75
        },
        "2602.04736v1": {
          "score": 0.7053582668304443,
          "rank": 76
        },
        "2602.05857v1": {
          "score": 0.705112636089325,
          "rank": 77
        },
        "2602.05242v1": {
          "score": 0.7048810720443726,
          "rank": 78
        },
        "2602.05012v1": {
          "score": 0.7045637369155884,
          "rank": 79
        },
        "2602.04649v1": {
          "score": 0.7044845819473267,
          "rank": 80
        },
        "2602.05167v1": {
          "score": 0.7043718695640564,
          "rank": 81
        },
        "2602.05635v1": {
          "score": 0.7042673826217651,
          "rank": 82
        },
        "2602.05762v1": {
          "score": 0.7041996717453003,
          "rank": 83
        },
        "2602.04775v1": {
          "score": 0.7040011882781982,
          "rank": 84
        },
        "2602.05193v1": {
          "score": 0.7038111090660095,
          "rank": 85
        },
        "2602.05797v1": {
          "score": 0.7037580609321594,
          "rank": 86
        },
        "2602.05852v1": {
          "score": 0.7034576535224915,
          "rank": 87
        },
        "2602.05333v1": {
          "score": 0.7033928036689758,
          "rank": 88
        },
        "2602.05704v1": {
          "score": 0.7033483982086182,
          "rank": 89
        },
        "2602.05865v1": {
          "score": 0.7031955122947693,
          "rank": 90
        },
        "2602.05734v1": {
          "score": 0.7027875781059265,
          "rank": 91
        },
        "2602.05846v1": {
          "score": 0.7026327252388,
          "rank": 92
        },
        "2602.06040v1": {
          "score": 0.702065110206604,
          "rank": 93
        },
        "2602.05059v1": {
          "score": 0.7018983364105225,
          "rank": 94
        },
        "2602.05227v1": {
          "score": 0.7018408179283142,
          "rank": 95
        },
        "2602.05849v1": {
          "score": 0.7017373442649841,
          "rank": 96
        },
        "2602.05056v1": {
          "score": 0.7013970613479614,
          "rank": 97
        },
        "2602.05068v1": {
          "score": 0.7013394832611084,
          "rank": 98
        },
        "2602.05403v1": {
          "score": 0.7011884450912476,
          "rank": 99
        },
        "2602.05591v1": {
          "score": 0.7010451555252075,
          "rank": 100
        }
      }
    },
    {
      "type": "llm_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "Comparison of genetic programming and neural symbolic regression techniques",
      "sim_scores": {
        "2602.05306v1": {
          "score": 0.7684454321861267,
          "rank": 1
        },
        "2602.05134v1": {
          "score": 0.7631412744522095,
          "rank": 2
        },
        "2602.05371v1": {
          "score": 0.7561100125312805,
          "rank": 3
        },
        "2602.04941v1": {
          "score": 0.7541742324829102,
          "rank": 4
        },
        "2602.05048v1": {
          "score": 0.7487457990646362,
          "rank": 5
        },
        "2602.05068v1": {
          "score": 0.7435674667358398,
          "rank": 6
        },
        "2602.05704v1": {
          "score": 0.7400549650192261,
          "rank": 7
        },
        "2602.05032v1": {
          "score": 0.7397909164428711,
          "rank": 8
        },
        "2602.05106v1": {
          "score": 0.739284873008728,
          "rank": 9
        },
        "2602.05391v1": {
          "score": 0.7387305498123169,
          "rank": 10
        },
        "2602.05709v1": {
          "score": 0.7382736206054688,
          "rank": 11
        },
        "2602.05848v1": {
          "score": 0.7370226383209229,
          "rank": 12
        },
        "2602.04761v1": {
          "score": 0.7364246845245361,
          "rank": 13
        },
        "2602.05688v1": {
          "score": 0.7363296747207642,
          "rank": 14
        },
        "2602.05146v1": {
          "score": 0.735980749130249,
          "rank": 15
        },
        "2602.05635v1": {
          "score": 0.7358809113502502,
          "rank": 16
        },
        "2602.05805v1": {
          "score": 0.7348380088806152,
          "rank": 17
        },
        "2602.05863v1": {
          "score": 0.7337397336959839,
          "rank": 18
        },
        "2602.04731v1": {
          "score": 0.73353111743927,
          "rank": 19
        },
        "2602.05798v1": {
          "score": 0.7334226965904236,
          "rank": 20
        },
        "2602.05999v1": {
          "score": 0.7334142923355103,
          "rank": 21
        },
        "2602.05182v1": {
          "score": 0.7334039211273193,
          "rank": 22
        },
        "2602.05036v1": {
          "score": 0.7321517467498779,
          "rank": 23
        },
        "2602.05786v1": {
          "score": 0.7318968772888184,
          "rank": 24
        },
        "2602.05323v1": {
          "score": 0.7315129041671753,
          "rank": 25
        },
        "2602.04649v1": {
          "score": 0.7312988638877869,
          "rank": 26
        },
        "2602.05675v1": {
          "score": 0.731093168258667,
          "rank": 27
        },
        "2602.05019v1": {
          "score": 0.7307606339454651,
          "rank": 28
        },
        "2602.05271v1": {
          "score": 0.7303587794303894,
          "rank": 29
        },
        "2602.05358v1": {
          "score": 0.7302732467651367,
          "rank": 30
        },
        "2602.05830v1": {
          "score": 0.7302231192588806,
          "rank": 31
        },
        "2602.06042v1": {
          "score": 0.7300096750259399,
          "rank": 32
        },
        "2602.06025v1": {
          "score": 0.7295864224433899,
          "rank": 33
        },
        "2602.05539v1": {
          "score": 0.7293399572372437,
          "rank": 34
        },
        "2602.05890v1": {
          "score": 0.7291878461837769,
          "rank": 35
        },
        "2602.04651v1": {
          "score": 0.7286659479141235,
          "rank": 36
        },
        "2602.05464v1": {
          "score": 0.7285816669464111,
          "rank": 37
        },
        "2602.05737v1": {
          "score": 0.7280097603797913,
          "rank": 38
        },
        "2602.05885v1": {
          "score": 0.7278100252151489,
          "rank": 39
        },
        "2602.05340v1": {
          "score": 0.7275173664093018,
          "rank": 40
        },
        "2602.05033v1": {
          "score": 0.7271934747695923,
          "rank": 41
        },
        "2602.04718v1": {
          "score": 0.7264500856399536,
          "rank": 42
        },
        "2602.05713v1": {
          "score": 0.7263789176940918,
          "rank": 43
        },
        "2602.05298v1": {
          "score": 0.726029098033905,
          "rank": 44
        },
        "2602.05333v1": {
          "score": 0.7253949642181396,
          "rank": 45
        },
        "2602.05893v1": {
          "score": 0.7249354124069214,
          "rank": 46
        },
        "2602.05712v1": {
          "score": 0.7248713970184326,
          "rank": 47
        },
        "2602.04785v1": {
          "score": 0.7245261073112488,
          "rank": 48
        },
        "2602.05616v1": {
          "score": 0.7241947650909424,
          "rank": 49
        },
        "2602.05000v1": {
          "score": 0.7237237691879272,
          "rank": 50
        },
        "2602.05152v1": {
          "score": 0.723610520362854,
          "rank": 51
        },
        "2602.05639v1": {
          "score": 0.7229454517364502,
          "rank": 52
        },
        "2602.05988v1": {
          "score": 0.7229205369949341,
          "rank": 53
        },
        "2602.05281v1": {
          "score": 0.72286057472229,
          "rank": 54
        },
        "2602.05136v1": {
          "score": 0.7228213548660278,
          "rank": 55
        },
        "2602.04774v1": {
          "score": 0.7224029302597046,
          "rank": 56
        },
        "2602.04832v1": {
          "score": 0.7222974896430969,
          "rank": 57
        },
        "2602.04736v1": {
          "score": 0.7220808267593384,
          "rank": 58
        },
        "2602.05006v1": {
          "score": 0.7218517065048218,
          "rank": 59
        },
        "2602.04837v1": {
          "score": 0.7218506932258606,
          "rank": 60
        },
        "2602.05172v1": {
          "score": 0.721366286277771,
          "rank": 61
        },
        "2602.05649v1": {
          "score": 0.7210414409637451,
          "rank": 62
        },
        "2602.05605v1": {
          "score": 0.720742404460907,
          "rank": 63
        },
        "2602.05184v1": {
          "score": 0.720712423324585,
          "rank": 64
        },
        "2602.05846v1": {
          "score": 0.7202916145324707,
          "rank": 65
        },
        "2602.05742v1": {
          "score": 0.7199826240539551,
          "rank": 66
        },
        "2602.05818v1": {
          "score": 0.7199144959449768,
          "rank": 67
        },
        "2602.05304v1": {
          "score": 0.7197852730751038,
          "rank": 68
        },
        "2602.05873v1": {
          "score": 0.7194761037826538,
          "rank": 69
        },
        "2602.05993v1": {
          "score": 0.7190158367156982,
          "rank": 70
        },
        "2602.06029v1": {
          "score": 0.7188733816146851,
          "rank": 71
        },
        "2602.05974v1": {
          "score": 0.7187711000442505,
          "rank": 72
        },
        "2602.05996v1": {
          "score": 0.7187533974647522,
          "rank": 73
        },
        "2602.04690v1": {
          "score": 0.7185237407684326,
          "rank": 74
        },
        "2602.05353v1": {
          "score": 0.7185216546058655,
          "rank": 75
        },
        "2602.05625v1": {
          "score": 0.7185029983520508,
          "rank": 76
        },
        "2602.05307v1": {
          "score": 0.7183114290237427,
          "rank": 77
        },
        "2602.06043v1": {
          "score": 0.7181870341300964,
          "rank": 78
        },
        "2602.04768v1": {
          "score": 0.718153715133667,
          "rank": 79
        },
        "2602.05859v1": {
          "score": 0.7179932594299316,
          "rank": 80
        },
        "2602.05589v1": {
          "score": 0.7178529500961304,
          "rank": 81
        },
        "2602.05472v1": {
          "score": 0.7177687883377075,
          "rank": 82
        },
        "2602.05523v1": {
          "score": 0.7175576686859131,
          "rank": 83
        },
        "2602.05187v1": {
          "score": 0.7174798250198364,
          "rank": 84
        },
        "2602.05139v1": {
          "score": 0.7172791957855225,
          "rank": 85
        },
        "2602.04643v1": {
          "score": 0.717151403427124,
          "rank": 86
        },
        "2602.05242v1": {
          "score": 0.7171022295951843,
          "rank": 87
        },
        "2602.05341v1": {
          "score": 0.716981828212738,
          "rank": 88
        },
        "2602.04811v1": {
          "score": 0.7169262170791626,
          "rank": 89
        },
        "2602.04634v1": {
          "score": 0.7168245315551758,
          "rank": 90
        },
        "2602.05370v1": {
          "score": 0.7167885303497314,
          "rank": 91
        },
        "2602.05695v1": {
          "score": 0.7167690396308899,
          "rank": 92
        },
        "2602.05114v1": {
          "score": 0.7166834473609924,
          "rank": 93
        },
        "2602.05735v1": {
          "score": 0.7164627313613892,
          "rank": 94
        },
        "2602.05193v1": {
          "score": 0.7162383198738098,
          "rank": 95
        },
        "2602.05630v1": {
          "score": 0.7159212231636047,
          "rank": 96
        },
        "2602.04879v1": {
          "score": 0.7158463001251221,
          "rank": 97
        },
        "2602.05494v1": {
          "score": 0.715724766254425,
          "rank": 98
        },
        "2602.05375v1": {
          "score": 0.71531742811203,
          "rank": 99
        },
        "2602.04810v1": {
          "score": 0.7151840925216675,
          "rank": 100
        }
      }
    }
  ]
}