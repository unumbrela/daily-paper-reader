Title: Theoretical Foundations of Scaling Law in Familial Models

URL Source: https://arxiv.org/pdf/2512.23407v1

Published Time: Tue, 30 Dec 2025 02:49:13 GMT

Number of Pages: 10

Markdown Content:
Theoretical Foundations of Scaling Law in Familial Models 

Huan Song , Qingfei Zhao , Ting Long , Shuyu Tian , Hongjun An , Jiawei Shao , Chi Zhang , Xuelong Li 

Institute of Artificial Intelligence (TeleAI), China Telecom Neural scaling law have become foundational for optimizing large language model (LLM) training, yet they typically assume a single dense model output. This limitation effectively overlooks “Familial models”, a transformative paradigm essential for realizing ubiquitous intelligence across heterogeneous device-edge-cloud hierarchies. Transcending static architectures, familial models integrate early exits with relay-style inference to spawn G deployable sub-models from a single shared backbone. In this work, we theoretically and empirically extend scaling law to capture this “one-run, many-models” paradigm by introducing Granularity ( G) as a fundamental scaling variable alongside model size ( N )and training tokens ( D). To rigorously quantify this relationship, we propose a unified functional form 

L(N, D, G ) and parameterize it using large-scale empirical runs. Specifically, we employ a rigorous IsoFLOP experimental design to strictly isolate architectural impact from computational scale. Across fixed budgets ( 10 18 –10 21 FLOPs), we systematically sweep model sizes ( N ) and granularities ( G) while dynamically adjusting tokens ( D). This approach effectively decouples the marginal cost of granularity from the benefits of scale, ensuring high-fidelity parameterization of our unified scaling law. Our results reveal that the granularity penalty follows a multiplicative power law with an extremely small exponent ( γ ≈ 0.041 ). Theoretically, this bridges fixed-compute training with dynamic architectures. Practically, it validates the “train once, deploy many” paradigm, demonstrating that deployment flexibility is achievable without compromising the compute-optimality of dense baselines.  

> Keywords:

familial models, scaling law, large language models, theoretical analysis, compute-optimal training  

> Correspondence to:

Xuelong Li ( xuelong_li@ieee.org )

# 1 Introduction 

In the landscape of modern Large Language Model (LLM) deployment, diverse applications impose varying constraints on latency and computational cost (Kwon et al., 2023; Hou et al., 2025; Semerikov et al., 2025). Practitioners are no longer satisfied with a single fixed model; instead, there is a pressing need for a flexible suite of models capable of spanning multiple cost tiers (Chen et al., 2023; Huang et al., 2025; Park et al., 2024). To address this, “Familial Models” have emerged as a transformative solution (An et al.). Going beyond standard early-exit architectures (Teerapittayanon et al., 2016), the proposed approach synergistically integrates Early Exiting with Scalable Branches (EESB) and Hierarchical Principal Component Decomposition (HPCD) (An et al.). Specifically, instead of merely attaching prediction heads to intermediate layers, lightweight, decomposable branch networks (Houlsby et al., 2019) are constructed to allow for fine-grained parameter tuning via low-rank matrix approximation (Hu et al., 2022). This architecture enables a single training run to produce G deployable sub-models (where G denotes granularity) that share a unified backbone and aligned hidden features (Kusupati et al., 2022). This structural consistency not only offers a continuous spectrum of depth-cost trade-offs but also inherently supports relay-style cooperative inference across heterogeneous (Kang et al., 2017) devices without additional middleware, thereby significantly enhancing the flexibility and efficiency of the “train once, deploy many” paradigm. To guide efficient model training and resource allocation, the field relies heavily on Neural Scaling Law. The foundational era began with Kaplan et al. (2020), who characterized test loss as a predictable power-law function of model size ( N ), dataset size ( D), and compute budget ( C). This paradigm was significantly 1

> arXiv:2512.23407v1 [cs.LG] 29 Dec 2025

refined by Hoffmann et al. (2022) through IsoFLOP analysis, establishing the “Chinchilla” scaling law which advocates for proportional scaling of parameters and data ( N ∝ D) to maximize efficiency. Recent rigorous replications have further solidified this foundation; despite identifying methodological flaws in the original Chinchilla study—such as optimizer early stopping and parameter rounding errors—researchers reaffirmed the validity of the compute-optimal frontier with corrected, statistically robust confidence intervals (Pearce and Song, 2024; Porian et al., 2024). As the field evolves, scaling law are expanding beyond dense models to specialized, efficient architectures. For instance, recent work on Mixture-of-Experts (MoE) introduced “Efficiency Leverage” (EL) to quantify the computational advantage over dense baselines (Tian et al., 2025). This research revealed that efficiency is governed by distinct architectural factors: EL scales as a power law with the activation ratio (sparsity) and exhibits a non-linear “U-shaped” sensitivity to expert granularity, with advantages amplifying significantly at larger compute budgets (Tian et al., 2025; Krajewski et al., 2024). These advances reflect a broader paradigm shift: as the community navigates potential saturation in pure scaling and explores new frontiers like post-training scaling and data quality, the focus is moving toward architecture-aware laws that ensure precise resource optimization. However, existing scaling law are inherently built upon a “one-run, one-model” paradigm(Yuan et al., 2025), characterizing loss solely as a function of N and D for a single output. This perspective fails to capture the unique “one-to-many” dynamics of familial model training , where the outcome is not a solitary model but a set of G interdependent sub-models derived from a single optimization process. Traditional laws overlook the architectural dimension of “Granularity” ( G, the number of exit points), and thus cannot quantify the potential interference or synergy between exits, nor predict the performance cost of making a model family “finer-grained.” To bridge this theoretical gap, we propose a unified scaling framework that explicitly incorporates Granularity ( G) as a fundamental scaling variable alongside N and D. Drawing inspiration from the architectural deconstruction approach of Tian et al. (2025), our methodology proceeds as follows: 1. Unified Loss Modeling: We model the pretraining loss as a joint function L(N, D, G ) to capture the “one-run, many-models” dynamic inherent in family architectures. 2. Familial Models Scaling Law: To quantify the modulatory effect of granularity, we adopt the formulation: 

L(N, D, G ) = 



E + AN α + BDβ



· Gγ , (1) This multiplicative structure isolates the granularity penalty Gγ from the standard power-law decay, effectively interpreting γ as the marginal “tax” imposed on the backbone for supporting multiple independent operating points. By parameterizing the unified functional form with data from our rigorous IsoFLOP experiments, we propose the Familial Models Scaling Law . For the representative experimental group, the fitted law is quantitatively established as: 

L(N, D, G ) = 



1.18 + 408 .69 

N 0.3006 + 3120 .14 

D0.3514 



· G0.041 , (2) In this equation, the irreducible loss E = 1 .18 represents the theoretical performance limit, while the small exponent γ ≈ 0.041 empirically confirms that the architectural overhead for supporting G exits is minimal, following a gentle multiplicative scaling rule. 3. Controlled Experiments: We conduct rigorous scaling runs under fixed compute budgets to systematically isolate the impact of architectural variables from computational scale. 4. Compute-Matched Parameterization: To ensure the scaling coefficients accurately reflect the architectural trade-offs of familial models, we parameterize the scaling law using a dataset derived from strict IsoFLOP (constant compute) constraints. By systematically varying model size N and granularity G within fixed compute budgets ( 10 20 –10 21 FLOPs), we generate a high-fidelity observation set. This design effectively decouples the marginal cost of granularity from computational scale, allowing for a precise isolation of the granularity exponent γ independent of the fitting algorithm employed. This work pioneers the theoretical formalization of the “Familial Models” paradigm, establishing the first unified scaling law that explicitly incorporates Granularity ( G) as a fundamental dimension alongside model 2size ( N ) and data ( D). Unlike prior studies limited to static dense models, we quantify the “cost of flexibility” by accurately modeling the three-dimensional loss surface. A critical discovery from our results is that the granularity exponent γ is extremely small ( ≈ 0.041 ), quantitatively proving that the architectural penalty for supporting multiple exit points is negligible. Furthermore, our analysis of the efficiency frontier redefines the compute-optimal landscape, demonstrating that the optimal allocation strategy remains robust even under dynamic architectural constraints. This fundamentally legitimizes the “one-run, many-models” strategy, confirming that practitioners can significantly increase deployment flexibility—obtaining a spectrum of sub-models—without deviating from the compute-optimal frontier established for dense models. 

# 2 Preliminaries 

2.1 Familial Models Architectures and Granularity Formulation 

We adopt a conventional dense Transformer equipped with a single final prediction head as the baseline, which corresponds to the special case G = 1. Within each compute-budget group, we fix the total training compute (FLOPs; ranging from 1e20 to 1e21) and, for each architectural configuration, derive the corresponding training-token budget D implied by the fixed compute constraint. This ensures fair, compute-matched comparisons across models within the same group. A Familial model is built upon a shared backbone and augments it with multiple early-exit prediction heads placed at selected intermediate layers. This design allows a single trained trunk to produce multiple deployable sub-models with different effective depths and inference costs. Formally, let the trunk contain L transformer layers. We attach exit heads at a set of intermediate layers { l 1 ,..., l G-1 } and also retain the standard output at the final layer L. This yields a total of G usable exits (including the final exit). We define G as the granularity factor, where a larger G corresponds to more available operating points (i.e., more depth/cost tiers) and thus finer deployment granularity. Training typically optimizes all exits jointly by minimizing a weighted sum of exit-specific language-modeling losses: 

Lfamily =

> G

X

> g=1

wg Lg , (3) Where Lg denotes the language-modeling loss at exit g, and wg is the corresponding weight. In our implementation, we assign equal weights to all exits (i.e., wg = 1 /G ), such that the total familial models loss is defined as the arithmetic mean of the individual losses across all exits. Within each experimental group, we keep the exit-weighting scheme and training protocol fixed, so that the primary controlled variables for scaling analysis are (N, D, G ).

2.2 Extending Scaling Law to Familial Models 

Modern deployment environments are rarely uniform; they often demand a versatile suite of models spanning a wide range of latency and cost tiers to adapt to varying hardware constraints (e.g., server-side vs. on-device) and dynamic query complexities. Relying on a single fixed operating point is inefficient, yet training independent models for each desired tier incurs a prohibitive computational cost that scales linearly with the number of models. Familial Models offer an elegant solution to this dilemma by training a shared Transformer trunk equipped with multiple intermediate exits. In this architecture, a single training run yields G deployable sub-models, each representing a distinct effective depth and inference cost. Here, the granularity G serves as a critical architectural hyperparameter, directly quantifying the density of valid operating points available from a single backbone. It effectively measures the “deployment resolution” of the model family—a higher G

implies a finer-grained ability to trade off accuracy for speed without the need for retraining. Classical scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022) have provided robust guidelines for predicting how loss varies with parameter count N and training tokens D. However, these frameworks operate under the assumption of distinct, independently trained models, failing to account for the internal dependencies and 3weight sharing inherent in multi-exit architectures 1. In the familial model setting, the training outcome is not a solitary scalar loss, but a trajectory of losses across G entangled sub-models derived from the same optimization process. To capture this “one-run, many-models” paradigm within a unified theoretical framework, we explicitly incorporate G as a third fundamental scaling dimension alongside N and D. By studying the joint scaling function L(N, D, G ), we aim to rigorously quantify the marginal cost of granularity—determining whether the architectural overhead of supporting multiple exits alters the fundamental compute-optimal frontier established for dense models. 

# 3 Scaling Law with Granularity 

Next, we define the functional form of the familial model scaling law and use it to outline our objectives and experimental roadmap. 

3.1 Proposed functional form 

Following empirical scaling-law literature—particularly compute-optimal scaling analyses (e.g., Hoffmann et al.)—we model pretraining loss as a smooth, monotone function of model size and data scale, exhibiting diminishing returns and approaching a non-zero irreducible floor. In the dense setting, this behavior is well captured by an additive decomposition in which loss approaches an irreducible term and decays as power laws in N and D. To extend this perspective to Familial models, we introduce granularity ( G) and propose a unified scaling law, utilizing the methodology of Hoffmann et al. to fit our extended parametric form to strictly compute-matched training runs: 

L(N, D, G ) = 



E + AN α + BDβ



Gγ , (4) where E is the irreducible loss floor as N, D → ∞ , A and B are positive scale coefficients, and α, β > 0 govern the rates of power-law improvement from increasing model size and data scale. The term Gγ captures the multiplicative effect of granularity on the learnable component of the loss. This formulation reduces to the standard (N, D ) scaling law when G = 1 , while remaining compact, interpretable, and straightforward to fit from empirical runs. 

3.2 Fitting Procedure 

A standard decomposition-based fitting procedure is adopted in the log domain, combining robust regression with multi-start initialization to ensure numerical stability and reliable parameter estimation. 

• Log-domain decomposition with LSE: To facilitate gradient-based optimization and ensure numerical stability, the standard scaling law formulation is reformulated as: 

L(N, D, G ) =  E + AN −α + BD −β  Gγ , (4) Ensuring positivity and optimization stability, the coefficients are parameterized by: 

E = exp( e), A = exp( a), B = exp( b), (5) For a specific run i, the predicted log value log ˆLi is calculated using these exponential terms: 

log ˆLi = log (exp( e) + exp( a − α log Ni) + exp( b − β log Di)) + γ log Gi, (6) Specifically, we implement the log of the positive sum via a log-sum-exp operator defined as: LSE (x, y, z ) = log (exp( x) + exp( y) + exp( z)) , (7) This results in the final formulation: 

log ˆLi = LSE (e, a − α log Ni, b − β log Di) + γ log Gi, (8)  

> 1Unlike independent models, sub-models in a family architecture share the majority of their parameters. This creates a multi-objective optimization landscape where gradients from deeper exits can regularize or potentially interfere with shallower ones, a dynamic not captured by traditional scaling laws for dense models.

4• Robust objective: Huber loss on log-residuals: The discrepancy between the model’s prediction and the observed data is quantified by the log-residual ri, which is defined as: 

ri = log ˆLi − log Li, (9) The model is fitted by minimizing the sum of Huber losses: 

min 

> a,b,e,α,β,γ

X

> i∈R

Huber δ (ri).

We use δ = 10 −3 for Huber robustness 2. Empirically, larger δ tends to overfit lower-compute regimes and predict held-out larger-compute runs poorly, while δ < 10 −3 does not materially change the resulting predictions—consistent with robust behavior. 

• Optimization: L-BFGS with grid initialization: As the objective function is non-convex, L-BFGS is employed to locate high-quality local minima. To further reduce sensitivity to initialization, optimization is initialized from a grid of starting points, which improves stability and consistency of the fitted solutions. The solution achieving the lowest final objective value is selected. In our experiments, the optimal solution does not occur at the boundary of the initialization grid, indicating that the resulting fit is unlikely to be an artifact of the chosen grid limits. 

α ∈ { 0, 0.5, . . . , 2}, β ∈ { 0, 0.5, . . . , 2}, e ∈ {− 1, −0.5, . . . , 1},a ∈ { 0.5, . . . , 25 }, b ∈ { 0.5, . . . , 25 }.

3.3 Experimental Design 

We conduct a series of experimental groups designed to support reliable scaling-law estimation under controlled compute conditions. In each group, we fix the overall training compute budget (FLOPs). For every configuration (dense baselines and familial model variants), we derive the corresponding training-token budget 

D implied by the fixed compute constraint, accounting for the fact that different exit layouts may alter the effective per-token training cost 3.Under the same compute budget, we then sweep across a range of parameter scales N and evaluate multiple familial model designs with different granularity settings G implemented by varying both the number and placement of intermediate exits. Each training run yields an observation ( N i ,D i ,G i ,L i ). The union of runs across all groups forms the dataset used to fit the proposed scaling law. For clarity, we present one representative experimental group to illustrate concrete architecture choices and exit placements. Each experimental group contains both a dense model baseline and a familial model variant; the DenseNet experimental setup is summarized in Table 1, and the familial model experimental setup is summarized in Table 2. Unless otherwise noted, all reported scaling-law parameters are obtained by fitting to the full set of runs across all groups. 

Dense d_model ffn_size num_attention_heads n_layers 

1B 1536 4608 12 19 2B 2048 6144 16 27 3B 2304 6912 18 36 4B 2560 7680 20 41  

> Table 1

Architectural hyperparameters of the dense transformer baselines used in our experiments.     

> 2Training runs occasionally exhibit transient loss spikes or instabilities, particularly in early phases. Unlike Mean Squared Error (MSE), which heavily penalizes these outliers and can skew the fitted curve, Huber loss transitions to linear scaling for large residuals, thereby effectively ignoring these non-representative data points.
> 3Specifically, the forward and backward pass computations of the additional exit heads are included in the total FLOPs count. Therefore, for a fixed compute budget, increasing the granularity G(i.e., adding more heads) results in a slightly higher per-token cost, necessitating a compensatory reduction in the training token count Dcompared to a dense baseline.

5Family d_model ffn_size num_attention_heads n_layers exit_layer 

2B 2048 6144 16 27 10 

3B 2304 6912 18 36 6, 20 

4B 2560 7680 20 41 4, 16, 18  

> Table 2

Architectural hyperparameters of the familial model variants and their exit configurations. 

# 4 Results 

4.1 Analysis And Visualization of Fitted Familial Models Scaling Law 

For the representative experimental group, the fitted scaling relation takes the form: 

L(N, D, G ) = 



1.18 + 408 .69 

N 0.3006 + 3120 .14 

D0.3514 



· G0.041 , (2) To facilitate interpretation, we visualize this relationship as a three-dimensional loss surface over (N, D, G ).In the rendered 3D plot (Figure 1), the horizontal axes represent model scale N and training token count D,while the vertical axis shows the fitted loss L. The surface reveals the characteristic smooth decay in loss as both model size and data scale increase, along with a mild upward tilt along the granularity dimension G,reflecting the small but consistent multiplicative penalty encoded by the exponent γ = 0 .041 . 

> Figure 1 Three-dimensional visualization of the fitted familial model scaling law.

The horizontal axes represent the model size N and training token count D (both in log scale), while the vertical axis shows the fitted loss L. The surface reveals a smooth decay in loss as scale increases. The slight upward variation across granularity levels G reflects the minimal multiplicative penalty encoded by the small exponent γ ≈ 0.041 , indicating that Familial Models maintain near-optimal scaling behavior. 

64.2 Efficiency Frontier 

Figure 2 illustrates the implied efficiency frontier for a fixed granularity of G = 3, providing a quantitative framework to guide the training and design of Familial Models. By mapping the relationship between model size N and data scale D under a specific granularity, this frontier facilitates a principled approach to the “one-run, many-models” paradigm. Specifically, the plot overlays the theoretical efficient frontier on top of the empirical isoloss contours derived from our fitted scaling law L(N, D, G = 3) . We observe that the frontier maintains the characteristic power-law shape found in dense model scaling (e.g., Chinchilla), suggesting that the fundamental trade-off between parameter efficiency and data efficiency persists even when optimizing for multiple exits. However, the presence of granularity imposes a constraint that slightly shifts the optimal allocation. The results indicate that to support three distinct operating points without compromising the performance of the backbone, practitioners should adopt a model-data ratio that closely tracks the compute-optimal trajectory of dense models, with only marginal adjustments to compensate for the shared capacity requirements. This visualization indicates that for a given compute budget, the scaling law can be used to identify the optimal parameter count and token budget required to maintain efficiency across multiple exits. For example, by intersecting a specific IsoFLOP line with the blue efficient frontier curve, one can pinpoint the exact (N ∗, D ∗)

pair that minimizes the joint loss. Such a framework allows practitioners to navigate the trade-offs between training costs and deployment flexibility, aiming to ensure that the unified pipeline produces sub-models that adhere to near-optimal scaling behavior, effectively amortizing the training cost across the entire family.         

> Figure 2 Compute-efficient frontier for granularity G= 3 .The plot displays the isoloss contours and the implied efficiency frontier (blue line) across varying FLOPs budgets. It indicates that moderate increases in training tokens Dcan compensate for smaller model sizes Nto maintain constant loss.

# 5 Discussion 

The establishment of the familial model scaling law L(N, D, G ) not only optimizes the training of general-purpose LLMs but also provides a theoretical foundation for diverse downstream applications requiring dynamic resource adaptation.  

> Empirical Validation of Granularity Efficiency.

Our rigorous fitting results reveal that the granularity exponent γ

is extremely small, indicating that under matched model size ( N ) and data scale ( D), increasing granularity 

G—i.e., adding more exit layers so that a single training run yields more deployable sub-models—induces only a very mild multiplicative change in loss. In practical terms, the fitted factor Gγ stays close to 1 over a wide range of G, meaning that the loss surface is only weakly sensitive to the number of exits. This implies a favorable architectural trade-off: familial model training can amortize high pretraining costs across multiple 7deployment sizes with minimal degradation in predictive quality. By attaching intermediate exits to a shared trunk, practitioners can obtain a spectrum of sub-models at different inference budgets from a single training run, while largely preserving the scaling behavior expected from dense single-exit training.  

> Extending to Complex Modalities and Tasks.

This inherent flexibility holds significant promise for other domains. For instance, in Multi-Intent Spoken Language Understanding (SLU), recent surveys highlight the necessity of joint modeling to capture complex intent-slot interactions (Wu et al., 2025). Familial architectures could adaptively allocate compute based on the complexity of the user’s utterance, efficiently handling multi-intent scenarios with lower latency. Similarly, in the realm of multimedia security, frameworks like Aperture have demonstrated the value of patch-aware mechanisms for joint forgery detection and localization ( ?). Future work could explore integrating familial backbones into such detection systems, allowing for rapid, coarse-grained screening at early exits and fine-grained, pixel-level localization at deeper layers.  

> Enabling Collaborative Ecosystems.

Furthermore, the “relay-style” inference capability of familial models aligns naturally with the emerging trend of multi-model collaboration (Shao and Li, 2025). As demonstrated by recent advances in enhanced tool invocation, decoupling reasoning from format normalization via specialized collaborative models significantly improves reliability (Zhang et al., 2025). Familial models can serve as the efficient infrastructure for such agentic workflows, where shallower sub-models handle routine formatting or filtering tasks, while deeper sub-models are reserved for complex reasoning and tool selection, thereby realizing a truly ubiquitous and equitable intelligence ecosystem.  

> Ongoing Work.

Finally, it is important to note that the current parameterization of our scaling law is based on a specific range of model configurations. To further enhance the extrapolability and robustness of the proposed formula, large-scale experiments involving broader spans of model parameters are currently ongoing. We are continuously refining the theoretical framework and validating the law across wider computational regimes to ensure its universality for future large-scale foundation models. 

# 6 Conclusion 

Based on the classic scaling law formulation, this study introduces a granularity factor G to extend the scaling law specifically for familial models, providing a theoretical foundation for the “train once, obtain multiple models” paradigm. By fitting over 50 sets of experimental data, we derive the following unified formula: 

L(N, D, G ) = 



1.18 + 408 .69 

N 0.3006 + 3120 .14 

D0.3514 



· G0.041 , (2) Experimental results indicate that the exponent γ for granularity G is extremely small ( γ ≈ 0.04 ). This suggests that for a given model size N and training token count D, increasing granularity G incurs only a negligible penalty on the loss function. Since the value of Gγ remains very close to 1 across a wide range, the average loss of familial models exhibits extremely low sensitivity to variations in granularity. This characteristic offers significant advantages for engineering practice: under an equivalent compute budget, familial models can spawn multiple sub-models of varying sizes in a single training run without significantly compromising performance, thereby adaptively meeting diverse application requirements. Currently, the model parameter sizes utilized to fit the familial models scaling law are relatively concentrated; thus, the extrapolability of the formula warrants further experimental verification. To enhance the robustness and generalization of the proposed law, ongoing experiments are being conducted involving model parameters with significantly larger spans. our research preliminarily demonstrates the superiority of familial models in engineering practice. By enabling the acquisition of multiple differing-sized models through a single training run, this architecture satisfies the demand for diverse deployment scales under fixed compute budgets while maintaining performance comparable to dense model baselines. 

# References  

> Hongjun An, Wenhan Hu, Sida Huang, Siqi Huang, Ruanjun Li, Yuanzhi Liang, Jiawei Shao, Yiliang Song, Zihan Wang, Cheng Yuan, et al. Ai flow: Perspectives, scenarios, and approaches (2025). arXiv preprint arXiv:2506.12479 .

8Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176 , 2023. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022. Xinyi Hou, Yanjie Zhao, and Haoyu Wang. Llm applications: Current paradigms and the next frontier. arXiv preprint arXiv:2503.04596 , 2025. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning , pages 2790–2799. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR , 1(2):3, 2022. Keke Huang, Yimin Shi, Dujian Ding, Yifei Li, Yang Fei, Laks Lakshmanan, and Xiaokui Xiao. Thriftllm: On cost-effective selection of large language models for classification queries. arXiv preprint arXiv:2501.04901 , 2025. Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor Mudge, Jason Mars, and Lingjia Tang. Neurosurgeon: Collaborative intelligence between the cloud and mobile edge. ACM SIGARCH Computer Architecture News , 45(1): 615–629, 2017. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 ,2020. Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pióro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Król, Tomasz Odrzygóźdź, Piotr Sankowski, et al. Scaling laws for fine-grained mixture of experts. 

arXiv preprint arXiv:2402.07871 , 2024. Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems , 35:30233–30249, 2022. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica. vllm: Easy, fast, and cheap llm serving with pagedattention. See https://vllm. ai/(accessed 9 August 2023) , 2023. Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, and Jae W Lee. Any-precision llm: Low-cost deployment of multiple, different-sized llms. arXiv preprint arXiv:2402.10517 , 2024. Tim Pearce and Jinyeop Song. Reconciling kaplan and chinchilla scaling laws. arXiv preprint arXiv:2406.12907 , 2024. Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, and Yair Carmon. Resolving discrepancies in compute-optimal scaling of language models. Advances in Neural Information Processing Systems , 37:100535–100570, 2024. Serhiy O Semerikov, Tetiana A Vakaliuk, Olga B Kanevska, Oksana A Ostroushko, and Andrii O Kolhatin. Edge intelligence unleashed: a survey on deploying large language models in resource-constrained environments. Journal of Edge Computing , 4(2):179–233, 2025. Jiawei Shao and Xuelong Li. Ai flow at the network edge. IEEE Network , pages 1–1, 2025. doi: 10.1109/MNET.2025. 3541208. Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd international conference on pattern recognition (ICPR) , pages 2464–2469. IEEE, 2016. Changxin Tian, Kunlong Chen, Jia Liu, Ziqi Liu, Zhiqiang Zhang, and Jun Zhou. Towards greater leverage: Scaling laws for efficient mixture-of-experts language models. arXiv preprint arXiv:2507.17702 , 2025. Di Wu, Ruiyu Fang, Liting Jiang, Shuangyong Song, Xiaomeng Huang, Shiquan Wang, Zhongqiu Li, Lingling Shi, Mengjiao Bao, Yongxiang Li, et al. Multi-intent spoken language understanding: a survey of methods, trends, and challenges. Vicinagearth , 2(1):20, 2025. 

9Cheng Yuan, Zhening Liu, Jiashu Lv, Jiawei Shao, Yufei Jiang, Jun Zhang, and Xuelong Li. Task-oriented feature compression for multimodal understanding via device-edge co-inference. ArXiv , abs/2503.12926, 2025. URL 

https://api.semanticscholar.org/CorpusID:277066029 .Yudian Zhang, Hao Sun, Mengxi Jia, Haijiang Zhu, Dell Zhang, and Xuelong Li. Enhanced tool invocation method through multi-model collaboration. Vicinagearth , 2(1):18, 2025. 

10