Title: HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation

URL Source: https://arxiv.org/pdf/2512.23464v1

Published Time: Tue, 30 Dec 2025 02:50:43 GMT

Number of Pages: 16

Markdown Content:
Tencent Hunyuan 

# HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation 

Tencent Hunyuan 3D Digital Human Team 

https://huggingface.co/tencent/HY-Motion-1.0 

https://github.com/Tencent-Hunyuan/HY-Motion-1.0 

Abstract 

We present HY-Motion 1.0 , a series of state-of-the-art, large-scale, motion genera-tion models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Trans-former (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm — including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models — to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity. Locomotion  

> Daily
> Activities
> Fitness & Outdoor Activities
> Game
> Character
> Actions
> Social
> Interactions &
> Leisure
> Sports &
> Athletics
> Locomotion
> Daily
> Activities
> Fitness & Outdoor Activities
> Game
> Character
> Actions
> Social
> Interactions &
> Leisure
> Sports &
> Athletics
> Ours
> Instruction-Following Capability Motion Quality

Figure 1: Top: Comparison of HY-Motion 1.0 to state-of-the-art text-to-motion models (DART [45 ], LoM [ 3], GoToZero [ 7], and MoMask [ 9]). Bottom: Example results generated by HY-Motion 1.0 (retargeted to different characters). 1

> arXiv:2512.23464v1 [cs.CV] 29 Dec 2025

1 Introduction 

The creation of high-quality 3D content has become a cornerstone of modern digital experiences, driving innovation in fields ranging from virtual and augmented reality (VR/AR) and interactive entertainment to robotics and digital human applications. Traditionally, generating realistic 3D human animations has required complex technical skills and a significant amount of time for well-trained animators, even with the assistance of expensive motion capture (MoCap) systems. In recent years, the advance of generative AI has opened a new frontier: text-to-motion generation, a paradigm that aims to democratize 3D human animation by allowing creators to generate complex motions simply by describing them in natural language. Significant progress in this domain has been fueled by various motion generation models [39, 9, 45, 5, 44 , 43 , 4, 14 , 11 , 40 , 30 ] and motion datasets [ 10 , 24 , 25 , 18 , 14 , 12 , 15 , 8], with recent efforts exploring scaling motion generation with Large Language Models (LLMs) [ 42 , 13 , 3, 23 , 1, 7]. On one hand, models such as MoMask [ 9] and DART [ 45 ] have shown promising results. However, being relatively small in model size, their generative capabilities are often limited, struggling to understand user instructions and produce motions of high complexity. On the other hand, models like LoM [ 3] and GoToZero [ 7], have leveraged Large Language Models to generate motions by expanding vocabulary with discrete motion tokenizers. While this approach enhances semantic diversity, the quantization process inherent in discrete representations often leads to a degradation in motion quality, resulting in outputs that lack smoothness and naturalness. These divergent approaches highlight several persistent challenges that hinder the widespread adoption and commercial viability of text-to-motion models. First, motion fidelity and realism remain a primary concern; models frequently produce results with noticeable artifacts such as foot sliding, unnatural physics, or the aforementioned lack of smoothness. Second, the semantic alignment between the input text and the output motion remains a significant hurdle, as models often fail to accurately interpret complex or nuanced instructions. Third, unlike in image and video generation, the scaling of motion generation diffusion models has been relatively underexplored, leaving their full potential for enhanced quality and instruction-following largely untapped. Finally, all these challenges are compounded by a data bottleneck: the lack of large-scale, diverse, and meticulously cleaned and annotated motion datasets. To address these critical gaps, we present HY-Motion 1.0, a new series of state-of-the-art, large-scale models for text-to-motion generation. Our work marks a significant leap forward by being the first to successfully scale a Diffusion Transformer (DiT)-based architecture [27], which leverages a flow matching objective [ 19 , 6, 17 ], to the billion-parameter scale in the motion generation domain. This scaling, combined with a novel and comprehensive training methodology, enables HY-Motion 1.0 to achieve unprecedented levels of motion quality and instruction-following precision, significantly outperforming existing open-source solutions. To the best of our knowledge, we are the first to successfully scale a Diffusion Transformer (DiT)-based architecture [ 27 ] to a billion-parameter scale, outperforming previous open-source state-of-the-art methods by a substantial margin, in terms of motion quality and the instruction-following capability. Specifically, the foundation of our success lies in a full-stage training paradigm. Firstly, a large-scale pretraining is conducted on a motion dataset over 3,000 hours, including various motion categories and variations featuring maximal diversity and coverage, to grasp the motion structure and the semantic correlation between text and motion. Secondly, we conduct fine-tuning on 400-hour high-quality text-motion pairs curated through meticulous motion filtering and caption filtering, resulting in better motion quality and more accurate instruction-following. Crucially, we introduce a final alignment stage using reinforcement learning (RL) from both human feedback and reward models to ensure the generated motions are not only physically plausible but also precisely aligned with human preferences and textual commands. This entire framework is supported by our newly developed meticulous data processing pipeline, which integrates automatic motion data cleaning, processing, and captioning with extensive manual refinement, resulting in a dataset with the industry’s widest coverage of over 200 distinct motion categories. In this paper, we detail the data pipeline, model architecture, and training process of HY-Motion 1.0. Our primary contributions can be summarized as follows: • Scaling Law for Text-to-Motion : We are the first to successfully scale a DiT-based flow matching model to over one billion parameters for text-to-motion generation, demonstrating significant improvements in instruction following capabilities of text-to-motion models. 2• Comprehensive Full-Stage Training Paradigm : We implement a three-stage training framework for the text-to-motion model training — large-scale pretraining, high-quality fine-tuning, and reinforcement learning — that holistically enhances model performance. • Meticulous Data Curation Pipeline : We designed and implemented a meticulous data curation pipeline, including both automated processing and extensive manual refinement for data cleaning and captioning. This rigorous process has yielded a large-scale, high-quality motion dataset, enabling the training of a large-scale network capable of generating high-quality motions accurately following the text instruction. • Open-Source Release : We are releasing the HY-Motion 1.0 models to the open-source community to foster further research and accelerate the development of commercially viable 3D motion generation technologies. 

2 Data 

Our data curation process consisted of acquisition, processing, filtering, and captioning, as illustrated in Fig. 2. We first describe our data acquisition, processing, and filtering in Sec. 2.1. Then we present our motion captioning process in Sec. 2.2 and motion taxonomy in Sec. 2.3. Motion   

> Retargeting
> Video Motion
> Estimation
> Motion Filtering
> & Canonicalization
> Motion
> Captioning
> Human Verification In -the -wild
> Videos
> MoCap Data
> & Animation Assets
> Final
> Dataset

Figure 2: Overview of the data processing pipeline. 

2.1 Data Acquisition and Filtering Data Acquisition. Our dataset was constructed from three complementary data sources: in-the-wild human motion videos, motion capture data, and 3D animation assets. Human motion videos provided rich and diverse actions captured across varied real-world scenarios. Motion capture data, typically recorded in controlled indoor environments, offered high-quality motion data but were limited in scene diversity. 3D animation assets, hand-crafted by professional artists for game production, exhibited exceptional motion quality but were relatively expensive and limited in quantity. For video data, we started with a massive collection of 12 million high-quality, in-the-wild video clips from HunyuanVideo [ 38 ]. We first processed these clips through a rigorous pre-processing stage, employing shot boundary detection to segment videos into coherent scenes and a human detector to search for clips containing human subjects. Subsequently, for the resulting candidate clips, we utilized a state-of-the-art human motion extraction algorithm GVHMR [ 36 ], to reconstruct 3D human tracks of SMPL-X [ 26 ] parameters. For motion capture data and 3D animation assets, we acquired approximately 500 hours of motion sequences. 

Motion Processing and Filtering To facilitate downstream model training, we standardized all motion data onto a unified SMPL-H skeleton. We processed all motions in three steps: retargeting, low-quality motion filtering, and canonicalization. We retargeted all motions onto a neutral SMPL-H skeleton [ 33 ]. For motion data originally in SMPL [ 22 ], SMPL-H [ 33 ], or SMPL-X [ 26 ] formats, we employed mesh fitting to convert them into the unified SMPL-H representation. For data with other skeletal structures, we applied a retargeting tool to map them to the SMPL-H skeleton. For all retargeted data, we established a comprehensive filtering pipeline to remove low-quality motion clips. This included removal of duplicates, abnormal poses, outliers of joint velocity, detection of anomalous displacements, pruning of static motions, and detection of artifacts such as foot-sliding. 3After filtering, we applied canonicalization to standardize the data. All motions were resampled to 30 fps, and sequences longer than 12 seconds were segmented into multiple clips. Each motion was normalized to a canonical coordinate frame: Y-axis up, the starting position centered at the origin, the lowest body point aligned on the ground plane, and initial facing direction along the positive Z-axis. Ultimately, we obtained over 3000 hours of motion data, including 400 hours of high-quality 3D motion data. 

2.2 Motion Captioning 

Our captioning pipeline primarily leveraged vision-language models (VLMs) for automatic annotation, followed by manual verification and refinement. Finally, we employed large language models (LLMs) to diversify and enrich the textual descriptions. For video-sourced data, we directly used the original videos corresponding to each motion sequence. For 3D motion data, we textured and rendered the SMPL-H model to generate synthetic videos. These videos, along with optimized prompts dedicated to human motion, were fed into a VLM (e.g ., Gemini-2.5-Pro) to obtain preliminary captions and action keywords. To obtain as accurate a text-motion pair as possible for the high-quality data (i.e., rendered motions), we conducted manual verification on the VLM outputs, correcting erroneous descriptions and supplementing missing key motion components. For all generated captions, we utilized an LLM to standardize the caption structure while preserving the original semantics and to create diverse paraphrases for data augmentation. 

2.3 Taxonomy 

To organize our dataset, we established a three-level hierarchical motion taxonomy based on textual captions and keywords. As illustrated in Figure 3, the hierarchy began with six coarse-grained, top-level categories: (a) Locomotion, (b) Sports & Athletics, (c) Fitness & Outdoor Activities, (d) Daily Activities, (e) Social Interactions & Leisure, and (f) Game Character Actions. These categories were progressively subdivided, culminating in over 200 fine-grained motion classes at the leaf level. 

3 Model Design 

The overview framework of HY-Motion 1.0 is shown in Fig. 4. The core is our HY-Motion DiT model, which accepts a text prompt and an expected duration of the motion as inputs and generates a clip of 3D human motion as outputs. We first detail our motion representation in Sec. 3.1. Then the HY-Motion DiT model is described in Sec. 3.2. Besides the core model, we train an independent LLM (Sec. 3.3) to analyze user prompts and predict the expected duration of the motion clips, and additionally, convert user prompts into better-structured prompts. 

3.1 Motion Representation 

We employ the skeleton definition of SMPL-H [ 34 ] (22 joints without hands). Formally, we denote a motion sequence as x = {f1, f2, . . . , fN }, where each frame is represented as a vector f ∈ R201 ,comprising the global root translation t ∈ R3, the global body orientation r ∈ R6, the local joint rotations jr ∈ R21 ×6, and the local joint positions jp ∈ R22 ×3. All rotational parameters adhere to the continuous 6D representation [ 46 ]. Noticeably, our representation is similar to DART [ 45 ], where the rotation representation is compatible with typical animation workflows, which differs from the commonly used HumanML3D [ 10 ] representation. We further remove the explicit temporal derivatives (velocities) and foot contact labels, as we observed faster training convergence. 

3.2 HY-Motion DiT Model Architecture. Similar to HunyuanVideo [ 16 ], we employ a hybrid Transformer that com-bines dual-stream and single-stream processing to model the joint distribution of motion and text. The network begins with dual-stream blocks, where motion latents and text tokens are processed via independent QKV projections and MLPs. Crucially, they interact through a joint attention mecha-nism, allowing motion features to query semantic cues from the text while preserving their distinct, 4Locomotion 

> Sports & Athletics

Fitness & Outdoor Activities 

> Daily Activities

Social Interactions & Leisure 

> Game Character Actions
> Horizontal Movement
> Special Movement
> Vertical Movement
> Ball Sports
> Precision Sports
> High Jump
> Long Jump
> Throwing Events

Track Events 

Gym & Strength Training 

Stretching 

Yoga 

> Aerial Sports
> Land Exploration
> Outdoor Games
> Water Sports
> Winter Sports
> Basic Object Interaction
> Basic Postures
> Eating & Cooking
> Fine Motor Skills
> Housework
> Involuntary Actions
> Office & Study
> Personal Care

Relaxation & Rest 

Multi-person Contact 

Solo Rhythmic Gestures 

Solo Semantic Gestures 

Dance 

Gymnastics & Acrobatics 

Imitation & Role-play 

Instrument Playing 

Martial Arts 

> Theatrical Performance
> Defense
> Firearm Attack
> Hit Reaction
> Magic Attack
> Melee Attack
> Melee Weapon Attack
> Land Vehicles
> Crawling
> Climbing
> Tennis
> Running High Jump
> Shot Put
> Crunch
> Plank
> Leg Press Stretch
> Child’s Pose
> Pigeon Pose
> Warrior I Pose
> Skydiving
> Throwing Paper Air..
> Curling
> Lifting
> Sitting
> Twisting a Cap
> Sweeping
> Making a Phone Call
> Applying (Lotion/C..
> Shaving
> Waking Up
> Hand Gestures
> Cha-Cha
> Modern Dance
> Handstand
> Piano
> Catwalk / Runway W..
> Cannon/Bazooka Fir..
> Staff Spell
> One-handed Sword S..
> Riding a Motorcycle

Motion Categories Figure 3: The hierarchy of our motion categories. User 

Prompt Duration Prediction 

& Prompt Rewrite 

Predicted Duration 

Optimized 

Prompt LLM 

Encoder 

HY -Motion 

Di T

Text Embeddings 

Noisy Latents 

“Kick a ball” 

“A person kicks a 

ball, extending 

their leg forward.” 

“5 seconds” 

Generated Motion 

Figure 4: Overview of the HY-Motion 1.0 framework. modality-specific representations. These streams subsequently merge in the single-stream blocks, where motion and text tokens are concatenated into a unified sequence. Here, we employ parallel spatial and channel attention modules to facilitate deep multimodal fusion and information exchange. 

Text Encoders. We leverage a hierarchical dual-conditioning strategy to integrate both fine-grained and global text guidance. Firstly, we employ Qwen3-8B [ 41 ] to extract rich, token-wise seman-tic embeddings. Addressing the limitation that LLMs typically utilize causal attention—which restricts the context for non-autoregressive generation, we follow HunyuanVideo [ 16 ] by adopting a Bidirectional Token Refiner. This module transforms the causal LLM features into bidirectional representations before injecting them into the dual-stream blocks, thereby providing holistically 5HY -Motion DiT            

> Timestep Optimized Prompt Noisy Motion
> PE Qwen3 -8B
> Token Refiner
> CLIP -L
> MLP Linear MLP
> Double Stream
> Block
> Single Stream
> Block
> 1/3 Nx
> 2/3 Nx
> LayerNorm
> Scale & Shift
> Linear
> Mod
> +y
> Output
> Mod
> y
> Attention ( w/full RoPE )
> LayerNorm
> Scale & Shift
> Linear
> Linear
> QK -Norm
> Gate
> LayerNorm
> Scale & Shift
> MLP
> Gate
> Motion Tokens
> Motion Output
> LayerNorm
> Scale & Shift
> Linear
> Linear
> QK -Norm
> Gate
> LayerNorm
> Scale & Shift
> MLP
> Gate
> Text Tokens
> Text Output
> Double Stream Block
> Mod
> y
> Single Stream Block
> Mod
> yInput
> LayerNorm
> Scale & Shift
> Linear
> MLP QK -Norm
> Attention
> (w/full RoPE )
> Linear
> Gate
> Output

Figure 5: Model architecture of our HY-Motion DiT. contextualized guidance for the diffusion model. Complementarily, we utilize CLIP-L [ 31 ] to extract a global text embedding. This embedding is concatenated with the timestep embedding and injected via a separated AdaLN [ 28 ] mechanism, where layer-specific modulation parameters are learned to adaptively regulate feature statistics throughout the network. 

Attention Mechanism and Positional Encoding. We devise a composite Attention Mechanism that incorporates specific masking strategies for both cross-modal interaction and temporal modeling. First, to regulate multimodal information flow, we enforce an asymmetric attention mask. While motion tokens attend globally to the text sequence to extract semantic cues, text tokens are explicitly masked from the motion latents. This unidirectional constraint prevents the diffusion noise inherent in the motion states from propagating back to the text embeddings, thereby preserving the integrity of the semantic conditioning. Second, for temporal modeling within the motion branch, we implement a narrow band mask strategy. Predicated on the hypothesis that kinematic dynamics are governed primarily by local continuity, we restrict attention to a sliding window of 121 frames under 30 fps. This imposes a locality inductive bias that decomposes long sequences into coherent substructures, effectively focusing the model on complex local dynamics while ensuring linear computational complexity. To spatially ground these attention interactions, we optimize the Positional Encoding by adopting full Rotary Positional Embeddings (RoPE) [ 37 ]. Diverging from disjoint encoding schemes, we concatenate text and motion embeddings into a single sequence prior to encoding. By applying RoPE across this unified sequence, we establish a continuous relative coordinate system, enabling the model to inherently resolve the positional correspondence between specific textual tokens and temporal motion frames. 

Flow Matching Objective. We employ Flow Matching [19] to construct a continuous probability path that bridges the standard Gaussian noise distribution and the complex motion data distribution. We adopt the optimal transport path, defined as a linear interpolation xt = (1 − t)x0 + tx1, which implies a constant target velocity [ 21 ]. The training objective is to minimize the Mean Squared Error (MSE) between the predicted and ground-truth velocity: 

LFM = Et, x0,x1 [|| vθ (xt, c, t ) − vt|| 22], (1) where x1 represents the clean motion data, x0 ∼ N (0 , I) is the initial noise, and the target velocity is given by vt = x1 − x0. During inference, the generation process is formulated as an Ordinary Differential Equation (ODE): dx/d t = vθ (xt, c, t ). Starting from random noise x0, we recover the clean motion x1 by numerically integrating this ODE along the predicted velocity field using an ODE solver ( e.g ., Euler). 63.3 Duration Prediction & Prompt Rewrite 

To accommodate diverse user inputs, we employ a dedicated LLM for both duration prediction and prompt rewrite. This module first predicts the target motion’s duration from the user prompt. It then transforms the prompt into a structured format optimized for our DiT model. This design is motivated by the observation that LLMs possess inherent common-sense knowledge about the typical duration of human activities, enabling them to infer temporal length from textual descriptions. To further enhance its accuracy, we fine-tune the LLM on a dataset of ground-truth motion durations, which aligns its predictions with our training data distribution. Crucially, this fine-tuning preserves the model’s ability to generalize to unseen motion descriptions. 

Data Synthesis. Our dataset for fine-tuning the LLM consists of data triplets { user prompt , opti-mized prompt , duration }. The “optimized prompt” and “duration” are the ground-truth text caption and duration of each motion clip in our motion dataset, respectively. To simulate realistic “user prompt”, we employed a powerful LLM ( e.g ., Gemini-2.5-Pro) to synthesize a dataset of user queries. Using the ground-truth text captions as a basis, we prompted the LLM to generate inputs that mimic the nature of casual human commands. These synthetic queries are intentionally diverse, encompass-ing informal language, a mix of English and Chinese, and varying levels of specificity, including queries that are deliberately brief and ambiguous. 

Two-Stage Fine-Tuning. The duration prediction & prompt rewrite module is fine-tuned from a Qwen3-30B-A3B [41] model with a two-stage training procedure. • Supervised Fine-Tuning (SFT): The model is first fine-tuned on the constructed dataset with ground-truth supervision. It learns to sequentially generate “optimized prompt” and “duration” from the “user prompt”, effectively grounding the temporal prediction in the semantic context of the rewritten text. • Reinforcement Learning (RL): To further mitigate hallucinations and enhance fidelity, we reinforce the model using Group Relative Policy Optimization (GRPO) [ 35 ]. We utilize a more powerful model (Qwen3-235B-A22B-Instruct-2507 [ 41 ]) as a reward judge to evaluate multiple candidate outputs for each prompt. The reward function assesses two critical dimensions: semantic consistency, ensuring the rewrite remains faithful to the user’s intent, and temporal plausibility, verifying that the predicted duration aligns with the physical complexity of the described action. By optimizing against the relative advantages of these candidates, GRPO steers the policy toward generating instructions that are both semantically precise and temporally coherent. 

4 Model Training 

The training process for HY-Motion DiT consists of three stages: large-scale pretraining, high-quality fine-tuning, and reinforcement learning. To reconcile the trade-off between generalization (requiring massive scale) and precision (requiring high-quality data), we implement a “scale-then-refine” curriculum for the supervised training phases (Sec. 4.1 and Sec. 4.2). We begin with large-scale pretraining on an expansive dataset comprising over 3, 000 hours of motion data, enabling the model to assimilate a broad motion prior and learn a generalized representation of human movement. This is followed by a high-quality fine-tuning stage on nearly 400 hours of meticulously curated and annotated data, which serves to sharpen the model’s capabilities and refine the details of the generation. While these supervised stages effectively model the underlying data distribution, strictly imitating the training distribution does not necessarily maximize perceptual quality or semantic responsiveness. Therefore, we introduce a final reinforcement learning phase (Sec. 4.3) to bridge the gap between “statistical likelihood” and “human preference”. By incorporating feedback from reward models, this stage fine-tunes the model to optimize for perceptual motion quality and maximize adherence to complex user instructions—metrics that are often difficult to capture through supervised loss functions alone. 

4.1 Large-Scale Pretraining 

The primary objective of this foundational stage is general motion prior acquisition. To maximize the model’s exposure to diverse semantic scenarios and kinematic patterns, we train the model on our full 7motion dataset Dall consisting of over 3, 000 hours of motion data. This dataset prioritizes coverage over precision, comprising both high-quality captures and noisy, in-the-wild video extractions, accompanied by text descriptions ranging from manual annotations to VLM-generated annotations. In this phase, we employ a standard Flow Matching objective (Eq. (1)) with a constant learning rate 

ηpre , aiming to rapidly establish a broad support for the motion distribution. From a distributional perspective, this stage allows the model to effectively “learn to move”. We observe a rapid convergence in training loss, indicating that the model successfully captures the fundamental dynamics of human movement and establishes a robust mapping between diverse textual prompts and plausible poses. The model demonstrates strong semantic generalization, capable of responding to a vast vocabulary of actions. However, the generated motion quality inevitably mirrors the mixed nature of the input distribution. Since a significant portion of Dall contains noisy data, the model’s output distribution remains high-entropy and loosely constrained. Consequently, while semantically correct, the generated sequences often exhibit artifacts such as high-frequency jitter, foot sliding, and minor anatomical inconsistencies. We treat this result as an expected outcome: the model has successfully learned a generalized but coarse prior, providing a semantic-rich initialization that is ready to be "sharpened" in the subsequent fine-tuning stage. 

4.2 High-Quality Fine-Tuning 

Following the broad acquisition of motion priors, the second stage focuses on distribution refinement. To bridge the gap between “plausible” and “high-quality”, we transition the training source to the meticulously curated subset DHQ . This dataset is characterized by high-quality, rigorously filtered motion data, paired with manually corrected textual descriptions. The objective here is to concentrate the model’s probability density around the optimal modes of the motion manifold. Crucially, to refine fine-grained kinematic details while preventing the forgetting of the broad semantic priors learned in Sec. 4.1, we decay the learning rate to ηft = 0 .1 × ηpre . This conservative optimization strategy ensures that the model polishes its output distribution without collapsing its generative diversity. Empirically, this phase drives a qualitative evolution from “roughly correct” to “precise and smooth”. Kinematically, we observe that the model effectively suppresses the noise patterns inherited from the pretraining phase, drastically reducing high-frequency jitter and foot sliding artifacts while enforcing stricter anatomical consistency. Semantically, the alignment becomes significantly more rigorous; the model demonstrates enhanced sensitivity to fine-grained anatomical instructions—such as accurately distinguishing between “waving the left hand” and “waving the right hand”—which were often ambiguous in the noisy pretraining data. Importantly, this sharpening of quality and control is achieved without a significant degradation in motion diversity, validating the efficacy of our coarse-to-fine curriculum. 

4.3 Reinforcement Learning 

While supervised learning establishes a strong kinematic prior, optimizing for data likelihood does not strictly equate to maximizing perceptual fidelity or instruction adherence. Generative models often suffer from a misalignment gap where statistically probable motions may still exhibit physical artifacts or semantic ambiguity. To bridge this gap, we propose a synergistic two-phase reinforcement learning, progressing from human preference learning to explicit objective constraint satisfaction. 

Human Preference Alignment via DPO. We first aim to internalize the nuanced standards of human perception. Since defining a closed-form reward function for complex semantic alignment and motion quality is intractable, we leverage Direct Preference Optimization (DPO) [ 32 ] to steer the policy directly from human feedback. We utilize the Stage-2 model to generate candidate pairs for a diverse prompt set. From an annotated pool of 40 , 000 pairs, we curate a high-information subset of 9, 228 pairs where human judges identified a distinct “winner”( xw) and “loser”( xl) based on instruction adherence and visual plausibility. By maximizing the likelihood margin between these pairs, DPO implicitly models the latent reward function of human perception: 

LDPO (πθ ; πref ) = −E(c,xw ,xl)∼D pref 



log σ



β log πθ (xw|c)

πref (xw|c) − β log πθ (xl|c)

πref (xl|c)

 

, (2) 8Table 1: Instruction-following capability comparison with state-of-the-art text-to-motion models. Motion categories: (a) Locomotion, (b) Sports & Athletics, (c) Fitness & Outdoor Activities, (d) Daily Activities, (e) Social Interactions & Leisure, and (f) Game Character Actions. Method Catg. (a) (b) (c) (d) (e) (f) Avg. SSAE MoMask [9] 2.98 2.41 2.09 2.07 2.38 1.97 2.31 58.0% GoToZero [7] 2.80 2.23 2.07 2.00 2.32 1.74 2.19 52.7% DART [45] 2.91 2.47 2.03 2.07 2.40 2.05 2.31 42.7% LoM [3] 2.81 2.07 1.95 2.00 2.39 1.84 2.17 48.9% 

HY-Motion 1.0 3.76 3.18 3.15 3.06 3.25 3.01 3.24 78.6% 

Table 2: Motion quality comparison with state-of-the-art text-to-motion models. Motion categories: (a) Locomotion, (b) Sports & Athletics, (c) Fitness & Outdoor Activities, (d) Daily Activities, (e) Social Interactions & Leisure, and (f) Game Character Actions. Method Catg. (a) (b) (c) (d) (e) (f) Avg. MoMask [9] 3.05 2.91 2.58 2.66 2.77 2.81 2.79 GoToZero [7] 3.11 3.01 2.69 2.72 2.89 2.81 2.86 DART [45] 3.38 3.33 2.94 2.95 3.06 3.07 3.11 LoM [3] 3.14 3.08 2.98 3.01 3.14 3.01 3.06 

HY-Motion 1.0 3.59 3.51 3.28 3.37 3.43 3.41 3.43 

where πref represents the frozen reference policy, and Dpref = {(ci, xiw, xil )}Ni=1 . This phase effec-tively acts as a semantic anchor, drastically improving the model’s “Pass Rate” by pruning low-quality modes that, while kinetically valid, fail to align with user intent. 

Physics & Semantic Refinement via Flow-GRPO. Building upon the preference-aligned policy, we address the limitations of DPO in enforcing strict physical and semantic boundaries. We employ Flow-GRPO [ 20 ], a variant of Group Relative Policy Optimization tailored for flow matching models, to optimize explicit objectives. Unlike standard PPO which requires a value network, Flow-GRPO stabilizes training by normalizing advantages within a group of sampled outputs for a given prompt. We maximize the following objective: 

LFlow-GRPO (πθ ; πref ) = Ec∼D GRPO ,{xi}Gi=1 ∼πref (·| c)

h

f (r, ˆA, θ, ϵ, β )

i

, (3) where the objective function f is averaged over time steps T :

f (r, ˆA, θ, ϵ, β ) = 1

G

> G

X

> i=1

1

T

> T

X

> t=1



min  rit(θ) ˆAi, clip (rit(θ), 1 − ϵ, 1 + ϵ) ˆAi

− βD KL (πθ ∥πref )



, with rit(θ) = pθ (xit−1|xit, c)

pref (xit−1|xit, c) .

(4) Here, DGRPO denotes the prompt dataset used for GRPO, G denotes the group size, and ˆAi is the standardized advantage derived from the composite reward R(xi, c). Specifically, the reward function integrates two critical metrics: a semantic reward ( Rsem ) evaluated by a custom-trained Text-Motion Retrieval (TMR) model [ 29 ], and a physical reward ( Rphy ) that imposes hard penalties on artifacts such as foot sliding and root drift. By explicitly optimizing this objective, the model is fine-tuned to satisfy rigorous kinematic constraints while maximizing semantic precision. 

5 Evaluation 

5.1 Comparison with State-of-the-Art 

To evaluate HY-Motion 1.0 against state-of-the-art text-to-motion models, we constructed a diverse test set comprising over 2000 text prompts. This test set spans six major categories (Sec. 2.3) and 9Table 3: Instruction-following capability comparison of different model sizes. Motion categories: (a) Locomotion, (b) Sports & Athletics, (c) Fitness & Outdoor Activities, (d) Daily Activities, (e) Social Interactions & Leisure, and (f) Game Character Actions. The model “DiT-0.46B-400h” is trained only on the 400-hour high-quality dataset, while the other models are pretrained on the 3,000-hour dataset. Method Catg. (a) (b) (c) (d) (e) (f) Avg. DiT-0.05B 3.68 2.95 2.85 3.10 3.12 2.90 3.10 DiT-0.46B 3.93 3.15 3.02 3.10 3.15 2.88 3.20 DiT-0.46B-400h 3.76 2.95 2.73 2.93 3.17 2.75 3.05 DiT-1B 3.95 3.32 3.23 3.36 3.27 2.92 3.34 

Table 4: Motion quality comparison of different model sizes. Motion categories: (a) Locomotion, (b) Sports & Athletics, (c) Fitness & Outdoor Activities, (d) Daily Activities, (e) Social Interactions & Leisure, and (f) Game Character Actions. Method Catg. (a) (b) (c) (d) (e) (f) Avg. DiT-0.05B 3.32 2.54 2.95 2.86 3.00 2.80 2.91 DiT-0.46B 3.68 3.32 3.10 3.10 3.22 3.13 3.26 DiT-0.46B-400h 3.83 3.02 3.23 3.29 3.27 3.20 3.31 DiT-1B 3.78 3.20 3.33 3.26 3.27 3.23 3.34 

covers simple atomic actions as well as concurrent and sequential action combinations. Human annotators were asked to rate the generated motions on a scale of 1 to 5 from two perspectives: instruction-following capability and motion quality. Fig. 1 and Tabs. 1–2 show the comparison with four models: DART [ 45 ], LoM [ 3], GoToZero [ 7], and MoMask [ 9 ]. Fig. Fig. 6 presents some examples of the visual comparison. Our model significantly outperforms the other models in terms of both instruction-following capability and motion quality. We further adopt an automatic evaluation approach, namely Structured Semantic Alignment Evalua-tion (SSAE) [ 2], which utilizes powerful video-VLMs to assess the generated motions. This approach transforms the text-motion alignment problem into a video-question-answering task. It works by decomposing each input prompt into a series of “yes” or “no” questions. These questions are then presented to a video-VLM ( e.g ., Gemini-2.5-Pro) along with the rendered video of the generated motion. For example, given the prompt “a person swings their arm while shooting a soccer ball”, the decomposed questions might include: “is the person kicking their leg?”, “is the person swinging their arm?”, and “does the person appear to be shooting a soccer ball?”. The correct rate of the VLM’s answers across all test prompts constitutes the model’s SSAE score. A comparison of SSAE scores with other models is provided in Tab. 1. 

5.2 Scaling Experiments 

To understand the effects of scale, we investigated how model size and data volume impact per-formance. The results of our human evaluation on models of varying sizes are presented in Tabs. 3–4. Our findings indicate that while instruction-following capability consistently improves with larger models, motion quality reaches a saturation point beyond the 0.46B parameter size. Notably, a comparison between the "DiT-0.46B" and "DiT-0.46B-400h" models (Tab. 3) underscores the critical role of the larger 3,000-hour training dataset in enhancing instruction-following capability. Consequently, we plan to release "DiT-1B" as the primary HY-Motion 1.0 model and "DiT-0.46B" as a lightweight "Lite" version. 

6 Conclusion 

In this work, we presented HY-Motion 1.0, a series of large-scale motion generation models that establish a new state-of-the-art in the text-to-motion domain. By successfully scaling a DiT-based flow matching architecture and implementing a meticulous data curation pipeline alongside a com-prehensive, full-stage training paradigm, we have provided a clear and effective path for developing 10 high-performance, instruction-following motion generation models. Our work yields key insights into the principles of scaling text-to-motion models: • The Duality of Data Scale and Quality : Our experiments reveal a clear distinction in how data properties affect model capabilities. We found that scaling the volume of training data is the primary driver for enhancing instruction following and semantic understanding. In contrast, improving the quality of the data through careful curation is the decisive factor for increasing motion fidelity and physical realism. • Effectiveness of the Multi-Stage Training Paradigm : We demonstrate that our three-stage training framework — large-scale pretraining, high-quality fine-tuning, and reinforcement learning — is essential. This "coarse-to-fine" approach effectively balances the trade-off between motion diversity and precision, suggesting that this data-centric, multi-stage optimization strategy is a robust path forward for the field. 

Limitations. Despite these advancements, HY-Motion 1.0 exhibits certain limitations that point towards future research directions: • Complex Instructions : While our model significantly outperforms baselines in semantic alignment, it still faces challenges with highly detailed or complex instructions. This is partially due to the inherent difficulty of our data captioning pipeline, i.e., creating complete and accurate textual descriptions for nuanced and intricate motions remains a significant challenge for both VLM-based captioning and the manual refinement process. • Human-Object Interaction (HOI) : As our current dataset primarily focuses on body kinemat-ics without explicit object geometry, the model may struggle to generate physically accurate interactions with external objects (e.g., precise contact points when holding a tool). We are releasing HY-Motion 1.0 to the open-source community and hope it serves as a solid baseline, inspiring further exploration and accelerating the development of scalable, high-quality motion generation technologies. 11 Contributors 

• Project Sponsors: Jie Jiang, Linus, Yuhong Liu • Project Supervisor: Xiaolong Li • Project Leader: Linchao Bao • Core Contributors: Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen • Contributors: – Product/Project Managers: Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An 

– Artists: Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang 

– Engineering: Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen 

– Data: Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang 

– Evaluation: Zhongyi Fan, Yifan Li, Zhichao Hu 12 A person turns their body to face the front-left . A person does Triangle Pose to the left side .

MoMask 

GoToZero 

DART 

LoM 

Ours A person draws a bow to its fullest and holds, keeping their body and head stable and facing forward. A person swings their arm while shooting a soccer ball .

MoMask 

GoToZero 

DART 

LoM 

Ours Figure 6: Examples of visual comparison to state-of-the-art models. 13 References 

[1] Bin Cao, Sipeng Zheng, Ye Wang, Lujie Xia, Qianshan Wei, Qin Jin, Jing Liu, and Zongqing Lu. A real-time controllable vision-language-motion model. In ICCV , 2025. [2] Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951 ,2025. [3] Changan Chen, Juze Zhang, Shrinidhi K Lakshmikanth, Yusu Fang, Ruizhi Shao, Gordon Wetzstein, Li Fei-Fei, and Ehsan Adeli. The language of motion: Unifying verbal and non-verbal language of 3d human motion. In CVPR , 2025. [4] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In CVPR , page 18000–18010, Jun 2023. [5] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. In ECCV , pages 390–408, 2025. [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning , 3 2024. [7] Ke Fan, Shunlin Lu, Minyue Dai, Runyi Yu, Lixing Xiao, Zhiyang Dou, Junting Dong, Lizhuang Ma, and Jingbo Wang. Go to zero: Towards zero-shot motion generation with million-scale data. In ICCV , pages 13336–13348, October 2025. [8] Chuan Guo, Inwoo Hwang, Jian Wang, and Bing Zhou. Snapmogen: Human motion generation from expressive texts. 7 2025. [9] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In CVPR , pages 1900–1910, 2024. [10] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 5152–5161, 2022. [11] Ziyan Guo, Zeyu Hu, De Wen Soh, and Na Zhao. Motionlab: Unified human motion generation and editing via the motion-condition-motion paradigm. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 13869–13879, October 2025. [12] Félix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in-betweening. 

ACM Transactions on Graphics , 39, 7 2020. [13] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a foreign language. NeurIPS , 36, 2024. [14] Boeun Kim, Hea In Jeong, JungHoon Sung, Yihua Cheng, Jeongmin Lee, Ju Yong Chang, Sang-Il Choi, Younggeun Choi, Saim Shin, Jungho Kim, and Hyung Jin Chang. Personabooth: Personalized text-to-motion generation. In CVPR , pages 22756–22765, 3 2025. [15] Makito Kobayashi, Chen-Chieh Liao, Keito Inoue, Sentaro Yojima, and Masafumi Takahashi. Motion capture dataset for practical use of ai-based motion editing and stylization. 7 2023. [16] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: A systematic framework for large video generative models. Technical report, Tencent, 12 2024. [17] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 

14 [18] Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Yansong Tang, and Xiu Li. Finedance: A fine-grained choreography dataset for 3d full body dance generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 10234–10243, 8 2023. [19] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR 2023 , 10 2022. [20] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. In Advances in neural information processing systems 39 , 10 2025. [21] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023 , 9 2022. [22] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. ACM TOG , 2015. [23] Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, and Ruimao Zhang. Scamo: Exploring the scaling law in autoregressive motion generation model. In 

Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR) , pages 27872–27882, June 2025. [24] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In ICCV , pages 5442–5451, 2019. [25] Ian Mason, Sebastian Starke, and Taku Komura. Real-time style modelling of human locomotion via feature-wise transformations and local motion phases. Proceedings of the ACM on Computer Graphics and Interactive Techniques , pages 1–18, 1 2022. [26] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from a single image. In 

CVPR , 2019. [27] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV , pages 4195–4205, 2023. [28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 4195–4205, 12 2023. [29] Mathis Petrovich, Michael J Black, and Gül Varol. Tmr: Text-to-motion retrieval using contrastive 3d human motion synthesis. In ICCV , pages 9488–9497, 2023. [30] Ekkasit Pinyoanuntapong, Muhammad Saleem, Korrawe Karunratanakul, Pu Wang, Hongfei Xue, Chen Chen, Chuan Guo, Junli Cao, Jian Ren, and Sergey Tulyakov. Maskcontrol: Spatio-temporal control for masked motion synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 9955–9965, 2025. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International conference on machine learning , pages 8748–8763, 2 2021. [32] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Advances in neural information processing systems 37 , pages 53728–53741, 7 2023. [33] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) , 36(6), November 2017. [34] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) , 36, 11 2017. [35] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Technical report, 4 2024. [36] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia Conference Proceedings , 2024. 

15 [37] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing , 568:127063, 2024. [38] Tencent Hunyuan Foundation Model Team. Hunyuanvideo 1.5 technical report, 2025. [39] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In ICLR , 2023. [40] Lixing Xiao, Shunlin Lu, Huaijin Pi, Ke Fan, Liang Pan, Yueer Zhou, Ziyong Feng, Xiaowei Zhou, Sida Peng, and Jingbo Wang. Motionstreamer: Streaming motion generation via diffusion-based autoregressive model in causal latent space. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 10086–10096, October 2025. [41] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. Technical report, 5 2025. [42] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2M-GPT: Generating human motion from textual descriptions with discrete representations. In CVPR , 2023. [43] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE TPAMI , 2024. [44] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. ReMoDiffuse: Retrieval-augmented motion diffusion model. In ICCV , pages 364–373, October 2023. [45] Kaifeng Zhao, Gen Li, and Siyu Tang. DartControl: A diffusion-based autoregressive motion model for real-time text-driven motion control. In ICLR , 2025. [46] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation represen-tations in neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. , pages 5745–5753, 6 2019. 

16