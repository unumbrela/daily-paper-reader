
# CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models

**Authors**: Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang
**Date**: 2025-12-29

**Tags**: <span class="tag-label tag-green">keywords: symbolic regression</span> <span class="tag-label tag-green">keywords: transformer</span>

---

## Abstract
...


---

## 论文详细总结（自动生成）

以下是对论文《CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models》的详细结构化总结。

### 1. 论文的核心问题与整体含义（研究动机和背景）

*   **核心问题**：大型视觉语言模型在多模态理解和生成任务中取得了显著进展，但仍然倾向于产生“幻觉”，即生成的文本内容与视觉输入不一致。这种现象限制了其在现实应用中的可靠性。
*   **研究动机**：现有的缓解幻觉的方法大多依赖额外的监督信号或重新训练模型，这限制了其可扩展性。虽然近年来出现了无需训练的解码策略（如对比解码），但现有的基于反馈的方法通常存在以下局限：
    *   仅依赖原始图像的单分辨率输入，忽略了多尺度的视觉结构。
    *   生成反馈通常以事后方式使用，仅用于重排序，未深度介入 token 级别的生成过程。
    *   缺乏细粒度、逐步的反馈修正机制。
*   **整体含义**：受人类视觉从全局场景感知到细节检查的认知过程启发，本文旨在提出一种无需训练的解码框架，通过“由粗到细”的生成式反馈机制，在解码过程中整合多层级视觉线索，以抑制幻觉并提高生成内容的忠实度。

### 2. 论文提出的方法论：核心思想、关键技术细节、公式或算法流程

*   **核心思想**：提出 **CoFi-Dec** 框架，构建一个“由粗到细”的生成式自反馈循环。通过将图像分解为粗粒度和细粒度视图，生成相应的文本假设，再将这些文本重构为合成图像，最后利用这些合成图像作为视觉反馈引导解码。
*   **关键技术细节**：
    1.  **分层视觉分解**：
        *   将输入图像 $I_0$ 分解为两组互补的图块：
            *   **粗粒度视图** $I_c$：通过均匀下采样将图像分割为低分辨率图块，保留全局空间结构。
            *   **细粒度视图** $I_f$：基于显著性区域或不确定区域的高分辨率裁剪，突出局部判别特征。
    2.  **多粒度响应生成与合成**：
        *   利用 LVLM 分别基于原图、粗视图和细视图生成文本响应 $R_0, R_c, R_f$。
        *   使用文本到图像生成模型（如 Stable Diffusion）将粗粒度和细粒度文本响应转化为伪图像 $v_c$ 和 $v_f$。这些图像代表了模型对自身信念的“视觉想象”。
    3.  **自校正解码与融合**：
        *   在解码的每一步 $t$，分别计算基于原图、合成粗图像和合成细图像的条件 Token 分布。
        *   引入 **Wasserstein 重心优化** 机制来融合这三个分布。与简单的平均不同，该方法在语义 Token 空间中寻找几何一致的共识，鼓励平滑且有语义意义的修正。
*   **算法流程**：
    *   输入：图像 $I_0$ 和文本提示 $T$。
    *   处理：生成 $I_c, I_f$ -> 生成文本 $R_c, R_f$ -> 生成图像 $v_c, v_f$。
    *   解码：在每一步通过 Wasserstein Barycenter 融合 $P(y_t|I_0, \dots), P(y_t|v_c, \dots), P(y_t|v_f, \dots)$ 来生成下一个 Token。

### 3. 实验设计：使用了哪些数据集 / 场景，它的 benchmark 是什么，对比了哪些方法

*   **注**：提供的文本在“实验设计”部分前截断，以下信息主要基于摘要和引言部分提取。
*   **数据集 / 场景**：文中提到在 **六个专注于幻觉的基准测试** 上进行了广泛实验。这些基准涵盖了**实体级**和**语义级**的幻觉评估场景。
*   **Benchmark**：虽然具体名称在截断文本中未完全列出，但摘要指出涵盖了典型的多模态幻觉评估任务（如图像描述和视觉问答）。
*   **对比方法**：与现有的**解码策略**进行了对比，特别是那些无需训练的基线方法（例如对比解码等）。

### 4. 资源与算力

*   **状态**：提供的文本截断于方法论部分，未包含实验设置或硬件资源相关的章节。
*   **说明**：文中未明确提及使用的 GPU 型号、数量或具体的训练/推理时长。考虑到该方法属于“训练-free”框架，主要算力消耗应集中在推理阶段，特别是引入了文本到图像模型的生成过程，这会增加推理时的计算成本。

### 5. 实验数量与充分性

*   **实验数量**：摘要提到在 **六个** 幻觉基准测试上进行了实验。
*   **充分性评估**：
    *   从描述来看，实验覆盖了不同维度的幻觉（实体级和语义级），显示出实验设计具有一定的全面性。
    *   文本声称该方法“大幅减少”了幻觉并“优于现有解码策略”，表明作者进行了充分的对比验证。
    *   **客观性**：由于缺乏具体的实验数据表格，无法直接评估其统计显著性或消融实验的细节，但框架设计本身的模块化（粗/细分支、Wasserstein 融合）通常意味着作者会在正文中进行消融实验以验证各组件的有效性。

### 6. 论文的主要结论与发现

*   **主要结论**：CoFi-Dec 能够在无需额外训练的情况下，有效缓解 LVLMs 的幻觉问题。
*   **具体发现**：
    *   结合粗粒度和细粒度的视觉信息对于避免误导性的全局解释至关重要（例如，避免将红绿灯误认为是红墨镜）。
    *   通过生成式视觉反馈和 Wasserstein 几何融合机制，能够调和高层语义一致性和细粒度视觉基础，从而产生更稳健、更忠实的输出。
    *   该框架是模型无关的，可广泛应用于各种预训练的 LVLMs。

### 7. 优点：方法或实验设计上有哪些亮点

*   **无需训练**：不需要对原始 LVLM 进行微调或额外的权重训练，降低了部署门槛，保持了模型原有的通用能力。
*   **仿生学设计**：模仿人类视觉“先粗后细”的认知过程，通过多粒度视图分解，有效捕捉了不同尺度的语义信息。
*   **创新的融合机制**：摒弃了简单的概率平均，采用 Wasserstein barycenter 进行分布融合，尊重了词汇空间的几何结构，使得修正过程更符合语义逻辑。
*   **闭环反馈**：将文本假设转化为视觉反馈并重新输入模型，形成了一个自验证的自校正回路，增强了生成内容与视觉信号的对齐。

### 8. 不足与局限

*   **推理延迟**：引入了文本到图像生成模型（如 Stable Diffusion）来合成反馈图像，这会显著增加推理时间和计算开销，可能不适用于对实时性要求极高的场景。
*   **反馈质量依赖**：方法的有效性部分依赖于文本到图像模型生成的质量。如果生成的伪图像质量较差或未能准确反映文本假设，可能会引入错误的反馈。
*   **数据覆盖局限**：虽然实验了六个基准，但在更复杂的长文本生成或特定领域的专业图像上的表现仍有待验证（基于文本截断情况的推断）。