Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans
Sky CH-Wang•
Justin Svegliato◦
Helen Appel◦
Jason Eisner⊖
•Columbia University
◦Microsoft
⊖Johns Hopkins University
skywang@cs.columbia.edu, {jsvegliato, helenappel}@microsoft.com, jason@cs.jhu.edu
Abstract
We present a method and dataset for fine-
tuning language models with preference su-
pervision using feedback-driven improvement
chains. Given a model response, an annota-
tor provides fine-grained feedback by mark-
ing “liked” and “disliked” spans and specifying
what they liked or disliked about them. The
base model then rewrites the disliked spans ac-
cordingly, proceeding from left to right, form-
ing a sequence of incremental improvements.
We construct preference pairs for direct align-
ment from each adjacent step in the chain, en-
abling the model to learn from localized, tar-
geted edits. We find that our approach out-
performs direct alignment methods based on
standard A/B preference ranking or full con-
trastive rewrites, demonstrating that structured,
revision-based supervision leads to more effi-
cient and effective preference tuning.
1
Introduction
Large language models (LLMs) achieve strong per-
formance across natural language tasks by fine-
tuning on human preferences (Ouyang et al., 2022;
Bai et al., 2022), also known as reinforcement learn-
ing from human feedback (RLHF). Direct align-
ment methods such as direct preference optimiza-
tion (DPO) (Rafailov et al., 2023) tune the LLM
directly on preference pairs.1 To construct a pref-
erence pair, it is typical to sample two random re-
sponses from a model and then ask humans to rate
which one is better (Ouyang et al., 2022). Unfor-
tunately, these judgments can be difficult to make
and noisy in nature (Chowdhury et al., 2024), as
it is rare for either response to dominate the other
in the sense of being better in all aspects. We pro-
pose an alternative human feedback framework in
which sampled responses are revised to produce
preference pairs with clearer and more meaningful
preference distinctions. In our framework, humans
1Alternative methods tune it using a reward model trained
to predict preferences on such pairs (Christiano et al., 2017).
are used not to compare responses but rather to
indicate opportunities for revision (Figure 1).
We introduce a novel dataset in which human
annotators highlight specific spans that they “like”
or “dislike” in (long) model responses. They also
indicate why they liked or disliked each span, using
a taxonomy we propose. This provides far more
information than a one-bit A/B preference ranking,
while adding negligible annotation overhead (§3).
Our novel dataset could help improve an LLM in
many ways, such as by providing multi-objective
and localized reward signals (Wu et al., 2023).
However, in this paper, we go beyond extracting
reward signals from the span-level annotations. In-
stead, we prompt the original response model to ex-
amine all the span-level feedback jointly and make
targeted revisions to address it, yielding improved
responses that dominate the originals and can be
used as preference pairs for direct alignment meth-
ods. We test how different strategies for building
preference pairs with this feedback—single-span
rewrites, full all-at-once rewrites, and cumulative
step-by-step rewrites—affect preference learning
and downstream models under direct alignment.
We find that cumulative rewrites yield the strongest
models and may improve sample efficiency.
If the LLM represents an infinite population of
responses, our approach to improving it resembles
the Lamarckian theory of evolution. Through re-
vision, each response adapts to its environment of
human judges; fine-tuning then ensures that these
beneficial adaptations are inherited by the next gen-
eration. Standard RLHF uses the less efficient Dar-
winian mechanism of selecting for adaptive traits
that already appear in the population through nat-
ural variation.2 We find that our Lamarckian ap-
proach leads to better alignment with less training.
2In this analogy, traits are not passed into the next genera-
tion of LLM responses by producing descendants of individ-
ual “organisms” as in evolution. Instead, the LLM parameters
are tuned to place more probability on either the adapted re-
sponses (Lamarckian) or the adaptive responses (Darwinian).
This tends to increase the prevalence of their traits in the next
generation by also raising the probability of similar responses.
arXiv:2512.23693v1  [cs.CL]  29 Dec 2025


———————
———————
———————
———————
———————
———————
———
Write me 
an actionable
business plan
for this 
academic paper.
Att. [arXiv paper]
…factually wrong.
…too many options…
…misrepresents….
…engaging …funny.
…quotes from …document.
It draws a sensible conclusion.
Okay, …
———————
———————
———————
———————
———————
———————
———
———————
———————
———————
———————
————— ——
———————
——
———————
———————
———————
———————
—————— —
———————
———
———————
———————
———————
———————
———————
—————
…
(a,a1)
(a1,a2)
a*
+ 1 Edit
Fully Edited
+ 1 Edit
+ 1 Edit
Figure 1: An overview of our critique-guided improvement chain framework. A model response is annotated
with span-level feedback (left) in the form of like and dislike highlights—each accompanied by labeled rationale
checkboxes—and then revised step-by-step (right) into a final fully-edited response (a∗). Each adjacent pair
(ai, ai+1) in the improvement sequence is used as a preference pair for direct alignment. Blue indicates a rewrite.
2
Related Work
Feedback
Formats.
Traditional
preference-
tuning approaches often rely on simple A/B
comparisons of complete responses (Ouyang et al.,
2022): human or LLM judges (Cui et al., 2024)
select the preferred response without providing
any rationale. More recent efforts have sought
richer supervision data. Human or LLM judges
may deliver fine-grained and/or multidimensional
feedback on individual responses (Wu et al., 2023;
Wang et al., 2024; Ye et al., 2025), preference
pairs (Cui et al., 2024; Just et al., 2025), or
revision pairs (Guo et al., 2023). Feedback on real
chatbot responses can also be imputed from the
conversational reactions that they elicit (Lin et al.,
2024; Shi et al., 2024).
More recently, another paradigm invites do-
main experts to directly edit generated responses
(Chakrabarty et al., 2025). In our evolution analogy,
this corresponds to intelligent design. But when hu-
mans are the editors, it often imposes a high cogni-
tive load and struggles to scale. By contrast, in our
lower-effort Lamarckian method, human reviewers
simply highlight favored and disfavored segments
of text and select and/or write brief explanations.
The responses then “adapt” to this environmental
feedback.
Self-training.
Since Huang et al. (2022), many
papers obtain high-quality responses through some
expensive multi-step LLM workflow, and then use
those responses for supervised fine-tuning (which
teaches the LLM to generate them in a single step)
or for preference optimization (which teaches the
LLM to prefer them to its current single-step re-
sponses: Dong et al., 2024; D’Oosterlinck et al.,
2025). Our approach fits into this line of work, but
we aim for human alignment by identifying a step
of our multi-step workflow—feedback on spans—
where human supervision is relatively cheap.3 The
final responses are still generated by the current
LLM and are therefore, we hope, achievable by
fine-tuning. This is reminscent of supervising ma-
chine translation through “hope” responses that are
generated by the current machine translation sys-
tem, but under a modified objective that encourages
them to be similar to a human-provided reference
response (Chiang, 2012).
Preference Optimization.
Direct Preference Op-
timization (DPO) (Rafailov et al., 2023) enables
stable fine-tuning from preference pairs without
learning an explicit reward function. However,
its effectiveness depends heavily on how prefer-
ence data are constructed. Recent work explores
improvements including curriculum learning (Pat-
tnaik et al., 2024), optimal preference pair selection
(Xiao et al., 2025), and alternative loss formula-
tions (Pal et al., 2024).
Since our preference pairs differ only by small
local edits, we experiment with various training
losses for direct alignment (D’Oosterlinck et al.,
2025; Pal et al., 2024) that are designed to handle
highly similar preference pairs. Several studies
have reported that DPO can fail catastrophically
in this setting, for example due to “likelihood dis-
placement” (Razin et al., 2025).
3
Data
Response Generation.
Our study focuses on
long-form in-context generation—e.g., the gen-
eration stage of retrieval-augmented generation.
The length and informational density of typical
responses in this setting make it natural for users to
form varying judgments about different parts of the
output. This creates a natural fit for localized feed-
back, yet traditional preference learning methods
3In future work, we would like to evaluate the impact on
cost and alignment quality of automating this step.
2


Domain
N
H+
H−
ΣA
#w
Yelp
86
45
397
1589
36k
News
81
49
379
1693
31k
Wikipedia
67
24
334
1375
28k
arXiv
43
27
193
863
19k
Total
277
145
1303
5520
115k
Table 1: Annotation statistics by domain. N is the num-
ber of responses annotated. H+ and H−are the number
of span-level like and dislike highlights. ΣA is the total
number of categorical attributes selected (checkboxes
ticked), and #w is the total number of words high-
lighted, as calculated via whitespace tokenization.
offer only global comparisons, making it difficult
to isolate what exactly should be improved.
To construct a set of model responses for hu-
man annotation, we first sample documents from
four diverse long-context domains: (1) Yelp re-
views (Zhang et al., 2015), (2) News articles from
Multi-News (Fabbri et al., 2019), (3) Wikipedia
pages (Wikimedia Foundation, 2024), and (4) AI-
related arXiv papers.4 For each document, we use
GPT-4.1 (OpenAI, 2025) to generate 5 challenging
user queries—designed to elicit responses requir-
ing reasoning, synthesis, or subjective judgment—
that could plausibly retrieve the document in a
retrieval-augmented generation setup. For each
query, we sample 2 independent responses from
Llama-3.1-8B-Instruct (Grattafiori et al., 2024)
with 0.8 temperature and 0.95 top-p sampling, us-
ing a fixed prompt template that includes the source
document. Our prompt templates are provided in
Appendix A with example queries and responses.
Annotation.
We began with a pilot study in
which 3 annotators were presented with the query,
source document, and the 2 model responses, and
asked to consider what they would have wanted to
see in a good response to the query, given the doc-
ument. Annotators highlighted spans they liked or
disliked in 100 model responses to 50 queries and
gave a brief explanation for each marked span as a
natural-language phrase or sentence. We conducted
thematic analysis following Braun and Clarke
(2006)’s 6-phase approach to derive a taxonomy of
common preference attributes (20 like/19 dislike).
With the full taxonomy defined (Appendix B), we
augmented the annotation interface to let annota-
tors provide categorical feedback by selecting at-
tribute checkboxes for each highlighted span.
4https://hf.co/datasets/jamescalam/ai-arxiv2
We then presented 4 new annotators with pairs
of Llama responses. They (1) highlighted spans
they liked or disliked in each response, (2) marked
span attributes (plus optional free-text feedback),
and (3) made an A/B preference judgment with a
brief explanation. Dataset statistics are in Table 1.
On average, they marked 0.5 like and 4.7 dislike
spans per response, with 3.8 attributes per span.
Our instructions to annotators are in Appendix C.
To measure inter-annotator agreement, 100 items
were annotated by all annotators. Agreement on
A/B preference rankings was moderate (Fleiss
κ = 0.47), reflecting the difficulty of such judg-
ments (see §1). Agreement on exact spans and their
attributes was negligible. This is unsurprising for a
fine-grained subjective feedback task and does not
mean that annotators actively disagreed: an annota-
tor was not asked to label every good and bad aspect
of the response and might omit many labels from
other annotators5 that they would nonetheless en-
dorse as reasonable if asked. Thus, we treat the ob-
served variability as additional information rather
than noise: it reveals the breadth of human prefer-
ences that the LLM should accommodate. Because
each decision is paired with checkbox rationales,
the dataset still provides richly grounded, inter-
pretable supervision from which models can learn.
Improvement Sequence Generation.
To con-
struct a step-wise improvement sequence from
a response annotated by a particular annoa-
tor, we prompt the original response model
(Llama-3.1-8B-Instruct) with four inputs: the
initial response, this annotator’s complete feedback
on liked and disliked spans, the source document,
and an instruction explicitly requesting a sequence
of incremental edits—where each step addresses
exactly one disliked span, proceeding from left to
right. The goal is to produce a chain of responses
in which each adjacent pair differs by a single, tar-
geted revision. An overview of the framework is in
Figure 1, with full prompts in Appendix A.
Note that the annotator is free to mark a narrow
span (e.g., highlighting an imprecise word), and
the LLM is equally free to edit beyond the span’s
boundaries if needed to fix the problem (e.g., re-
thinking the whole clause, or changing the word
but also adjusting the adjacent context to maintain
coherence and fluency). We do apply a Levenshtein
distance heuristic to ensure structural compliance:
5Or deviate from them, conveying very similar feedback
with slightly different span boundaries or checkboxes.
3


sequences that fail this check (e.g., due to multi-
edit steps or non-contiguous rewrites) are discarded
and regenerated. This process yields 277 valid im-
provement sequences for a total of 1303 unique
improvement steps.
Annotation Time.
The time and financial cost
of the LLM calls was negligible compared to the
human annotation. We logged how long annotators
spent on each item, omitting outliers beyond 1.5
times the inter-quartile range from the median to
reduce the influence of breaks and distractions. No-
tably, annotating a pair of responses with our full
protocol (455 seconds) was only 9% slower than
simple A/B preference ranking (419 seconds). This
low overhead likely results from A/B ranking al-
ready being difficult, especially in our task. Annota-
tors had to carefully read the deliberately challeng-
ing query and both long responses and implicitly
reason about their preferences. Prior work similarly
finds that making such implicit rationales explicit
adds little additional burden (Zaidan et al., 2007).
Notice that A/B ranking produces only 1 pair for
preference tuning, while our protocol can produce
1 + 4.7 + 4.7 = 10.4 pairs on average (the A/B
comparison, plus one synthetic pair per dislike span
on each of A and B). This makes our protocol more
than 9× faster per pair.6
In future work, it would be worth investigating
reduced-cost protocols. Was it necessary to require
the comparison of two responses (to force careful
reading), or could the annotator simply read and an-
notate a single response? What if they only marked
the 1 or 2 spans that they felt most strongly about?
What if they skipped the like spans (which were not
rewritten), or the span attributes (so that the LLM
revision model had to guess what was wrong with
the span)? Finally, could spans be marked by the
actual user who solicited the response, as a richer
alternative to thumbs-up/thumbs-down feedback?
4
Experiments
With our dataset, we preference-tune Llama-3.1
-8B-Instruct, seeking to understand how differ-
ent forms of feedback-derived preference pairs af-
fect model learning and performance. Specifically—
where (x, y) denotes a pair where y is the winner
6We do caution against simply counting pairs. Perhaps
not all types of pairs are equally likely for training (and our
method did not even train on the A/B comparison). But overall,
Table 2 shows that we improved ELO from 1612 to 1634 with
only a 9% increase in human annotation time.
response and x is the loser—we compare:
• Preference Pairs (a, b) where a and b are two
ranked Llama responses to the same prompt.
• First Edits (a0, a1) where a1 is a single edit that
improves just the first disliked span.
• Full Rewrites (a0, a∗) where a∗is the final cu-
mulative rewrite of the original response a0.
• Stepwise Edits (ai, ai+1) where ai+1 is a step-
wise revision of ai, generated to address a single
span-level critique. Each pair corresponds to two
adjacent steps in the improvement sequence.
We also evaluate performance against two base-
lines: the base model (Llama-3.1-8B-Instruct)
and an SFT baseline (a∗SFT), in which the base
model is fine-tuned on fully-improved a responses.
Following standard post-training protocols, all
preference-tuning methods are applied on top of
the a∗SFT model, with the exception of the (a, b)
setting (in which a∗is unavailable).
We treat
the training loss function as a hyperparameter,
considering DPO (Rafailov et al., 2023), DPO-
Positive (Pal et al., 2024), APO-zero, and APO-
down (D’Oosterlinck et al., 2025). That is, for each
preference pair construction method, we fine-tune
with each training loss and evaluate the method on
test data using the fine-tuned model that performed
best on separate validation data. This procedure
selected APO-down—well-suited for contrastive
preference data—for all methods except (a, b), for
which DPO was better. Hyperparameters for train-
ing configurations are in Appendix D.
A possible advantage of Stepwise Edits is that it
produces more training signal by converting each
annotated response into multiple preference pairs
(ai, ai+1). To assess how much of this method’s
benefit comes from the increased number of pairs
vs. from their diversity (relative to (a0, a1)) or min-
imality (relative to (a0, a∗)), we also try down-
sampling these preference pairs (ai, ai+1)ds to
match the number to other methods.
We evaluate each model based on its Elo score
(Appendix D), computed from 263 pairwise com-
parisons judged by 4 human (ELOH) annotators
on responses generated with a temperature of 0.8
and top-p sampling of 0.95 on a held-out set of
40 prompts. We also report automated annota-
tor (ELOM) results on responses to 138 held-out
prompts (2898 pairwise comparisons), using the
alpaca_eval_gpt4 evaluator in Li et al. (2023)
with the highest correlation to human judgments.
Table 2 shows that our approach significantly
4


Method
ELOH
ELOM
base
1383
(-305,-71)
1355
(-314,-223)
a∗SFT
1377
(-310,-70)
1353
(-313,-234)
(a, b)
1465
(-247,-4)
1376
(-291,-207)
(a0, a1)
1525
(-183,+52)
1435
(-232,-147)
(a0, a∗)
1612
(-155,+52)
1617
(-57,+31)
(ai, ai+1)ds
1620
(-120,+105)
1602
(-76,+28)
(ai, ai+1)
1634
1629
Table 2: Comparative quality of the 7 models’ responses
using ELO scores under human judgments (ELOH) and
alpaca_eval_gpt4 judgments (ELOM). We show a
bootstrap 95% confidence interval for each model’s dif-
ference from the best-performing model. (ai, ai+1) ben-
efits from training on all 1303 stepwise preference pairs;
all other methods generate only 277 pairs.
outperforms standard A/B preference ranking.
Given the negligible annotation overhead relative
to A/B preferences, this highlights the efficiency of
our framework in gathering actionable and effec-
tive preference data from human annotators. When
analyzing how best to utilize this feedback for di-
rect alignment, even preference pairs derived from
single-step edits—targeted revisions addressing a
single highlighted span—outperform A/B prefer-
ence data. Further gains are achieved through cu-
mulative rewriting, where iterative revisions pro-
duce a final response that is often better than any
model output sampled directly. Using the origi-
nal and final responses in this sequence as a pref-
erence pair yields a large improvement in model
quality.7 Finally, incorporating the intermediate
steps as additional training pairs—teaching the
model to prefer each version over its predecessor—
provides a small but not statistically significant
boost. Overall, these results suggest that our an-
notation framework—centered on span-level feed-
back and iterative revision—provides a more effec-
tive/scalable way to elicit high-quality preference
data, enabling better model alignment with mini-
mal additional annotation effort.
5
Conclusion
We offer a low-overhead framework for preference-
tuning that uses span-level feedback and stepwise
rewrites to generate structured improvement se-
quences. By constructing preference pairs from ad-
jacent edits in these sequences, our method enables
models to learn from localized, fine-grained super-
7Such a preference pair could be generated more cheaply
in a single step, rather than through a series of intermediate
steps.
vision. Experiments show that this fine-grained ap-
proach to preference optimization outperforms ex-
isting alignment techniques, highlighting the value
of targeted feedback and minimally contrastive
training signals in shaping model behavior.
We will release our annotation tool and dataset.
Our novel feedback taxonomy is provided in Ap-
pendix B.
Limitations
Alignment Objective.
How to train on revision
data in a principled way is an open question. For
convenience, we experimented with existing pref-
erence optimization methods, but it might be more
appropriate to design new ones. The standard meth-
ods are based on the notion that when humans
stochastically rank a ≺b, this provides additional
evidence that reward(a) < reward(b). In con-
trast, our revision process may tend to improve the
reward of a response, but we do not have a way
of estimating by how much.8 It is not even clear
that reward does improve at each revision step—
perhaps the intermediate steps in (a0, a1, . . . a∗)
have lower reward (and should have lower prob-
ability) because they are internally inconsistent
in style or goals.9 Empirically, we observe that
some standard methods fail: when using DPO on
stepwise-edit-constructed preference pairs, training
can exhibit catastrophic degeneration. It remains
an open question how to design direct alignment
methods that faithfully reflect the structure and se-
mantics of revision-constructed preference data,
while preserving stability and effectiveness during
training.
Frontier Models.
Our experiments are with an
8B-parameter model. It is unknown whether this
workflow would also be able to improve today’s
frontier models.
Removing the Humans.
In principle, the span-
level feedback could itself be provided by the LLM.
We have not yet experimented with this workflow.
At that point, we would be using the LLM to re-
flect on its own answers and improve them, and
then training it to produce or prefer the improved
8Without asking human or LLM judges.
9We also note a technical roadblock: we cannot compute
the closed-form probability of generating a certain revised
response during training. That rules out clipped importance
sampling methods like GRPO, which require this proposal
probability for use as a denominator.
5


answers. Recent work on standard RLHF does in-
creasingly substitute high-quality LLM preferences
for human preferences (e.g., Cui et al., 2024). Com-
pared to humans, AI-generated feedback is more
scalable, easier to collect, and significantly cheaper.
On the other hand, retaining some response-level
feedback from actual humans might be important
to achieve or at least evaluate alignment with actual
human values.
Annotation Instructions.
Our annotation guide-
lines did not tell annotators how many spans to
mark per response (or how many explanations to
mark per span). Best practices for such guidelines
or incentives remain to be worked out. (However,
the (ai, ai+1)ds result in Table 2 suggests that mark-
ing even a single span per response can work well.)
Other Settings.
Our taxonomy that guided the
human feedback was developed specifically for
our RAG setting. Feedback on tasks such as so-
cial dialogue, advice, therapy, tutoring, or code
generation—to give a few examples—would pre-
sumably look quite different; those settings would
require developing new taxonomies. Our annota-
tion interface and revision prompt might also not
extend to a setting like code, where perhaps feed-
back should no longer be attached to textual spans.
References
Yuntao Bai,
Saurav Kadavath,
Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, et al. 2022. Constitutional AI: Harm-
lessness from AI feedback. Computing Research
Repository (CoRR), arXiv:2212.08073.
Virginia Braun and Victoria Clarke. 2006. Using the-
matic analysis in psychology. Qualitative Research
in Psychology, 3(2):77–101.
Tuhin Chakrabarty, Philippe Laban, and Chien-Sheng
Wu. 2025. AI-slop to AI-polish? aligning language
models through edit-based writing rewards and test-
time computation. Computing Research Repository
(CoRR), arXiv:2504.07532.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, 13(40):1159–1187.
Sayak Ray Chowdhury, Anush Kini, and Nagarajan
Natarajan. 2024. Provably robust DPO: Aligning
language models with noisy feedback. In Proceed-
ings of the 41st International Conference on Machine
Learning, pages 42258–42274.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. Ad-
vances in Neural Information Processing Systems,
30.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie,
Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong
Sun. 2024. ULTRAFEEDBACK: Boosting language
models with scaled AI feedback. In Proceedings of
the 41st International Conference on Machine Learn-
ing, volume 235 of Proceedings of Machine Learning
Research, pages 9722–9744. PMLR.
Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui,
and Furu Wei. 2024. Self-boosting large language
models with synthetic preference data. arXiv preprint
arXiv:2410.06961.
Karel D’Oosterlinck, Winnie Xu, Chris Develder,
Thomas Demeester, Amanpreet Singh, Christopher
Potts, Douwe Kiela, and Shikib Mehri. 2025. An-
chored preference optimization and contrastive revi-
sions: Addressing underspecification in alignment.
Transactions of the Association for Computational
Linguistics, 13:442–460.
Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and
Dragomir Radev. 2019. Multi-news: A large-scale
multi-document summarization dataset and abstrac-
tive hierarchical model. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics, pages 1074–1084, Florence, Italy. Asso-
ciation for Computational Linguistics.
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad Al-
Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten,
Alex Vaughan, et al. 2024. The Llama 3 herd of
models. Computing Research Repository (CoRR),
arXiv:2407.21783.
Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin
Zhao, and Ji rong Wen. 2023. Beyond imitation:
Leveraging fine-grained quality signals for alignment.
arXiv preprint arXiv:2311.04072.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,
Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.
Large language models can self-improve.
arXiv
preprint arXiv:2210.11610.
Hoang Anh Just, Ming Jin, Anit Sahu, Huy Phan, and
Ruoxi Jia. 2025. Data-centric human preference with
rationales for direct preference alignment.
arXiv
preprint arXiv:2407.14477.
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B. Hashimoto. 2023. AlpacaEval: An au-
tomatic evaluator of instruction-following models.
https://github.com/tatsu-lab/alpaca_eval.
6


Ying-Chun Lin, Jennifer Neville, Jack W Stokes,
Longqi Yang, Tara Safavi, Mengting Wan, Scott
Counts, Siddharth Suri, Reid Andersen, Xiaofeng
Xu, et al. 2024. Interpretable user satisfaction estima-
tion for conversational systems with large language
models. arXiv preprint arXiv:2403.12388.
OpenAI. 2025. Introducing GPT-4.1 in the API. Ac-
cessed: 2025-04-24.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback.
Advances in Neural
Information Processing Systems, 35:27730–27744.
Arka Pal, Deep Karkhanis, Samuel Dooley, Manley
Roberts, Siddartha Naidu, and Colin White. 2024.
Smaug: Fixing failure modes of preference optimisa-
tion with DPO-positive. Computing Research Repos-
itory (CoRR), arXiv:2402.13228.
Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji,
Vikas Yadav, and Sathwik Tejaswi Madhusudhan.
2024. Enhancing alignment using curriculum learn-
ing & ranked preferences. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2024,
pages 12891–12907, Miami, Florida, USA. Associa-
tion for Computational Linguistics.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2023.
Direct preference optimization: Your lan-
guage model is secretly a reward model. Advances in
Neural Information Processing Systems, 36:53728–
53741.
Noam Razin, Sadhika Malladi, Adithya Bhaskar, Danqi
Chen, Sanjeev Arora, and Boris Hanin. 2025. Unin-
tentional unalignment: Likelihood displacement in
direct preference optimization. In The Thirteenth In-
ternational Conference on Learning Representations.
Taiwei Shi, Zhuoer Wang, Longqi Yang, Ying-Chun Lin,
Zexue He, Mengting Wan, Pei Zhou, Sujay Kumar
Jauhar, Xiaofeng Xu, Xia Song, and Jennifer Neville.
2024. Wildfeedback: Aligning LLMs with in-situ
user interactions and feedback. In NeurIPS 2024
Workshop on Behavioral Machine Learning.
Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams,
Makesh Narsimhan Sreedhar, Daniel Egert, Olivier
Delalleau, Jane Scowcroft, Neel Kant, Aidan Swope,
and Oleksii Kuchaiev. 2024.
HelpSteer: Multi-
attribute helpfulness dataset for SteerLM. In Pro-
ceedings of the 2024 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (Volume
1: Long Papers), pages 3371–3384, Mexico City,
Mexico. Association for Computational Linguistics.
Wikimedia Foundation. 2024. Wikimedia downloads.
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane
Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari
Ostendorf, and Hannaneh Hajishirzi. 2023. Fine-
grained human feedback gives better rewards for lan-
guage model training. Advances in Neural Informa-
tion Processing Systems, 36:59008–59033.
Yao Xiao, Hai Ye, Linyao Chen, Hwee Tou Ng, Li-
dong Bing, Xiaoli Li, and Roy Ka-wei Lee. 2025.
Finding the sweet spot: Preference data construction
for scaling preference optimization. arXiv preprint
arXiv:2502.16825.
Zihuiwen Ye, Fraser David Greenlee, Max Bartolo, Phil
Blunsom, Jon Ander Campos, and Matthias Gallé.
2025. Improving reward models with synthetic cri-
tiques. In Findings of the Association for Computa-
tional Linguistics: NAACL 2025, pages 4506–4520,
Albuquerque, New Mexico. Association for Compu-
tational Linguistics.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using “annotator rationales” to improve machine
learning for text categorization. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 260–267, Rochester, New York. Associa-
tion for Computational Linguistics.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text classi-
fication. Advances in Neural Information Processing
Systems, 28.
7


A
Prompts and Example Responses
Query Generation Prompt.
The following
prompt was provided to gpt-4.1-2025-04-14,
with “{{DOCUMENT}}” replaced by a document
from the target domain (e.g., Wikipedia, Yelp re-
views, news articles, or arXiv papers), in order to
generate queries.
You’re collaborating with a retrieval-
augmented generation (RAG) system.
The document below was retrieved in re-
sponse to a user query.
Your task: Invent five highly creative
and cognitively demanding queries that
could have caused this document to be
retrieved.
These queries should not just use the
document—they should stretch it. Think
of tasks that require deep reasoning, syn-
thesis of ideas, subjective judgment, or
unconventional thinking. Your goal is to
make the model work hard.
Some examples:
• “Propose
a
novel
startup
idea
grounded in the methodologies of
this academic paper, and outline a
go-to-market strategy.”
• “Evaluate the emotional tone across
customer reviews for this hotel and
design a retraining program for staff
based on your findings.”
Guidelines:
• The user did not know the contents
of this document before issuing the
query.
• Each query must necessitate re-
trieving and deeply using the con-
tent of this document.
• Be original: No two queries should
sound alike. Avoid formulaic pat-
terns or generic phrasing.
• Simple
questions,
trivia-style
queries, or those answerable in a
few words are not acceptable.
• Creativity is rewarded. Redun-
dancy is penalized.
Return your output as valid JSON in the
following format: "queries": ["...", "...",
"...", "...", "..."]
## Document
{{DOCUMENT}}
Query Examples.
The queries below are a ran-
dom sample illustrating the types of queries gener-
ated for each domain.
Example 1 (arXiv): Analyze how the
design choices in MERLOT Reserve
regarding time alignment and masking
strategies for audio and text affect the
model’s ability to learn temporal com-
monsense knowledge, and propose an
ablation study to isolate these effects.
Example 2 (Wikipedia): Analyze how
the spontaneous social interactions on
beep lines prefigured the emergence
of online social networks, identifying
unique features and limitations. Com-
pose a comparative critique highlighting
lessons for the design of future virtual
communities.
Example 3 (Yelp): Drawing inspiration
from the review’s depiction of customer
service and venue ambiance, design a
staff training program that balances ‘sass’
with professionalism for quick-service
restaurants.
Example 4 (News): Using the method-
ologies and results described in the study,
propose a hypothetical follow-up experi-
ment that would distinguish between the
effects of pet exposure and other con-
founding factors, such as rural versus
suburban environments or breastfeeding
rates, on infant health outcomes.
Response Generation Prompt.
The following
prompt scaffold was provided to meta-llama/
Llama-3.1-8B-Instruct to generate responses to
the queries.
You are a helpful and intelligent assis-
tant.
Directly answer the user query
while using information in the document
provided.
## Document
{{DOCUMENT}}
## Query
{{QUERY}}
8


Figure 2: Left: the original response. Right: the fully edited response a∗after incorporating all user-highlighted
feedback. Differences are highlighted.
Response Examples.
Figure 2 shows a par-
tial example of an original response generated
by meta-llama/Llama-3.1-8B-Instruct and a
fully rewritten version that reflects the cumulative
result of all step-wise edits addressing user feed-
back.
Response Rewrite Prompt.
The following
system prompt was provided to meta-llama/
Llama-3.1-8B-Instruct to generate step-wise
cumulative improvement sequences. Dislike IDs
provided are randomized.
You are tasked with improving the fol-
lowing generated response based on user
feedback, which includes highlighted
likes and dislikes. Your goal is to pro-
duce a series of incremental and cumu-
lative edits that improve the response by
directly addressing the user’s dislikes.
You will be provided with the following:
User Query: The original request from
the user.
Knowledge Source: A col-
lection of relevant information retrieved
by the system to support the response.
Generated Response with User Feedback
Highlights: The final output produced
by a large language model based on
the query and retrieved knowledge, with
highlighted sections based on user feed-
back. User Feedback Highlight Reasons:
The specific reasons provided by the user
explaining their likes and dislikes for
each highlighted portion of text. These
reasons apply only to the highlighted seg-
ments, not to the response as a whole.
Your instructions: Improve the response
in a sequence of steps. In each step, ad-
dress only one <dislike> span based on
its corresponding feedback. You must
output the entire edited response at each
step—not just the edited portion.
In
each step, remove the <dislike> high-
light tag you are addressing from the
text. Highlight tags for unaddressed
dislikes must be preserved in the edited
text. You may revise surrounding text
if needed to maintain coherence, flow,
or tone. Carefully check and fix any
formatting errors at each step (e.g., in-
correct numbering, bullet points used in-
correctly, inconsistent list formatting).
Avoid expanding or shrinking the over-
all length of the response more than nec-
essary. Try to keep each edit roughly the
same length as the original text unless
the feedback clearly calls for a change.
Do not over-optimize for conciseness
if it comes at the expense of fully ad-
dressing the feedback. Prioritize directly
addressing the user’s feedback meaning-
fully over unnecessary rewording. Impor-
tant: Make sure to do the edits in the OR-
DER OF DISLIKE IDS. That is, start
with ID = 1, then proceed to ID = 2, and
so on.
The user prompt given with this system prompt
is structured as follows:
## User Query
{{User Query}}
9


## Knowledge Source
{{Knowledge Source}}
## Generated Response with User Feed-
back Highlights
{{Response with Feedback Highlights}}
## User Feedback Highlight Reasons
{{Highlight Reasons}}
An example displaying the formatting of the re-
sponse with highlights together with the feedback
reasons that are fed into the prompt above are as
follows:
...Firstly, the document highlights the
half-price offer after 11 PM, which is
particularly appealing to college students.
This pricing strategy allows students to
affordably enjoy a meal, making it a
staple for freshmen and beyond. <like
id=’1’>The document states, "I started
coming here, as all Pitt students do,
freshman year, for the luxury of half-
price food after 11." This suggests
that the economic benefit of the half-
price offer has fostered loyalty and
encouraged repeat visits among stu-
dents.</like id=’1’>...
...{‘id’: 1, ‘explanation’: “I like this be-
cause it states a useful fact. I like this
because it quotes/cites/paraphrases from
the retrieved document.”}
B
Feedback Taxonomy
B.1
Feedback on like spans
• Utility: I like this because...
– It directly answers my question.
– It smoothly leads up to answering my
question.
– It helps my general understanding of the
topic.
– It gives a quick recap/summary.
• Where the information comes from: I like this
because...
– It quotes/cites/paraphrases from the re-
trieved document.
– It echos/repeats/reiterates my query.
– The assistant generated it on its own.
• What the span contributes: I like this be-
cause...
– It states a useful fact.
– It draws a sensible conclusion.
– It assesses how reliable the info is.
– It suggests one or more possibilities (e.g.,
examples, options, explanations, consid-
erations).
– It defines a term or explains a concept.
– It offers an opinion.
– It
corrects
or
clarifies
my
ques-
tion/instructions.
– It flags an important caveat or potential
pitfall.
– It acknowledges a limitation or the un-
certainty involved.
• Craft & style: I like this because...
– It shows careful attention to the details
of my query.
– It’s well-written.
– It’s well-organized.
– It’s engaging—maybe even a little funny.
• Other: Describe why you liked this span.
B.2
Feedback on dislike spans
• Poor content: I dislike this because...
– It states something that’s factually
wrong.
– The opinion or advice it gives is weak or
low-quality.
– It doesn’t add anything useful—totally
unnecessary.
– It’s off-topic or irrelevant to my question.
– The wording is toxic or offensive.
• Misleading sourcing: I dislike this because...
– It credits the wrong source for a claim.
– It misrepresents what the source actually
says.
• Craft & style problems: I dislike this be-
cause...
– It dumps too many options on me at
once.
– The writing is confusing or hard to fol-
low.
10


– It repeats itself unnecessarily.
– It’s too wordy.
– The tone or style doesn’t fit what I ex-
pect.
• Other issues: I dislike this because...
– It feels generic or incomplete.
– It shows the assistant misunderstood my
question or instructions.
– It ignores the instructions I gave.
– It’s too one-sided and misses other per-
spectives.
– It lacks depth or useful detail.
– It just copies the source without adding
insight.
– I just disagree with it.
• Other: Describe why you disliked this span.
C
Annotation Guidelines and Interface
Preamble.
For each annotation item, first take
some time to read the Query. Imagine that you’ve
posed the following query to an AI assistant that
will search the internet for related documents and
webpages to answer your query. Think about what
you would like to see in a given response to this
query. Next, briefly skim the Knowledge Source.
Suppose that this is the document the assistant
found on the internet to base its response to your
query off of. The information present in this doc-
ument and the knowledge present in the language
model behind the assistant will be the only infor-
mation the assistant can use to answer your query.
Given this, think about what you would like to see
in a given response to the query, that is based on
the knowledge present in this knowledge source
document.
Span Highlighting.
You are presented with two
responses to the query above, both of which are
based on the information present in the knowledge
source and the knowledge present in the large lan-
guage model underlying an AI assistant. Read each
response carefully. Your task is to identify and
highlight any sections of text that you liked or dis-
liked in each response. To highlight a span, simply
drag your cursor over the specific text itself; you
can change your current highlight mode (like or
dislike) by clicking on the Like Highlight Mode /
Dislike Highlight Mode button switcher at the top.
Spans you like will be highlighted in green; red for
dislike.
When you finish highlighting a single span, a
popup window will appear asking you why you
liked or disliked the span you highlighted. Read
through the options, ticking the corresponding
checkbox(es) if the reason(s) correspond to why
you liked or disliked the span you just highlighted.
You can tick as many checkboxes as you want here;
if there are any other reasons in addition to those
in the checkbox that made you like the given span,
please tick the final Other checkbox and briefly de-
scribe your reason in the corresponding text box.
Press Save when you’re finished highlighting the
reasons behind your highlight.
You can revisit and edit your answers to each
checkbox easily by simply clicking on the span
you highlighted, which will bring up your answers
you chose for the corresponding span. The span
highlight will darken slightly in color, to indicate
that the window popup currently corresponds to the
indicated span.
Please make these highlights for both Response
A and Response B. When deciding how large or
how small a span to highlight, imagine that you
were directing someone’s attention to the fact that
you liked or disliked a given span. Which parts, if
any, for each would you highlight, for another to
check this as quickly as possible?
Overlapping Spans.
You may like and dislike a
part of or a given span for different reasons—for
example, you may like the content of a span, but
dislike its style or how the information is presented.
In such cases, it’s encouraged to note this down by
having overlapping spans either partially or com-
pletely overlapping spans of text; simply select
the opposite highlight mode and highlight over an
already highlighted span of text.
Response-Level Questions.
Once you’ve fin-
ished highlighting spans of text that you liked and
disliked in both responses, you are now to highlight
why you may have liked or disliked each given
response as a whole. Tick off the corresponding
checkboxes if you feel that they apply to each given
response, and write out any reasons why you may
have liked or disliked a given response as a whole
for reasons not present in the checkboxes given.
AB Preference Ranking.
Finally, think about
both responses, and the annotations you made.
Make a decision—which response did you pre-
11


Figure 3: Span highlighting and rationale providing annotation interface, showing the spans highlighted by a user
and the provided reasons for those highlights.
fer more? Indicate your choice, and write down
why you did so, in the provided space below. Im-
portant: The tie option should be used very, very
rarely—reserve it for the exceptional cases where
the two responses are genuinely indistinguishable
(or equally poor). Try to make a decisive and justi-
fied decision.
Annotation Interface.
Figure 3 illustrates the
annotation interface, displaying two responses side
by side. In the example, a user has selected a span
in Response B. The example reveals the specific
reasons the user provided for disliking the span.
D
Details
Hyperparameters.
Direct alignment models
were post-trained with a maximum sequence length
of 8192 tokens and a global batch size of 4. Op-
timization was performed using AdamW with a
learning rate of 5e-7, scheduled via cosine decay
and no warm-up. Mixed-precision (fp16) training
and gradient checkpointing were enabled, and a
DPO β value of 0.1 was used. SFT models were
fine-tuned with a learning rate of 5e-6. Grid search
on the learning rate was performed for both SFT
and direct alignment models over the values of 5e-
6, 2e-6, 1e-6, 5e-7, and 2e-7.
Elo Score Calculation.
Pairwise comparison
counts (wins = 1, draws = 0.5, losses = 0) were
expanded into individual game outcomes and fitted
with a Bradley-Terry (logistic Elo) model, where
the probability that model i defeats model j is
Pij =
1
1 + 10(Rj−Ri)/400 .
Ratings Ri were obtained by maximizing the
joint log-likelihood.
As individual A/B judg-
ments are not IID, we repeatedly drew 1,000 boot-
strap samples by resampling the set of prompts—
keeping all A/B judgment-level outcomes among
its responses—with replacement, refit the Elo
model on each sample, and took the 2.5th/97.5th
percentiles of the resulting pair-wise rating differ-
ences as the 95% confidence limits.
12
