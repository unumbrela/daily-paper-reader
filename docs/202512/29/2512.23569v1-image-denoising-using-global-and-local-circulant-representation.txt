1
Image Denoising Using Global and Local
Circulant Representation
Zhaoming Kong, Xiaowei Yang, and Jiahuan Zhang
Abstract—The proliferation of imaging devices and countless
image data generated every day impose an increasingly high
demand on efficient and effective image denoising. In this paper,
we establish a theoretical connection between principal compo-
nent analysis (PCA) and the Haar transform under circulant
representation, and present a computationally simple denoising
algorithm. The proposed method, termed Haar-tSVD, exploits a
unified tensor singular value decomposition (t-SVD) projection
combined with Haar transform to efficiently capture global
and local patch correlations. Haar-tSVD operates as a one-step,
parallelizable plug-and-play denoiser that eliminates the need for
learning local bases, thereby striking a balance between denoising
speed and performance. Besides, an adaptive noise estimation
scheme is introduced to improve robustness according to eigen-
value analysis of the circulant structure. To further enhance the
performance under severe noise conditions, we integrate deep
neural networks with Haar-tSVD based on the established Haar-
PCA relationship. Experimental results on various denoising
datasets demonstrate the efficiency and effectiveness of proposed
method for noise removal. Our code is publicly available at
https://github.com/ZhaomingKong/Haar-tSVD.
Index Terms—Efficient image denoising, circulant representa-
tion, tensor-SVD projection, Haar transform, real-world datasets.
I. INTRODUCTION
T
He rapid development of modern imaging systems and
technologies has significantly enriched the information
captured and conveyed by images, delivering a more faithful
representation for real scenes. However, images are inevitably
corrupted by noise during acquisition and transmission, which
can severely degrade the visual quality of acquired data.
Therefore, effective image denoising remains a fundamental
task in many applications, including feature extraction, object
tracking, and medical diagnosis [1], [2].
Many state-of-the-art image denoising techniques weigh
highly on deep neural network (DNN) architectures, which
are usually trained on datasets containing clean and noisy
image pairs. For example, Zhang et al. [3] incorporated batch
normalization (BN) [4], rectified linear unit (ReLU) [5] and
residual learning [6] into the convolutional neural network
(CNN) model. Chen et al. [7] considered an efficient activation
free network. Zamir et al. [8] introduced transformer [9] to
capture long-range image pixel interactions. While the data-
driven models have shown impressive performance for image
Z. Kong and X. Yang are with the School of Software Engineering,
South China University of Technology, Guangzhou, 510006, China (e-mail:
kong.zm@mail.scut.edu.cn; xwyang@scut.edu.cn).
J. Zhang is with the Department of Clinical Laboratory Medicine, Guang-
dong Provincial People’s Hospital, Southern Medical University, Guangzhou,
510000, China (email: zhangjiahuan@gdph.org.cn).
restoration, their performance may degrade sharply when the
test images do not match the distribution of the current scene.
Besides, collecting high-quality data is time-consuming and
expensive, and the platform required for training and inference
may be inaccessible to ordinary users and researchers.
Therefore, it is interesting to devise denoising methods with
less dependence on ground-truth data and low computational
complexity. As an alternative to DNN models, the classic
patch-based denoising framework still shows competitive per-
formance in various tasks [10]. In general, related works filter
out noise based solely on the input noisy observation with
different regularization terms and image priors [11], [12].
The representative BM3D method [13] integrated the NLSS
characteristic of natural images [14], sparse representation [15]
and transform-domain techniques [16] into a subtle paradigm.
Gu et al. [17] replaced the sparsity constraint with the low-
rank assumption. Xu et al. [18] employed the Maximum A-
Posterior (MAP) estimation technique [19] and proposed a
trilateral weighted sparse coding scheme.
Despite steady progress, existing methods still face sev-
eral inherent limitations [20], including the need for solving
complex optimization problems in the test phase, reliance on
manually tuned parameters, and insufficient exploitation of
auxiliary information. To circumvent these issues and achieve
a balance between denoising performance and computational
resources, we take advantage of the circulant representation
[21], [22] and propose Haar-tSVD to capture the local and
nonlocal similarity. Furthermore, we investigate an effective
combination of DNNs and the proposed denoiser to utilize
both external information and internal image structures. Fol-
lowing the patch-based denoising paradigm, the proposed
method is distinguished by several characteristics.
First, we exploit the global circulant representation to model
patch-level correlation, which can be efficiently captured by a
pair of shared t-SVD bases. We then extend this global patch
circulant structure to a local group-level circulant formula-
tion, thereby revealing an intrinsic connection between the
Haar transform and PCA. The resulting global-local circulant
correlation leads to a unified integration of t-SVD and Haar
bases, eliminating the need for local transform learning, yield-
ing a parallelizable and plug-and-play denoiser. Besides, to
improve the adaptability and avoid manual parameter tuning,
we develop an adaptive variant termed A-Haar-tSVD, which
adjusts the noise estimation value based on the eigenvalue
characteristics of circulant structures. Moreover, to improve
robustness under severe noise, we propose fusing DNNs
with the established Haar–PCA link to leverage both external
information and intrinsic image features.
arXiv:2512.23569v1  [cs.CV]  29 Dec 2025


2
Our main contributions can be summarized as follows:
• Unified bases: We propose to leverage the circulant
representation to capture both the intra- and inter-
correlations among image patches. Furthermore, by build-
ing the connection between the Haar transform and PCA
via the eigenvalue decomposition (EVD) of the circulant
structure, we demonstrate that the global-local circulant
similarity can be efficiently exploited by a unified t-SVD
projection and Haar transform.
• Adaptive scheme: We develop an adaptive scheme to
enhance the flexibility and robustness of the proposed
method by exploring a CNN-based noise estimator along-
side eigenvalue characteristics of circulant structures. To
reduce computational overhead, a fast implementation is
achieved through parallel programming techniques.
• Enhancement strategy: We leverage the established
Haar–PCA relationship and introduce a learning-based
enhancement strategy that integrates a data-driven module
with the proposed denoiser, improving robustness and
denoising performance in challenging scenarios.
• Diverse datasets: We evaluate the applicability of the
proposed method across different real-world denoising
tasks such as images, videos and hyperspectral imaging
(HSI). Experiments demonstrate the competitive perfor-
mance of the proposed method in terms of both effec-
tiveness and efficiency.
The rest of the paper is structured as follows. Section II
summarizes related works. Section III describes the proposed
Haar-tSVD denoising method and its variants in detail. Sec-
tion IV presents datasets, experimental settings and results.
Besides, discussions of the ablation studies are included in
this section. Section V concludes this work.
II. RELATED WORKS
A. Symbols and Notations
We follow the tensor notations in [23] for image repre-
sentation. Vectors and matrices are first- and second-order
tensors, denoted by bold lowercase letters a and bold capital
letters A, respectively. A higher order tensor is denoted by
calligraphic letters, e.g., A. An Nth-order tensor is denoted
as A ∈RI1×I2×···×IN . The n-mode product of a tensor A by
a matrix U ∈RPn×In, denoted by A ×n U is also a tensor.
The mode-n unfolding of A is denoted by A(n).
B. Patch-based Framework
Patch-based denoisers typically exploit the NLSS property
through three stages: grouping, collaborative filtering, and
aggregation, as illustrated in Fig. 1. For a reference patch
Pn ∈Rps×ps×c, the grouping step stacks K similar patches
within a local window W into a 4D group Gn ∈Rps×ps×c×K
based on certain patch-matching criteria [24]–[26]. Collabora-
tive filtering then estimates the underlying clean group via
ˆGc = arg min
Gc ∥Gn −Gc∥2
F + ρ · Ψ(Gc),
(1)
where Ψ(·) encodes prior knowledge. Finally, the clean
patches in ˆGc are aggregated back to their original locations
to suppress residual noise.
Fig. 1. Illustration of the patch-based framework for traditional denoisers.
C. Circulant Representation and t-SVD for Image Denoising
Recently, circulant structure and representation [22], [27],
[28] has been utilized for image restoration due to its ef-
fectiveness of exploring redundancy and encoding the inner
structure of an image patch by cyclic shift. The block circulant
representation (BCR) [22] of an image patch P ∈Rps×ps×c
models patch-level redundancy as
bcirc(P) =





P(1)
P(c)
· · ·
P(2)
P(2)
P(1)
· · ·
P(3)
...
...
...
...
P(c)
P(c−1)
· · ·
P(1)




,
(2)
where P(i) = P(:, :, i) denotes the i-th frontal slice, and
bcirc(P) ∈Rpsc×psc is a block circulant matrix. This repre-
sentation preserves the patch structure while capturing inter-
slice correlations, making it well suited for multi-dimensional
image data. Moreover, explicit construction of BCR is un-
necessary, since block circulant operations can be efficiently
implemented via the tensor t-product [29], [30].
Definition II.1 (T-product). Suppose A ∈Rn1×m×n3 and B ∈
Rm×n2×n3, then the t-product C = A ∗B ∈Rn1×n2×n3 is
defined as
bcirc(C) = bcirc(A)bcirc(B).
(3)
Equ. (3) can be efficiently computed in the Fourier domain
C(i)
F = b
A(i) bB(i),
i = 1, 2, . . . , m,
(4)
where b
A is obtained by applying the fast Fourier transform
(FFT) along the third mode of A via
b
A = A ×3 WF F T ,
(5)
where WF F T refers to the FFT matrix. Then the popular t-
SVD [30] can be defined based on the t-product.
Definition II.2 (T-SVD). For A ∈Rn1×n2×n3, its t-SVD
decomposition is given by
A = U ∗S ∗VT ,
(6)
where U ∈Rn1×n1×n3 and V ∈Rn2×n2×n3 are orthogonal
tensors, and the entries in S ∈Rn1×n2×n3 can be viewed as
singular values or coefficients of A.
t-SVD has proven effective for denoising [22], [31], [32],
by exploring the block circulant structure of image patches,
alleviating tensor imbalance issues [33], and sharing prop-
erties with matrix-based formulations. However, existing t-
SVD–based methods typically require iterative optimization
and local basis learning during the test phase. This motivates
the design of a simple and effective denoising scheme with
t-SVD and circulant representation.


3
Fig. 2. Flowchart of the proposed Haar-tSVD method. It is a one-step filtering algorithm built upon global and local circulant representation.
D. Haar Transform
As a popular transformation in image filtering [34], [35],
the Haar transform belongs to the wavelet family, which is
derived from the Haar matrix [36]. Specifically, starting from
a simple 2 × 2 Haar transformation matrix
H2 =
1
√
2
1
1
1
−1

.
(7)
We can define the general 2N × 2N Haar matrix by
H2N =
1
√
2
 HN ⊗[1, 1]
IN ⊗[1, −1]

,
(8)
where IN
∈RN×N denotes the identity matrix and ⊗
represents the Kronecker product [23]. The Haar matrix plays
an important role in the analysis of the localized features due
to its simplicity and orthogonality. In this paper, we study the
Haar transform from the perspective of circulant structure, and
discuss how it can be efficiently and effectively integrated into
the patch-based denoising framework.
III. METHOD
In this section, we first provide a detailed description of
the proposed Haar-tSVD method, whose flowchart is depicted
in Fig. 2. We then introduce its adaptive variant and the
corresponding robust enhancement strategy.
A. Searching Similar Patches
Directly computing the Euclidean distance between two
patches Pi, Pj
∈Rps×ps×3 is both time-consuming and
sensitive to noise. Based on the observation that the green
channel generally has higher SNR [37], we adopt a GCP-based
patch-matching method [38], which computes the distance dij
between two patches via
dij =



∥PG
i −PG
j ∥, ∥PG
i ∥≥max( 1
γ ∥PR
i ∥, 1
γ ∥PB
i ∥)
∥Pavg
i
−Pavg
j
∥, otherwise,
(9)
where PR, PG and PB represents the R, G and B channels of
an image patch P, respectively. Pavg is the average value of
RGB channels. The weight parameter γ is used to measure
the importance of the green channel. Similar to [38], we
empirically set γ = 1.2. This scheme reduces patch-matching
time by
2
3 and improves robustness to noise by leveraging
the cleaner green channel, facilitating group-level sparsity for
collaborative filtering.
B. Global Circulant Reperesentation
In the collaborative filtering step, the choice of algebraic
representation plays a crucial role. In this section, we present
an extension of the nonlocal circulant representation to a
global formulation for patch modeling, enabling efficient cap-
ture of latent structures across different images.
1) Nonlocal Circulant Formulation: Following [28], by
integrating the BCR into the nonlocal SVD problem and
leveraging the notations of t-SVD and t-product, we can
explore the patch-level correlation for each group via
min
K
X
i=1
∥Pi −U ∗Si ∗VT ∥2,
s.t
UT U = I, VT V = I,
(10)
where Si ∈Rps×ps×3 is the coefficient tensor, and U, V ∈
Rps×ps×3 are orthogonal bases. This nonlocal t-SVD can be
reduced to independent SVDs in the Fourier domain:
min
K
X
i=1
∥bP(j)
i
−bU(j) bS(j)
i
bV(j)T ∥2,
s.t
bU(j)T bU(j) = I, bV(j)T bV(j) = I,
∀j ∈1, 2, 3,
(11)
where bP(j)
i
represents the j-th frontal slice of patch Pi in the
Fourier domain. The projection pairs bU(j) and bV(j) are given
by the eigenvectors of the ensemble row-wise and column-wise
correlation matrices Crow = PK
i=1 bP(j)
i
bP(j)T
i
and Ccol =
PK
i=1 bP(j)T
i
bP(j)
i
, respectively.
2) Global Patch-Level Projection: It is noticed that if the
observed patches are corrupted by additive white Gaussian
noise (AWGN) n ∼N(0, σ2), then for a large K
K
X
i=1
bP(j)
noisyi bP(j)T
noisyi =
K
X
i=1
bP(j)
cleani bP(j)T
cleani + σ2I.
(12)
This implies that the noise-free t-SVD bases of Pclean can
be approximated using numerous patches. To assess the effect
of K, we appy a t-SVD denoiser [28] on real-world datasets
(CC15 [39], HighISO [40], SIDD [41]). As shown in Fig. 3,
performance gains become marginal for K > 45, indicating
that excessively large groups are unnecessary.
However, learning separate bases for each group [28] intro-
duces additional complexity. Since an image patch may share
features with patches across the entire image, we consider to
estimate a global projection pair U and V from all reference


4
Fig. 3. Bases U and V learned from noisy and clean patches for varying K.
patches according to Equ. (11). We refer to this implemen-
tation as globally-learned t-SVD. Besides, it is interesting to
investigate if the t-SVD bases acquired from a single image
can be transferred to other images. Specifically, we utilize the
first image from the CC15 dataset to learn a global pair of
U and V, and then reuse them for all subsequent images.
This variant is termed the globally-reused t-SVD. Fig. 4
illustrates different strategies of obtaining U and V. As shown
in Table I, the globally-reused approach achieves reliable
estimation with reduced computational cost. This suggests that
the learned bases can be directly applied to new observations
while maintaining performance.
Fig. 4. Different strategies for obtaining t-SVD bases U and V.
TABLE I
PSNR/SSIM [42] COMPARISON OF T-SVD BASES U AND V OBTAINED
WITH DIFFERENT STRATEGIES.
Dataset
Locally-learned
Globally-learned
Reused (Ours)
Clean
CC15
37.86/0.956
37.89/0.957
37.90/0.957
37.92/0.957
HighISO
40.05/0.968
40.13/0.969
40.15/0.970
40.19/0.970
SIDD-val
34.35/0.876
34.39/0.876
34.38/0.876
34.45/0.876
Time (s)
6.95
6.13
6.02
-
C. Local Circulant Structure
To further promote sparsity and suppress noise in the
transform domain, exploiting group-level correlations among
similar patches is essential. A common approach is to learn
an invertible transform Ugroup ∈RK×K along the grouping
dimension via PCA [28], [43], [44]. However, performing local
PCA for each group incurs considerable computational cost.
In this section, we present an alternative approach to bypass
the explicit PCA learning.
1) Group circulant formulation: Due to limited search
windows and noise contamination, only a small number of
similar patches can usually be identified. To better exploit their
correlations, we propose to extend the circulant structure to
the stacked similar patches, enabling recursive modeling of
their interdependencies. Specifically, the circulant structure of
a group G containing K image patches can be defined as
circ(G) =





pT
1
pT
K
· · ·
pT
2
pT
2
pT
1
· · ·
pT
K−1
...
...
...
...
pT
K
pT
K−1
· · ·
pT
1




∈RK×3Kps2,
(13)
where pi ∈R3ps2×1 is the vector form of the i-th patch
Pi of G. This construction replicates each patch across all
rows, thereby explicitly encoding group-level redundancy. To
efficiently exploit such correlations without explicit learning
on the large matrix circ(G), we examine its structural property.
Let rT
i denotes the i-th row of circ(G), then
rT
i =

pT
(j+i−2 mod K)+1
K
j=1, ∀i = 1, 2, . . . , K.
(14)
From Equ. (13) and Equ. (14), we notice that each row is
a cyclic permutation of K patches, thus the Gram matrix
circ(G)circ(G)T ∈RK×K preserves the circulant structure:
circ(G)circ(G)T =





rT
1 r1
rT
1 r2
· · ·
rT
1 rK
rT
2 r1
rT
2 r2
· · ·
rT
2 rK
...
...
...
...
rT
Kr1
rT
Kr2
· · ·
rT
KrK




.
(15)
2) Relationship to PCA and the Haar Transform: The EVD
of the matrix in Equ. (15) is given by
circ(G)circ(G)T u = λu,
(16)
where λ and u ∈RK denote an eigenvalue and its corre-
sponding eigenvector, respectively. An interesting observation
of the circulant pattern circ(G) from Equ. (13) and Equ. (14)
is that the sum of each row and column of circ(G) is identical.
Denoting this common row sum by rT
sum, we have
rT
sum =
K
X
i=1
pT
i .
(17)
Leveraging the circulant property of both circ(G) and its Gram
matrix in Equ. (15), the dominant eigenpair (λmax, umax) of
circ(G)circ(G)T can be obtained by
λmax =
K
X
i=1
pT
i
K
X
i=1
pi = rT
sumrsum,
umax =
1
√
K
(1, 1, . . . , 1)T .
(18)
Interestingly, when K is a power of 2, the dominant eigen-
vector uT
max coincides with the first row of the Haar trans-
form matrix defined in Equ. (8). This reveals that the Haar
transform inherently encodes the first principal component of
circ(G)circ(G)T . Since noise can be suppressed by discarding
small coefficients in the transform domain, the orthogonal
Haar bases provide an efficient data-agnostic alternative to
PCA under circulant formulation. Accordingly, the group-level
transform can be modeled by the Haar transform.
Employing the Haar transform enjoys two computational
benefits. First, it enables modeling the group-level redundancy
without explicitly constructing the circulant matrix. Second,
the predefined Haar matrix UHaar eliminates the need to
obtain a distinct Ugroup for each group Gnoisy.


5
D. The Haar-tSVD Transform
1) One-step Filtering: The above analysis of the circulant
representation allows us to get rid of local transform learning.
By combining the patch-level projection pair (U, V) with the
group-level Haar matrix Ugroup, we obtain a plug-and-play
transform. As a result, the proposed Haar-tSVD method admits
a simple one-step collaborative filtering scheme, as summa-
rized in Algorithm 1. Specifically, the coefficients Snoisy can
be derived by performing the forward transform Tforward via
Snoisy = UT ∗Gnoisy ∗V ×4 UHaar.
(19)
Hard-thresholding Tthreshold [45] is then applied to shrink the
coefficients of Snoisy under threshold τ via
Struncate =
(
Snoisy,
|Snoisy| ≥τ,
0,
|Snoisy| < τ.
(20)
The estimated clean group Gestimate is recovered by taking
the inverse Tinverse of Equ. (19) with Struncate:
Gestimate = U ∗Struncate ∗VT ×4 U−1
Haar.
(21)
Finally, the denoised patches in Gestimate are aggregated
back to their original image locations.
Algorithm 1 Haar-tSVD
Input: Noisy image Y, patch size ps, number of similar
patches K, search window size W, and noise level σ.
Output: Estimated clean image X.
1: for each reference patch Pref do
2:
Grouping: Search K most similar patches within win-
dow W using Equ. (9) to form group Gnoisy.
3:
Filtering: Apply the forward-threshold-inverse trans-
form in Equs. (19)–(21) for the estimated group via
Gestimate = Tinverse ◦Tthreshold ◦Tforward(Gnoisy) .
4:
Aggregation: Averagely write all patches of Gestimate
back to their original locations.
5: end for
2) Complexity analysis: The computational cost of the
proposed Haar-tSVD for each local group consists of three
parts: (i) the search of K similar patches within a window
O(KW 2ps), (ii) the global t-SVD projection O(Kps3), and
(iii) the Haar transform projection O(K3). By adopting the
fast Haar transform [36], the group-level projection can be
reduced to O(K log K). Therefore, an overall computational
complexity of Haar-tSVD is O([KW 2ps+Kps3 +KlogK]).
Compared to many effective t-SVD based approaches [22],
[28], [38], [46], [47], the proposed method is more efficient,
because it does not involve any local learning procedure.
3) Fast implementation: The proposed method consists of
two major components: patch-matching and global-local trans-
form, both of which are inherently parallelizable. Specifically,
patch-matching involves only Euclidean distance computa-
tions, while the transform bases U, V and UHaar are shared
by all patch groups. Therefore, the computation is dominated
by matrix–vector multiplications, making the method suitable
for parallel implementation. We achieve over 10× speedup
compared to the baseline MATLAB serial implementation. In
practice, further acceleration can be obtained by caching inter-
mediate results such as patch indices and transform coefficients
Snoisy during parameter selection.
E. Adaptive Scheme and Enhancement Strategy
1) Adaptive noise estimation: Apart from an effective col-
laborative filtering scheme, noise estimation is crucial for an
efficient denoiser. For Haar-tSVD, the noise level σ charac-
terizes the sparsity of coefficients in the transform domain.
To model the sparsity level, we can reformulate the noise
estimation problem as a classification task and train a noise
estimator using CNN [38], as illustrated in Fig. 5.
Fig. 5. The CNN-based noise estimation approach.
However, we notice that the rationale for incorporating a
DNN-based noise estimator also hinges on the robustness
of local adaptive transform to small perturbations in σ. The
prediction accuracy of the CNN estimator on real-world data
reported in Table II indicates limited generalization results.
Misclassification of the noise level σ can degrade the per-
formance of Haar-tSVD. As shown in Fig. 6, although Haar-
tSVD achieves denoising quality comparable to PCA-tSVD
[38], its fixed bases are more sensitive to noise-level variations,
potentially leading to oversmoothing effects.
TABLE II
AVERAGE PREDICTION ACCURACY OF THE CNN-BASED ESTIMATOR ON
REAL-WORLD DATASETS.
Dataset
CC15
HighISO
SIDD-validation
Accuracy (%)
69.3
71.2
75.1
(a) Comparison of Haar and PCA
(b) Visual effects (σ = 40)
Fig. 6. Denoising effects of the Haar and PCA transforms for Ugroup.
To enhance adaptiveness, we further analyze the eigenvalue
characteristics of the circulant matrix circ(G)circ(G)T . Due
to its circulant structure, an eigenpair (ˆλ, ˆu) is given by
ˆλ =
K
X
i=1
(−1)ipT
i
K
X
i=1
(−1)ipi,
ˆu =
1
√
K
(−1, 1, . . . , −1, 1)T .
(22)
Equ. (22) indicates that the eigenpair captures the alternat-
ing contrast between adjacent patches within a local group


6
G. Therefore, we can use ˆλ to measure the inner-group
similarity. Specifically, in low-noise conditions, high inner-
group similarity yields small ˆλ, whereas severe noise disrupts
patch-matching and shifts ˆλ toward larger eigenvalues. To
demonstrate the impact of noise, we denote by a the rank
position of ˆλ among the K eigenvalues sorted in ascending
order. As illustrated in Fig. 7, for K = 32, a increases
significantly with noise severity, suggesting that a can serve
as an indicator of noise strength.
(a) Noise-free image
(b) Noisy image
Fig. 7. Influence of real-world noise on patch-search and rank position a.
Accordingly, the CNN-estimated noise level σest is adap-
tively adjusted as
ˆσ =



1
β σest,
a ≤γ,
σest,
a > γ,
(23)
where ˆσ is the final estimated noise level, β and γ are weight-
ing parameters empirically set to 1.2 and 13, respectively.
From Equ. (23), we notice that it is unnecessary to perform
full EVD to determine a. To further reduce computational
complexity, we avoid adjusting σest for every local group G.
Instead, we randomly sample a few groups within a subimage
and obtain the corresponding ˆσ through majority voting.
We term the proposed method with the adaptive noise
estimation strategy as A-Haar-tSVD. As illustrated in Fig.
8, the adaptive approach incorporates the effectiveness of
CNNs, preserves the simplicity of patch-based framework and
leverages the adaptability of eigenvalue analysis.
Fig. 8. Flowchart of the adaptive variant A-Haar-tSVD.
2) Enhancement under severe noise: Classic patch-based
denoisers are particularly subject to severe noise [1], [10],
which degrades patch-matching accuracy and contaminates
transform-domain coefficients. As illustrated in Fig. 9, both
the proposed Haar-tSVD and the state-of-the-art BM3D exhibit
noticeable color artifacts, even when the noise level is properly
estimated. To further improve robustness of the proposed
method under severe noise, we present an enhancement strat-
egy, termed RA-Haar-tSVD, which integrates a lightweight,
trainable refinement module into the Haar-tSVD framework.
Fig. 9 shows its effectiveness to remove artifacts and restore
natural color in the challenging high-noise scenario.
According to the Haar-PCA link in Equ. (18), the first
(a) Noisy
(b) CBM3D
(c) A-Haar-tSVD (d) RA-Haar-tSVD
Fig. 9. Effects of the robust enhancement strategy under severe noise.
row of the Haar matrix corresponds to the dominant eigen-
vector umax and captures the weighted mean of all patches
pnoisy ∈RK×1 of a group Gnoisy in the transform domain:
Gnoisy ×4 uT
max =
1
√
K
K
X
i=1
pnoisyi =
√
K · pnoisy,
(24)
where the weighted mean patch
√
K ·pnoisy is the first row of
the group-level projection Gnoisy×4UHaar and preserves most
of its signal energy. However, pnoisy is severely contaminated
due to the rare-patch effect, which propagates visible artifacts
to the reconstructed image. To address this issue, we employ a
plain fully connected network (FCN) [48] to obtain a cleaner
version of the dominant coefficients pest via
pest = FCNθ(pnoisy).
(25)
As illustrated in Fig. 10, the estimated mean patch pest
of the FCN is used to refine the first row of the group-
level projection GHaar. This strategy offers three advantages:
(i) it utilizes the feature extraction ability of the network to
obtain the dominant group component under severe noise; (ii)
abundant noisy-clean pairs of mean patches are available from
overlapping groups for training; and (iii) only the dominant
Haar coefficients are modified, the remaining components are
preserved and local structural information is retained.
Fig. 10. Flowchart of the RA-Haar-tSVD enhancement strategy for a group.
F. Related Patch-based Denoisers
Table III compares the proposed Haar-tSVD and its variants
with representative patch-based denoisers in terms of learning
strategy and implementation. Built upon the t-SVD and Haar
transforms, the proposed method absorbs the ideas from the
simple and effective design of classic denoisers. Instead of
recursively solving complex optimization problems, the pro-
posed Haar-tSVD aims at one-step filtering that eliminates
the need to train local bases for each image. Under circulant
representation, we employ a unified global t-SVD and Haar
transform to efficiently capture patch- and group-level correla-
tion. Besides, EVD is leveraged for adaptive noise estimation,
and the integration of NNs with the Haar-tSVD provides an
extra enhancement mechanism under severe noise.


7
TABLE III
REPRESENTATIVE TRADITIONAL DENOISERS WITH DIFFERENT ALGEBRAIC REPRESENTATIONS AND ENHANCEMENT STRATEGIES.
Category
Algebra
Methods
Parallel impl.
Global transform
One-step filter
Adaptive scheme
Enhancement
Key words
Patch-based
denoisers
Matrix
DCT [49]
✓
✓
✓
-
-
Discrete cosine transform filter
NLPCA [43]
✓
-
-
-
✓
Nonlocal PCA
MCWNNM [50]
-
-
-
-
✓
Weighted nuclear norm
TWSC [18]
-
-
-
✓
✓
Trilateral sparse coding
GID [51]
-
-
-
✓
✓
External + internal prior
NLH [35]
✓
✓
-
-
✓
Haar-based NLM
Bitonic [52]
✓
-
-
✓
✓
Bitonic filtering
Tensor
BM4D [53]
✓
✓
-
-
✓
4D extension of BM3D
HOSVD [54]
-
✓
✓
-
✓
4D HOSVD transform
TDL [55]
-
-
-
-
✓
Tucker dictionary learning
LLRT [56]
-
-
-
✓
✓
LR tensor + Laplacian
MSt-SVD [28]
✓
-
✓
-
-
One-step t-SVD
LTDL [46]
-
-
-
-
✓
LR tensor dictionary learning
GCP-ID [38]
✓
-
✓
✓
-
Green channel prior + t-SVD
Haar-tSVD (Ours)
✓
✓
✓
-
-
Global-local circulant structure
A-Haar-tSVD (Ours)
✓
✓
✓
✓
-
Adaptive noise estimation
RA-Haar-tSVD (Ours)
✓
✓
✓
✓
✓
Adaptive noise estimation
IV. EXPERIMENTS
In this section, we evaluate about 50 denoising methods
across images, video, and HSI datasets. For each method,
original implementations or published results are used, with
parameters/models carefully chosen for optimal performance.
GPU-accelerated methods are executed using the computa-
tional resources of Google Colab Pro, while all remaining
experiments are conducted on a workstation equipped with
an Intel Core i7-10700F CPU @ 2.9 GHz and 16 GB RAM.
More details are provided in the supplementary material.
A. Implementation Details
Haar-tSVD. It involves four primary parameters: the patch
size ps, the number of similar patches K within a group, the
window size W for patch search, and the hard-thresholding
parameter τ. For image, video and HSI data, we set ps = 8,
K = 32, W < 20 and τ = σ
p
2log(cKps2), where c denotes
the number of channels or spectral bands.
A-Haar-tSVD and RA-Haar-tSVD. A multi-layer CNN
[57]
and
a
FCN
are
adopted
for
noise
level
predic-
tion and mean-patch estimation, respectively. The SIDD
small and CC60 [51] data are used for training. For
A-Haar-tSVD, each input is divided into subimages of
size
128 × 128,
and
the
noise
level
σ
is
selected
from {1.25, 5, 10, 20, 30, 40, 50, 60, 80, 100, 120}. For RA-
Haar-tSVD, each input pnoisy is the mean patch of a local
group, and the network is trained using residual-learning.
B. Color Image Denoising
We perform synthetic and real-world experiments. For syn-
thetic denoising, we use the Kodak dataset [58] and add
independent noise to each RGB to simulate channel-specific
corruption. In case 1, the noise variances are [15, 10, 20], and
in case 2, they are [40, 30, 50]. For Gaussian denoisers, the
noise levels σ are set to 20 and 50, respectively.
1) Objective results: Table IV evaluates the results of tra-
ditional patch-based denoisers and supervised/self-supervised
DNN models. Overall, Haar-tSVD achieves competitive per-
formance, its adaptive variant A-Haar-tSVD improves results
across datasets, and the enhancement strategy RA-Haar-tSVD
further boosts performance under severe noise. On the Kodak
dataset, we notice that RA-Haar-tSVD is particularly bene-
ficial when the noise level is high, whereas at lower noise
levels, NN-based estimation may suffer from oversmoothness.
Therefore, on real-world datasets, we apply the enhancement
scheme when σ > 30. Beyond Kodak, A-Haar-tSVD achieves
PSNR gains of at least 0.24dB and 0.84dB over CBM3D
and MSt-SVD on DND and SIDD, respectively, while RA-
Haar-tSVD shows additional improvements in challenging
scenarios. On CC, PolyU, HighISO, and IOCI, the proposed
methods demonstrate their adaptiveness and generalization
compared with many DNN models that degrade when training
or validation data are not available.
Ideally, the enhancement strategy should be applied only
when necessary. To better understand its potential, Table
V assesses its effects in cases where measurable gains are
observed. Under this ideal setting, RA-Haar-tSVD consistently
outperforms the adaptive baseline, with obvious advantages
under severe noise, demonstrating its potential in handling
different degradations.
TABLE V
EVALUATION OF RA-HAAR-TSVD UNDER IDEAL SETTINGS.
Method
DND
SIDD
CC15
PolyU
HighISO
IOCI
A-Haar-tSVD
38.25
35.58
38.24
38.89
40.62
41.52
RA-Haar-tSVD (ideal)
38.64
37.48
38.56
38.95
40.68
41.60
Improvement
0.39
1.90
0.32
0.06
0.06
0.08
2) Visual evaluations: Visual evaluations are presented in
Fig. 11 to Fig. 13. Specifically, Fig. 11 shows the effectiveness
of integrating NNs with circulant representation for noise
removal and detail preservation, while compared methods
slightly over-smooth textures in certain regions. Fig. 12 high-
lights the limitations of patch-based denoisers when handling
heavily corrupted images, where noise disrupts grouping and
local transforms. Nevertheless, the proposed method produces
less distortions and color artifacts compared to SASL and
Bitonic, thanks to the redundancy encoded in the global-local
circulant structures and the enhancement strategy. Fig. 13
shows that when confronted with unseen noise patterns, the
well-trained DNN models may leave unwanted artifacts and
suffer from over-smooth effects. By comparison, the adaptive
scheme shows its strengths by exploiting both the CNN estima-
tor and nonlocal information, which render certain robustness
and adaptability, therefore achieving a balance between noise
suppression and detail recovery.


8
TABLE IV
DENOISING COMPARISON ON REAL-WORLD SRGB COLOR IMAGE DATASETS. ‘*’: THE RESULTS ARE FROM THE AUTHORS’ PAPERS.
Methods/Models
Kodak (case 1)
Kodak (case 2)
DND [59]
SIDD [41]
CC15 [39]
PolyU [60]
HighISO [40]
IOCI [10]
Traditional
denoisers
Bitonic [52]
-
-
37.85/0.936
36.67/0.933
35.22/0.924
36.64/0.939
37.37/0.943
39.10/0.954
MCWNNM [50]
28.40/0.792
26.54/0.686
37.38/0.929
29.54/0.888
37.02/0.950
38.26/0.965
39.89/0.970
41.04/0.972
NLHCC [35]
-
-
38.85/0.953
35.31/0.930
38.49/0.965
38.36/0.965
40.29/0.971
41.22/0.974
MSt-SVD [28]
33.99/0.906
29.12/0.770
38.01/0.938
34.38/0.901
37.95/0.959
38.85/0.971
40.49/0.974
41.48/0.977
CBM3D [61]
33.51/0.893
28.90/0.780
37.73/0.934
34.74/0.922
37.70/0.957
38.69/0.970
40.35/0.974
41.46/0.976
Haar-tSVD (ours)
34.13/0.910
29.16/0.780
38.11/0.939
35.05/0.914
38.10/0.961
38.78/0.969
40.51/0.973
41.45/0.977
Traditional
+ DNN
A-Haar-tSVD (ours)
34.04/0.907
29.18/0.783
38.25/0.944
35.58/0.925
38.24/0.963
38.89/0.971
40.62/0.974
41.52/0.978
RA-Haar-tSVD (ours)
33.89/0.901
29.19/0.787
38.64/0.945
37.48/-
38.30/0.962
38.85/0.970
40.63/0.975
41.50/0.977
Pixel2Pixel [62]
30.26/0.841
26.88/0.718
-
34.34*/-
35.51/0.923
37.19/0.951
37.77/0.945
-
DNN models
(Self-supervised)
APR-RD [63]
26.12/0.713
23.21/0.634
38.57/0.942
38.23*/-
35.83/0.946
37.01/0.953
38.75/0.968
39.36/0.964
B2UB [64]
29.91/0.816
26.80/0.698
-
-
36.51/0.935
38.25/0.968
39.03/0.962
40.29/0.972
Noise2VST [65]
34.08/0.907
28.34/0.771
-
-
33.96/0.868
36.60/0.929
35.99/0.893
39.11/0.946
SASL [66]
26.52/0.728
23.69/0.638
38.01/0.936
-
34.93/0.936
37.13/0.954
38.24/0.964
39.44/0.964
TBSN [67]
27.47/0.745
24.63/0.662
37.79/0.940
39.01/0.945
35.73/0.931
36.51/0.954
38.84/0.966
40.03/0.966
Zero-shot [68]
-
-
-
35.05/0.922
37.20/0.948
37.88/0.959
-
-
DNN models
(Supervised)
ClipDe [69]
30.94/0.871
20.97/0.618
39.57/0.954
39.42/0.956
35.40/0.915
36.87/0.939
37.61/0.954
40.03/0.969
Condformer [70]
-
-
40.10*/0.956*
40.23*/-
36.34/0.922
37.27/0.947
37.79/0.927
39.86/0.956
DeepSN [71]
-
-
39.92/0.956
39.79/0.958
35.89/0.936
37.25/0.956
38.12/0.950
39.04/0.966
DIDN [72]
27.68/0.802
21.85/0.654
39.64/0.953
39.78/0.958
36.06/0.946
37.36/0.953
38.24/0.950
39.86/0.964
DMID [73]
-
-
-
-
37.09*/-
37.59/0.946
37.90/0.931
39.16/0.951
FFDNet [74]
33.57/0.891
28.94/0.765
37.61/0.942
38.27/0.948
37.67/0.956
38.76/0.970
40.28/0.973
41.49/0.977
IDF [75]
31.88/0.851
27.96/0.764
33.72/0.811
-
36.43/0.942
37.77/0.962
38.63/0.960
40.23/0.970
MaIR [76]
32.86/0.873
28.61/0.781
-
39.92/0.959
36.05/0.938
37.64/0.958
38.38/0.953
40.11/0.969
NAFNet [7]
-
-
38.36/0.943
40.15/0.960
34.39/0.923
36.38/0.947
37.88/0.954
38.27/0.939
Restormer [8]
31.51/0.845
27.31/0.745
40.03/0.956
40.02/0.960
36.33/0.941
37.66/0.956
38.29/0.948
40.10/0.966
(a) Reference
(b) Noisy
(c) FFDNet
(d) MaIR
(e) Restormer
(f) CBM3D
(g) A-Haar-tSVD (h) RA-Haar-tSVD
Fig. 11. Denoising comparison on the Kodak (case2) dataset.
(a) Noisy
(b) Bitonic
(c) NLHCC
(d) SASL
(e) DeepSN
(f) Restormer
(g) NAFNet
(h) RA-Haar-tSVD
Fig. 12. Denoising comparison on the DND dataset.
3) Denoising efficiency: As reported in Table VI, super-
vised DNN models benefit from modern GPUs and achieve
fast inference. For example, Restormer and Condformer can
process an image of size 512 × 512 × 3 within 1 second.
The proposed Haar-tSVD is among the few methods parallel
with the state-of-the-art BM3D in terms of both efficiency
(a) Mean
(b) Noisy
(c) Condformer
(d) DeepSN
(e) MaIR
(f) Noise2VST
(g) Restormer
(h) A-Haar-tSVD
Fig. 13. Denoising comparison on the IOCI datset.
and effectiveness, as its grouping is performed only on the
green/opponent channel and it avoids recursive learning of
local transforms. Moreover, compared with other complex
network architectures, RA-Haar-tSVD is lightweight and en-
joys considerably low complexity. In particular, the simple
CNN noise estimator and the FCN used in the adaptive and
enhancement strategies require less than
1
50 of the training
time of an advanced model like Restormer. This combination
of channel-wise prior, lightweight design, and avoidance of
repeated local learning makes the proposed method highly
practical for real-world denoising applications.
TABLE VI
COMPUTATIONAL COMPLEXITY OF DIFFERENT DENOISING METHODS
WHEN PROCESSING 512 × 512 × 3 SRGB IMAGES.
Method
CBM3D
NLHCC
RA-Haar-tSVD
DIDN
Condformer
Restormer
Test time (s)
3.6
41.2
4.5
7.3
0.9
0.8
Train time (m)
–
–
23.2
–
–
> 1000
# Params (M)
–
–
7.5
22.1
26.4
25.3


9
TABLE VII
QUANTITATIVE COMPARISON ON REAL-WORLD COLOR VIDEO DENOISING DATASETS. BEST RESULTS ARE HIGHLIGHTED IN BOLD.
Dataset
Traditional denoisers
Traditional + DNN
DNN models
MSt-SVD
VBM4D
VIDOSAT
Haar-tSVD
A-Haar-tSVD
RA-Haar-tSVD
VNLNet
DVDNet
FastDVDNet
FloRNN
STBN
UDVD
ViDeNN
VRT
[28]
[77]
[78]
(Ours)
(Ours)
(Ours)
[79]
[80]
[81]
[82]
[83]
[84]
[85]
[86]
CRVD [87]
36.66
34.14
34.16
36.80
37.00
37.13
36.11
34.50
35.84
36.66
36.51
-
32.31
36.94
0.946
0.908
0.938
0.953
0.961
0.963
0.945
0.949
0.931
0.960
0.960
-
0.845
0.956
IOCV [10]
38.22
38.76
-
38.83
38.92
39.04
38.76
38.53
37.57
38.64
38.76
35.02
36.13
38.50
0.974
0.976
-
0.976
0.977
0.978
0.977
0.975
0.970
0.974
0.977
0.966
0.951
0.967
C. Real-world Color Video Denoising
Videos are more informative than images with dynamic
objects and temporal continuity. The proposed method can
be readily extended to handle video sequences. Specifically,
for Haar-tSVD, the patch search is applied to both spatial
and temporal dimension. The trained CNN-based estimator
and the adaptive noise adjustment of A-Haar-tSVD can be
adopted in a frame-by-frame fashion, while the enhancement
strategy is imposed on groups obtained from spatio-temporal
patch search. In our experiments, all video sequences of CRVD
and IOCV are used for evaluations. The objective metrics are
computed as the average across all frames [81]. Table VII lists
the quantitative results of compared methods. The proposed
method achieves very competitive results on both datasets,
and the enhancement strategy RA-Haar-tSVD outperforms
baselines by a margin of at least 0.28dB in PSNR. These
results demonstrate the effectiveness of combining the global
t-SVD with the Haar transform in capturing the nonlocal
characteristics of video data across frames.
Visual comparisons are provided in Fig. 14 to Fig. 16.
As shown in Fig. 14 and Fig. 15, even under moderate
noise levels, the pretrained DNN models tend to exhibit more
obvious over-smooth effects. In particular, Fig. 15 depicts a
scene with static background, where the toy girl in blue is
dynamic and moves in more than one directions. Consequently,
some details and textures appear only in certain frames.
Benefiting from the adaptive noise estimation strategy and the
nonlocal characteristics, the proposed A-Haar-tSVD method
effectively captures spatiotemporal similarity and preserves
more structural information. Besides, Fig. 16 demonstrates the
robustness of the proposed enhancement strategy to severe
noise corruptions. By leveraging the patch- and group-level
redundancy across different frames, the proposed method is
able to suppress noise and mitigate color artifacts.
(a) Mean
(b) Noisy
(c) VBM4D
(d) FastDVDNet
(e) FloRNN
(f) VRT
(g) VNLNet
(h) A-Haar-tSVD
Fig. 14. Denoising comparison on the IOCV dataset.
(a) Mean
(b) Noisy
(c) VBM4D
(d) FastDVDNet
(e) FloRNN
(f) VRT
(g) VNLNet
(h) A-Haar-tSVD
Fig. 15. Denoising comparison on the CRVD dataset (ISO = 6400).
(a) Mean
(b) Noisy
(c) FastDVDNet
(d) STBN
(e) VBM4D
(f) VRT
(g) VNLNet
(h) RA-Haar-tSVD
Fig. 16. Denoising comparison on the CRVD dataset (ISO = 12800).
D. Real-world HSI Denoising
HSI plays a vital role in a variety of remote sensing applica-
tions [98]. To handle an HSI data Y ∈RH×W ×Nbands, we treat
each local patch as a long tube and set ps = 8 × 8 × Nbands,
while other parameters are kept the same as those used in
the image denoising task. To apply the CNN noise estimator
without retraining, we estimate the noise of Y based on the
mean value across all spectral bands. Objective results of
compared methods are given in Table VIII. By effectively
capturing the rich spatial–spectral correlations in HSI data with
circulant structures, the proposed Haar-tSVD and its adaptive
variant achieve competitive performance compared with state-
of-the-art approaches. Meanwhile, by eliminating the need for
complex iterative filtering and local transform learning, our
method attains inference time comparable to advanced DNN
models. These properties make it a practical and efficient
solution for large-scale HSI denoising.
Visual comparisons of competitive methods on the Real-
HSI dataset are shown in Fig. 17. We can see that The
proposed method achieves a good balance between smoothness
and detail preservation. In contrast, BM4D tends to produce


10
TABLE VIII
DENOISING RESULTS OF COMPARED METHODS ON THE REAL-HSI DATASET.
Datasets
Metrics
Traditonal denoisers
DNN methods
BM4D
LLRT
LTDL
MSt-SVD
NGMeet
SSTPTV
Haar-tSVD
A-Haar-tSVD
FlexDID
HSI-DeNet
Mac-Net
QRNN3D
RAS2S
sDeCNN
[53]
[56]
[46]
[28]
[88]
[89]
(Ours)
(Ours)
[90]
[91]
[92]
[93]
[94]
[95]
Real-HSI
PSNR ↑
25.88
25.90
25.80
25.86
25.87
25.21
25.82
25.81
25.31
25.63
25.88
25.82
25.87
25.70
SSIM ↑
0.865
0.861
0.841
0.866
0.866
0.815
0.865
0.866
0.820
0.853
0.863
0.869
0.868
0.860
SAM [96] ↓
0.066
0.060
0.074
0.063
0.051
0.072
0.063
0.064
0.075
0.092
0.056
0.064
0.054
0.093
EGRAS [97] ↓
222.68
224.05
223.32
222.64
222.69
233.51
222.96
223.12
232.12
232.73
222.84
225.32
222.83
227.88
Platform
-
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU/GPU
GPU (T4)
GPU (T4)
GPU (T4)
GPU (T4)
GPU (T4)
GPU (T4)
Time
minutes
4.1
16.5
35.0
2.8
4.1
4.9
0.1
0.2
6.1
0.8
0.2
0.1
0.1
0.7
(a) Mean
(b) Noisy
(c) BM4D
(d) OLRT
(e) FlexDLD
(f) QRNN3D
(g) RAS2S
(h) A-Haar-tSVD
Fig. 17. Denoising comparison on the Real-HSI dataset.
obvious oversmooth effects, since its predefined patch-level
transforms may not fully exploit the correlation across all
the spectral bands. Additionally, the state-of-the-art low-rank
tensor method OLRT struggles to preserve high-frequency
components such as edges and textures. These observations
suggest that increasing the number of iterations and similar
patches may not help preserve fine details and structure of HSI
data. Moreover, although the DNN-based methods FlexDLD
and RAS2S demonstrate impressive noise suppression, they
introduce mild artifacts. Fig. 18 illustrates the spectral re-
flectance curves of the ground-truth and the reconstructed
HSIs, further confirming the competitive performance of our
method in this challenging scenario.
Fig. 18. Spectral curves of randomly selected points across different regions.
In many cases, the noise distribution can be highly non-
uniform across spectral bands, with a few bands suffering from
severe contamination. To address this issue, we perform band-
wise noise estimation, enabling the identification of heavily
corrupted bands based on the predicted noise level σest, as
illustrated in Fig. 19. For severely contaminated bands (defined
as σest > 2σmean, where σmean denotes the mean noise level
across all bands), we adopt the highest noise estimation output
to ensure noise suppression. The idea is to exploit the rich
redundancy information inherent in HSI tubes to achieve noise
removal. However, this may cause oversmooth effects to the
cleaner bands. Therefore, for the less corrupted spectral bands,
we can refine the denosing result by adopting their mean noise
estimation value σmean to preserve fine details.
(a) PaviaU
(b) Urban
Fig. 19. Estimated noise σest for different bands with the proposed strategy.
Visual evaluations for several real-world HSI datasets are
provided in Fig. 20 to Fig. 22, illustrating the proposed
method’s performance on challenging cases. The qualitative
evaluations suggest that benefiting from the adaptive noise
estimation scheme, the proposed method can achieve promis-
ing results. In particular, Fig. 21 and Fig. 22 demonstrate its
ability of handling complex noise patterns, showing a rea-
sonable ability to suppress both random noise and structured
striping artifacts. Moreover, fine-grained textural details are
well preserved without introducing noticeable artifacts.
(a) Noisy
(b) BM4D
(c) RAS2S
(d) A-Haar-tSVD
Fig. 20. Denoising comparison on the PaviaU data [99].
(a) Noisy
(b) BM4D
(c) RAS2S
(d) A-Haar-tSVD
Fig. 21. Denoising comparison on the Urban data [100].
(a) Noisy
(b) BM4D
(c) RAS2S
(d) A-Haar-tSVD
Fig. 22. Denoising comparison on the EO1 data [101].


11
E. Parameter Analysis
Following the classic patch-based denoising paradigm, the
proposed Haar-tSVD transform relies on several key parame-
ters such as the patch size ps, search window range W and
number of nonlocal similar patches K. Fig. 23 evaluates the
impact of these parameters on the denoising performance of
Haar-tSVD across different datasets. For the patch size ps, a
comparison between the CC15 and SIDD datasets shows that
larger patch size can improve robustness against heavy noise
contamination. However, increasing ps may drastically raise
computational burden. Similarly, expanding the search range
W and choosing more grouped patches K do not guarantee
performance enhancement due to the rare patch effect. Based
on these observations, we mainly set ps = 8, W = 18 and
K = 32 in our experiments to achieve a tradeoff between
denoising performance and speed.
(a) Patch size ps
(b) Search range W
(c) Similar patches K
Fig. 23. Influence of different parameters on the Haar-tSVD transform.
F. Discussion
1) Ablation study: The effectiveness and robustness of the
effective adaptive variant A-Haar-tSVD largely stems from the
local adjustment of noise levels through eigenvalue analysis
in Equ. (22). To further assess its impact, we conduct an
ablation study. As shown in Table IX, A-Haar-tSVD con-
sistently benefits from the adaptive noise level adjustment
scheme in Equ. (23) across different scenarios, while incurring
minimal computational cost, which justifies the efficiency and
applicability of the proposed adaptive mechanism.
TABLE IX
COMPARISON OF THE PROPOSED A-HAAR-TSVD WITH AND WITHOUT
LOCAL NOISE LEVEL ADJUSTMENT.
Dataset
Without local adjustment
With local adjustment
CC15
38.10/0.961
38.24/0.963
PolyU
38.77/0.969
38.89/0.971
HighISO
40.53/0.974
40.63/0.974
IOCI
41.39/0.977
41.52/0.978
SIDD-val
35.20/0.893
35.28/0.894
Time (s)
3.65
3.93
The adaptive variant A-Haar-tSVD introduces two weight-
ing parameters, β and γ, which enhance robustness and adap-
tiveness based on the eigenvalue analysis in (22). We conduct
a sensitivity study to evaluate the impact of β and γ, with the
results presented in Fig. 24. We notice that a small β(< 1)
tends to overestimate the noise level, lead to oversmoothing
and lower PSNR. On the other hand, choosing an excessively
large γ may fail to capture the inner-group similarity under
noisy conditions. Hence, a reasonable range for γ is between
12 and 16. Based on these findings, β and γ are empirically
set to 1.2 and 13 in our experiments, respectively.
(a) β
(b) γ
Fig. 24. Impact of weighting parameters β and γ on A-Haar-tSVD.
2) Limitations and potential enhancement: From Table
IV, classic patch-based transforms struggle with severely-
corrupted images, where noise energy may spread across all
frequencies and all principal directions, thus the coefficients
of noise overlap in magnitude with those of the true image.
Hence, the thresholding operation fails to separate noise from
signal. Beyond integrating DNNs with Haar-tSVD, it is in-
teresting to devise an alternative approach within the patch-
based paradigm to handle severe noise. Recently, Zontak et
al. [102] observed that down-sampled noisy images contain
patches with reduced noise and structures similar to clean
ones. Therefore, instead of directly filtering the large-size
noisy observation, an alternative is to first handle downsized
image, and then restore the original resolution using image
super-resolution techniques [103], [104].
V. CONCLUSION
In this paper, we present Haar-tSVD, an efficient and
effective one-step method for image denoising. We leverage
global and local circulant representation to capture similar-
ity and correlation among image patches at both patch and
group levels. Under the circulant formulation, we establish
a theoretical connection between the Haar transform and
PCA, and demonstrate that Haar-tSVD can be modeled by a
unified t-SVD and Haar bases, resulting in a plug-and-play
and parallelizable filtering approach. The proposed method
reduces computational cost by avoiding the training of local
transform bases. To achieve further acceleration, we also
design and implement fast, parallelizable strategies. Moreover,
we investigate the integration and combination of neural net-
works with the proposed Haar-tSVD. This leads to a flexible
noise estimation scheme and an enhancement strategy based
on the eigenvalue characteristics of circulant structures. The
promising results motivate exploration of the proposed method
and its adaptive scheme for broader applications in other
imaging domains [105], [106].
REFERENCES
[1] M. Elad, B. Kawar, and G. Vaksman, “Image denoising: The deep
learning revolution and beyond—a survey paper,” SIAM J. Imaging
Sci., vol. 16, no. 3, pp. 1594–1654, 2023.
[2] B. Wen, “Reproducible denoising methods,” [Online]. Available: https:
//github.com/wenbihan/reproducible-image-denoising-state-of-the-art.
[3] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, “Beyond a
gaussian denoiser: Residual learning of deep cnn for image denoising,”
IEEE Trans. Image Process., vol. 26, no. 7, pp. 3142–3155, 2017.
[4] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” in Proc. Int.
Conf. Mach. Learn., 2015, pp. 448–456.


12
[5] V. Nair and G. E. Hinton, “Rectified linear units improve restricted
boltzmann machines,” in Proc. Int. Conf. Mach. Learn., 2010, pp. 807–
814.
[6] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,
pp. 770–778.
[7] L. Chen, X. Chu, X. Zhang, and J. Sun, “Simple baselines for image
restoration,” in Eur. Conf. Comput. Vis.
Springer, 2022, pp. 17–33.
[8] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-
H. Yang, “Restormer: Efficient transformer for high-resolution image
restoration,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022,
pp. 5728–5739.
[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Proc.
Advances Neural Inf. Process. Syst., vol. 30, 2017.
[10] Z. Kong, F. Deng, H. Zhuang, X. Yang, J. Yu, and L. He, “A comparison
of image denoising methods,” arXiv preprint arXiv:2304.08990, 2023.
[11] V. Katkovnik, A. Foi, K. Egiazarian, and J. Astola, “From local kernel
to nonlocal multiple-model image denoising,” Int. J. Comput. Vis.,
vol. 86, no. 1, pp. 1–32, 2010.
[12] D. Zoran and Y. Weiss, “From learning models of natural image patches
to whole image restoration,” in Proc. IEEE Int. Conf. Comput. Vis.,
2011, pp. 479–486.
[13] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Image denoising
by sparse 3-d transform-domain collaborative filtering,” IEEE Trans.
Image Process., vol. 16, no. 8, pp. 2080–2095, 2007.
[14] A. Buades, B. Coll, and J.-M. Morel, “A review of image denoising
algorithms, with a new one,” Multiscale Model. Simul., vol. 4, no. 2,
pp. 490–530, 2005.
[15] M. Elad and M. Aharon, “Image denoising via sparse and redundant
representations over learned dictionaries,” IEEE Trans. Image Process.,
vol. 15, no. 12, pp. 3736–3745, 2006.
[16] L. P. Yaroslavsky, K. O. Egiazarian, and J. T. Astola, “Transform
domain image restoration methods: review, comparison, and interpreta-
tion,” in Proc. Nonlinear Image Process. Pattern Anal. XII, vol. 4304,
2001, pp. 155–169.
[17] S. Gu, L. Zhang, W. Zuo, and X. Feng, “Weighted nuclear norm
minimization with application to image denoising,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit., 2014, pp. 2862–2869.
[18] J. Xu, L. Zhang, and D. Zhang, “A trilateral weighted sparse coding
scheme for real-world image denoising,” in Eur. Conf. Comput. Vis.,
2018, pp. 20–36.
[19] K. P. Murphy, Machine learning: a probabilistic perspective.
MIT
press, 2012.
[20] A. Lucas, M. Iliadis, R. Molina, and A. K. Katsaggelos, “Using deep
neural networks for inverse problems in imaging: beyond analytical
methods,” IEEE Signal Process. Mag., vol. 35, no. 1, pp. 20–36, 2018.
[21] G. J. Tee, “Eigenvectors of block circulant and alternating circulant
matrices,” N. Z. J. Math., vol. 36, no. 8, pp. 195–211, 2007.
[22] Z. Zhang and S. Aeron, “Exact tensor completion using t-svd,” IEEE
Trans. Signal Process., vol. 65, no. 6, pp. 1511–1526, 2016.
[23] T. G. Kolda and B. W. Bader, “Tensor decompositions and applica-
tions,” SIAM Rev., vol. 51, no. 3, pp. 455–500, 2009.
[24] A. Foi, V. Katkovnik, and K. Egiazarian, “Pointwise shape-adaptive
dct for high-quality denoising and deblocking of grayscale and color
images,” IEEE Trans. Image Process., vol. 16, no. 5, pp. 1395–1411,
2007.
[25] A. Buades, J.-L. Lisani, and M. Miladinovi´c, “Patch-based video
denoising with optical flow estimation,” IEEE Trans. Image Process.,
vol. 25, no. 6, pp. 2573–2586, 2016.
[26] Y. M¨akinen, L. Azzari, and A. Foi, “Collaborative filtering of correlated
noise: Exact transform-domain variance for improved shrinkage and
patch matching,” IEEE Trans. Image Process., 2020.
[27] F. Chen, X. Zeng, and M. Wang, “Image denoising via local and
nonlocal circulant similarity,” J. Vis. Commun. Image Represent.,
vol. 30, pp. 117–124, 2015.
[28] Z. Kong and X. Yang, “Color image and multispectral image denoising
using block diagonal representation,” IEEE Trans. Image Process.,
vol. 28, no. 9, pp. 4247–4259, 2019.
[29] M. E. Kilmer and C. D. Martin, “Factorization strategies for third-order
tensors,” Linear Algebra Appl., vol. 435, no. 3, pp. 641–658, 2011.
[30] M. E. Kilmer, K. Braman, N. Hao, and R. C. Hoover, “Third-order
tensors as operators on matrices: A theoretical and computational
framework with applications in imaging,” SIAM J. Matrix Anal. Appl.,
vol. 34, no. 1, pp. 148–172, 2013.
[31] G. M. PD, B. Madathil, and S. N. George, “Entropy-based reweighted
tensor completion technique for video recovery,” IEEE Trans. Circuit
Syst. Video Technol., vol. 30, no. 2, pp. 415–426, 2019.
[32] J. Xue, Y.-Q. Zhao, T. Wu, and J. C.-W. Chan, “Tensor convolution-like
low-rank dictionary for high-dimensional image representation,” IEEE
Trans. Circuit Syst. Video Technol., vol. 34, no. 12, pp. 13 257–13 270,
2024.
[33] J. A. Bengua, H. N. Phien, H. D. Tuan, and M. N. Do, “Efficient tensor
completion for color image and video recovery: Low-rank tensor train,”
IEEE Trans. Image Process., vol. 26, no. 5, pp. 2466–2479, 2017.
[34] T. Blu and F. Luisier, “The sure-let approach to image denoising,” IEEE
Trans. Image Process., vol. 16, no. 11, pp. 2778–2786, 2007.
[35] Y. Hou, J. Xu, M. Liu, G. Liu, L. Liu, F. Zhu, and L. Shao, “Nlh:
A blind pixel-level non-local method for real-world image denoising,”
IEEE Trans. Image Process., vol. 29, pp. 5121–5135, 2020.
[36] Roeser and Jernigan, “Fast haar transform algorithms,” IEEE Trans.
Comput., vol. 100, no. 2, pp. 175–177, 1982.
[37] S. Guo, Z. Liang, and L. Zhang, “Joint denoising and demosaicking
with green channel prior for real-world burst images,” IEEE Trans.
Image Process., vol. 30, pp. 6930–6942, 2021.
[38] Z. Kong, F. Deng, and X. Yang, “Image denoising using green channel
prior,” IEEE Trans. Image Process., 2025.
[39] S. Nam, Y. Hwang, Y. Matsushita, and S. Joo Kim, “A holistic approach
to cross-channel image noise modeling and its application to image
denoising,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,
pp. 1683–1691.
[40] H. Yue, J. Liu, J. Yang, T. Q. Nguyen, and F. Wu, “High iso jpeg image
denoising by deep fusion of collaborative and convolutional filtering,”
IEEE Trans. Image Process., vol. 28, no. 9, pp. 4339–4353, 2019.
[41] A. Abdelhamed, S. Lin, and M. S. Brown, “A high-quality denoising
dataset for smartphone cameras,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., 2018, pp. 1692–1700.
[42] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
Trans. Image Process., vol. 13, no. 4, pp. 600–612, 2004.
[43] L. Zhang, W. Dong, D. Zhang, and G. Shi, “Two-stage image denoising
by principal component analysis with local pixel grouping,” Pattern
Recognit., vol. 43, no. 4, pp. 1531–1549, 2010.
[44] Q. Guo, C. Zhang, Y. Zhang, and H. Liu, “An efficient svd-based
method for image denoising,” IEEE Trans. Circuit Syst. Video Technol.,
vol. 26, no. 5, pp. 868–880, 2015.
[45] D. L. Donoho and J. M. Johnstone, “Ideal spatial adaptation by wavelet
shrinkage,” biometrika, vol. 81, no. 3, pp. 425–455, 1994.
[46] X. Gong, W. Chen, and J. Chen, “A low-rank tensor dictionary learn-
ing method for hyperspectral image denoising,” IEEE Trans. Signal
Process., vol. 68, pp. 1168–1180, 2020.
[47] Q. Shi, Y.-M. Cheung, and J. Lou, “Robust tensor svd and recovery
with rank estimation,” IEEE Trans. Cybern., vol. 52, no. 10, pp. 10 667–
10 682, 2022.
[48] H. C. Burger, C. J. Schuler, and S. Harmeling, “Image denoising:
Can plain neural networks compete with bm3d?” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit., 2012, pp. 2392–2399.
[49] L. P. Yaroslavsky, “Local adaptive image restoration and enhancement
with the use of dft and dct in a running window,” in Proc. SPIE, vol.
2825, 1996, pp. 2–13.
[50] J. Xu, L. Zhang, D. Zhang, and X. Feng, “Multi-channel weighted
nuclear norm minimization for real color image denoising,” in Proc.
IEEE Int. Conf. Comput. Vis., 2017, pp. 1096–1104.
[51] J. Xu, L. Zhang, and D. Zhang, “External prior guided internal prior
learning for real-world noisy image denoising,” IEEE Trans. Image
Process., vol. 27, no. 6, pp. 2996–3010, 2018.
[52] G. Treece, “Real image denoising with a locally-adaptive bitonic filter,”
IEEE Trans. Image Process., vol. 31, pp. 3151–3165, 2022.
[53] M. Maggioni, V. Katkovnik, K. Egiazarian, and A. Foi, “Nonlocal
transform-domain filter for volumetric data denoising and reconstruc-
tion,” IEEE Trans. Image Process., vol. 22, no. 1, pp. 119–133, 2012.
[54] A. Rajwade, A. Rangarajan, and A. Banerjee, “Image denoising using
the higher order singular value decomposition,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 35, no. 4, pp. 849–862, 2012.
[55] Y. Peng, D. Meng, Z. Xu, C. Gao, Y. Yang, and B. Zhang, “De-
composable nonlocal tensor dictionary learning for multispectral image
denoising,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014,
pp. 2949–2956.
[56] Y. Chang, L. Yan, and S. Zhong, “Hyper-laplacian regularized unidi-
rectional low-rank tensor recovery for multispectral image denoising,”
in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 4260–
4268.


13
[57] PyTorch Team. Training a classifier. [Online]. Available: https:
//docs.pytorch.org/tutorials/beginner/blitz/cifar10 tutorial.html
[58] Kodak, “Kodak gallery dataset,” [Online]. Available: http://r0k.us/
graphics/kodak.
[59] T. Plotz and S. Roth, “Benchmarking denoising algorithms with real
photographs,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
2017, pp. 1586–1595.
[60] J. Xu, H. Li, Z. Liang, D. Zhang, and L. Zhang, “Real-world noisy
image denoising: A new benchmark,” arXiv preprint arXiv:1804.02603,
2018.
[61] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Color image
denoising via sparse 3d collaborative filtering with grouping constraint
in luminance-chrominance space,” in Proc. IEEE Int. Conf. Image
Process., 2007, pp. 313–316.
[62] Q. Ma, J. Jiang, X. Zhou, P. Liang, X. Liu, and J. Ma, “Pixel2pixel: A
pixelwise approach for zero-shot single image denoising,” IEEE Trans.
Pattern Anal. Mach. Intell., 2025.
[63] H. Kim and N. I. Cho, “Apr-rd: Complemental two steps for self-
supervised real image denoising,” in Proc. AAAI, vol. 39, no. 4, 2025,
pp. 4257–4265.
[64] Z. Wang, J. Liu, G. Li, and H. Han, “Blind2unblind: Self-supervised
image denoising with visible blind spots,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit., 2022, pp. 2027–2036.
[65] S. Herbreteau and M. Unser, “Self-calibrated variance-stabilizing trans-
formations for real-world image denoising,” 2025.
[66] J. Li, Z. Zhang, X. Liu, C. Feng, X. Wang, L. Lei, and W. Zuo, “Spa-
tially adaptive self-supervised learning for real-world image denoising,”
in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023.
[67] J. Li, Z. Zhang, and W. Zuo, “Rethinking transformer-based blind-
spot network for self-supervised image denoising,” arXiv preprint
arXiv:2404.07846, 2024.
[68] Y. Quan, T. Zheng, Z. Ma, and H. Ji, “Zero-shot blind-spot image
denoising via implicit neural sampling,” in IEEE Conf. Comput. Vis.
Pattern Recog., 2025, pp. 7502–7512.
[69] J. Cheng, D. Liang, and S. Tan, “Transfer clip for generalizable image
denoising,” in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp.
25 974–25 984.
[70] Y.
Huang
and
H.
Huang,
“Beyond
image
prior:
Embedding
noise prior into conditional denoising transformer,” arXiv preprint
arXiv:2407.09094, 2024.
[71] X. Deng, C. Zhang, L. Jiang, J. Xia, and M. Xu, “Deepsn-net: Deep
semi-smooth newton driven network for blind image restoration,” IEEE
Trans. Pattern Anal. Mach. Intell., 2025.
[72] S. Yu, B. Park, and J. Jeong, “Deep iterative down-up cnn for image
denoising,” in Proc. Conf. Comput. Vis. Pattern Recognit. Workshops,
2019.
[73] T. Li, H. Feng, L. Wang, L. Zhu, Z. Xiong, and H. Huang, “Stimulating
diffusion model for image denoising via adaptive embedding and
ensembling,” IEEE Trans. Pattern Anal. Mach. Intell., 2024.
[74] K. Zhang, W. Zuo, and L. Zhang, “Ffdnet: Toward a fast and flexible
solution for cnn-based image denoising,” IEEE Trans. Image Process.,
vol. 27, no. 9, pp. 4608–4622, 2018.
[75] D. Kim, J. Ko, M. K. Ali, and T. H. Kim, “Idf: Iterative dynamic
filtering networks for generalizable image denoising,” in Int. Conf.
Comput. Vis., 2025.
[76] B. Li, H. Zhao, W. Wang, P. Hu, Y. Gou, and X. Peng, “Mair: A
locality- and continuity-preserving mamba for image restoration,” in
IEEE Conf. Comput. Vis. Pattern Recog., Nashville, TN, Jun. 2025.
[77] M. Maggioni, G. Boracchi, A. Foi, and K. Egiazarian, “Video de-
noising, deblocking, and enhancement through separable 4-d nonlocal
spatiotemporal transforms,” IEEE Trans. Image Process., vol. 21, no. 9,
pp. 3952–3966, 2012.
[78] B. Wen, S. Ravishankar, and Y. Bresler, “Vidosat: High-dimensional
sparsifying transform learning for online video denoising,” IEEE Trans.
Image Process., vol. 28, no. 4, pp. 1691–1704, 2018.
[79] A. Davy, T. Ehret, J.-M. Morel, P. Arias, and G. Facciolo, “A non-local
cnn for video denoising,” in Proc. IEEE Conf. Int. Image Process.,
2019, pp. 2409–2413.
[80] M. Tassano, J. Delon, and T. Veit, “Dvdnet: A fast network for deep
video denoising,” in Proc. IEEE Conf. Int. Image Process., 2019, pp.
1805–1809.
[81] ——, “Fastdvdnet: Towards real-time deep video denoising without
flow estimation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
June 2020.
[82] J. Li, X. Wu, Z. Niu, and W. Zuo, “Unidirectional video denoising
by mimicking backward recurrent modules with look-ahead forward
ones,” in Eur. Conf. Comput. Vis.
Springer, 2022, pp. 592–609.
[83] Z. Chen, T. Jiang, X. Hu, W. Zhang, H. Li, and H. Wang, “Spa-
tiotemporal blind-spot network with calibrated flow alignment for self-
supervised video denoising,” in Proc. AAAI, vol. 39, no. 3, 2025, pp.
2411–2419.
[84] D. Y. Sheth, S. Mohan, J. L. Vincent, R. Manzorro, P. A. Crozier, M. M.
Khapra, E. P. Simoncelli, and C. Fernandez-Granda, “Unsupervised
deep video denoising,” in Proc. IEEE Int. Conf. Comput. Vis., 2021,
pp. 1759–1768.
[85] M. Claus and J. Van Gemert, “Videnn: Deep blind video denoising,” in
Proc. Conf. Comput. Vis. Pattern Recognit. Workshops, 2019, pp. 0–0.
[86] J. Liang, J. Cao, Y. Fan, K. Zhang, R. Ranjan, Y. Li, R. Timofte,
and L. Van Gool, “Vrt: A video restoration transformer,” IEEE Trans.
Image Process., 2024.
[87] H. Yue, C. Cao, L. Liao, R. Chu, and J. Yang, “Supervised raw video
denoising with a benchmark dataset on dynamic scenes,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit., 2020, pp. 2301–2310.
[88] W. He, Q. Yao, C. Li, N. Yokoya, and Q. Zhao, “Non-local meets
global: An integrated paradigm for hyperspectral denoising,” in Proc.
IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 6861–6870.
[89] J.-L. Zhao, T.-H. Zhang, S. Fang, J.-F. Gao, J.-Y. Wang, and M.-
G. Gong, “Spatial-spectral texture-preserved total variation: A novel
regularization for hyperspectral image denoising,” IEEE Trans. Circuit
Syst. Video Technol., 2025.
[90] Y. Chen, H. Zhang, Y. Wang, Y. Yang, and J. Wu, “Flex-dld: Deep
low-rank decomposition model with flexible priors for hyperspectral
image denoising and restoration,” IEEE Trans. Image Process., 2024.
[91] Y. Chang, L. Yan, H. Fang, S. Zhong, and W. Liao, “Hsi-denet:
Hyperspectral image restoration via convolutional neural network,”
IEEE Trans. Geosci. Remote Sens., vol. 57, no. 2, pp. 667–682, 2018.
[92] F. Xiong, J. Zhou, Q. Zhao, J. Lu, and Y. Qian, “Mac-net: Model-
aided nonlocal neural network for hyperspectral image denoising,”
IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–14, 2021.
[93] K. Wei, Y. Fu, J. Yang, and H. Huang, “A physics-based noise
formation model for extreme low-light raw denoising,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit., 2020, pp. 2758–2767.
[94] J. Xiao, Y. Liu, and X. Wei, “Region-aware sequence-to-sequence
learning for hyperspectral denoising,” in Eur. Conf. Comput. Vis.
Springer, 2024, pp. 218–235.
[95] A. Maffei, J. M. Haut, M. E. Paoletti, J. Plaza, L. Bruzzone, and
A. Plaza, “A single model cnn for hyperspectral image denoising,”
IEEE Trans. Geosci. Remote Sens., vol. 58, no. 4, pp. 2516–2529,
2019.
[96] R. H. Yuhas, J. W. Boardman, and A. F. Goetz, “Determination of semi-
arid landscape endmembers and seasonal trends using convex geometry
spectral unmixing techniques,” ratio, vol. 4, p. 22, 1990.
[97] L. Wald, Data fusion: definitions and architectures: fusion of images of
different spatial resolutions.
Les Presses de l’Ecoledes Mines, 2002.
[98] J. Song, J.-H. Jeong, D.-S. Park, H.-H. Kim, D.-C. Seo, and J. C. Ye,
“Unsupervised denoising for satellite imagery using wavelet directional
cyclegan,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 8, pp. 6823–
6839, 2020.
[99] https://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote
Sensing Scenes.
[100] L. S. Kalman and E. M. Bassett III, “Classification and material
identification in an urban environment using hydice hyperspectral data,”
in Imaging Spectrometry III, vol. 3118.
SPIE, 1997, pp. 57–68.
[101] E. M. Middleton, S. G. Ungar, D. J. Mandl, L. Ong, S. W. Frye, P. E.
Campbell, D. R. Landis, J. P. Young, and N. H. Pollack, “The earth
observing one (eo-1) satellite mission: Over a decade in space,” IEEE
J. Sel. Top. Appl. Earth Obs. Remote. Sens., vol. 6, no. 2, pp. 243–256,
2013.
[102] M. Zontak, I. Mosseri, and M. Irani, “Separating signal from noise
using patch recurrence across scales,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit., 2013, pp. 1195–1202.
[103] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using
deep convolutional networks,” IEEE Trans. Pattern Anal. Mach. Intell.,
vol. 38, no. 2, pp. 295–307, 2015.
[104] Z. Wang, J. Chen, and S. C. Hoi, “Deep learning for image super-
resolution: A survey,” IEEE Trans. Pattern Anal. Mach. Intell., 2020.
[105] H. Fu, J. Liang, Z. Fang, J. Han, F. Liang, and G. Zhang, “Weconvene:
Learned image compression with wavelet-domain convolution and
entropy model,” in Eur. Conf. Comput. Vis. Springer, 2024, pp. 37–53.
[106] Y. Luo, X. Zhao, and D. Meng, “Revisiting nonlocal self-similarity
from continuous representation,” IEEE Trans. Pattern Anal. Mach.
Intell., 2024.
