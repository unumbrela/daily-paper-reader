Title: Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study

URL Source: https://arxiv.org/pdf/2512.23435v1

Published Time: Tue, 30 Dec 2025 02:26:38 GMT

Number of Pages: 5

Markdown Content:
# Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study 

Saifelden M. Ismail *

University of Science and Technology, Zewail City 

Giza, Egypt Email: s-saifelden.ismail@zewailcity.edu.eg ORCID: 0009-0002-8867-6533 

Abstract —Speech Emotion Recognition (SER) has signifi-cant potential for mobile applications, yet deployment remains constrained by the computational demands of state-of-the-art transformer architectures. This paper presents a mobile-efficient SER system based on DistilHuBERT, a distilled and 8-bit quantized transformer that achieves 92% parameter reduction compared to full-scale Wav2Vec 2.0 models while maintaining competitive accuracy. We conduct a rigorous 5-fold Leave-One-Session-Out (LOSO) cross-validation on the IEMOCAP dataset to ensure speaker independence, augmented with cross-corpus training on CREMA-D to enhance generalization. Cross-corpus training with CREMA-D yields a 1.2% improvement in Weighted Accuracy, a 1.4% gain in Macro F1-score, and a 32% reduction in cross-fold variance, with the Neutral class showing the most substantial benefit at 5.4% F1-score improvement. Our approach achieves an Unweighted Accuracy of 61.4% with a quantized model footprint of only 23 MB, representing approximately 91% of full-scale baseline performance. Cross-corpus evaluation on RAVDESS reveals that the theatrical nature of acted emotions causes predictions to cluster by arousal level rather than valence—happiness is systematically confused with anger due to acoustic saturation in high-energy expressions. Despite this theatricality effect reducing overall RAVDESS accuracy to 43.29%, the model maintains robust arousal detection with 97% recall for anger and 64% for sadness. These findings establish a Pareto-optimal tradeoff between model size and accuracy, enabling practical affect recognition on resource-constrained mobile devices. 

Keywords —Speech Emotion Recognition, Model Compression, DistilHuBERT, Cross-Corpus Validation, Mobile Deployment, Affective Computing 

I. L IST OF ABBREVIATIONS 

SER Speech Emotion Recognition LOSO Leave-One-Session-Out WA Weighted Accuracy UA Unweighted Accuracy UAR Unweighted Average Recall CV Cross-Validation VAD Voice Activity Detection ONNX Open Neural Network Exchange NAS Neural Architecture Search II. I NTRODUCTION 

Speech Emotion Recognition (SER) is critical for respon-sive mobile applications, including mental health monitoring, customer service analytics, and adaptive human-computer interaction. The ability to automatically detect emotional states from vocal cues enables systems to provide personalized feedback, detect distress signals, and enhance user experience through affectively aware interfaces. However, the deployment of SER systems on mobile devices faces a fundamental challenge: state-of-the-art models based on self-supervised transformer architectures such as Wav2Vec 2.0 [8] and HuBERT [5] achieve high accuracy but require hundreds of megabytes of storage and substantial computational resources, making them impractical for edge deployment. Recent advances in knowledge distillation [10] offer a potential solution to this efficiency bottleneck. DistilHuBERT [4], a layer-wise distilled variant of the HuBERT architecture, compresses the model to approximately 23 MB after 8-bit quantization [11] while preserving much of the original model’s representational capacity. This dramatic reduction in foot-print enables on-device inference without cloud dependencies, addressing privacy concerns and reducing latency for real-time applications. However, the performance implications of such aggressive compression in the context of emotion recognition—a task requiring nuanced extraction of prosodic and spectral features—remain underexplored. Beyond architectural efficiency, a second critical challenge in SER research is generalization. Many published results rely on cross-validation protocols that inadvertently introduce speaker leakage, where the same speaker appears in both training and test sets. This methodological flaw leads to inflated accuracy estimates, as models learn speaker-specific vocal traits rather than generalizable emotional patterns. To address this, we adopt a strict 5-fold Leave-One-Session-Out (LOSO) protocol on the IEMOCAP dataset [1], ensuring that each test fold contains entirely unseen speakers. This rigorous evaluation standard, while resulting in lower absolute accuracy, provides a more realistic estimate of deployment performance. Furthermore, we investigate whether cross-corpus training can serve as an effective regularization strategy. By incor-porating the CREMA-D dataset [6] as a permanent training component, we expose the model to 91 additional speakers across diverse recording conditions. This approach aims to reduce overfitting to the idiosyncrasies of a single corpus and improve robustness to acoustic variability. However, cross-corpus training introduces its own complexities, particularly when datasets differ in emotional expressiveness. CREMA-D features theatrical, acted emotions, while IEMOCAP contains 

> arXiv:2512.23435v1 [cs.SD] 29 Dec 2025

more naturalistic, conversational affect. Understanding how these domain differences affect model behavior is essential for designing robust SER systems. This paper makes the following contributions: (1) We demonstrate that DistilHuBERT achieves 91% of full-scale Wav2Vec 2.0 performance while reducing model size by 92%, establishing a new Pareto frontier for mobile SER. (2) We conduct a methodologically rigorous 5-fold LOSO evaluation, avoiding the speaker leakage issues prevalent in prior work. (3) We quantify the regularization benefits of cross-corpus training with CREMA-D, showing a 1.2% gain in Weighted Accuracy and a 32% reduction in cross-fold variance. (4) We provide detailed error analysis on RAVDESS [7], revealing that the model maintains robust arousal detection (97% recall for anger) despite domain shift, with errors systematically clustered by energy level rather than random misclassification. (5) We propose a practical deployment architecture utilizing Voice Activity Detection and temporal aggregation to mitigate neutral-class bias in long-form voice note analysis. The remainder of this paper is organized as follows: Section II details our methodology, including dataset integration, acoustic preprocessing, and the Adaptive Focal Loss [9] training objective. Section III presents experimental results and comparative benchmarking against both high-capacity and efficiency-oriented baselines. Section IV describes the production model and cross-corpus error analysis. Section V concludes with a discussion of limitations and future directions. III. M ETHODOLOGY 

In this section, we detail our approach to Speech Emotion Recognition (SER), emphasizing a pipeline designed for mobile deployment and cross-corpus robustness. Our methodology is characterized by a multi-stage acoustic workflow, the use of a distilled transformer architecture, and a specialized training objective to handle class imbalance. 

A. Experimental Framework and Dataset Integration 

We utilize the IEMOCAP dataset [1] as our primary benchmark, focusing on four categorical emotions: anger, happiness (including excitement), neutral, and sadness. To evaluate speaker independence, we implement a strict 5-fold Leave-One-Session-Out (LOSO) cross-validation, following the protocol of Pepino et al. [2]. In each fold, the model is trained on three sessions, validated on one, and tested on the remaining session. To enhance robustness, we incorporate the CREMA-D dataset [6] as a permanent training component. In adher-ence to rigorous evaluation standards, CREMA-D samples are exclusively used for training and are never included in validation or test sets. This configuration ensures that our performance metrics reflect true generalization to unseen IEMOCAP speakers across distinct recording environments. 

B. Acoustic Processing and Data Augmentation 

Unlike standard approaches that rely on Mel-frequency Cepstral Coefficients (MFCCs) [3], we process raw audio waveforms sampled at 16,000 Hz. The preprocessing pipeline begins with the removal of non-informative intervals using a 20 dB top-threshold trim. To compensate for natural spectral tilt and emphasize high-frequency prosodic cues, a pre-emphasis filter ( H(z) = 1 − 0.97 z−1) is applied. Finally, all utterances are peak-normalized to a maximum absolute amplitude of 0.95 to eliminate volume bias across heterogeneous recording setups. To improve generalization, we implement a stochastic augmentation pipeline applied during the training phase. This includes random gain adjustments between -18.0 and +6.0 dB, the addition of Gaussian noise with amplitudes ranging from 0.001 to 0.015, pitch shifting within ±2 semitones, and polarity inversion. All audio segments are fixed to a maximum duration of 8.0 seconds, with shorter clips being padded and longer clips truncated to maintain batch consistency. 

C. Feature Extraction and Architecture Selection 

For feature extraction, we prioritize mobile-deployability through the use of DistilHuBERT [4]. While standard Wav2Vec 2.0 models [2] offer high performance, their parameter count is often prohibitive for edge devices. DistilHuBERT utilizes knowledge distillation to compress the Hidden-unit BERT (HuBERT) architecture into a shallower transformer encoder. Our choice is motivated by hardware efficiency; upon 8-bit quantization, the model footprint is approximately 23 MB. In our implementation, the feature encoder remains frozen to leverage pre-trained acoustic representations, while the downstream classification head is fine-tuned for categorical emotion recognition. 

D. Optimization Strategy and Hyperparameters 

The model is trained using the AdamW optimizer [12] for 25 epochs with a batch size of 16. We employ a cosine learning rate scheduler with a peak learning rate of 5 × 10 −5 and a warmup ratio of 0.1 to stabilize early training. To mitigate overfitting, a weight decay of 0.01 is applied, and we implement an early stopping callback with a patience of 6 epochs based on validation Unweighted Accuracy (UA). To address the class imbalance detailed in Table II, we utilize an Adaptive Focal Loss function. This objective integrates balanced class weights ( αt) and a focusing parameter ( γ =2.0) to prioritize difficult, under-represented samples. The loss function is further regularized with a label smoothing factor of 0.1, defined as: 

F L (pt) = −αt(1 − pt)γ log( pt) (1) By combining this focal objective with the weight decay and dropout layers inherent to the transformer architecture, the model is encouraged to learn robust, speaker-invariant emotional features. IV. R ESULTS AND DISCUSSION 

The experimental results for the 5-fold LOSO cross-validation on the IEMOCAP dataset are detailed in Table I. Our analysis examines the trade-offs between architectural effi-ciency, the regularization effects of multi-corpus training, and the methodological rigor required for real-world deployment. TABLE I PERFORMANCE COMPARISON OF 5-F OLD LOSO C ROSS -V ALIDATION ON                                                                                      

> IEMOCAP
> Metric / Class IEMOCAP Only + CREMA-D Gain ( ∆)
> Global Metrics
> WA ↑0.595 ±0.031 0.607 ±0.021 +1.2% UA (UAR) ↑0.616 ±0.039 0.614 ±0.030 -0.2% Macro F1 ↑0.596 ±0.040 0.610 ±0.028 +1.4%
> Class-wise F1-Score
> Anger 0.684 ±0.029 0.692 ±0.040 +0.8% Happiness 0.541 ±0.080 0.563 ±0.073 +2.2% Neutral 0.512 ±0.059 0.566 ±0.029 +5.4% Sadness 0.647 ±0.044 0.618 ±0.034 -2.9% Note: Bold values indicate superior performance. TABLE II DISTRIBUTION OF EMOTIONAL CLASSES ACROSS IEMOCAP AND
> CREMA-D
> Dataset Anger Hap/Exc Neutral Sad Total
> IEMOCAP 1,103 1,636 1,708 1,084 5,531 CREMA-D (Train) 1,271 1,271 1,087 1,271 4,900
> Combined Total 2,374 2,907 2,795 2,355 10,431
> Avg. Class Weight 1.065 0.913 0.974 1.070 —

A. Cross-Corpus Regularization via CREMA-D 

A primary focus of this study was the integration of the CREMA-D dataset as a permanent training anchor. As seen in Table II, the inclusion of 4,900 additional samples nearly doubled the available training data. This strategy yielded a 1.2% increase in Weighted Accuracy (WA) and a 1.4% improvement in Macro F1-score. More critically, the standard deviation of the WA across the five folds decreased from 

0.031 to 0.021 , representing a 32% reduction in cross-fold variance. This substantial decrease in performance variability is a strong indicator that CREMA-D functions as an effective regularization mechanism. The acoustic diversity provided by the 91 additional speakers in CREMA-D—each with distinct vocal tract characteristics, recording equipment, and expressive styles—serves as a stabilizing regularizer, reducing the model’s sensitivity to the idiosyncratic vocal characteristics of individual IEMOCAP sessions. Rather than merely increasing dataset size, the introduction of cross-corpus heterogeneity prevents the model from overfitting to corpus-specific artifacts such as room acoustics, microphone frequency response, or the limited phonetic coverage of a single speaker pool. The class-wise analysis reveals a 5.4% gain in the Neutral class F1-score, suggesting that multi-corpus exposure helps the model calibrate a more robust baseline for vocal energy. However, we observed a 2.9% decrease in Sadness recall. This discrepancy points to a ”theatricality gap”: CREMA-D’s sadness is characterized by high-intensity, acted prosody, whereas IEMOCAP features naturalistic, low-intensity sadness. The model’s exposure to the theatrical variant appears to have shifted the decision boundary, leading to increased confusion with neutral speech in the test set. Despite this, the overall gain in F1-score and the dramatic improvement in cross-fold stability justify the use of CREMA-D for broader acoustic coverage and robust generalization. 

B. Comparative Benchmarking and Methodological Rigor 

To evaluate the proposed DistilHuBERT approach, we contextualize our findings against both high-capacity and efficiency-oriented benchmarks. First, we consider the work of Pepino et al. [2], who utilized a full Wav2Vec 2.0 Base model (approx. 318 MB) to achieve an Unweighted Accuracy (UA) of 67.2%. Our distilled implementation achieved a UA of 61.4%, representing approximately 91% of the performance of the full-scale baseline while utilizing a 23 MB quantized footprint. This 92% reduction in parameter size is critical for mobile deployment, where memory and thermal constraints are paramount. Furthermore, we address the performance of the LIGHT-SERNET architecture by Aftab et al. [3], which reported a UA of 70.76%. While their absolute accuracy is higher, there is a significant methodological distinction: their study utilized a 10-fold cross-validation protocol. In the context of SER, standard k-fold CV often involves a random shuffle of the dataset, which is susceptible to ”speaker leakage.” In such cases, the model may inadvertently learn speaker-specific biological identities rather than generalized emotional cues. By contrast, our adherence to a strict 5-fold LOSO protocol ensures the model generalizes to entirely unseen speakers. This rigor accounts for the lower absolute UA but confirms the model’s viability for deployment with unknown users. 

C. Architectural Capacity and the Pareto Frontier 

The results suggest that our implementation may be ap-proaching the theoretical capacity limit for a two-layer distilled transformer on the SER task. Emotion recognition requires the simultaneous extraction of low-level prosody and high-level temporal dependencies. The reduced depth of DistilHuBERT, while efficient, likely limits the model’s ability to resolve the most subtle emotional nuances found in naturalistic interactions. However, the efficacy of the Adaptive Focal Loss (weights in Table II) ensured the model did not collapse into majority-class prediction strategies. Our system thus represents a ”Pareto-optimal” point—maximizing accuracy per megabyte of model weight. This confirms that a carefully regularized, distilled model can provide reliable affect recognition within the strict envelopes of edge hardware, bridging the gap between laboratory performance and real-world mobile feasibility. V. P RODUCTION MODEL DEVELOPMENT AND 

CROSS -C ORPUS ROBUSTNESS 

A. Production Model Formulation and Performance 

To transition from an experimental framework to a deploy-able system, a final production model was developed using a training protocol designed to maximize feature extraction from the full breadth of the IEMOCAP corpus. This model utilized a random-shuffle strategy with a 5% hold-out for validation and testing, allowing the architecture to learn from the widest possible range of speaker identities and linguistic variations within the dataset. The resulting model achieved a Weighted Accuracy (WA) of 75.48% and an Unweighted Accuracy (UA) of 76.40%. To facilitate low-latency inference on mobile hardware, the final model was exported to the ONNX format for efficient cross-platform deployment. 

B. Generalization and the Theatricality Effect in Error Analysis 

The robustness of the production model was scrutinized through a cross-corpus evaluation on the RAVDESS dataset. While the domain shift between the semi-spontaneous IEMO-CAP data and the theatrical RAVDESS expressions resulted in an overall accuracy of 43.29%, the model demonstrated a significant capability to identify underlying physiological arousal states. As illustrated in Fig. 1, the model’s errors are systematically clustered by arousal levels rather than random distribution. We hypothesize that this phenomenon is a direct result of the theatrical nature of the RAVDESS corpus. In RAVDESS, actors prioritize clarity and communicative intent, leading to ”over-acted” expressions that emphasize vocal intensity and pitch range at the expense of subtlety. For a model trained on the more nuanced, semi-spontaneous features of IEMOCAP, this theatricality creates an acoustic ”bleeding” effect. Specifically, high-energy samples (Happiness and Anger) become acousti-cally saturated, causing the model to lose the subtle spectral distinctions required to differentiate valence; consequently, it defaults to the more acoustically dominant ”Anger” class. Similarly, the low-energy theatricality of ”Neutral” samples often mimics the prosodic markers of ”Sadness,” leading to a significant recall drop for the neutral class. Despite this valence confusion, the model remains a highly robust detector of arousal intensity, maintaining a 97% recall for Anger and a 64% recall for Sadness across disparate recording environments. 

C. Proposed Deployment Architecture 

Given the model’s high sensitivity to high-arousal and negative-valence states, we propose a specialized deployment pipeline for processing long-form voice notes. This architecture initiates with a Voice Activity Detection (VAD) layer to eliminate non-speech segments, thereby mitigating ”Neutral” bias during inference. The filtered audio is then segmented into 8-second windows to capture necessary macro-prosodic contours. Finally, the system employs an aggregated averaging mechanism across all windows within a voice note. This pooling strategy serves as a temporal filter, smoothing out momentary classification jitter and providing a stable emotional sentiment for the entire communication. By focusing on longitudinal emotional trends, this pipeline leverages the model’s strengths in identifying anger and sadness while compensating for the cross-corpus precision drop in positive affect caused by domain-specific theatricality. VI. C ONCLUSION 

This work demonstrates that carefully distilled transformer ar-chitectures can provide a viable path toward mobile-deployable Anger Happy Neutral Sad              

> Predicted Emotion
> Anger
> Happy
> Neutral
> Sad
> Actual Emotion
> 0.97 0.01 0.01 0.01
> 0.85 0.03 0.04 0.08
> 0.15 0.05 0.20 0.60
> 0.05 0.01 0.30 0.64
> Normalized Cross-Corpus Confusion Matrix (RAVDESS)
> 0.2
> 0.4
> 0.6
> 0.8
> Proportion of Predictions
> Fig. 1. Cross-corpus confusion matrix on RAVDESS. The results highlight the model’s tendency to cluster high-arousal (Anger/Happy) and low-arousal (Neutral/Sad) states, suggesting that theatrical over-emphasis in the target corpus obscures the subtle valence cues learned during training. Overall Accuracy: 43.29%; Unweighted Recall: 46.18%.

Speech Emotion Recognition systems. Our implementation of DistilHuBERT achieves 61.4% Unweighted Accuracy under a strict 5-fold LOSO protocol, representing 91% of full-scale Wav2Vec 2.0 performance while operating within a 23 MB quantized footprint. This 92% reduction in model size is critical for practical deployment on resource-constrained edge devices, where memory and thermal limitations prohibit the use of larger transformer models. The integration of CREMA-D as a cross-corpus training component yielded measurable improvements in generaliza-tion, increasing Weighted Accuracy by 1.2% and reducing cross-fold variance by 32%. This reduction in performance variability suggests that multi-corpus training serves as an effective acoustic regularizer, reducing the model’s sensitivity to speaker-specific idiosyncrasies. However, our class-wise analysis revealed a significant tradeoff: while neutral emotion recognition improved by 5.4%, sadness F1-score decreased by 2.9%. This degradation is attributed to a ”theatricality gap” between the acted, high-intensity sadness in CREMA-D and the naturalistic, low-energy sadness in IEMOCAP. This finding underscores the importance of careful corpus selection when augmenting training data, particularly for low-arousal emotional states. Cross-corpus evaluation on RAVDESS further revealed the model’s strengths and limitations. While overall accuracy dropped to 43.29% due to domain shift, the error analysis demonstrated that misclassifications were not random but systematically clustered by arousal level. The model maintained 97% recall for anger and 64% recall for sadness, indicating robust detection of physiological arousal states despite the acoustic mismatch between semi-spontaneous and theatrical speech. This behavior suggests that the model has learned gener-alizable arousal-related features, even if valence discrimination remains sensitive to recording environment and expressive style. From a methodological perspective, our adherence to LOSO cross-validation ensures that reported performance metrics reflect true generalization to unseen speakers, avoiding the speaker leakage artifacts that inflate accuracy in less rigorous evaluation protocols. While our absolute accuracy is lower than some published results using standard k-fold cross-validation, this rigor provides a more honest estimate of real-world deployment performance. The results suggest that our distilled architecture may be approaching the theoretical capacity limit for shallow transformers on the SER task, representing a Pareto-optimal tradeoff between model size and accuracy. Future work should explore several directions to further improve mobile SER systems. First, the theatricality gap identified in our cross-corpus analysis suggests that domain adaptation techniques, such as adversarial training or style transfer, could help align representations across corpora with differing expressive intensities. Second, the proposed VAD-based deployment pipeline for long-form voice notes requires empirical validation to quantify its effectiveness in reducing neutral-class bias. Third, extending the model to support fine-grained dimensional emotion prediction (arousal and valence) rather than categorical labels could provide more nuanced affect recognition while leveraging the model’s demonstrated strength in arousal detection. Finally, investigation of model pruning and neural architecture search may reveal even more compact architectures without further sacrificing accuracy. In conclusion, this work establishes that distilled transformer models, when combined with rigorous evaluation protocols and strategic cross-corpus training, can bridge the gap between laboratory performance and practical mobile deployment. The resulting system provides a foundation for privacy-preserving, low-latency emotion recognition in real-world applications, from mental health monitoring to adaptive conversational agents. ACKNOWLEDGMENT 

All experimental code and trained models are publicly available to facilitate reproducibility and future research. APPENDIX 

To ensure full transparency and reproducibility of our results, all experimental code has been made publicly available on Kaggle. Table III provides annotated links to each stage of the experimental pipeline, including cross-validation folds, production model training, and cross-corpus evaluation. REFERENCES [1] C. Busso et al., “IEMOCAP: Interactive emotional dyadic motion capture database,” Language Resources and Evaluation , vol. 42, no. 4, pp. 335– 359, 2008. [2] L. Pepino, P. Riera, and L. Ferrer, “Emotion Recognition from Speech Using wav2vec 2.0 Embeddings,” in Proc. Interspeech 2021 , Brno, Czech Republic, 2021, pp. 3400–3404. TABLE III PUBLICLY AVAILABLE EXPERIMENTAL NOTEBOOKS 

> IEMOCAP-Only (Folds 1–5)
> 5-fold LOSO CV on IEMOCAP. Baseline in Table I. kaggle.com/code/amrproject5/distilhubert-on-iemocap
> IEMOCAP + CREMA-D (Folds 1–3)
> First 3 folds with CREMA-D. See Table I. kaggle.com/code/amrproject5/distilhubert-on-crema-d-and-iemocap
> IEMOCAP + CREMA-D (Folds 4–5)
> Final 2 folds with CREMA-D. See Table I. kaggle.com/code/saifeldenmohamed8/distilhubert-on-iemocap-and-crema-d-2
> Production Model
> Full training, 95/5 split. 75.48% WA, 76.40% UA. 8-bit ONNX export. kaggle.com/code/saifeldenmohamed8/production-script
> Cross-Corpus (RAVDESS)
> Generalization test. Confusion matrix in Fig. 1. kaggle.com/code/saifeldenismail/cross-corpus-analysis

[3] A. Aftab, A. Morsali, S. Ghaemmaghami, and B. Champagne, “LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Singapore, 2022, pp. 6912–6916. [4] H. Chang, S. Yang, and H. Lee, “DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Singapore, 2022, pp. 7087–7091. [5] W. Hsu et al., “HuBERT: Self-Supervised Speech Representation Learn-ing by Masked Prediction of Hidden Units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 29, pp. 3451–3460, 2021. [6] H. Cao et al., “CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset,” IEEE Transactions on Affective Computing , vol. 5, no. 4, pp. 377–390, 2014. [7] S. R. Livingstone and F. A. Russo, “The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English,” PLoS ONE , vol. 13, no. 5, p. e0196391, 2018. [8] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,” in 

Advances in Neural Information Processing Systems , vol. 33, 2020, pp. 12449–12460. [9] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal Loss for Dense Object Detection,” in Proc. IEEE International Conference on Computer Vision (ICCV) , Venice, Italy, 2017, pp. 2980–2988. [10] G. Hinton, O. Vinyals, and J. Dean, “Distilling the Knowledge in a Neural Network,” arXiv preprint arXiv:1503.02531 , 2015. [11] B. Jacob et al., “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Salt Lake City, UT, USA, 2018, pp. 2704–2713. [12] I. Loshchilov and F. Hutter, “Decoupled Weight Decay Regularization,” in Proc. International Conference on Learning Representations (ICLR) ,2019.