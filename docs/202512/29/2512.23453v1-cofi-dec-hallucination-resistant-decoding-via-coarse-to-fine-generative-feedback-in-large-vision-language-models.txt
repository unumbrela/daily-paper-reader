Title: CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models

URL Source: https://arxiv.org/pdf/2512.23453v1

Published Time: Tue, 30 Dec 2025 03:08:51 GMT

Number of Pages: 15

Markdown Content:
# CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models 

## Zongsheng Cao âˆ—

agiczsr@gmail.com Researcher 

## Yangfan He âˆ—

he00577@umn.edu UMN 

## Anran Liu âˆ—â€  

anniegogo1008@gmail.com Researcher 

## Jun Xie 

xiejun@lenovo.com PCIE 

## Feng Chen 

chenfeng@lenovo.com PCIE 

## Zepeng Wang â€ 

wangzpb@lenovo.com PCIE 

## Abstract 

Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose CoFi-Dec , a training-free decoding frame-work that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the hu-man visual process from global scene perception to detailed inspec-tion, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mecha-nism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec sub-stantially reduces both entity-level and semantic-level hallucina-tions, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seam-lessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec. 

## CCS Concepts 

â€¢ Computing methodologies â†’ Natural language processing .

## Keywords 

Mitigating Hallucinations, Large Vision-Language Models  

> âˆ—Equal contribution
> â€ Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
> MMâ€™25, October 27â€“31, 2025, Dublin, Ireland
> Â©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-2035-2/2025/10 https://doi.org/10.1145/3746027.3754791

ACM Reference Format: 

Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, and Zepeng Wang. 2025. CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models. In Proceedings of the 33th ACM International Conference on Multimedia (MM â€™25), October 27â€“31, 2025, Dublin, Ireland. ACM, New York, NY, USA, 15 pages. https: //doi.org/10.1145/3746027.3754791 

## 1 Introduction 

In recent years, large vision-language models (LVLMs) have at-tracted extensive attention, and achieved impressive results across a range of multimodal tasks, including image captioning and vi-sual question answering, by extending the representational power of Large Language Models (LLMs) to process visual data [ 2, 51 ]. Despite their success in jointly modeling visual and textual infor-mation, LVLMs remain prone to generating hallucinations: outputs that contradict or deviate from the actual visual input. [ 18 , 30 ,49 , 52 ]. Such behavior poses serious risks of misinformation, un-dermining the trustworthiness of these models and limiting their applicability in safety-critical or real-world scenarios [3, 34, 58]. A growing body of work attributes this issue to the models â€™ tendency to overfit to language priors, a byproduct of imbalanced training data that leads them to prioritize linguistic patterns over grounded visual evidence [ 3, 26 , 34 ]. To address this, several meth-ods have focused on hallucination suppression through additional supervision or enhanced training schemes [5, 30, 57]. While these approaches have shown effectiveness, their dependence on exten-sive retraining and additional annotated data significantly limits their scalability and usability in downstream applications. To overcome these limitations, a newer class of approaches shifts focus from training-time interventions to decoding-time strate-gies [ 13 , 20 , 25 ]. In particular, recent advances in contrastive decod-ing, achieved without any model retraining, have shown promising results in reducing hallucinations [ 29 ]. These methods work by contrasting token predictions conditioned on faithful visual input against those generated under intentionally biased or weakened conditions, such as missing or corrupted images [ 14 , 26 ], noisy prompts [ 46 ], or truncated intermediate layers [ 10 ]. This inference-time strategy provides an efficient and generalizable pathway to mit-igate hallucinations without incurring the overhead of additional training. Moreover, some prior work has explored hallucination mitigation through either architectural modifications [ 28 , 33 , 44 ], fine-tuning with annotated data, or post-hoc filtering. More re-cently, decoding-time strategies have attracted increasing attention due to their training-free and model-agnostic properties. Among  

> arXiv:2512.23453v1 [cs.CV] 29 Dec 2025 MMâ€™25, October 27â€“31, 2025, Dublin, Ireland Zongsheng Cao et al.

Figure 1: The illustration of the importance of combining coarse-grained and fine-grained visual information to avoid misleading global interpretations. By decomposing the scene, we uncover that the perceived red sunglasses are actually traffic lights, highlighting the need for multi-scale reasoning in visual understanding. 

these, feedback-based decoding methods attempt to revise or ver-ify model predictions by incorporating auxiliary signals, such as retrieved factual knowledge or images synthesized from textual hypotheses. While conceptually promising, these approaches are often lim-ited in three critical aspects. As shown in Fig.1. First, most feedback-based methods rely solely on the original image input and operate at a single resolution, overlooking the inherent multiscale structure of visual information. As a result, they may fail to detect inconsis-tencies that manifest at different semantic levels, such as global layout versus object details. Second, generative feedback is typi-cally used in a post-hoc fashion, serving merely as a reference for re-ranking or answer replacement. This weakens its influence on the actual token-level generation process. Third, existing strategies rarely incorporate feedback in a fine-grained, step-by-step manner that allows cumulative correction and refinement during decoding. In this work, we propose CoFi-Dec , a new decoding framework that addresses these limitations through coarse-to-fine guided gen-erative feedback . Our key insight stems from the observation that hallucinations often result from imbalanced attention across visual granularities. To address this, we design a human-inspired decoding process that mimics perceptual strategies in visual cognition, where coarse scanning precedes focused inspection. Motivated by previ-ous work [ 35 , 56 ], given an input image, CoFi-Dec first constructs two visual pathways by decomposing the image into coarse-grained and fine-grained representations. The coarse view captures global context by uniformly downsampling the image into low-resolution patches, while the fine view highlights local semantics through high-resolution crops around salient or ambiguous regions. These two complementary inputs are used to generate independent textual responses using an LVLM. Each response serves as a semantic hy-pothesis derived from a different perceptual lens. To convert these hypotheses into actionable feedback, we employ a generative vi-sion model (e.g., Stable Diffusion) to synthesize two pseudo-images corresponding to the coarse- and fine-grained responses. These generated images serve as the modelâ€™s self-imagined reflections of its own beliefs, capturing what the model â€œseesâ€ at different levels of abstraction. Rather than treating these synthetic images as aux-iliary evidence, we integrate them along with the original image into the decoding loop. This integration guides subsequent token generation. At each decoding step, CoFi-Dec computes three conditional token distributions, each based on a distinct visual condition: the original image, the synthesized coarse image, and the synthesized fine image. These distributions are then fused using a Wasserstein barycenter optimization , which computes a geometry-aware consen-sus across the semantic token space. Unlike naÃ¯ve averaging, this method respects the underlying structure of the vocabulary space and encourages smooth, semantically meaningful corrections. This multi-path decoding design allows CoFi-Dec to cross-reference visual semantics at multiple scales, verify generated content from self-imagined feedback, and continuously refine output in a self-correcting fashion. Importantly, the entire framework is training-free, modular, and compatible with a wide range of pre-trained LVLMs, making it practical for real-world deployment. To summarize, our main contributions are: 

â€¢ We propose CoFi-Dec , a novel training-free decoding frame-work that incorporates coarse-to-fine generative visual feed-back to mitigate hallucinations in LVLMs. 

â€¢ We design a multi-granular visual decomposition mecha-nism that mirrors human-like inspection patterns, enabling distinct semantic views for generative self-verification. 

â€¢ We introduce a Wasserstein-based fusion strategy that jointly considers original and feedback-conditioned predictions to produce geometrically consistent token-level decoding. 

â€¢ We conduct extensive experiments across six hallucination-sensitive benchmarks, demonstrating that CoFi-Dec consis-tently outperforms state-of-the-art decoding baselines in hallucination mitigation. 

## 2 Related Work 

Hallucination in LVLMs . Autoregressive large language models such as LLaMA 2 [ 45 ], PaLM 2 [ 9], and Vicuna [ 8] have catalysed a rapid shift from text-only modelling to large visionâ€“language models (LVLMs) [ 2, 12 , 33 , 51 ]. In a typical LVLM, raw image fea-tures are passed through a lightweight adapter or projection layer, allowing pixel-level signals to share a common embedding space with tokens; this simple alignment lets a single backbone reason fluidly across modalities and has driven state-of-the-art results on image captioning, visual question answering, and allied bench-marks [3, 34]. Performance gains, however, mask a persistent liability, which is called hallucination. LVLMs still invent objects, attributes, or relations that are absent from the scene [ 3, 30 , 34 ]. Current miti-gation strategies fall into three broad camps. Alignment methods (e.g. RLHF) try to steer generation toward fidelity [ 18 , 43 ]. Training-time regularisers embed auxiliary losses that penalise visual-textual mismatch [ 7, 21 ]. Data-centric approaches toughen the model with noise or adversarial samples [ 31 , 54 ]. A complementary, post-hoc line edits outputs via verifierâ€“editor cascades [ 52 , 59 ]. While all CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models MMâ€™25, October 27â€“31, 2025, Dublin, Ireland 

show promise, they typically demand heavy curation or repeated fine-tuning, which hampers scalability and real-world adoption. A growing body of work now targets training-free, zero-tuning add-ons that enhance pretrained LVLMs without touching their weights. Two main families have emerged. Contrastive-decoding schemes [ 14 , 26 ] suppress hallucinations by ranking or subtracting continuations produced under different sampling rules, whereas guided-decoding techniques [ 6, 13 , 48 ] inject auxiliary signals di-rectly into the token-generation loop. Most recently, Zhang et al .[56] extends this trend: they create synthetic images as iterative â€œvi-sual critiques,â€ allowing the model to refine its own drafts and curb hallucinations in a manner reminiscent of human double-checking. 

Text-to-Image Synthesis .Text-to-image synthesis seeks to trans-late a free-form sentence into a picture that is both semantically faithful and visually convincing [ 16 , 60 ]. Breakthroughs in deep generative modelling [ 17 , 55 ] have propelled the field forward, pro-ducing three flagship families: diffusion models [ 19 , 24 , 36 , 39 , 40 ], generative adversarial networks (GANs) [ 23 , 41 ], and autoregres-sive token predictors [ 4 , 53 ]. Diffusion methods now dominate thanks to their ability to render highly detailed, photorealistic im-agery while offering precise user control [ 11 , 50 ]. When exposed to web-scale corpora such as LAION [ 42 ], they learn tight textâ€“image correspondences that translate into strong zero-shot performance on downstream tasks, from image classification [ 27 ] to semantic segmentation [1, 47]. Recent work by Jiao et al . [22] shows that diffusion generators can improve fine-grained recognition. Using Stable Diffusion XL [ 37 ], they built the Img-Diff corpus, which contains paired images with subtle variations, and demonstrated that fine-tuning LVLMs on this synthetic set increases accuracy on multiple VQA benchmarks. In contrast to such data-augmentation-and-retraining pipelines, Zhang et al . [56] takes a training-free approach. During inference, a pretrained diffusion model acts as a visual feedback loop: it converts the LVLMâ€™s initial text into synthetic images and feeds them back to the model for self-correction. This iterative process improves factual consistency and visual grounding while keeping the orig-inal LVLM weights unchanged, thus maintaining its deployment footprint. However, figures often contain both coarse-grained and fine-grained features, which previous work has overlooked. Ad-dressing these features remains an open challenge for hallucination resistance. 

## 3 Methodology 

In this paper, we introduce CoFi-Dec , a novel training-free frame-work designed to enhance the reliability of LVLM responses through recursive refinement using feedback from a text-to-image genera-tive model, as depicted in Figure 2. 

Problem Setting. We assume access to a vision-language model (LVLM) parameterized by ğœƒ , which receives a visual input ğ‘£ and a textual query x, and aims to generate a coherent and relevant textual response sequence y in an autoregressive fashion. The image ğ‘£ is first encoded by a vision encoder and subsequently mapped into a sequence of visual tokens via a vision-language projection module, such as a Q-Former [ 28 ] or a linear projection layer [ 33 ], which aligns the visual features with the language modelâ€™s embedding space. These visual tokens, together with the tokenized textual query, are fed into the language encoder to condition the generative process. Formally, the generation at each time step ğ‘¡ is governed by the following distribution: 

ğ‘¦ ğ‘¡ âˆ¼ ğ‘ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£, x, y<ğ‘¡ ) âˆ exp ğ‘“ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£, x, y<ğ‘¡ ), (1) where ğ‘¦ ğ‘¡ denotes the token generated at step ğ‘¡ , and y<ğ‘¡ = [ğ‘¦ 0, . . . , ğ‘¦ ğ‘¡ âˆ’1]

represents the sequence of previously generated tokens. The func-tion ğ‘“ ğœƒ outputs the unnormalized logit scores over the vocabulary 

V, which are then transformed into probabilities for sampling. This autoregressive decoding continues until an end-of-sequence token is generated, yielding the final response y = [ğ‘¦ 0, . . . , ğ‘¦ ğ‘‡ ].

## 3.1 Generative Feedback with Multi-Granular Conditioning 

Despite recent advances, hallucination remains a persistent chal-lenge for Large Vision-Language Models (LVLMs), especially in scenarios requiring precise grounding of visual entities. Existing decoding strategies, leverage auxiliary synthetic visual signals to refine the output, yet they predominantly rely on the original single-scale image input. Such representations may fail to capture critical visual nuances that are essential for resolving ambiguity or verify-ing semantic consistency. Motivated by the multi-scale processing mechanism in human vision, where observers first perform a coarse scan of the visual field before engaging in fine-detail scrutiny, we propose a novel decoding framework that jointly incorporates coarse-grained, fine-grained, and original visual cues. By structuring the visual input into a hierarchical multi-resolution context and integrating it into a generative feedback loop, our model achieves higher robustness and fidelity in grounding visual content during generation. 

Hierarchical Visual Decomposition. Given an input image ğ¼ 0 âˆˆ

Rğ» Ã—ğ‘Š Ã—3 and an associated textual prompt ğ‘‡ , we first decompose the image into two complementary sets of patches that capture different granularity levels: 

â€¢ Coarse-grained views Iğ‘ = {ğ¼ 1 

> ğ‘

, . . . , ğ¼ ğ‘› ğ‘ }: Each ğ¼ ğ‘– ğ‘ is ob-tained by uniformly dividing ğ¼ 0 into ğ‘› non-overlapping low-resolution patches, preserving the global spatial structure while discarding fine details. This resembles a downsampled global scan that preserves contextual semantics. 

â€¢ Fine-grained views Iğ‘“ = {ğ¼ 1 

> ğ‘“

, . . . , ğ¼ ğ‘š ğ‘“ }: Each ğ¼ ğ‘— ğ‘“ is a high-resolution crop focusing on salient or uncertain regions, de-rived from either learned attention maps or region proposal algorithms. These patches highlight local discriminative fea-tures that are potentially omitted by coarse processing. In this way, the full multi-resolution visual input is denoted by the unified set: 

I = ğ¼ 0 âˆª I ğ‘ âˆª I ğ‘“ . (2) To this end, the initial response ğ‘… 0 is generated by an LVLM conditioned only on the original image ğ¼ 0 and the prompt ğ‘‡ :

ğ‘… 0 = LVLM (ğ¼ 0,ğ‘‡ ). (3) We then perform two conditional generations under different gran-ularity contexts from arse- and fine-grained perspectives: 

ğ‘… ğ‘ = LVLM (I ğ‘ ,ğ‘‡ ), (4) 

ğ‘… ğ‘“ = LVLM (I ğ‘“ ,ğ‘‡ ). (5) MMâ€™25, October 27â€“31, 2025, Dublin, Ireland Zongsheng Cao et al. 

Figure 2: Overview of our proposed CoFi-Dec. Our method follows a coarse-to-fine generative feedback framework to enhance the fidelity of image-grounded text generation. By generating both coarse- and fine-grained textual descriptions from the original image and synthesizing corresponding images, we obtain multi-perspective visual feedback. These signals are then fused using a Wasserstein barycenter to produce a final response that is semantically rich, visually grounded, and resistant to hallucination. 

To enable self-verification, a generative vision model ğº (e.g., Sta-ble Diffusion) is used to synthesize a pseudo-image based on the textual hypothesis. The synthesized image serves as a reflection of the modelâ€™s internal belief and facilitates visual grounding in subse-quent refinement stages. We obtain the corresponding synthesized figures ğ‘£ ğ‘ and ğ‘£ ğ‘“ as follows: 

ğ‘£ ğ‘ = ğº (ğ‘… ğ‘ ), ğ‘£ ğ‘“ = ğº (ğ‘… ğ‘“ ). (6) 

## 3.2 Self-Correcting Decoding with Generative Feedback 

In this part, we describe how to incorporate the complementary cues from coarse-grained, fine-grained, and original image views into the decoding process in a more fine-grained and dynamic man-ner. While previous sections focus on generating separate responses conditioned on different visual granularities, these responses are inherently static and limited in their ability to reconcile seman-tic discrepancies across views. To address this, we move beyond fixed-level conditioning and explore a decoding-time integration strategy that operates at the token level. Our goal is to enable more precise alignment between multimodal inputs and language out-puts by leveraging the unique strengths of each visual perspective: the original image captures layout and raw semantics, the coarse-grained view emphasizes holistic structure, and the fine-grained view provides object-level precision. Specifically, rather than relying on isolated end-to-end responses from each visual condition, we propose a self-correcting decoding 

strategy that dynamically fuses the predictive signals across gran-ularities for each generated token. This allows the model to adap-tively weigh global context, structural composition, and localized evidence in a principled way at every step of generation, thus en-abling more faithful and grounded outputs. Let ğ‘£ , ğ‘£ ğ‘ , and ğ‘£ ğ‘“ denote the visual embeddings extracted from the original image, the synthesized coarse-grained context, and the fine-grained visual reference, respectively. For a given prompt x

and previously generated sequence y<ğ‘¡ , the model produces three conditional probability distributions over the vocabulary for the next token ğ‘¦ ğ‘¡ :

ğ‘ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£, x, y<ğ‘¡ ) =Softmax [ğ‘“ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£, x, y<ğ‘¡ )] ,ğ‘ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£ ğ‘ , x, y<ğ‘¡ ) =Softmax [ğ‘“ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£ ğ‘ , x, y<ğ‘¡ )] ,ğ‘ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£ ğ‘“ , x, y<ğ‘¡ ) =Softmax ğ‘“ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£ ğ‘“ , x, y<ğ‘¡ ) .

(7) To obtain a unified prediction distribution that effectively inte-grates multi-granular visual information, we leverage the concept of Wasserstein barycenters to fuse the token-level output distri-butions conditioned on the original image ğ‘£ , the coarse-grained reference ğ‘£ ğ‘ , and the fine-grained reference ğ‘£ ğ‘“ . Unlike simple aver-aging or heuristic weighting schemes, the Wasserstein barycenter provides a principled way to compute a central distribution that CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models MMâ€™25, October 27â€“31, 2025, Dublin, Ireland 

minimizes the overall transportation cost with respect to the con-stituent distributions, thereby preserving the underlying semantic geometry of the token space. Formally, for a fixed decoding timestep ğ‘¡ , we denote the three pre-dictive distributions over the vocabulary as ğ‘ƒ (ğ‘£ ) 

> ğ‘¡

= ğ‘ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£, x, y<ğ‘¡ ),

ğ‘ƒ (ğ‘ ) 

> ğ‘¡

= ğ‘ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£ ğ‘ , x, y<ğ‘¡ ), and ğ‘ƒ (ğ‘“ ) 

> ğ‘¡

= ğ‘ ğœƒ (ğ‘¦ ğ‘¡ |ğ‘£ ğ‘“ , x, y<ğ‘¡ ), each repre-sented as a probability vector in the |V | -dimensional simplex, where V is the vocabulary. To compute the fused distribution ğ‘ƒ (fused ) 

> ğ‘¡

that serves as the final prediction for ğ‘¦ ğ‘¡ , we solve the following optimization problem: 

ğ‘ƒ (fused ) 

> ğ‘¡

= arg min  

> ğ‘ƒ âˆˆÎ”|V |



ğ‘Š (ğ‘ƒ, ğ‘ƒ (ğ‘£ ) 

> ğ‘¡

) + ğ‘Š (ğ‘ƒ, ğ‘ƒ (ğ‘ ) 

> ğ‘¡

) + ğ‘Š (ğ‘ƒ, ğ‘ƒ (ğ‘“ ) 

> ğ‘¡

)



, (8) where ğ‘Š (ğ‘ƒ, ğ‘„ ) denotes the Wasserstein distance between two dis-tributions ğ‘ƒ and ğ‘„ . This formulation ensures that the fused dis-tribution is not only probabilistically valid but also geometrically faithful to the structure of the input distributions. Once the barycenter ğ‘ƒ (fused ) 

> ğ‘¡

is computed for each timestep ğ‘¡ ,it is used in place of the individual conditional distributions for token selection during generation. Notably, this fusion mechanism allows the model to adaptively reconcile the high-level contextual cues from ğ‘£ ğ‘ with the fine-grained semantic alignment from ğ‘£ ğ‘“ ,grounded by the original image ğ‘£ , yielding a robust and geometri-cally consistent prediction. This Wasserstein-based fusion strategy thus constitutes a key component of our modelâ€™s capability to per-form multi-granular visual reasoning during language generation. 

Remark 3.1. Our proposed framework introduces a unified decod-ing paradigm that explicitly models multi-resolution visual reason-ing and aligns it with generative feedback. Compared to prior work that treats feedback as a post-hoc correction, we integrate it into a multi-path decoding framework where each visual granularity serves as an independent yet complementary source of evidence. By mirroring human-like inspection patterns, in which a global preview is followed by selective zoom-in, the model acquires a more holistic and reliable understanding of the visual scene. This reduces reliance on spurious correlations and encourages grounded generation, especially in scenarios with ambiguous, cluttered, or fine-detailed imagery. 

## 4 Experiments 

In this section, we evaluate our model through a suite of benchmark trials designed to gauge its ability to suppress hallucinations in LVLMs, and we juxtapose the resulting metrics with those reported by the strongest contemporary baselines. 

## 4.1 Experimental Settings 

LVLMs . To evaluate our model, we ran a comprehensive test suite on three headline open-source LVLMs: the upgraded LLaVA-1.5 [ 32 ], 

InstructBLIP [12 ], and Alibabaâ€™s Qwen-VL [2]. The first two share an identical textual backbone, Vicuna-7B [8 ], itself a dialogue-oriented adaptation of LLaMA [ 45 ]. By contrast, Qwen-VL is an-chored in the 7-billion-parameter Qwen family. For our own trials, we simply plug the proposed CoFi-Dec module into the publicly released Qwen-VL-Chat checkpoint, leaving all original weights intact. 

Benchmarks . Following previous work [ 20 , 56 ], our empirical study taps into six publicly available testbeds that jointly probe a modelâ€™s resistance to hallucination and its overall visualâ€“reasoning prowess: (1) POPE [ 30 ] focuses on object hallucination by pre-senting binary questions that test whether the model can cor-rectly identify the presence or absence of objects in an image. (2) 

CHAIR [38 ] examines hallucinations in free-form image captions, requiring models to describe images randomly sampled from the MSCOCO validation set, with attention to visual grounding. (3) 

MME- Hallucination [15 ] provides a fine-grained evaluation of both object-level and attribute-level hallucinations via four sub-tasks: existence , count , position , and color . (4) MMBench [ 35 ] serves as a broad-spectrum evaluation suite, covering 20 aspects of multi-modal reasoning to test LVLMsâ€™ general understanding capabilities. (5) MMVP [ 44 ] targets fine-grained recognition through CLIP-blind image pairs, challenging models with binary questions across 150 curated examples. (6) LLaVA-Bench comprises 24 images ranging from complex real-world scenes to artistic renderings (e.g., memes, paintings, and sketches), paired with 60 intricate questions designed to test both visual perception and contextual comprehension. 

Baselines . We begin with a straightforward baseline in which de-coding is performed in the conventional manner, that is, each token is sampled directly from the softmax-normalized output probabili-ties. Beyond this, we evaluate our approach against three state-of-the-art decoding strategies: VCD [ 26 ], M3ID [ 14 ], and RITUAL [ 48 ]. For benchmarks such as CHAIR [ 38 ] and MME-Hallucination [ 15 ], we expand the comparison set to include additional recent methods, namely Woodpecker [ 6], HALC [ 6 ], DoLa [ 10 ], OPERA [ 20 ], and DeGF [ 56 ]. Performance results for these baselines are derived from our own re-implementation based on their officially released source code. 

Implementation Details . In all experiments, we maintain consis-tency with the standard input formatting adopted by LLaVA-1.5 [ 32 ]and InstructBLIP [ 12 ]. To account for variability, we run each MME benchmark experiment three times with different random seeds and report both the mean accuracy and standard deviation. 

## 4.2 Results and Discussions 

Results on POPE . Table 1 presents a comparative analysis of our proposed method and several strong baselines on the POPE bench-mark, evaluated under three distinct negative sampling strategies across three different datasets. More details can refer to Appen-dix. Our approach consistently yields superior performance over all competitors across the 18 evaluation settings, achieving the highest accuracy, precision, and F1 score on both LVLM architec-tures. Specifically, we observe significant gains in different metrics when compared to the next-best method. These results indicate that integrating a generative reference allows the model to better capture fine-grained visual cues, effectively mitigating object hallu-cinations. Additionally, while many decoding approaches exhibit overconfidence and often default to affirmative answers, our self-refining decoding strategy demonstrates a more cautious response pattern. This is reflected in its consistently higher precision, indicat-ing a stronger ability to avoid false positives and resist generating misleading outputs. MMâ€™25, October 27â€“31, 2025, Dublin, Ireland Zongsheng Cao et al. 

> Table 1: Results on POPE [ 30 ] benchmark. Higher ( â†‘) accuracy, precision, recall, and F1 indicate better performance. The best results are bolded, and the second-best are underlined.

Setup Method LLaVA-1.5 InstructBLIP Qwen-VL 

Acc. â†‘ Prec. â†‘ F1 â†‘ Acc. â†‘ Prec. â†‘ F1 â†‘ Acc. â†‘ Prec. â†‘ F1 â†‘

> MS-COCO

Random Regular 83.13 81.94 83.44 83.07 83.02 83.08 87.43 93.56 86.48 VCD 87.00 86.13 87.15 86.23 88.14 85.88 88.80 93.89 88.11 M3ID 87.50 87.38 87.52 86.67 88.09 86.41 89.83 95.44 89.17 RITUAL 88.87 89.23 88.81 88.83 90.48 88.60 89.47 96.32 88.62 DeGF 89.23 90.17 89.11 89.30 90.68 89.10 89.73 93.19 89.31 Ours 90.33 89.05 89.38 90.12 91.23 91.38 90.11 96.87 88.92 Popular Regular 81.17 78.28 82.08 77.00 73.82 78.44 84.70 88.24 83.96 VCD 83.10 79.96 83.94 80.07 77.67 80.89 85.13 87.27 84.69 M3ID 84.30 81.58 84.95 80.97 77.93 81.85 86.27 89.19 85.73 RITUAL 85.83 84.17 86.17 81.97 78.90 82.87 84.57 84.09 84.67 DeGF 86.1 84.73 86.37 82.50 79.64 83.31 86.50 86.87 85.71 

Ours 87.67 85.25 88.43 83.52 80.12 83.69 86.34 91.24 86.38 

Adversarial Regular 77.43 73.31 79.26 74.60 71.26 76.45 79.83 80.13 79.73 VCD 77.17 72.18 79.47 77.20 74.29 78.49 81.33 80.60 81.55 M3ID 78.23 73.51 80.22 77.47 73.68 79.14 82.03 81.47 82.19 RITUAL 78.80 74.43 80.54 78.73 74.57 80.39 82.80 83.15 82.71 DeGF 79.47 75.14 81.09 78.8 78.43 80.11 83.47 84.49 82.98 

Ours 81.67 76.93 82.22 79.61 77.64 79.86 84.36 85.03 83.69 

> A-OKVQA

Random Regular 81.90 76.63 83.53 80.63 76.82 81.92 86.27 90.66 85.48 VCD 83.83 78.05 85.34 84.20 80.90 85.00 87.87 90.06 87.53 M3ID 84.67 79.25 85.97 85.43 81.77 86.23 88.13 92.06 87.55 RITUAL 85.17 79.79 86.40 87.13 83.92 87.71 87.73 92.49 87.01 DeGF 86.17 80.84 87.27 87.4 84.67 88.02 87.90 89.16 87.58 

Ours 88.67 83.63 89.38 88.94 85.32 89.21 88.33 91.46 88.31 

Popular Regular 75.07 68.58 78.77 75.17 70.15 77.91 84.60 87.99 83.88 VCD 76.63 69.59 80.19 78.63 73.53 80.72 86.23 87.30 86.03 M3ID 77.80 70.98 80.91 78.80 73.38 81.00 86.50 89.59 85.95 RITUAL 78.83 71.99 81.68 78.73 72.83 81.17 86.36 88.73 86.20 DeGF 79.07 72.11 81.09 80.47 75.61 82.35 86.47 90.74 86.52 

Ours 80.11 72.64 82.36 80.79 76.29 83.76 87.71 90.96 87.26 

Adversarial Regular 67.23 61.56 73.70 69.87 64.54 74.54 76.90 75.59 77.48 VCD 67.40 61.39 74.21 71.00 65.41 75.45 79.13 76.04 80.30 M3ID 68.60 62.22 75.11 70.10 64.28 75.16 79.50 77.54 80.21 RITUAL 68.57 62.26 74.99 70.27 64.15 75.55 80.20 79.08 80.58 DeGF 70.7 66.7 76.86 71.87 65.65 75.96 80.75 80.37 80.46 

Ours 71.3 68.1 78.26 73.44 66.97 77.16 81.26 80.97 81.04   

> An important observation is the methodâ€™s robustness in the more difficult popular and adversarial scenarios. Unlike the random set-ting, these configurations are characterized by frequent inclusion and co-occurrence of non-existent negative objects [ 30 ], which tend to trigger hallucinations in existing models. Despite this in-creased complexity, our method experiences significantly smaller drops in performance compared to all other baselines. This sug-gests that our generative feedback mechanism not only enhances the modelâ€™s visual grounding but also helps it discern misleading CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models MMâ€™25, October 27â€“31, 2025, Dublin, Ireland

Table 2: Results on CHAIR [ 38 ] benchmark. We limit the maximum number of new tokens to 64. Lower ( â†“) CHAIR ğ‘† , CHAIR ğ¼ 

and higher ( â†‘) recall and length indicate better performance. The best results in each setting are bolded, and the second-best are underlined. 

Method LLaVA-1.5 InstructBLIP 

CHAIR ğ‘† â†“ CHAIR ğ¼ â†“ Recall â†‘ Length â†‘ CHAIR ğ‘† â†“ CHAIR ğ¼ â†“ Recall â†‘ Length â†‘

Regular 26.1 9.3 58.6 53.3 31.3 11.0 59.1 53.5 VCD 24.3 7.8 63.4 54.4 29.9 10.2 61.9 54.1 M3ID 21.5 6.1 64.1 53.6 30.7 10.3 62.5 53.3 RITUAL 22.3 6.8 63.1 54.7 26.7 8.8 63.5 55.2 Woodpecker 24.8 7.6 60.9 49.6 31.1 10.7 62.4 51.4 HALC 21.6 7.2 64.9 53.5 24.6 8.1 64.2 54.8 DeFG 18.4 6.4 62.9 54.3 24.0 7.7 63.5 55.0 

Ours 18.1 6.1 65.2 55.7 23.2 7.3 68.9 55.8 

Table 3: Results on MME-Hallucination [ 15 ] and MMBench [ 35 ] benchmark. We present the average MME scores along with their standard deviations over three random seeds for each subset. Additionally, the final column reports the overall accuracy of each method on the MMBench benchmark. Higher values ( â†‘) denote better performance. The best results are highlighted in bold, while the second-best are marked with underlining. 

Method Object-level Attribute-level MME Score â†‘ MMBench â†‘

Existence â†‘ Count â†‘ Position â†‘ Color â†‘

Regular 173.75 (Â±4.79 ) 121.67 (Â±12 .47 ) 117.92 (Â±3.69 ) 149.17 (Â±7.51 ) 562.50 (Â±3.96 ) 64.1 DoLa 176.67 (Â±2.89 ) 113.33 (Â±10 .41 ) 90.55 (Â±8.22 ) 141.67 (Â±7.64 ) 522.22 (Â±16 .78 ) 63.8 OPERA 183.33 (Â±6.45 ) 137.22 (Â±6.31 ) 122.78 (Â±2.55 ) 155.00 (Â±5.00 ) 598.33 (Â±10 .41 ) 64.4 VCD 186.67 (Â±5.77 ) 125.56 (Â±3.47 ) 128.89 (Â±6.73 ) 139.45 (Â±12 .51 ) 580.56 (Â±15 .13 ) 64.5 M3ID 186.67 (Â±5.77 ) 128.33 (Â±10 .41 ) 131.63 (Â±5.00 ) 151.67 (Â±20 .88 ) 597.50 (Â±20 .35 ) 64.3 RITUAL 187.53 (Â±2.89 ) 139.58 (Â±7.64 ) 125.00 (Â±10 .27 ) 163.33 (Â±6.87 ) 626.29 (Â±20 .38 ) 64.0 Woodpecker 187.53 (Â±2.89 ) 126.25 (Â±2.17 ) 126.66 (Â±2.89 ) 149.17 (Â±17 .34 ) 589.58 (Â±10 .00 ) 64.2 HALC 183.33 (Â±0.00 ) 133.33 (Â±5.77 ) 109.58 (Â±3.69 ) 155.00 (Â±5.00 ) 581.24 (Â±9.07 ) 64.4 DeFG 188.33 (Â±2.89 ) 142.50 (Â±6.64 ) 131.33 (Â±3.85 ) 163.17 (Â±3.47 ) 625.83 (Â±9.18 ) 65.2 

Ours 190.26 (Â±2.31 ) 144.43 (Â±5.27 ) 133.71 (Â±3.14 ) 165.62 (Â±4.08 ) 627.39 (Â±8.53 ) 65.9 

object associations, thus improving its resistance to context-driven hallucinations. 

Results on CHAIR . We evaluate our approach on the open-ended image captioning task and compare it with several state-of-the-art decoding strategies. The CHAIR scores, recall values, and average response lengths are summarized in Table 2. These evaluations are conducted on two representative LVLMs and consistently show that our method outperforms all baselines. Notably, our approach surpasses the second-best method by margins of significant gain on the CHAIR ğ‘† metric, respectively. Furthermore, it generates more informative responses than standard decoding, as reflected by its higher recall and longer average output length. These findings highlight the strength of our generative feedback mechanism in enhancing the fidelity and richness of model outputs. By leveraging synthesized visual references during decoding, our method helps the model better ground its predictions in visual content, leading to a significant reduction in hallucinated objects during caption generation. 

Results on MME-Hallucination and MMBench . To evaluate our method beyond object-level hallucinations, we further conduct experiments on the MME-Hallucination benchmark, which covers both object existence and attribute-based hallucination scenarios. As reported in Table 3, our approach consistently surpasses com-peting baselines, achieving notable improvements in the total score metric, outperforming the second-best method by +18.19 on LLaVA-1.5 and +21.11 on InstructBLIP. Across the eight subcategories, our method secures the highest performance in six, underscoring its robustness across different hallucination types. Notably, our approach yields significant gains on the color subset, which is par-ticularly difficult due to its reliance on accurate interpretation of subtle visual attributes. These findings confirm the strength of our MMâ€™25, October 27â€“31, 2025, Dublin, Ireland Zongsheng Cao et al. 

Figure 3: Case study on the LLaVA-Bench benchmark. Responses from standard decoding and our method (LLaVA-1.5) are shown with GPT-4V-assisted evaluations. Hallucinated and correct contents are highlighted in red and green, respectively. Table 4: Ablation study. We present the performance of our approach on multiple components.                                                     

> Models POPE Acc. CHAIR ğ‘† CHAIR ğ¼ MME Score Coarse Response(sdv1.5) 89.04 20.3 7.2 626.6 Fine-grained Response(sdv1.5) 89.67 19.8 6.9 634.15 Fine-grained - Coarse Response(sdv1.5) 89.78 18.7 6.3 644.37
> Remove Wasserstein Fusion(sdv1.5) 88.01 22.4 6.2 624.29 Coarse Response(sdxl-v1.0) 88.26 19.9 6.7 643.62 Fine-grained Response(sdxl-v1.0) 88.45 19.4 6.4 648.73 Fine-grained - Coarse Response(sdxl-v1.0) 88.60 18.3 5.8 649.38
> Remove Wasserstein Fusion(sdxl-v1.0) 87.13 21.8 6.6 638.02 Coarse Response(sd-v2.1) 88.36 20.2 7.3 634.4 Fine-grained Response(sd-v2.1) 88.54 19.25 7.1 644.89 Fine-grained - Coarse Response(sd-v2.1) 88.69 18.44 6.9 647.24
> Remove Wasserstein Fusion(sd-v2.1) 87.19 21.3 7.8 631.26

generative feedback mechanism in mitigating both coarse-grained and fine-grained hallucinations. More details for experiments on other datasets and more case studies can refer to the Appendix. 

## 4.3 Ablation Studies 

Study for Components. We conduct an ablation study to evaluate the contributions of coarse-to-fine reasoning and Wasserstein fu-sion across different diffusion backbones (sdv1.5, sdxl-v1.0, sd-v2.1). As shown in Table 4, results consistently show that combining fine- and coarse-grained responses outperforms using either alone, indicating that multi-level semantic aggregation improves factual grounding and descriptive accuracy. For example, the Fine-grained -Coarse Response setting yields the best CHAIR and MME scores across all backbones, demonstrating reduced hallucinations and en-hanced alignment with image content. Removing the Wasserstein fusion module leads to noticeable performance drops, confirming its key role in consolidating diverse semantic cues into a unified and accurate response. The improvements are particularly evident in the CHAIR metrics, highlighting the frameworkâ€™s robustness in mit-igating hallucinated object mentions. Overall, these results validate the effectiveness and generalizability of our proposed fine-to-coarse feedback and fusion strategy. 

Effects of Different Generative Models . Table A.3 compares sev-eral CoFi-Dec variants that utilize different versions of Stable Diffu-sion as the generative module, all evaluated using the same LLaVA-1.5 backbone. The results demonstrate that the performance of our method remains stable across different diffusion model choices. Re-gardless of the specific generative variant, all configurations deliver consistent improvements over the baseline regular decoding. While SD-XL-v1.0 [ 37 ] provides marginally better performance, we adopt SD-v1.5 as the default due to its substantially faster image gener-ation time, making it more practical for large-scale or real-time applications. 

## 5 Conclusion 

In this work, we present a multi-granularity generative feedback framework to mitigate hallucination in large vision-language mod-els. Inspired by the hierarchical nature of human visual perception, our method integrates the original image with both coarse-grained global context and fine-grained local visual evidence, enabling a structured multi-resolution reasoning process. By leveraging a gen-erative self-verification loop, our approach allows the model to refine its output based on internal visual imagination and cross-granularity consistency. Experimental results demonstrate that our method significantly improves response faithfulness and visual grounding across multiple benchmarks, particularly in scenarios in-volving ambiguous or detail-intensive visual inputs. Our framework is model-agnostic, modular, and compatible with existing LVLMs, offering a generalizable pathway toward more reliable multimodal generation. Future work includes exploring adaptive granularity selection via tree-structured exploration and incorporating learned feedback quality estimation to further enhance self-correction ca-pabilities. 

## 6 Acknowledgments 

This work was supported by the Science and Technology Innovation 2030-Key Project under Grant 2021ZD0201404. CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models MMâ€™25, October 27â€“31, 2025, Dublin, Ireland 

## References 

[1] Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior Wolf. 2021. Segdiff: Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390 

(2021). [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023). [3] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. 2024. Hallucination of multimodal large language models: A survey. arXiv preprint arXiv:2404.18930 (2024). [4] Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Patrick Murphy, William T Freeman, Michael Rubinstein, et al . 2023. Muse: Text-To-Image Generation via Masked Generative Transformers. In International Conference on Machine Learning . PMLR, 4055â€“ 4075. [5] Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, and Heng Tao Shen. 2024. Al-leviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization. Advances in Neural Information Processing Systems (2024). [6] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou. 2024. HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding. In International Conference on Machine Learning . PMLR, 7824â€“7846. [7] Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, and Ming Tang. 2023. Mitigating hallucination in visual language models with visual supervision. arXiv preprint arXiv:2311.16479 (2023). [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/ [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-bastian Gehrmann, et al . 2023. Palm: Scaling language modeling with pathways. 

Journal of Machine Learning Research 24, 1, Article 240 (2023), 113 pages. [10] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. 2024. DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. In International Conference on Learning Representations .https://openreview.net/forum?id=Th6NyL07na [11] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. 2023. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 9 (2023), 10850â€“10869. [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP: towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems 36 (2023), 49250â€“49267. [13] Ailin Deng, Zhirui Chen, and Bryan Hooi. 2024. Seeing is believing: Mitigating hallucination in large vision-language models via clip-guided decoding. arXiv preprint arXiv:2402.15300 (2024). [14] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pra-muditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. 2024. Multi-modal hallucination control by visual information grounding. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .14303â€“14312. [15] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al . 2023. MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv:2306.13394 (2023). [16] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. 2023. Expressive text-to-image generation with rich text. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 7545â€“7556. [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. Advances in Neural Information Processing Systems 27 (2014). [18] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting and preventing halluci-nations in large vision language models. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 18135â€“18143. [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems 33 (2020), 6840â€“6851. [20] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2024. Opera: Alleviating hal-lucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13418â€“13427. [21] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qing-hao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. 2024. Hallucination augmented contrastive learning for multimodal large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 27036â€“27046. [22] Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, and Ying Shen. 2024. Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models. arXiv preprint arXiv:2408.04594 (2024). [23] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. 2023. Scaling up gans for text-to-image synthesis. In 

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .10124â€“10134. [24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 2022. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems 35 (2022), 26565â€“26577. [25] Junho Kim, Hyunjun Kim, Yeonju Kim, and Yong Man Ro. 2024. CODE: Contrast-ing Self-generated Description to Combat Hallucination in Large Multi-modal Models. Advances in Neural Information Processing Systems (2024). [26] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2024. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13872â€“13882. [27] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. 2023. Your diffusion model is secretly a zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 2206â€“2217. [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning . PMLR, 19730â€“19742. [29] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori B Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023. Contrastive Decoding: Open-ended Text Generation as Optimization. In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics . 12286â€“12312. [30] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating Object Hallucination in Large Vision-Language Models. In 

Proceedings of the Conference on Empirical Methods in Natural Language Processing .292â€“305. [31] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2024. Mitigating Hallucination in Large Multi-Modal Models via Ro-bust Instruction Tuning. In International Conference on Learning Representations .https://openreview.net/forum?id=J44HfH4JCg [32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 26296â€“26306. [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual in-struction tuning. Advances in Neural Information Processing Systems 36 (2023), 34892â€“34916. [34] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. 2024. A survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253 (2024). [35] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al . 2024. Mmbench: Is your multi-modal model an all-around player?. In European Conference on Computer Vision . Springer, 216â€“233. [36] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In International Conference on Machine Learning . PMLR, 16784â€“16804. [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach. 2024. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. In International Conference on Learning Representations . https://openreview.net/forum?id=di52zR8xgf [38] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object Hallucination in Image Captioning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . 4035â€“4045. [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In 

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .10684â€“10695. [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al . 2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems 35 (2022), 36479â€“36494. [41] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. 2023. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image syn-thesis. In International Conference on Machine Learning . PMLR, 30105â€“30118. [42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al . 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems 

35 (2022), 25278â€“25294. [43] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al . 2023. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525 (2023). MMâ€™25, October 27â€“31, 2025, Dublin, Ireland Zongsheng Cao et al. 

[44] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9568â€“9578. [45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al . 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [46] Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. 2024. Mitigat-ing hallucinations in large vision-language models with instruction contrastive decoding. In Findings of the Association for Computational Linguistics ACL 2024 .15840â€“15853. [47] Julia Wolleb, Robin SandkÃ¼hler, Florentin Bieder, Philippe Valmaggia, and Philippe C Cattin. 2022. Diffusion models for implicit image segmentation ensembles. In International Conference on Medical Imaging with Deep Learning .PMLR, 1336â€“1348. [48] Sangmin Woo, Jaehyuk Jang, Donguk Kim, Yubin Choi, and Changick Kim. 2024. RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs. arXiv preprint arXiv:2405.17821 (2024). [49] Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, Xiaoshuai Sun, and Rongrong Ji. 2024. Evaluating and Analyzing Relationship Hallucinations in LVLMs. In International Conference on Machine Learning . PMLR, 53553â€“53570. [50] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023. Diffusion models: A comprehensive survey of methods and applications. Comput. Surveys 56, 4 (2023), 1â€“39. [51] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. 2024. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13040â€“13051. [52] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023. Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint arXiv:2310.16045 

(2023). [53] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. 2022. Scaling Autoregressive Models for Content-Rich Text-to-Image Generation. Transactions on Machine Learning Research (2022). https: //openreview.net/forum?id=AFDcYJKhND [54] Zihao Yue, Liang Zhang, and Qin Jin. 2024. Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective. In Proceedings of the Annual Meeting of the Association for Computational Linguistics . 11766â€“11781. [55] Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu, Lingjie Liu, Adam Kortylewski, Christian Theobalt, and Eric Xing. 2023. Multimodal image synthesis and editing: A survey and taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 12 (2023), 15098â€“15119. [56] Ce Zhang, Zifu Wan, Zhehan Kan, Martin Q Ma, Simon Stepputtis, Deva Ra-manan, Russ Salakhutdinov, Louis-Philippe Morency, Katia Sycara, and Yaqi Xie. 2025. Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models. ICLR (2025). [57] Jinrui Zhang, Teng Wang, Haigang Zhang, Ping Lu, and Feng Zheng. 2024. Re-flective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models. In European Conference on Computer Vision . https://www.ecva.net/ papers/eccv_2024/papers_ECCV/papers/08550.pdf [58] Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Tongshuang Wu, and Jianshu Chen. 2024. Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models. In Findings of the Association for Compu-tational Linguistics ACL 2024 . 8702â€“8718. [59] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2024. Analyzing and Mitigating Object Hallucination in Large Vision-Language Models. In International Conference on Learning Representations . https://openreview.net/forum?id=oZDJKTlOUe [60] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. 2019. Dm-gan: Dynamic mem-ory generative adversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 5802â€“5810. CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models MMâ€™25, October 27â€“31, 2025, Dublin, Ireland 

Table A2: Efficiency comparison. For each method, we present the average inference latency per instance and peak GPU memory. Experiments are conducted on a single RTX A6000 Ada GPU. 

Method Avg. Latency â†“ GPU Memory â†“ CHAIR ğ‘† â†“

Regular 3.44 s (Ã—1.00) 15778 MB (Ã—1.00) 55.0 VCD 6.91 s (Ã—2.01) 16634 MB (Ã—1.05) 54.4 OPERA 24.70 s (Ã—7.18) 22706 MB (Ã—1.44) 52.6 Woodpecker 10.68 s (Ã—3.10) 22199 MB (Ã—1.41) 57.6 HALC 22.61 s (Ã—6.51) 23084 MB (Ã—1.46) 51.0 DeFG 13.89 s (Ã—4.04) 19119 MB (Ã—1.21) 48.8 

Ours 20.83 s (Ã—6.05) 18597 MB (Ã—1.28) 46.7 

Table A3: Effects of different generative models. We report the performance of different variants of our method, uti-lizing various stable diffusion models, on the LLaVA-1.5 backbone. 

Models POPE Acc. CHAIR ğ‘† CHAIR ğ¼ MME Score Regular 83.13 26.2 9.4 562.50 SD-v1.1 88.37 19.3 6.5 638.33 SD-v1.5 89.03 18.4 6.1 644.44 SD-v2.1 88.70 18.8 6.7 632.22 SD-XL-v0.9 88.87 18.6 6.1 642.50 SD-XL-v1.0 88.60 17.9 5.8 648.33 

Figure 1: Results on MMVP [ 44 ]. We apply our approach to LLaVA-1.5 [ 32 ] and compare its performance against other hallucination mitigation methods. Table A1: GPT-4V-aided evaluation on LLaVA-Bench 

> . Higher accuracy and detailedness ( â†‘) indicate better performance. The evaluation is performed on LLaVA-1.5 [32].

Method LLaVA-1.5 InstructBLIP 

Acc. â†‘ Det. â†‘ Acc. â†‘ Det. â†‘

Regular 2.88 3.29 3.42 3.96 DeFG 4.29 4.54 4.38 4.79 

Ours 4.33 4.57 4.33 4.86 

VCD 3.62 3.83 3.71 4.21 DeFG 4.04 4.38 4.17 4.58 

Ours 4.12 4.31 4.26 4.63 

M3ID 3.88 4.08 4.00 4.33 DeFG 4.04 4.29 4.08 4.50 

Ours 4.13 4.18 4.27 4.51 

## A More Experimental Results and Analysis 

Results on MMVP . To further evaluate the modelâ€™s ability in fine-grained visual discrimination, we run experiments on the MMVP benchmark. As illustrated in Figure 1, applying our self-refining decoding technique to LLaVA-1.5 elevates performance from 22.67% to 27.33%. Compared to other hallucination mitigation baselines, our method yields more substantial improvements, particularly in tasks that require distinguishing visually similar images with subtle differences. These results highlight our approachâ€™s ability to improve precision in visual recognition by reducing misinter-pretations and hallucinated features, thus delivering more accurate outputs in nuanced visual tasks. 

Results on LLaVA-Bench . Figure 3 showcases a qualitative com-parison using LLaVA-Bench, where we examine the responses gen-erated by standard decoding and our CoFi-Dec-enhanced decoding with LLaVA-1.5. Regular decoding often produces vague or hallu-cinated descriptions, such as references to â€œ the island below the mountain â€ or general observations like â€œ cloudy sky â€ and â€œcohesive landscape â€, which lack specificity. In contrast, our method produces more grounded and informative outputs, accu-rately identifying elements such as the volcano, the road, nearby vegetation, and residential areas. GPT-4V-assisted evaluation re-sults, summarized in Table A1, further validate these observations, showing that our method outperforms baselines like VCD and M3ID in both response accuracy and descriptive detail. 

Efficiency Comparison. In Table A2, we present a comparative analysis of the computational efficiency of our method versus other baseline approaches on the CHAIR benchmark, utilizing the LLaVA-1.5 model with a maximum sequence length of 128 tokens. Our method requires two forward passes and integrates a text-to-image generation step to suppress hallucinations, which leads to a latency increase of approximately 4.04 Ã— and a GPU memory overhead of 1.21 Ã— relative to standard decoding. The full inference process in our framework consists of three sequential phases: (1) initial re-sponse generation, (2) visual feedback generation via a diffusion model, and (3) refinement of the original response. On average, these stages take 3.4s, 3.8s, and 6.6s per sample, respectively. Al-though our method is less efficient than simpler techniques like standard or contrastive decoding, it is notably more efficient than computationally intensive approaches such as OPERA and HALC. Importantly, our framework consistently achieves the lowest hallu-cination rates across all evaluated methods. MMâ€™25, October 27â€“31, 2025, Dublin, Ireland Zongsheng Cao et al.  

> Table A4: Another results on POPE [ 30 ] benchmark. Higher ( â†‘) accuracy, precision, recall, and F1 indicate better performance. The best results are bolded, and the second-best are underlined.

Setup Method LLaVA-1.5 InstructBLIP Qwen-VL 

Acc. â†‘ Prec. â†‘ F1 â†‘ Acc. â†‘ Prec. â†‘ F1 â†‘ Acc. â†‘ Prec. â†‘ F1 â†‘

> GQA

Random Regular 82.23 76.32 84.03 79.67 76.05 80.99 84.90 89.51 83.96 VCD 83.23 76.73 85.05 82.83 80.16 83.56 85.21 92.05 84.21 M3ID 84.20 78.00 85.77 83.07 80.06 83.87 85.69 93.11 84.67 RITUAL 86.10 80.30 87.31 84.87 82.52 85.39 86.1 93.78 84.81 DeGF 87.09 80.46 87.96 85.40 85.64 85.12 85.95 94.22 85.08 

Ours 89.03 81.1 89.06 86.78 87.06 86.39 87.14 94.65 86.32 

Popular Regular 73.47 66.83 77.84 73.33 68.72 76.26 81.33 83.38 80.74 VCD 72.37 65.27 77.58 76.13 71.10 78.68 81.97 82.82 81.73 M3ID 73.87 66.70 78.49 75.17 69.94 78.04 82.13 84.58 81.48 RITUAL 74.80 67.50 79.15 74.50 69.17 77.61 81.13 85.48 81.03 DeGF 75.12 71.56 80.98 75.34 71.89 77.96 82.10 86.39 81.85 

Ours 78.56 73.28 83.34 76.18 73.17 78.65 83.54 87.24 83.09 

Adversarial Regular 68.60 62.43 74.84 68.60 63.94 73.10 79.03 80.43 78.54 VCD 68.83 62.26 75.43 71.00 65.75 75.14 80.87 81.07 80.80 M3ID 68.67 62.16 75.28 71.17 65.79 75.36 81.03 82.93 80.94 RITUAL 68.23 61.75 75.10 70.17 64.76 74.78 81.07 83.29 80.41 DeGF 74.07 67.42 78.22 72.45 68.52 75.32 81.13 84.18 80.57 

Ours 75.21 68.34 79.76 73.25 69.68 76.87 81.69 84.96 81.67 

> Figure A2: Case study on the LLaVA-Bench benchmark. We compare the responses generated by regular decoding and our method using LLaVA-1.5. GPT-4V-aided evaluation results are also provided alongside the responses. Hallucinated and accurate content is highlighted in red and green.

CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models MMâ€™25, October 27â€“31, 2025, Dublin, Ireland 

Figure A3: Case study on the LLaVA-Bench benchmark. 

Figure A4: Case study on the LLaVA-Bench benchmark. 

Figure A5: Case study on the LLaVA-Bench benchmark. MMâ€™25, October 27â€“31, 2025, Dublin, Ireland Zongsheng Cao et al. 

Figure A6: Case study on the LLaVA-Bench benchmark. 

Figure A7: Case study on the LLaVA-Bench benchmark. 

Figure A8: Case study on the LLaVA-Bench benchmark. CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models MMâ€™25, October 27â€“31, 2025, Dublin, Ireland 

Figure A9: Case study on the LLaVA-Bench benchmark.