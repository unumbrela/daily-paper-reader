Title: A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem

URL Source: https://arxiv.org/pdf/2601.15038v1

Published Time: Thu, 22 Jan 2026 01:55:59 GMT

Number of Pages: 6

Markdown Content:
# A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem 

Mertcan Daysalilar 1, Fuat Uyguroglu 2, Gabriel Nicolosi 3, Adam Meyers 1

> 1

Industrial and Systems Engineering, University of Miami, Coral Gables, FL, USA 

> 2

Faculty of Engineering, Cyprus International University 99258 Nicosia, North Cyprus, via Mersin 10, Turkey 

> 3

Engineering Management and Systems Engineering, Missouri University of Science and Technology, Rolla ,MO, USA 

mxd4222@miami.edu , fuyguroglu@ciu.edu.tr , gabrielnicolosi@mst.edu , axm8336@miami.edu 

## Abstract 

The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alterna-tive to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability—failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparame-ters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heteroge-neous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N = 10 customers, the model demonstrates robust generalization to unseen instances ranging from N = 5 to N = 100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability. 

## Keywords 

Electric vehicle routing problem with time windows, curriculum learning, combinatorial optimization, proximal policy optimization, constraint decomposition 

## 1. Introduction 

The electrification of logistics fleets is a critical step toward sustainable industrial operations and supply chains, yet it introduces significant operational challenges. Unlike internal combustion engine vehicles, electric vehicles (EVs) are constrained by significantly shorter driving ranges, nonlinear charging times, and the scarcity of recharging infras-tructure [1, 2]. When coupled with strict customer time windows [3], the electric vehicle routing problem with time windows (EVRPTW) becomes an NP-hard optimization problem. The primary objective in industrial applications is not only to minimize distance but to optimize the total operational cost, which heavily weights the minimization of fleet size due to the high capital expenditure of electric vehicles [4]. Traditional solvers, such as branch-and-price or adaptive large neighborhood search, provide high-quality solutions but suffer from computational bottlenecks that limit their utility in real-time, dynamic dispatching scenarios [2, 4, 5]. Recently, deep reinforcement learning (DRL) has emerged as a promising alternative, capable of generating solutions in seconds once the model is trained. Foundational works by Kool et al. [7] and Nazari et al. [6] demonstrated that attention-based neural networks can effectively solve the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP). Building on this, recent research has begun applying DRL to electric vehicle constraints. For 1

> arXiv:2601.15038v1 [cs.LG] 21 Jan 2026

instance, Lin et al. [1] adapted the attention mechanism to capture EV-specific states, such as battery levels and charg-ing station availability, while Wang et al. [8] explored attention-enhanced strategies to optimize energy consumption. Despite these advances, applying end-to-end RL to highly constrained variants remains an open challenge due to the sparsity of feasible solutions in the search space. A major limitation of standard “end-to-end” DRL models is their tendency to fail when trained on the EVRPTW from random initialization [1]. This instability stems from the highly constrained nature of the problem: random explo-ration rarely yields a feasible solution, resulting in a sparse reward signal where the agent receives almost no positive feedback to guide learning. Instead, the agent encounters frequent negative feedback due to constraint violations (e.g., a depleted battery or missed deadline), leading to unstable gradient updates and degenerate policies that avoid hard-to-serve customers to minimize violation penalties [1]. To overcome such convergence issues, curriculum learning [9] has been proposed to gradually increase training diffi-culty. In the context of combinatorial optimization, Lisicki et al. [10] and Zhang et al. [11] demonstrated that training on smaller graph sizes before scaling to larger instances improves generalization for the TSP and CVRP. However, these approaches primarily address scaling problem size ( N) rather than the complexity of operational constraints. In the context of EVRPTW, effective training requires disentangling the learning of routing topology from ensuring feasibility under complex constraints [1]; an agent must first learn to construct a set of feasible routes before it can learn to optimize delivery timing. To address this instability, we propose a robust, curriculum-based deep reinforcement learning (CB-DRL) framework. Our primary contribution is a three-phase constraint curriculum that stabilizes training by decomposing the problem complexity. Specifically, the agent sequentially learns routing topology (phase A), energy management (phase B), and scheduling (phase C). We investigate whether this structured progression allows a neural policy to achieve near-optimal performance and zero-shot generalization on benchmark instances, effectively solving the feasibility collapse that plagues standard approaches. 

## 2. Problem Formulation 

We define the EVRPTW on a complete graph G = ( V, E), where V = {0} ∪ C ∪ S consists of a depot node 0, customer set C with cardinality |C | = N, and charging station set S with |S| = M. Each edge (i, j) ∈ E has an associated distance 

di j and travel time ti j . Each customer i has a demand qi, service time si, and hard time window [ei, li].The objective is to find a set of routes to serve all customers that minimizes the total operational cost ( J), defined as the weighted sum of the total travel distance and the size of the vehicle fleet used: 

J = ∑

> k∈K

# ∑ 

> (i,j)∈τk

di j + λ · | K| (1) where K denotes the set of active vehicles used and τk denotes the specific trajectory (sequence of edges) traversed by vehicle k ∈ K. This objective function J serves as the reward signal for the constructed solution set. Here, λ reflects the capital cost of activating an additional EV, making fleet size reduction a primary operational objective. All customers must be visited exactly once in exactly one route, and each route starts and ends at the depot. The constraints include the following: (i) the accumulated demand of customers visited by any single vehicle must not exceed the maximum capacity Qload , (ii) a vehicle’s battery state-of-charge must remain non negative with full recharging allowed at stations, and (iii) service at customer i must begin within its time window [ei, li] [1]. 

## 3. Methodology 

The proposed framework combines a heterogeneous encoder-decoder architecture with a curriculum-based training controller as shown in Fig. 1. The architecture consists of a constraint decomposition module and a policy optimization loop. The controller adjusts training complexity based on the current epoch k by determining the active constraint set 

Ck. This schedule divides training into three phases. Phase A ( k < 10) enforces only topological and capacity limits, phase B (10 ≤ k < 20) injects energy constraints and battery limits, and phase C ( k ≥ 20) activates the full problem by enforcing time windows. These constraints define the transition dynamics and reward function within the environment. The neural agent πθ interacts with this environment by observing state st and generating action at . The environment returns reward rt based on operational costs and constraint violations. The resulting trajectories are used to update model parameters θ.2Policy Optimization Loop Constraint Decomposition                                

> PPO Algorithm
> Objective: ∇θJ(θ)
> Phase-Adaptive Params
> Neural Agent πθ
> (Hetero-Graph Encoder) Action at∼πθ(·| st)
> EVRPTW Environment
> Dynamic Constraints Active Set Ck
> Curriculum Controller
> Selects Active Ck
> Epoch k
> Phase A: Topology
> min ∑di j s.t. Capacity Q
> Phase B: + Energy
> s.t. Battery bt≥0
> Phase C: + Time
> s.t. ei≤ai≤lik < 10  10 ≤k<20
> k ≥ 20
> st,rt
> Action at
> Trajectory τUpdate θ

Figure 1: Overview of the proposed curriculum-based deep reinforcement learning (CB-DRL) framework. 

3.1 Heterogeneous Graph Attention Encoder 

Our neural policy network employs a heterogeneous graph attention encoder to effectively capture the distinct roles of different node types in the EVRPTW. Standard attention models treat all graph nodes identically, applying the same projection and attention mechanisms regardless of node type. However, in EVRPTW, nodes serve fundamen-tally different functions: charging stations provide battery replenishment (renewable resources), customers impose demand and time constraints (demand nodes), and the depot serves as the route origin and destination. To address this heterogeneity, and inspired by [8], we employ separate projection parameters W Qcust ,W Qstation , and W Qdepot . This al-lows the model to learn distinct relational dynamics—for example, that the distance between a customer and a station is critical for feasibility, whereas the distance between two stations is less relevant. The output embeddings are fed into a global-local attention edge encoder to fuse local neighborhood information with global routing context [12], effectively aggregating features across different spatial scales. 

3.2 The Phased Constraint Curriculum 

The core novelty of this work is the curriculum controller, which modulates the environment’s complexity to ensure stable convergence based on the principles of curriculum learning [9]. We resolve “gradient collapse” by structuring training into three phases. Phase A (topology learning) enforces only capacity constraints ( Qload ), while battery limits and time windows remain disabled. In this stage, the agent learns basic spatial routing and the “return to depot” logic, establishing a strong spatial foundation. Phase B (energy management) subsequently introduces battery-related ( Qbat )constraints and activates charging station nodes. During this phase, the agent learns to monitor its state of charge and detour to charging stations when necessary. Finally, phase C (full EVRPTW) enforces all constraints, including time windows [ei, li], requiring the agent to refine its routing and charging policies to strictly satisfy customer time windows. Each phase restricts the optimization problem to a reduced constraint set, yielding a simpler reward structure that mitigates conflicting optimization objectives (e.g., distance minimization versus time-window feasibility). By activating constraint-related penalties incrementally, the framework stabilizes policy-gradient updates and enables the agent to learn feasible routing behavior before optimizing more restrictive constraints. 

## Phase A 

DDepot 

1

2

3

Topology Only 

## Phase B 

D

1

2

3

S Station 

A + Energy Constraints 

## Phase C 

D

1

2

3

S

> [10-12]
> [14-16]
> [08-09]

B + Time Windows 

Figure 2: The three-phase constraint curriculum. 34. Experimental Design and Results 

4.1 Experimental Setup 

We utilize a custom instance generator that produces feasible EVRPTW instances. Customer and charging-station coordinates are sampled uniformly from the unit square [0, 1] × [0, 1], following standard benchmarking practice [1, 7], with travel distances and times proportional to Euclidean distance. The generator produces instances across nine spatiotemporal classes, denoted C, Cm, Ct, R, Rm, Rt, RC, RCm, and RCt. The spatial notation follows Solomon-style [5] distributions: C (clustered customers), R (randomly distributed customers), and RC (mixed random–clustered). Temporal tightness is indicated with suffixes: wide (no suffix), medium (m), and tight (t) time windows. Customer demands are sampled from a bounded integer range, and vehicle capacity is fixed to ensure feasibility. Charging stations are placed independently and allow full recharging. To assess zero-shot generalization (i.e., evaluating performance on unseen instances without retraining), the model is trained only on instances with N = 10 customers and M = 3 charging stations, and then evaluated without retraining on instance sizes N ∈ { 5, 10 , 20 , 30 , 40 , 50 , 100 }. During the evaluation phase, to ensure robust performance, we employ the policy-oriented Monte Carlo decoding scheme [13] with a fixed batch size of N starting nodes. We compare our curriculum-based deep reinforcement learning (CB-DRL) framework against three distinct baselines. First, we utilize the variable neighborhood search (VNS) heuristic proposed by Schneider et al. [2] for the EVRPTW. Second, we include a standard end-to-end proximal policy optimization (PPO) model trained directly on the full EVRPTW without curriculum learning. Finally, we employ an exact optimization approach based on a mixed-integer linear programming (MILP) formulation of the EVRPTW [3]. The MILP explicitly models routing, charging deci-sions, battery dynamics, and time-window constraints, and it is solved to optimality using the Gurobi Optimizer to provide ground-truth benchmarks and optimality gaps. We evaluate solution quality and computational efficiency using four complementary metrics, including total cost ( J), optimality gap ( ∆%), feasibility success rate, and runtime. The optimality gap ( ∆%) measures the relative deviation of the total operational cost from a baseline solution (exact, heuristic, or best found) and is computed as (JRL −

Jbase )/Jbase × 100, where J (Eq. 1) includes total travel distance and fleet activation cost. In all experiments, the fleet activation penalty is fixed at λ = 100. The feasibility success rate (%) serves as the primary metric for assessing the framework’s ability to consistently generate feasible solutions on unseen test instances under tight operational constraints. A strict time limit of 600 seconds was imposed on all solvers. Any method failing to converge within this limit—indicated by a hyphen (-) in the results—was considered incomplete. 

4.2 Solution Quality and Generalization 

Table 1 details the generalization performance. We report the total travel distance ( D)—corresponding to the first term of the objective function (Eq. 1)—alongside the fleet size ( K) and total operational cost ( J). We observe that while both neural models perform comparably on small instances ( N < 20), CB-DRL begins to outperform standard PPO on instances with N ≥ 20, maintaining tighter optimality gaps as complexity increases. Table 1: Detailed performance breakdown including distance ( D), fleet size ( K), total cost ( J), and optimality gap ( ∆)

Instance MILP (Exact) Heuristic (VNS) Standard PPO CB-DRL 

D K J D K J ∆% D K J ∆% D K J ∆%

C5S2 2.19 1.13 115.2 2.20 1.15 117.2 1.7 2.04 1.20 122.0 5.9 2.03 1.20 122.0 5.9 

C10S3 3.22 2.04 207.2 3.26 2.10 213.3 2.9 3.30 2.14 217.3 4.9 3.28 2.14 217.3 4.9 

C20S3 - - - 5.19 3.69 374.2 0.0 7.29 3.75 382.3 2.2 5.68 3.72 377.7 0.9 

C30S4 - - - 5.90 3.98 403.9 0.0 10.1 4.12 422.1 4.5 6.91 4.13 419.9 4.0 

C40S5 - - - - - - - 13.3 4.41 454.3 5.0 8.59 4.28 436.6 0.0 C50S6 - - - - - - - 16.4 4.82 498.4 6.0 10.4 4.61 471.4 0.0 C100S12 - - - - - - - 33.1 8.01 834.1 18.5 18.2 7.22 740.2 0.0           

> Note: Bold indicates the best solution (J) found across all methods. Baseline for ∆%is MILP (N ≤10 ), heuristic ( 20 ≤N≤30 ), and best found solution (N ≥40 ).

4Table 2: Feasibility Success Rate and Runtime on Unseen Test Instances                                                         

> N=5 N=10 N=20 N=30 N=40 N=50 N=100 Model Succ. Time Succ. Time Succ. Time Succ. Time Succ. Time Succ. Time Succ. Time
> MILP (Exact) 100 %0.21s 100 %64.4s ----------Heuristic (VNS) 97.6% 0.36s 99.6% 0.86s 98.2 % 3.75s 98.4 % 11.52s ------Standard PPO 94.8% 0.28s 96.2% 1.12s 92.1% 4.74s 78.3% 11.54s 66.2% 23.86s 61.1% 35.91s 64.4% 188.48s
> CB-DRL 94.5% 0.31s 95.5% 1.25s 92.3% 5.15s 87.4% 11.26s 74.7% 20.64s 68.9% 32.10s 64.7% 142.19s
> Succ. = Success rate (% feasible solutions). Time = Average inference time per instance.

As problem complexity increases ( N ≥ 40), CB-DRL achieves higher optimality than the baselines, with the exact solver and heuristic timing out at N ≥ 20 and N ≥ 40 instance sizes, respectively. As detailed in Table 1, CB-DRL gives the best-known solutions (bold) for N = 40, 50, and 100, significantly outperforming standard PPO, which suffers from increasingly large optimality gaps (e.g., an 18.5% gap at N = 100). This advantage is reinforced by the feasibility success rates in Table 2; on unseen large-scale instances ( N ≥ 40), CB-DRL consistently maintains a higher success rate than standard PPO (e.g., 74.7% vs. 66.2% at N = 40). Finally, CB-DRL shows improved computational efficiency; at N = 100, it is approximately 25% faster than standard PPO (142.19s vs. 188.48s) because it generates concise, feasible routes rather than the excessively long, detoured paths produced by the baseline (e.g., Fig. 3(c-d)). 

Figure 3: Performance metrics (A, B) and routing behavior (C, D) comparing standard PPO and CB-DRL. 5Figure 3 illustrates the behavioral mechanism driving these results. Quantitative metrics (top) confirm that the per-formance advantage of CB-DRL grows with problem scale. Qualitatively, the route maps (bottom)—visualizing a representative instance with N = 40 customers and M = 5 charging stations—reveal that the baseline model struggles with long-term planning, resulting in tangled, inefficient trajectories. In contrast, the curriculum-trained agent gener-ates clean, well-separated routes, minimizing unnecessary detours and allowing it to serve the same demand with a smaller fleet. 

## 5. Conclusion 

This study addresses the limitations of standard deep reinforcement learning in solving highly constrained optimization problems. We proposed a curriculum-based DRL (CB-DRL) framework that structurally decomposes the electric vehicle routing problem with time windows into a learnable hierarchy of topology, energy, and time. Our experimental results highlight three primary contributions regarding scalability and performance. First, the framework demonstrates superior scalability compared to traditional methods; while the exact solver and heuristic become intractable or exceed time limits for large problem sizes ( N ≥ 40), CB-DRL efficiently generates valid solutions up to N = 100. Second, this scalability translates to superior solution quality on large instances ( N ≥ 40), where the model achieves higher optimality and feasibility success rates than the standard PPO baseline. Third, the proposed method improves computational efficiency, yielding faster inference times on instances of N ≥ 30 by generating concise routes that minimize decoding steps. These findings suggest that for complex combinatorial tasks, introducing constraints sequentially allows neural solvers to learn robust foundational policies before tackling full operational complexity. Future research will focus on extending the CB-DRL framework to stochastic environments, incorporating uncertain travel times and dynamic customer requests to better reflect real-world operational volatility. Additionally, we aim to investigate the integration of this neural policy into meta-heuristic search operators, enabling scalable solutions for massive-scale logistics networks ( N ≥ 500) without sacrificing computational efficiency. 

## References 

[1] B. Lin, Z. Ghaddar, and M. Nathwani, “Deep Reinforcement Learning for the Electric Vehicle Routing Problem With Time Windows,” IEEE Trans. Intell. Transp. Syst. , vol. 22, no. 11, pp. 7051–7064, 2021. [2] M. Schneider, A. Stenger, and D. Goeke, “The Electric Vehicle-Routing Problem with Time Windows and Recharging Sta-tions,” Transp. Sci. , vol. 48, no. 4, pp. 500–520, 2014. [3] M. Daysalilar, C. Chen, and M. Erkoc, “Electric Vehicle Routing Problems for Heterogeneous Fleets with Partial Recharg-ing, Diverse Charger Types, and Soft Time Windows,” in IISE Annual Conference Proceedings , pp. 1–6, 2023. DOI: 10.21872/2023IISE_2943 [4] P. Toth and D. Vigo, Vehicle Routing: Problems, Methods, and Applications . SIAM, 2014. [5] M. M. Solomon, “Algorithms for the Vehicle Routing and Scheduling Problems with Time Window Constraints,” Oper. Res. ,vol. 35, no. 2, pp. 254–265, 1987. [6] M. Nazari, A. Oroojlooy, L. Snyder, and M. Takác, “Reinforcement Learning for Solving the Vehicle Routing Problem,” in 

Proc. NeurIPS , 2018. [7] W. Kool, H. van Hoof, and M. Welling, “Attention, Learn to Solve Routing Problems!” in Proc. ICLR , 2019. [8] C. Wang, R. Zhang, R. Hong, and H. Wang, “Attention-Enhanced Deep Reinforcement Learning for Electric Vehicle Routing Optimization,” IEEE Trans. Transp. Electrific. , Early Access, 2025. DOI: 10.1109/TTE.2025.3574546 [9] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum Learning,” in Proc. ICML , 2009, pp. 41–48. [10] M. Lisicki, A. Afkanpour, and G. Taylor, “Evaluating Curriculum Learning Strategies in Neural Combinatorial Optimization,” arXiv preprint arXiv:2006.06310, 2020. [11] Z. Zhang, Z. Zhang, X. Wang, and W. Zhu, “Learning to Solve Travelling Salesman Problem with Hardness-Adaptive Cur-riculum,” in Proc. AAAI , vol. 36, no. 8, pp. 9136–9144, 2022, doi:10.1609/aaai.v36i8.21817. [12] D. Yan, B. Ou, Q. Guan, Z. Zhu, and H. Cao, “Edge-Driven Multiple Trajectory Attention Model for Vehicle Routing Problems,” Applied Sciences , vol. 15, no. 5, p. 2679, 2025. [13] Y.-D. Kwon, et al., “POMO: Policy Optimization with Multiple Optima for Reinforcement Learning,” in Proc. NeurIPS ,2020. 

6