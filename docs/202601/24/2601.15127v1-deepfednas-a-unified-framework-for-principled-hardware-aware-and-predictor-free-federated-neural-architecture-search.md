# DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search
# DeepFedNAS：一个统一的、基于原理、硬件感知且无预测器的联邦神经架构搜索框架

**Authors**: Bostan Khan, Masoud Daneshtalab \
**Date**: 2026-01-21 \
**PDF**: https://arxiv.org/pdf/2601.15127v1 \
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-green">EAA</span> <span class="tag-label tag-green">EOH</span> \
**Score**: 6.0 \
**Evidence**: 自动化模型设计与架构启发式方法 \
**TLDR**: DeepFedNAS 通过多目标适应度函数和架构启发式方法自动化联邦学习模型设计。

---

## Abstract
Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a principled, multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS

## 摘要
联邦神经架构搜索 (FedNAS) 旨在为隐私保护的联邦学习 (FL) 自动化模型设计，但目前面临两个关键瓶颈：一是无引导的超网训练导致模型性能欠佳，二是训练后子网发现过程耗时数

---

## 速览摘要（自动生成）

**问题**：现有的联邦神经架构搜索（FedNAS）面临超网络训练缺乏
