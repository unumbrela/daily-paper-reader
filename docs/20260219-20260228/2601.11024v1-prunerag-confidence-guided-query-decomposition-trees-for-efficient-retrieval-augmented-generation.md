---
title: "PruneRAG: Confidence-Guided Query Decomposition Trees for Efficient Retrieval-Augmented Generation"
title_zh: PruneRAG：用于高效检索增强生成的置信度引导查询分解树
authors: "Shuguang Jiao, Xinyu Xiao, Yunfan Wei, Shuhan Qi, Chengkai Huang, Quan Z. Michael Sheng, Lina Yao"
date: 2026-01-16
pdf: "https://arxiv.org/pdf/2601.11024v1"
tags: ["query:sr-llm"]
score: 10.0
evidence: 用于高效检索增强生成的查询分解
tldr: 针对检索增强生成（RAG）在处理复杂推理任务时存在的证据遗忘和检索效率低下问题，本文提出PruneRAG框架。该框架通过构建置信度引导的查询分解树，引入自适应节点扩展、置信度剪枝及细粒度实体检索机制，在保证多跳推理准确性的同时显著降低了冗余检索开销，并定义了证据遗忘率指标以量化评估证据利用效率。
motivation: 现有RAG系统在多跳推理中常因不受控的查询扩展导致证据遗忘和严重的计算冗余。
method: 提出一种基于置信度的查询分解树框架，结合自适应树生长控制、分支剪枝策略以及实体级锚点检索来优化推理路径。
result: 在多个多跳问答基准测试中，PruneRAG在保持高准确率的同时，显著提升了检索效率并降低了证据遗忘率。
conclusion: 通过结构化的查询分解与置信度引导的剪枝，可以有效平衡RAG系统的推理深度、检索精度与计算成本。
---

## 摘要
检索增强生成（RAG）已成为在知识密集型和推理任务中增强大语言模型的强大框架。然而，随着推理链的加深或搜索树的扩展，RAG 系统经常面临两个持续性的失败：证据遗忘（即检索到的知识未被有效利用）和低效（由不受控的查询扩展和冗余检索引起）。这些问题揭示了当前 RAG 架构中检索与证据利用之间的关键鸿沟。我们提出了 PruneRAG，这是一个置信度引导的查询分解框架，它构建结构化的查询分解树以执行稳定且高效的推理。PruneRAG 引入了三个关键机制：调节树宽度和深度的自适应节点扩展；接受可靠答案并剪枝不确定分支的置信度引导决策；以及提取实体级锚点以提高检索精度的细粒度检索。这些组件共同在多跳推理过程中保留显著证据，同时显著降低检索开销。为了更好地分析证据误用，我们定义了“证据遗忘率”作为一种指标，用于量化检索到黄金证据但未被正确使用的情况。在各种多跳问答基准测试上的广泛实验表明，PruneRAG 在准确性和效率上均优于现有的最先进基准方法。

## Abstract
Retrieval-augmented generation (RAG) has become a powerful framework for enhancing large language models in knowledge-intensive and reasoning tasks. However, as reasoning chains deepen or search trees expand, RAG systems often face two persistent failures: evidence forgetting, where retrieved knowledge is not effectively used, and inefficiency, caused by uncontrolled query expansions and redundant retrieval. These issues reveal a critical gap between retrieval and evidence utilization in current RAG architectures. We propose PruneRAG, a confidence-guided query decomposition framework that builds a structured query decomposition tree to perform stable and efficient reasoning. PruneRAG introduces three key mechanisms: adaptive node expansion that regulates tree width and depth, confidence-guided decisions that accept reliable answers and prune uncertain branches, and fine-grained retrieval that extracts entity-level anchors to improve retrieval precision. Together, these components preserve salient evidence throughout multi-hop reasoning while significantly reducing retrieval overhead. To better analyze evidence misuse, we define the Evidence Forgetting Rate as a metric to quantify cases where golden evidence is retrieved but not correctly used. Extensive experiments across various multi-hop QA benchmarks show that PruneRAG achieves superior accuracy and efficiency over state-of-the-art baselines.