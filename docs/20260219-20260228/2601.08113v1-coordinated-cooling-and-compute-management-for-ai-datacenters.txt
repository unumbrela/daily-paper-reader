Title: Coordinated Cooling and Compute Management for AI Datacenters

URL Source: https://arxiv.org/pdf/2601.08113v1

Published Time: Wed, 14 Jan 2026 01:14:53 GMT

Number of Pages: 12

Markdown Content:
> IEEE TRANSACTIONS ON CLOUD COMPUTING 1

# Coordinated Cooling and Compute Management for AI Datacenters 

Nardos Belay Abera and Yize Chen, Member, IEEE 

Abstract â€”The AI datacenters are currently being deployed on a large scale to support the training and deployment of power-intensive large-language models (LLMs). Extensive amount of computation and cooling required in datacenters increase concerns about the energy use and carbon emissions of AI datacenters. Although current state-of-the-art has examined the energy efficiency of LLM inference, most prior research focused on optimizing compute-side scheduling without considering thermal objectives or constraints. Since GPU-intensive inference generates substantial heat that can degrade datacenter performance, ignoring thermal effects can increase total energy consumption and reduce the efficiency of LLM serving. To fill this gap, we profile the characteristics of GPU servers under varying cooling and AI jobs, and develop a joint cooling and computing modeling approach for AI datacenters. Built upon such workload and thermal dynamics models, a novel hierarchical control framework is proposed to co-optimize computing and thermal management by identifying the optimal GPU parallelism, frequency (DVFS), and cooling control knobs. Using real Azure inference traces and detailed GPU profiling, our model balances serving latency and thermal constraints in AI datacenters while significantly improving AI datacentersâ€™ energy efficiency. 

Index Terms â€”AI datacenters, DVFS, GPU profiling, hierarchi-cal control, LLM, latency. 

I. INTRODUCTION 

# ARTIFICIAL intelligence (AI) datacenters are designed to support computing-intensive tasks such as training and serving of the large language model (LLM) and the GenAI model. Due to AIâ€™s rapid and widespread adoption, hyper-scale inference clusters are now being deployed to serve trillions of requests per day [1], [2]. To meet the ever-increasing computational requirements of LLMs services, various solutions, such as enhanced datacenter architectures, scheduling algorithms, and accelerators, have been proposed to increase the efficiency of inference [3]â€“[6]. Although previous work primarily focused on performance aspects, one critical factor is largely overlooked: the energy perspective of large-scale AI datacenters [7]. This concern is amplified by the substantial economic and environmental impacts of the enormous amounts of energy consumed by modern AI models. In particular, training the 175-billion-parameter GPT-3 would require approximately 1287 MWh of electricity and emit 502 metric tons of CO 2 [8]. LLM inference is even gradually dominating the energy consumption landscape. In 

> N. B. Abera and Y. Chen are with the Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada (e-mail: nbelay@ualberta.ca; yize.chen@ualberta.ca). This work was supported in part by the Natural Sciences and Engineering Research Council of Canada, and in part by the Canada First Research Excellence Fund as part of the University of Albertaâ€™s Future Energy Systems Research Initiative.

fact, recent studies project that 80â€“90% of the total workload and energy consumption of LLMs deployed in production environments arise from inference rather than training [9], [10]. Although some previous work has aimed to reduce the energy consumption of LLM inference [11], these studies explicitly ignore the impact of thermal behavior and cooling systems [12]. Such energy-inefficient computing can be attributed to suboptimal thermal conditions in datacenters, where intense and dense heat generation leads to GPU performance degradation due to thermal throttling [13]. Load and temperature imbalances also cause sub-optimal scheduling and cooling in datacenters. Therefore, neglecting thermal dynamics could result in cooling units reacting only after temperature increases occur [14]. Not only does such operation result in degradation in computing performance, but the energy used for cooling is significant and also contributes approximately 45-50% of the total energy consumption in AI datacenters [15]. A major challenge in managing power and energy profile of AI datacenters is the highly heterogeneous and time-varying nature of LLM workload, which varies in context length, output length, and model size. This creates irregular compute demand and rapid heat generation patterns across GPU racks. This heterogeneity breaks the assumption of the traditional datacenter control system and overwhelms cooling systems, whose slow response leads to temperature overshoots, local hot spots, and inefficient use of cooling energy [16]. Existing data-center management approaches often decouple computing and cooling control as separate loops [17]. Although such methods improve inference throughput or cooling efficiency in isolation, they fail to capture the strong interdependence between computational load, heat generation, and cooling dynamics. This separation leads to suboptimal energy consumption and unstable temperature regulation, especially under dynamic LLM workloads, which ultimately affects the sustainability and performance of datacenters [18]. To address this research gap in AI datacenters energy management, this paper proposes the first hierarchical control framework, which jointly coordinates and controls computing and cooling resources to enable energy-efficient LLM inference. Such a hierarchical framework helps reduce computational com-plexity and disaggregates multiple control variables, allowing coordinated control across multiple layers. As a foundation for this design, we first model the power and energy characteristics of the LLM workloads and key cooling control parameters. LLM inference differs fundamentally from conventional CPU-centric datacenter workloads [19]â€“[22]. Taking into account heterogeneous computational behaviors of LLM inference, we develop the AI datacenter heat transfer  

> arXiv:2601.08113v1 [eess.SY] 13 Jan 2026 IEEE TRANSACTIONS ON CLOUD COMPUTING 2

model, and develop a novel hierarchical control framework operating across multiple levels. At the higher level, we predict the AI workload, and dynamically adjust the number of active servers to maximize utilization and reduce idle power. At the intermediate level, the controller dynamically changes GPU parallelism, involving the number of GPUs working together on one LLM inference instance, subject to the maximum available GPUs and temperature constraints. The cooling level dynamically adjusts parameters such as the supply air temperature and airflow rate, which vary with respect to workloads. On the lower level, dynamic voltage and frequency scaling (DVFS) is adopted to adjust the frequencies of GPUs according to the SLOs of workloads and thermal conditions. To achieve this, we introduce a novel temperature-aware DVFS mechanism that jointly considers workload intensity and GPU temperature when selecting the optimal operating frequency. This enables the controller to reduce energy consumption and regulate temperature without degrading inference performance. This approach also considers the sensitivity of DVFS to the types of work request. Requesting longer prompts is computationally intensive and more sensitive to frequencies, while requests involving short prompts but longer output are memory-bound, which is less sensitive [11]. The control framework uses detailed, realistic profiling LLM serving data obtained with varying workloads, GPU configurations, and temperatures to capture the correlation between energy, latency, and temperature, allowing the controller to design the optimal cooling and computing strategy. The overall scheme is illustrated in Fig. 1. 

Fig. 1: Schematic for proposed hierarchical control of joint coolingâ€“compute for AI datacenters. 

A. Main Contribution 

Building upon LLM serving characteristics and opportunities of co-management of GPU cooling and computing, this work makes the following key contributions:  

> â€¢

We propose a novel hierarchical computeâ€“thermal control framework for the AI datacenter that provides method-ology for jointly optimizing computing resources (GPU parallelism and frequency) and cooling resources (supply air temperature and inlet airflow rate). A novel temperature constraint is integrated to guaranty thermal safety while enabling significant energy savings.  

> â€¢

We develop a novel thermal-aware workload dispatch algorithm that schedules jobs based on the capacity of each GPU pool. This algorithm reduces the overhead associated with frequent reconfigurations and ensures smooth adaptation to heterogeneous, time-varying LLM inference workloads. We incorporate an LSTM-based model for forecasting token demand for proactive resource allocation and a DistilBERT-based classifier for classifying each LLM jobâ€™s length.  

> â€¢

We perform a comprehensive evaluation using Microsoft Azure LLM inference traces in combination with detailed thermal and power profiling of the GPU [23]. Simulation and experimental results demonstrate that the proposed approach reduces both IT-side and cooling-side power con-sumption while preserving SLOs. Overall, the proposed controller achieves 24.2% computing-energy savings and 31.2% cooling-energy savings, and lowers the average GPU temperature by 17.0%, while maintaining negligible impact on inference latency. 

B. Article Outline 

The remainder of this paper is organized as follows. Sec-tion II provides background on datacenter cooling systems and the major control knobs associated with AI datacenters, such as tensor parallelism (TP), DVFS, and LLM Workload Char-acteristics. Section III describes the cooling and computing characteristics of the AI datacenter. Section IV proposes our hierarchical control framework for both cooling and workload scheduling with a dispatch algorithm. Section V presents a comprehensive performance analysis using simulation and real-system experiments, which validate the effectiveness of the integrated cooling-computing control. Finally, Section VI concludes the paper and outlines directions for future work. II. BACKGROUND 

A. Datacenters Cooling Systems 

Cooling systems are major determinants of the thermal behavior of computing components such as GPUs and CPUs in the datacenter. Due to their importance in thermal management and energy efficiency, there are enormous opportunities to enhance performance datacenters using optimized cooling strategies. Therefore, it is important to determine their working characteristics to facilitate co-optimization of computing and cooling power consumption. Datacenters utilize air and liquid cooling to control the massive heat loads generated by high-performance GPUs [24]â€“[26]. Air cooling remains the most widely used approach due to its relatively low operating and maintenance costs [27]. Heat is removed by circulating cold air through the servers, which can be deployed at the level of the room, row, or rack. Room-level systems pump cold air through the raised floor plenum, but are subject to problems of cold air bypass and return of hot air, resulting in reduced efficiency in the cooling process [28], [29]. Row and rack cooling minimize airflow distances and increase the isolation between cold and hot streams to achieve higher thermal efficiency [30], [31]. According to ASHRAE guidelines, proper inlet and outlet air temperature must be maintained to ensure stable operation of IT equipment and to avoid hardware failure [32]. To improve cooling effectiveness, several key parameters can be actively IEEE TRANSACTIONS ON CLOUD COMPUTING 3

controlled, including supply air temperature, inlet airflow rate, cold inlet temperature, and return temperature. These parameters must be dynamically tuned to track the time-varying AI workloads while ensuring thermal safety. Effective control of these parameters is possible with an advanced control scheme. PID (Proportionalâ€“Integralâ€“Derivative) control is commonly used in temperature control in a datacenter [33]. Despite its simplicity, PID controller only relies on the previous error values for control, and it is incapable of considering forecasts for the error values. Hence, it can be slow and inaccurate in its responses [34]. In addition, PID controllers cannot incorporate optimization objectives. While model predictive control (MPC) leverages the knowledge obtained from the system model to predict its possible behaviors in the future [35]. With real-time temperature prediction and optimization of cooling systems, MPC has been extensively used in the field of traditional CPU-based datacenter energy savings and thermal management. In [36], an objective function of the MPC was performed that included operating costs, which reduced energy consumption. In [37], a data-driven subspace predictive control method was designed for air-cooled traditional datacenters. An economic MPC was proposed for room-based cooling systems in [38], [39].However, many of these methods were mainly adapted to CPU-centric datacenters, which have slower workload dynamics and fixed airflow actuation. Such assumptions do not hold for AI/LLM inference, which is highly time-varying and exhibits nonlinear performanceâ€“power behavior. 

B. Tensor Parallelism and Thermal Constraints 

As the computational requirements for LLMs inference remain high in terms of memory, large models are generally split into multiple GPUs using a concept referred to as model parallelism in LLMs for efficient execution. There exist two variants for model parallelism, namely pipeline parallelism (PP) and tensor parallelism (TP) [40]. In PP, the modelsâ€™ layers are assigned to different GPU in such a way that each GPU is only responsible for processing a separate stage within a specific layer, with communication between only two successive stages. In the other variant, called TP, operations in each layer are performed using multiple GPUs in a manner in which the GPUs need to execute processes in each layer synchronously [41]. Because most open-source LLMs can be accommodated on multiple GPUs within a single server [42], this paper focuses on the widely used TP configurations. Although choosing a smaller TP reduces power consumption, the computational workload for each separate GPU is also higher. Increased usage for each GPU leads to increased heat production by single device, possibly increasing the temperature with subsequent thermal throttling with reduced power [18]. Considering these operational challenges, TP decisions must incorporate temperature constraints to achieve efficient datacenter operation. 

C. DVFS and LLM Workload Characteristics 

DVFS is also widely utilized in standard CPU-based datacen-ters to save power under stable and predictable workloads [43]. DVFS holds the promise of efficiently reducing dynamic power consumption by lowering the operating frequency/voltage of processors during periods of low utilization without compromis-ing performance. In contrast, the workload of LLM serving has highly stochastic characteristics with variations in request sizes influenced by the input/output token lengths. There is also a significant variation in the computational requirements for each request, leading to fluctuations in GPU usage, power usage, and heat generation over a short period of time. As a result, traditional fixed GPU frequency approaches in AI datacenters may not be efficient [11]. To overcome this challenge, DVFS of LLM inference needs to adapt dynamically according to the real-time workload pattern of the arriving requests, in addition to considering the temperature factors. 

D. Performance Metrics 

Most prior studies evaluated the latency of LLM inference using the time to first token (TTFT) and the time between tokens (TBT) [44], [45]. In this work, to capture end-to-end request-level performance, we use throughput in queries per second (QPS). Additionally, in terms of measuring the energy consumption for each GPU (in watts-hour, Wh), together with the average GPU temperature during the inference phase, we intend to evaluate the power efficiency and thermal sustainability for different levels of workload, respectively. Cooling performance is evaluated using cooling energy in conjunction with temperature metrics, including the server return temperature and the inlet (cold-aisle) temperature. In this work, we aim at optimizing the overall AI datacenter performance by integrating both LLM serving and thermal management metrics. III. C OOLING AND COMPUTING CHARACTERISTICS OF AI DATACENTERS 

In this Section, we present the complete modeling and forecasting framework used in our coolingâ€“computing control system. Our goal is to clearly present the thermal dynamics, power modeling, and IT-load forecasting, and explain why each component is needed and how they connect to the hierarchical control. First, Sects. III-A and Sect. III-B develop the thermal model, which includes (i) a temperature-field model that describes rack-level heat transfer and airflow dynamics and (ii) a cooling power model that links supply temperature, airflow rate, and chiller/fan consumption. Together, these models describe how thermal states evolve in response to workload and cooling actuation. Next, Sect. III-C introduces the computing workload prediction and runtime workload characterization model. Although this section is not part of the thermal model itself, it is essential, since thermal dynamics depends directly on the computational load executed by the servers. Therefore, we use a forecasting and profiling framework based on LSTM token demand prediction and DistilBERT job classification to estimate the future workload. This forecast is fed into the thermal model so the controller can anticipate the upcoming heat generation, proactively adjust the cooling supply, and schedule workloads to ensure that temperature and latency constraints are satisfied. IEEE TRANSACTIONS ON CLOUD COMPUTING 4

A. General Framework for Rack-Based Air Cooling 

Rack-based cooling is widely adopted in modern datacenters to compensate for the limitations associated with room-/row-based cooling. In this design, rack cooling units (RCUs) are integrated within single server racks, usually at the bottom or at the back. The RCUs supply conditioned air to the cold aisle, drawn through server components such as GPUs and memory modules [46]. The heat-exhaled air is released into the hot aisle and fed back to the RCU to be cooled once more, with a closed-loop air flow cycle. To model the thermal dynamics, a zonal modeling method is utilized which divides the rack into distinct thermal zones, gen-erally categorized as the inlet and outlet zone. The assumption holds that every zone is subject to the same thermophysical properties, namely density and specific heat [47]. The model is given in multi-node state-space form, and it is governed through mass conservation principles that account for airflow and convective heat transport, with energy balance relations that illuminate thermal relations between zones. In server rack simulation, the model encapsulates the coupled relationships among airflow dynamics, internal heat generation, and the cooling reactions of the RCUs. 

B. Temperature Field Model and Power Model Development 

The thermal dynamics per-rack for the datacenters is built based on energy conservation laws [48]. Each rack (with server 

i = 1 , ..., N ) is divided into three temperature zones with Î¸c,i ,

Î¸s,i , and Î¸h,i denoting the temperatures for the cold, server exhaust, and hot zones, respectively. Let Î¦L, Î¦OH i , Î¦s,i , Î¦RCU 

and Î¦OC i denote the leakage flow rate between the hot and cold zones, the recirculated flow between the cold zone and the hot zone, the server fan flow, the RCU supply flow, and the cold-to-cold coupling flow, respectively. The RCU supply temperature is denoted by Î¸RCU , and bi indicates the distance from the RCU. Each server contains a m number of GPU, whose power consumption is modeled by PGPU (f (t), u (t)) as a function of the operating frequency of the GPU f (t) and the utilization of the workload u(t). Given air density Ï, heat capacity cp, cold zone volume Vc, hot zone volume Vh, and server thermal capacitance Cth , we can model the thermal dynamics for the cold zone, server exhaust and hot zone. The governing equations are expressed as follows. The cold-zone dynamics are 

Ë™Î¸c,i (t) = 1

Ïc pVc



biÎ¦RCU Ïc pÎ¸RCU 

| {z }

> RCU supply

+ Î¦ OC iâˆ’1Ïc pÎ¸c,i âˆ’1

| {z }

> upstream inflow

+ Î¦ LÏc pÎ¸h,i 

| {z }

> leakage

âˆ’ (Î¦ OC i + Î¦ s,i )Ïc pÎ¸c,i 

| {z }

> outflow



,

(1) The server exhaust temperature is expressed as 

Ë™Î¸s,i (t) = 1

Cth 



Î¦s,i Ïc p(Î¸c,i âˆ’ Î¸s,i )

| {z }

> convective exchange

+ mP GP U (f (t), u (t)

| {z }

> server power



,

(2) Finally, the hot-zone dynamics are 

Ë™Î¸h,i (t) = 1

Ïc pVh



Î¦s,i Ïc pÎ¸s,i 

| {z }

> exhaust inflow

âˆ’ Î¦LÏc pÎ¸h,i 

| {z }

> leakage to cold

+ Î¦ OH iâˆ’1 Ïc pÎ¸h,i âˆ’1

| {z }

> upstream inflow

âˆ’ Î¦OH i Ïc pÎ¸h,i 

| {z }

> recirculation out



,

(3) 

Thermal Load Extraction: In addition to the rack thermal dynamics, we model the cooling power consumption of the RCU. Once we collect measurements of the temperature of each hot zone Î¸h,i , the return temperature Î¸ret (t) is 

Î¸ret (t) = 1

N

> N

X

> i=1

Î¸h,i (t); (4) then the heat removed by the RCU can be calculated as 

Qload (t) = Ï Î¦RCU (t)cp

 Î¸ret (t) âˆ’ Î¸RCU (t). (5) 

Cooling Efficiency Metric: The efficiency of the RCU is quantified through the coefficient of performance (COP), rep-resented as a quadratic function of the outlet temperature [49]: 

COP  Î¸RCU (t) = Î±0 + Î±1Î¸RCU (t) + Î±2Î¸2

> RCU

(t), (6) where Î±0, Î± 1, Î± 2 are the empirical coefficients. The power drawn from the cooling source is proportional to the thermal load and inversely proportional to the COP: 

Pc,src (t) = Qload (t)

COP  Î¸RCU (t) . (7) The fan power consumption model is characterized by the following relationship: 

Pc, fan (t) = Î´0 + Î´1Î¦RCU (t) + Î´2Î¦2 

> RCU

(t), (8) where Î´0, Î´ 1, Î´ 2 are empirical parameters that reflect the performance of the fan. 

Cooling Power Model: To sum up everything, the total cooling power Pcooling (t) can be written as the sum of the cold-source consumption Pc, src (t) and the fan consumption 

Pc, fan (t) [50]: 

Pcooling (t) = Pc, src (t) + Pc, fan (t). (9) The Pc, src (t) and Pc, fan (t) are the main factors defining the power required to remove heat from the GPU rack. The main source of thermal generation originates from the computing components themselves. In modern AI datacenters, the GPU is the dominant contributor to total IT power, and its power consumption directly affects the heat released into the surrounding environment. Hence, an accurate model of GPU power consumption as a function of its operating conditions (e.g., working frequency, GPU utilization rate) is essential to characterize the coupling between computing workloads and thermal dynamics. In this work, GPU power consumption is modeled as a function of the time-varying operating frequency f (t) and utilization U (t) [51], which will be elaborated in Sec. III-D: 

PGP U (t) = a3f (t)U (t) + a2f (t) + a1U (t) + a0, (10) IEEE TRANSACTIONS ON CLOUD COMPUTING 5

where ai (i = 0 , 1, 2, 3) are constant coefficients. The GPU temperature dynamics is modeled as [11]: 

Ë™Î¸GPU (t) = Î²0 Î¸c(t) + Î²1 PGPU (t) + Î³, (11) where Î²0, Î²1, and Î³ are the model coefficients. Here, Î¸C (t)

denotes the cold air temperature, PGPU (t) represents the power of the GPU, and Ë™Î¸GPU (t) captures the rate of change of the GPU temperature. Server N                   

> Server i+1
> â€¦ â€¦
> Server i-1
> â€¦ â€¦
> Server i
> Server 1
> Cold Zone Hot Zone
> Î¸c, N
> Rack Cooling Units
> Î¸c,i+1
> Î¸c,i
> Î¸c,i-1
> Î¸c,1
> Î¸h,N
> Î¸h,i+1
> Î¸h,i
> Î¸h,i-1
> Î¸h,1
> Î¸RCU Î¸ret
> â€¦ â€¦
> â€¦ â€¦
> â€¦ â€¦
> â€¦ â€¦
> ðš½ L
> supply temp return temp
> ,
> ðš½ RCU

Fig. 2: Schematic of airflow in the hot and cold zone, with cooling from Rack Cooling Units (RCU). 

C. Workload Forecasting Model 

Precise workload prediction is critical to designing effective allocation of computing resources for large-scale inference clusters for LLM. By foreseeing future inference demands, the system identifies a proactive workload distribution of computing resources to adjust cooling, balance thermal reg-ulation, and maintain performance and energy efficiency as a whole. Moreover, datacenters can prevent abrupt changes in thermal and computing conditions ahead of time, thus attaining SLOs compliance with avoided unnecessary power usage. This predictive capability is especially important for serving LLM deployments, where token generation needs have strong temporal correlation with LLM serving characteristics, such as prefill-decoding schemes and transformer architectures. To achieve workload forecasting, we collect 30-minute interval data and train a sequence to sequence long-short-term memory (LSTM) network, which fulfills token workload prediction at the cluster level. At time step t, we define the input vector as 

Lt = [ nt, g t], where nt and gt denote the aggregated context length and generated tokens within this interval, respectively. Given the lookback length H, the next-step token demand is predicted as Ë†Lt+1 = fLSTM (Ltâˆ’H+1: t) where Ltâˆ’H+1: t

denotes the sequence {Ltâˆ’H+1 , . . . , L t}. The LSTM can preserve memory over time using its hidden states, and can address short-term variations in token demand together with long-term cycles in token demand. 

D. Impact of TP and DVFS on LLM Inference Workloads 

The performance of LLM inference is jointly determined by the workload characteristics, the degree of TP, and the underlying GPU frequency settings. Table I shows the profiling results obtained from our real GPU inference experiment using the LLaMA-2 model [23] with 8X Tesla V100 cluster. It illustrate how TP configurations (TP2, TP4, TP8) and token lengths would impact the power and computing profile. For a given total token size, increasing the TP decreases the load per GPU, consequently decreasing the average inference latency and temperature. While larger TP size are also responsible for increasing the overall power usage. This compromise requires determining the best TP configuration given observed workload and datacenter utilization. Second, Table II aggregates the effects of DVFS for workloads with varying token lengths. The request streams are categorized into three different sizes of workload based on the total length of the generated and incoming context token. This classification allows for an ordered investigation of how the workload size interacts with the GPU frequency. The table finds that higher frequency settings always reduce latency, but raises power consumption and GPU temperature. In particular, since token length directly affects each workloadâ€™s computation demand, the latency, power, and thermal readings are different for the three classes. The per class effect of DVFS is reported in Appendix B. These tables show the subset of experimental setup utilized to investigate the effects of workload variation for various TP and frequency tuning settings. By classifying incoming requests into small, medium and large sets based on the integrated context and generated token lengths, we can determine how the size of the workload influences computational requirements. For each type of workload, we vary the TP configuration and the operating frequency of the GPU, then record the resulting latency, power usage, and temperature levels. This methodology allows for the determination of the sensitivity of LLM inference performance to the change in size of the workload and hardware control parameters. Overall, these results highlight a clear trade-off: a higher TP and higher GPU frequency reduce latency, but they can increase power draw and temperature. These experimental observations motivate our choice of TP and DVFS as the decision variables in the proposed framework. IV. H IERARCHICAL COOLING AND COMPUTING CONTROL 

In this Section, we introduce the design of our hierarchical control architecture, which integrates four levels of decision making to jointly manage computing and cooling parameters in LLM inference. At the upper layer, a cluster-level controller determines how many GPU clusters to activate based on workload forecasts obtained from an LSTM prediction model, TABLE I: Performance metrics with different configurations of TP units for varying levels of total tokens, with 8 Ã—Tesla V100 GPUs (16 GB), on the Llama-2 7B model inference workload.                                                          

> Total Tokens Latency (s) Temperature (Â°C) Power (W)
> TP2 TP4 TP8 TP2 TP4 TP8 TP2 TP4 TP8 195k 0.473 0.395 0.365 54.7 50.8 47.4 145 299 598 179k 0.435 0.384 0.355 54.1 50.0 47.0 145 298 597 177k 0.423 0.378 0.355 53.6 49.6 46.6 145 298 596 168k 0.368 0.355 0.344 53.4 49.5 46.6 145 297 596 150k 0.355 0.279 0.233 53.2 49.0 46.6 140 289 581 IEEE TRANSACTIONS ON CLOUD COMPUTING 6

TABLE II: Performance metrics with varying GPU frequencies for 3 representative workload traces, using 8 Ã—Tesla V100 GPUs (16 GB) for Llama-2 7B inference with TP2.                                              

> Frequency 3047 Tokens 2373 Tokens 935 Tokens
> Lat. Power Temp Lat. Power Temp Lat. Power Temp 1000 4.169 87.71 42 3.673 68.90 42 3.546 67.17 40 1200 3.811 169.41 43 3.641 152.78 43 3.487 77.66 40 1400 3.780 204.46 45 3.623 187.50 44 3.466 163.87 43 1600 3.732 209.13 46 3.620 193.97 45 3.465 164.17 43 1800 3.712 219.99 47 3.615 194.85 45 3.463 156.58 43

updated every 30 minutes. In the second layer, the TP configurations are adjusted every 5 minutes to meet workload demands, while incorporating a novel thermal constraint to prevent overheating. In the third layer, the cooling control is updated every 60 seconds to regulate airflow and inlet temperature for thermal stability. Lastly, at the lowest layer, per-job DVFS scaling is applied, whereby GPU operating frequencies are chosen at run-time to meet latency, energy, and thermal constraints. Through this multi-layered design, the system can achieve coordinated control within various timescales, both the long-term workload variability and the fine-grained system changes. 

A. Cluster-Level Control 

At the cluster level, 30-minute intervals are utilized to adjust decisions to decide the number of active servers and the GPU budget subjected to lower layers. The aim is to allocate adequate computational capacity to process the forecasted token load while avoiding both under-provisioning and excessive idle capacity. The workload prediction is made based on the LSTM model trained on historical data to predict the sum of tokens in the next half hour. To map that demand to physical resources, we use profiled throughput information from a single TP8 (8-GPU) because it provides the maximum parallelism. This configuration serves as a reference for capacity estima-tion, ensuring that cluster-level sizing reflects the maximum achievable throughput under full GPU utilization. From these measurements, we obtain the effective 30-minute capacity 

C(30) 8 . The number of TP groups (servers) required to reliably serve the forecast load is calculated as N = âŒˆ Ë†L30 /C (30) 8 âŒ‰,where Ë†L30 denotes the total token demand predicted by the LSTM during the next 30 minute interval and C(30) 8 represents the profiled token-processing capacity (tokens per 30 minutes) of a TP8 server over the same horizon. This profile-anchored sizing translates the forecasted workload into a robust GPU budget, which is subsequently passed to the window-level mixed-integer linear program (MILP) for fine-grained allocation across TP modes. 

B. Window-Level TP Selection 

At the start of each control window, denoted as w (5 minutes), the system verifies the optimal TP configuration mix m âˆˆ M ,where M is the set of potential GPU parallelism settings, e.g. T P 2, T P 4, etc. The objective of optimization is to reduce overall power consumption while (i) meeting the anticipated token demand Ë†Lw, (ii) adhering to the specified GPU budget 

Gmax and (iii) maintaining compliance with thermal regulation. The forecast workload Ë†Lw (total tokens in window w) is given from an LSTM model that was trained on traces from the past. The decision variable Ym,w is the number of pools of TP m to be selected for window w. The choice of sets 

Yw = {Y2,w , Y 4,w , Y 8,w } is formulated as an MILP with the GPU budget and the temperature constraints. 

min 

> Ym,w

X

> m

X

> t

PGP U m,w,t Ym,w (12a) s.t. X

> m

Cm,w Ym,w â‰¥ Ë†Lw (12b) 

X

> m

m Y m,w â‰¤ Gmax (12c) 

Î¸GP U,w (Yw, Ë†Lw, Î¸ c) â‰¤ Î¸max  

> GP U

(12d) The MILP at the pool-level in (12a) â€“ (12d) determines the optimal number of TP pools Ym,w for each configuration m

during the control window w. Designed to minimize the energy consumption of the GPU (12a) . The constraint (12b) guaranties that the aggregate capacity of all active pools Cm,w is adequate to support the expected token demand; The constraint (12c) applies the constraint budget of the GPUs Gmax (total available GPUs), while (12d) will ensure that the resulting temperature 

Î¸GP U,w from the thermal dynamics in (11) remains below the maximum allowable limit, i.e., Î¸GPU (t) â‰¤ Î¸GPU ,max , under the selected TP configuration Yw and the forecast workload 

Ë†Lw.Here, Cm,w , PGP U m,w and Î¸GP U,w denote the profiled token-processing capacity (tokens per 5 min), the GPU power in the window, and the temperature that correspond to configura-tion TP m, respectively. When the runtime configuration for the pool is reached, expressed as Yâ‹†w = ( Y â‹†

> 2,w

, Y â‹†

> 4,w

, Y â‹†

> 8,w

), the runtime controller uses a smart-switch algorithm and maps the TP pools as desired to the currently idle GPUs. When sufficient idle resources exist, it allows the fast reconfiguration of TP without restarting the serving process or repeating expensive re-sharding. Moreover, to design a fair and deterministic dispatch of inference request allocations among the designated multi pools, we utilize a proportional deterministic dispatch (Algorithm 1). This algorithm assigns tasks to the pools with respect to their evaluated capacities, resulting in an alternating schedule among the pools with respect to their relative weights. Thus, two pools with the same type (e.g., two pools of TP 4) are load-balanced by design, with the consequence that job allocation is capacity-aware and predictable for each designated window. The Capacity-proportional deterministic dispatch algorithm allocates inference jobs across active GPU pools based on their effective capacities. Here, Râ€² indicates the set of active GPU pools considered in the current scheduling window, and 

p stands for the index of each of these pools in Râ€². Cp is the effective processing capacity of the pool p, defined as the number of tokens processed per window. The number of inference jobs that arrive in the current scheduling window is expressed as Jw, sp is the normalized capacity share of the pool p, and np denotes the number of jobs scheduled for the IEEE TRANSACTIONS ON CLOUD COMPUTING 7

Algorithm 1 Capacity-Proportional Deterministic Dispatch  

> 1:

Input: Active pools Râ€² = {p} with capacities Cp; window job list {(tj , n j )}Jw 

> j=1

sorted by arrival time. where nj

denotes the token length of job j. 

> 2:

Output: Mapping f : {1, . . . , J w} â†’ R â€² (job â†’ pool)  

> 3:

Compute shares sp â† Cp/ P 

> qâˆˆR â€²

Cq 

> 4:

Balanced-round np â‰ˆ spJw to integers np such that P 

> p

np = Jw 

> 5:

Build a deterministic weighted schedule S by interleaving each pool p exactly np times  

> 6:

for j = 1 to Jw do  

> 7:

f (j) â† S[j] 

> 8:

return f

pool p. The function f : {1, . . . , J w} â†’ R â€² assigns each job 

j to a specific group p based on the deterministic schedule determined by the algorithm. 

C. Cooling MPC Formulation 

Utilizing the computational power within a physics-based rack model framework, we regulate the temperature of the RCU supply and the flow rate. uk := ( Î¸RCU (k), Î¦RCU (k)) ,

reduce cooling power with the adoption of thermal safety. Let 

xk âˆˆ RNk denote the overall thermal state for each active server. for server i governed by the zonal energy balance equations (Eqs. 1â€“3), with Nk the number of active servers in the time interval k. We find the discretized dynamics for the sampling time interval âˆ†t by numerically combining the ODE Eqs. (1)-(3) 

xk+1 = f xk, u k, d k

, (13) where dk is the computing workload. At each time t, the controller solves the following optimization problem with horizon Np:

min  

> {uk}Np âˆ’1
> k=0
> Npâˆ’1

X

> k=0

Pcooling (k) (14a) s.t. xk+1 = f (xk, u k, d k), k = 0 , . . . , N p âˆ’ 1, (14b) 

Î¸min RCU â‰¤ Î¸RCU (k) â‰¤ Î¸max RCU (14c) 

Î¦min RCU â‰¤ Î¦RCU (k) â‰¤ Î¦max RCU (14d) 

Î¸c(k) â‰¤ Î¸max c (14e) 

Î¸ret (k) â‰¤ Î¸max ret (14f) 

Î¸GPU (k) â‰¤ Î¸max GPU (14g) In this cooling MPC, (14a) minimizes the total cooling power on the prediction horizon. Constraints (14e) â€“ (14g) enforce thermal safety by keeping the temperatures below their respective limits, while constraints (14c) â€“ (14d) impose actuator bounds on the cooling setpoints (Î¸RCU , Î¦RCU ). In practice, such a MPC framework can be solved iteratively with the fitted cooling dynamics (13). 

D. Per-Job DVFS with Class-Aware Constraints 

DVFS is a well-established mechanism in GPU power management that dynamically adjusts the GPU clock/voltage to trade inference performance (latency/throughput) for lower power consumption [52], where controllers rely on known execution time and workload size. However, for LLM inference workloads in the AI datacenter, the output length is inherently uncertain and depends on both the input prompt and the model generation process. This makes conventional DVFS policies inadequate, as frequency selection cannot be decided solely from static job parameters. To address this challenge, we introduce a DistilBERT-based job classifier that predicts the type of job ( short , medium or long ) based on the context token length (Table III). We adopt DistilBERT because it provides strong semantic representations with a smaller model footprint, leading to lower inference latency and overhead, which is critical for real-time scheduling and control decisions in LLM serving [53]. This classification enables the enforcement of class-specific latency and thermal constraints, allowing frequency selection to be formulated as an optimization problem that is token lengthâ€“aware and ensures compliance with SLOs. For each incoming job j, we formulate a binary MILP problem that selects a frequency f from the set F =

{1000 , 1200 , 1400 , 1600 , 1800 } MHz . The DVFS optimization problem minimizes GPU power while respecting both the per-class latency and the GPU temperature limits: 

min 

> xf,j

X 

> fâˆˆF

PGP U (f, n j ) xf,j (15a) s.t. X 

> fâˆˆF

xf,j = 1 , (15b) 

X 

> fâˆˆF

lat c(nj , f ) xf,j â‰¤ lat max  

> c(j)

, (15c) 

X 

> fâˆˆF

Î¸GPU (f, n j ) xf,j â‰¤ Î¸max  

> GP U

, (15d) 

xf,j âˆˆ { 0, 1}, âˆ€f âˆˆ F . (15e) Objective (15a) minimizes the power of the GPU by selecting the optimal operating frequency f for each job j, subject to several constraints. Constraint (15b) enforces a one-hot selection, ensuring that exactly one frequency is chosen per job. Constraint (15c) guaranties that the job latency at the selected frequency does not exceed the class-specific latency bound 

lat max  

> c(j)

, where c(j) âˆˆ { S, M, L} corresponds to short, medium or long job classes. Constraint (15d) ensures thermal safety by keeping the GPU temperature below its upper threshold 

Î¸max  

> GP U

. Here, nj denotes the context token length of the job j,while PGP U (f, n j ), Lc(nj , f ), and Î¸GPU (f, n j ) represent the GPU power, latency, and temperature profiles associated with frequency f , respectively. TABLE III: Job classification by input/output token thresholds (DistilBERT-based predictor).                

> Query Type Class Context Token Length Generated Token Length
> Short S<256 <100
> Medium M<1024 <350
> Long L<8192 â‰¥350

IEEE TRANSACTIONS ON CLOUD COMPUTING 8

TABLE IV: Parameters of the control algorithm. 

Notation Value Unit Description 

TS 30 sec Control sampling time 

Np 2 â€“ Prediction horizon of MPC 

Î¸min RCU 18 â—¦C Lower bound of supply-air temp 

Î¸max RCU 27 â—¦C Upper bound of supply-air temp 

Î¦min RCU 0.009 m3/s Lower bound of supply airflow rate 

Î¦max RCU 0.03 m3/s Upper bound of supply airflow rate 

Î¸max ret 70 â—¦C Upper limit of return-air temperature 

Î¸max c 12 â—¦C Upper limit of cold-aisle inlet temp 

Î¸max GPU 50 â—¦C Upper limit of GPU inlet temperature 

Kp 4.5 â€“ Proportional gain of PID controller 

Ki 0.18 â€“ Integral gain of PID controller 

Kd 0.1 â€“ Derivative gain of PID controller 

The proposed controller is designed within a specified operating range of temperature and airflow as shown in Table IV, allowing stable system cooling. For this purpose, the limits Î¸RCU and Î¦RCU constrain the operation of the cooling unit within the range, while Î¸max ret , Î¸max c , and Î¸max GPU enforce the thermal safety of the rack and GPU modules. The PID gains 

(Kp, K i, K d) define the dynamic response of the controller (for a detailed mathematical formulation, see Appendix D). V. N UMERICAL EXPERIMENTS 

A. Evaluation Setup 

We evaluate the proposed joint coolingâ€“computing frame-work through a one-day simulation on GPU clusters, which captures diurnal workload variations derived from the real-world Azure LLM Inference Trace. The hierarchical controller operates on multiple temporal scales: cluster-level planning every 30 minutes, pool-level TP scheduling every 5 minutes, cooling control per minute, and DVFS on per-job level. Before the DVFS stage, we employ a DistilBERT-based job classification model to predict the output length category from the input context tokens. The classifier achieves 91% 

accuracy on validation set, allowing the controller to anticipate the expected service duration and select appropriate GPU frequencies that balance latency and power efficiency. The MILP optimization problems for the TP and server allocation are solved using PuLP [54], while the cooling control is formu-lated as an MPC problem solved by SciPy SLSQP [55]. The simulation integrates real GPU profiling data and production-level inference traces, including a one-week Azure workload that covers coding and conversation traces. To further validate the practicality of our approach, we conducted one-hour real system experiments on servers equipped with 8 Ã— Tesla V100 GPUs (16 GB each) running the Llama-2-7B model [42]. The control algorithm is implemented on top of vLLM [56], allowing dynamic TP configuration, workload scheduling, and DVFS runtime adjustment. The proposed framework is compared with a baseline configuration consisting of a single TP8 pool operating at fixed, non optimized GPU frequency clocks. 

B. Hierarchical Controlâ€“Based Provisioning 

We simulate the proposed hierarchical control framework using a subset of the utilization profile shown in Fig. 3 (a), which is derived from the Microsoft Azure LLM inference dataset, which captures normalized token and job arrival patterns, providing realistic diurnal workload dynamics that serve as the foundation for LSTM-based forecasting and subsequent hierarchical control simulation. At the cluster level, the LP-based provisioning controller dy-namically manages the number of active GPU servers according to the workload forecast from the LSTM model. As illustrated in Fig. 3 (b), the controller activates or deactivates GPU servers every 30 minutes to align the computing capacity with the predicted demand. When off-peak conditions exist, only a few servers are kept online to reduce idle energy consumption, while more servers are brought online as utilization increases. This adaptive scaling demonstrates that the proposed control mechanism can effectively balance resource allocation and en-ergy efficiency. The workload utilization pattern shows diurnal characteristics with sharp peaks and valleys occurring along the cycles in user activity. By maintaining the right number 03 06 09 12 15 18 21                                                           

> Time (hour)
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0
> Token per second
> (a) TPS
> 03 06 09 12 15 18 21
> Time (hour)
> 0
> 2
> 4
> 6
> 8
> 10
> 12
> Servers
> (b) Active Servers
> 03 06 09 12 15 18 21
> Time (hour)
> 900
> 1000
> 1100
> 1200
> Freq (MHz)
> (c) Mean GPU Freq
> 03 06 09 12 15 18 21
> Time (hour)
> 10
> 20
> 30
> 40
> 50
> PComputing  (kW)
> (d) Computing Power
> 03 06 09 12 15 18 21
> Time (hour)
> 20
> 30
> 40
> 50
> 60
> 70
> ret  ( C)
> Max
> (e) Return Temp
> MPC
> PID
> 03 06 09 12 15 18 21
> Time (hour)
> 8.00
> 8.25
> 8.50
> 8.75
> 9.00
> 9.25
> 9.50
> c(t) (  C)
> (f) Inlet Temp
> MPC
> 03 06 09 12 15 18 21
> Time (hour)
> 18
> 20
> 22
> 24
> 26
> RCU  ( C)
> (g) MPC Supply Air Temp
> MPC
> 03 06 09 12 15 18 21
> Time (hour)
> 0.00900
> 0.00925
> 0.00950
> 0.00975
> 0.01000
> 0.01025
> 0.01050
> RCU  (m 3/s)
> (h) MPC Airflow Rate
> MPC
> 03 06 09 12 15 18 21
> Time (hour)
> 18
> 20
> 22
> 24
> 26
> RCU  ( C)
> (i) PID Supply Air Temp
> PID
> 03 06 09 12 15 18 21
> Time (hour)
> 0.0285
> 0.0290
> 0.0295
> 0.0300
> 0.0305
> 0.0310
> 0.0315
> RCU  (m 3/s)
> (j) PID Airflow Rate
> PID

Fig. 3: Simulation results of the hierarchical control framework integrating workload utilization, cluster-level provisioning, MILP-based frequency tuning, computing power consumption, and cooling regulation. The bottom panels show the comparison between PID and MPC controllers in the cooling layer, illustrating the control efforts required by supply air temperature and airflow rate from both controllers. IEEE TRANSACTIONS ON CLOUD COMPUTING 904:31 04:41 04:51 05:01 05:11 05:21 05:31      

> Time (hour)
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0
> Token Per Second
> 20:52 21:02 21:12 21:22 21:32 21:42 21:52
> Time (hour)
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0
> Token Per Second
> 04:31 04:41 04:51 05:01 05:11 05:21 05:31
> Time (30 min)
> 0
> 1
> 2
> 3
> 4
> 5
> Servers
> 20:52 21:02 21:12 21:22 21:32 21:42 21:52
> Time (30 min)
> 0
> 2
> 4
> 6
> 8
> 10
> 12
> Servers
> 04:31 04:41 04:51 05:01 05:11 05:21 05:31
> Time (hour)
> 45
> 50
> 55
> 60
> 65
> 70
> ret  ( C)
> max (70Â°C)
> 20:52 21:02 21:12 21:22 21:32 21:42 21:52
> Time (hour)
> 20
> 30
> 40
> 50
> 60
> 70
> ret  ( C)
> max (70Â°C)
> 04:31 04:41 04:51 05:01 05:11 05:21 05:31
> Time (hour)
> 8.6
> 8.8
> 9.0
> 9.2
> 9.4
> c ( C)
> 20:52 21:02 21:12 21:22 21:32 21:42 21:52
> Time (hour)
> 7.85
> 7.90
> 7.95
> 8.00
> 8.05
> 8.10
> c ( C)
> 04:31 04:41 04:51 05:01 05:11 05:21 05:31
> Time (hour)
> 18.0
> 18.5
> 19.0
> 19.5
> 20.0
> 20.5
> 21.0
> 21.5
> 22.0
> RCU  ( C)
> 20:52 21:02 21:12 21:22 21:32 21:42 21:52
> Time (hour)
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> 25
> RCU  ( C)
> 04:31 04:41 04:51 05:01 05:11 05:21 05:31
> Time (hour)
> 9.0
> 9.2
> 9.4
> 9.6
> 9.8
> RCU  (m 3/s)
> Ã—10 3
> 20:52 21:02 21:12 21:22 21:32 21:42 21:52
> Time (hour)
> 9.00
> 9.02
> 9.04
> 9.06
> 9.08
> 9.10
> 9.12
> 9.14
> 9.16
> RCU  (m 3/s)
> Ã—10 3
> 04:31 04:41 04:51 05:01 05:11 05:21 05:31
> Time (hour)
> 5
> 10
> 15
> 20
> 25
> Pcomputing  (kW)
> 20:52 21:02 21:12 21:22 21:32 21:42 21:52
> Time (hour)
> 5
> 10
> 15
> 20
> 25
> Pcomputing  (kW)
> 04:31 04:41 04:51 05:01 05:11 05:21 05:31
> Time (hour)
> 1000
> 1050
> 1100
> 1150
> 1200
> 1250
> 1300
> 1350
> Freq (MHz)
> 20:52 21:02 21:12 21:22 21:32 21:42 21:52
> Time (hour)
> 1000
> 1005
> 1010
> 1015
> 1020
> 1025
> 1030
> 1035
> 1040
> Freq (MHz)
> From low to high traffic transition 1-hour window [04:31 05:31] From high to low traffic transition 1-hour window [20:52 21:52]

Fig. 4: High-resolution one hour windows in two operating regimes: high-traffic (left, panels) and low-load (right, panels). Top rows show workload utilization, LP-based provisioning, and MILP-based GPU frequency optimization; bottom rows show MPC-regulated thermals: return temperature, inlet temperature, airflow rate, and supply temperature. This side-by-side view highlights control behavior and thermalâ€“power responses under peak versus trough demand. 

of active GPUs, the proposed controller prevents both under-utilization (too many idle GPUs) and overload (insufficient capacity). At the DVFS level, an MILP-based frequency-tuning controller determines the optimal GPU frequency per class such that the computing power is minimized with respect to latency and temperature limits. Fig. 3 (c), shows the dynamic scaling of the mean GPU frequency with workload intensity. This behavior demonstrates the effectiveness of the proposed controller at the instance-level in combining compute efficiency with real-time workload dynamics. The resulting computing power consumption, shown in Fig. 3 (d), follows the combined effect of provisioning and frequency control. Computing power increases with both workload intensity and active server count frequency tuning, and decreases when either utilization or operating frequency is reduced. At the cooling level, an MPC regulates two manipulated variables, Î¦RCU and Î¸RCU to control the thermal outputs: temperatures Î¸ret , Î¸c, and Î¸s. The objective of MPC controller is to guaranty thermal safety while simultaneously reducing cooling power consumption given GPU TP configuration and incoming workloads. Fig. 3 (e)-(h) illustrate the performance of the MPC-based cooling controller, which controls in response to the variable intensity of the workload. Fig. 3 (e) shows that Î¸ret stays below the limit of 70Â°C throughout the day, which confirms the effectiveness of the proposed controller. As workload and temperature increase, the MPC increases the cooling effort to stabilize Î¸ret . Fig. 3 (f) shows that Î¸c

increases as the workload is reduced, indicating that Î¸c becomes hotter as the workload decreases to minimize the cooling effort and vise versa. This adaptive response from the proposed control system shows that the MPC is working to adjust airflow in the cold aisle according to thermal and workload variations. As shown in Fig. 3 (g), the air temperature supplied by the RCU is reduced during periods of high utilization to provide a higher cooling rate and increased during periods of low utilization to reduce the power consumption of the chiller. As illustrated in Fig. 3 (h), in the proposed system, 

Î¦RCU increases during periods of high utilization, and it is reduced during periods of low utilization to reduce the fan power. In addition to the temporal response, the spatial temperature distribution across the rack indicates that servers positioned farther from the cooling unit have slightly higher temperatures due to reduced airflow uniformity and uneven workload distribution. However, every Î¸s is within the defined safety threshold, which confirms effective thermal management under spatially varying conditions (see Appendix A). To further evaluate the performance, the cooling controller was compared with that of a proposed MPC and a PID controller. In both controls, the workload was the same. The results indicate that the MPC-based cooling strategy significantly outperforms the PID controller in terms of energy savings with around 28% due to its performance in adapting both Î¦RCU and Î¸RCU in response 0 5 10 15 20 25 30 35 40 45 50 55 60                                     

> Time (min)
> 1
> 2
> 3
> 4
> 5
> 6
> Latency (s)
> (a)
> Proposed Controller
> Baseline
> 0510 15 20 25 30 35 40 45 50 55 60
> Time (min)
> 38
> 40
> 42
> 44
> 46
> 48
> 50
> Temperature (Â°C)
> (b)
> Proposed Controller
> Baseline
> 0510 15 20 25 30 35 40 45 50 55 60
> Time (min)
> 55.0
> 57.5
> 60.0
> 62.5
> 65.0
> 67.5
> 70.0
> 72.5
> Power (W/GPU)
> (c)
> Proposed Controller
> Baseline
> 0510 15 20 25 30 35 40 45 50 55 60
> Time (min)
> 100
> 200
> 300
> 400
> 500
> 600
> 700
> Cooling Power (W)
> (d)
> Proposed Controller
> Baseline

Fig. 5: Performance on the computing side versus the cooling side with the proposed control framework versus a baseline on the LLaMA-2-7B inference on a real 8 Ã—Tesla (16 GB) GPU server. The plots collectively depict (a) end-to-end inference latency (QPS), (b) GPU temperature, (c) average per-GPU power consumption, and (d) cooling power. IEEE TRANSACTIONS ON CLOUD COMPUTING 10 

to workload variations in advance (see the Appendix A). In both cases, the control objective, therefore, is to keep the return-air temperature Î¸ret below its set point while minimizing the cooling power. Fig. 4 shows a representative, high-resolution windows of one-hour around two operating regimes: low to high-traffic transition (left panels) and high to low traffic transition (right panels). Under high traffic times, utilization characteristics show dramatic changes, the LP scheduler activates more servers, and MILP will update GPU frequency to ensure throughput continues. The related MPC thermal control proactively adjusts to the response by increasing Î¦RCU and reducing Î¸RCU to minimize heat accumulation, thus keeping Î¸ret below the safety threshold. Meanwhile, workload and GPU frequencies are low in low-load regimes. MPC also reduces the energy consumption of the cooling by reducing Î¦RCU and increasing 

Î¸RCU , decreasing the power of the fan and the chiller, while keeping the limits of thermal safety. On the whole, the side-by-side comparison highlights the advantages of proposed coordinated operation of the hierarchical framework. The computation layer scales resources or frequency dynamically with demand. The cooling layer coordinately adjusts airflow or supply temperature all the time, while achieving joint power efficiency and thermal safety. It is shown that this proposed controller can achieve high performance during peaks and substantial energy savings during troughs without violating thermal or computing constraints. 

C. Experimental Evaluation 

We evaluate the effectiveness of the proposed hierarchical control framework using the setup environment described previously. The experiments use three key metrics: GPU power consumption, thermal behavior, and inference latency to assess the performance of the proposed control approach and compare it with the baseline setup. These metrics reflect the efficiency of GPU energy usage, its operating temperature safety, and its real-time operating status. Figs. 5(a) - 5(d) depict the efficiency of each control layer under experimental inference workloads. As shown in Fig. 5(a) the total end-to-end latency follows the baseline well, confirming that the SLOs are maintained without performance degradation. Fig. 5(b) shows that GPU temperatures remain stable within safe limits despite reduced power consumption. The maximum temperature per GPU remains below 50Â°C, the frameworkâ€™s capacity to maintain thermal reliability while reducing energy. Fig. 5(c) also shows that hierarchical control reduces the power per GPU compared to the static baseline. During 1 hour, the average reduction reaches approximately 24.2%, confirming that the TP configuration and the MILP-based frequency tuning based on the proposed control scheme enable efficient energy scaling without performance loss. Finally, Fig. 5 (d) highlights the contribution of the MPC cooling layer, which modulates 

Î¦RCU and Î¸RCU in coordination with the optimization of the compute side. The coordination therefore reduces the cooling capacity by 31. 2% while ensuring temperature stability. At the pool level, the MILP scheduler dynamically adjusts TP every five minutes based on workload demand and GPU TABLE V: TP configuration across five-minute windows under hierarchical control (LLaMA-2-7B).                                                 

> TP Mode W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11 W12
> TP2 000000000022TP4 000112221100TP8 111000000000

TABLE VI: Performance Comparison Between Baseline and Controlled Systems.                 

> Metric Baseline Controlled Improvement
> Computing energy (Wh/GPU) 54.8 41.6 24.2%
> Cooling energy (Wh/GPU) 291 202.2 31.2%
> Temperature (Â°C) 50.1 41.6 17.0%
> Latency (s) 2.31 2.28 â‰ˆ0

availability (see Table V). The optimizer switches from TP8 down through TP4 and finally TP2 with decreasing workload intensity to reduce energy consumption while keeping inference throughput high. Table VI shows that the proposed controller has a reduction in energy in both the cooling and the computing subsystems. The result suggests that the proposed hierarchical controller can efficiently reduce total energy costs while maintaining reliable inference performance. VI. C ONCLUSION 

This papers tackles the challenge of energy management of AI datacenters, and proposes a hierarchical design for joint com-puting and cooling control. The proposed framework integrates LP-driven resource allocation, MILP-based LLM scheduling, and MPC cooling control, which is further supported by an LSTM-based workload predictor and a latency-aware classifier. Extensive simulations and real system experiments using Azure LLM inference trace on 8 Ã—Tesla V100 GPU with LLaMA-2-7B demonstrate up to 24.2% reduction in per GPU energy consumption, 31.2% cooling energy savings, and 17% decrease in mean GPU temperature, resulting in consistent total energy reduction without latency degradation. Future research will extend the scope further to include advanced liquid and hybrid cooling schemes with the objective of enhancing energy efficiency and temperature stability. REFERENCES [1] I. Ozkaya, â€œApplication of large language models to software engineering tasks: Opportunities, risks, and implications,â€ IEEE Software , vol. 40, no. 3, pp. 4â€“8, 2023. [2] M. Lammertyn, â€œ60+ chatgpt statistics and facts you need to know in 2024,â€ Invgate Blog, 2024, [Online]. Available: https://blog.invgate.com/ chatgpt-statistics. [3] N. Kulkarni, G. Gonzalez-Pumariega, A. Khurana, C. A. Shoemaker, C. Delimitrou, and D. H. Albonesi, â€œCuttleSys: Data-driven resource man-agement for interactive services on reconfigurable multicores,â€ in Proc. 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO) , 2020, pp. 650â€“664. [4] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, â€œEfficient memory management for large language model serving with PagedAttention,â€ in Proc. 29th Symposium on Operating Systems Principles (SOSP) , 2023, pp. 611â€“626. [5] X. Miao, C. Shi, J. Duan, X. Xi, D. Lin, B. Cui, and Z. Jia, â€œSpotServe: Serving generative large language models on preemptible instances,â€ in  

> Proc. 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), Vol. 2 , 2024, pp. 1112â€“1127. IEEE TRANSACTIONS ON CLOUD COMPUTING 11

[6] Y. Zhao, D. Wu, and J. Wang, â€œAlisa: Accelerating large language model inference via sparsity-aware kv caching,â€ in Proc. 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA) , 2024, pp. 1005â€“1017. [7] S. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones, W. Bergeron, J. Kepner, D. Tiwari, and V. Gadepally, â€œFrom words to watts: Benchmarking the energy costs of large language model inference,â€ in Proc. 2023 IEEE High Performance Extreme Computing Conference (HPEC) , 2023, pp. 1â€“9. [8] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, and J. Dean, â€œCarbon emissions and large neural network training,â€ arXiv preprint arXiv:2104.10350 , 2021. [9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, â€œLanguage models are few-shot learners,â€ in Advances in Neural Information Processing Systems (NeurIPS) , 2020. [Online]. Available: https://arxiv.org/abs/2005.14165 [10] G. Leopold, â€œAWS to offer Nvidiaâ€™s T4 GPUs for AI inferencing,â€ HPCwire, Mar 2019, [Online]. Available: https://www.hpcwire.com/2019/ 03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/. [11] J. Stojkovic, C. Zhang, I. Goiri, J. Torrellas, and E. Choukse, â€œDynamollm: Designing LLM inference clusters for performance and energy efficiency,â€ in Proc. 2025 IEEE Int. Symp. on High Performance Computer Architecture (HPCA) , 2025, pp. 1348â€“1362. [12] M. Dayarathna, Y. Wen, and R. Fan, â€œData center energy consumption modeling: A survey,â€ IEEE Commun. Surveys & Tutorials , vol. 18, no. 1, pp. 732â€“794, 2015. [13] Å. Å ach and D. Svyetlichnyy, â€œAdvances in numerical modeling for heat transfer and thermal management: A review of computational approaches and environmental impacts,â€ Energies , vol. 18, no. 5, p. 1302, 2025. [14] S. Ilager, K. Ramamohanarao, and R. Buyya, â€œETAS: Energy and thermal-aware dynamic virtual machine consolidation in cloud data center with proactive hotspot mitigation,â€ Concurrency and Computation: Practice and Experience , vol. 31, no. 17, p. e5221, 2019. [15] V. Lopez and H. F. Hamann, â€œHeat transfer modeling in data centers,â€ 

International Journal of Heat and Mass Transfer , vol. 54, no. 25â€“26, pp. 5306â€“5318, 2011. [16] S. Cruzes, â€œData centers in the age of AI: A tutorial survey on infras-tructure, sustainability, and emerging challenges,â€ Authorea Preprints ,2025. [17] R. Lu and D. Wang, â€œA thermal-aware workload scheduler for high-performance LLM inference in cooling-regulated datacenters,â€ ACM SIGENERGY Energy Informatics Review , vol. 5, no. 2, pp. 98â€“104, 2025. [18] J. Stojkovic, C. Zhang, I. Goiri, E. Choukse, H. Qiu, R. Fonseca, J. Torrellas, and R. Bianchini, â€œTAPAS: Thermal-and power-aware scheduling for LLM inference in cloud platforms,â€ in Proc. 30th ACM Int. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS) , vol. 2, 2025, pp. 1266â€“1281. [19] S. Chen, A. Jin, C. Delimitrou, and J. F. Mart Â´Ä±nez, â€œRetail: Opting for learning simplicity to enable QoS-aware power management in the cloud,â€ in Proc. 2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA) , 2022, pp. 155â€“168. [20] C.-H. Hsu, Y. Zhang, M. A. Laurenzano, D. Meisner, T. Wenisch, J. Mars, L. Tang, and R. G. Dreslinski, â€œAdrenaline: Pinpointing and reining in tail queries with quick voltage boosting,â€ in Proc. 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA) , 2015, pp. 271â€“282. [21] H. Kasture, D. B. Bartolini, N. Beckmann, and D. Sanchez, â€œRubik: Fast analytical power management for latency-critical systems,â€ in Proc. 48th International Symposium on Microarchitecture (MICRO) , 2015, pp. 598â€“610. [22] L. Zhou, L. N. Bhuyan, and K. K. Ramakrishnan, â€œGemini: Learning to manage CPU power for latency-critical search engines,â€ in Proc. 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO) , 2020, pp. 637â€“649. [23] P. Patel, E. Choukse, C. Zhang, A. Shah, Â´I. Goiri, S. Maleki, R. Fonseca, and R. Bianchini, â€œSplitwise: Efficient generative LLM inference using phase splitting,â€ IEEE Micro , 2025. [24] T. Gao, M. David, J. Geer, R. Schmidt, and B. Sammakia, â€œA dynamic model of failure scenarios of the dry cooler in a liquid cooled chiller-less data center,â€ in Proc. 2015 31st Thermal Measurement, Modeling & Management Symposium (SEMI-THERM) , 2015, pp. 113â€“119. [25] B. Tschudi and O. VanGeet, â€œBest practices guide for energy-efficient data center design,â€ NREL, Tech. Rep. NREL/BR-7A40-47201, Mar 2011, [Online]. Available: https://datacenters.lbl.gov/sites/default/files/ eedatacenterbestpractices.pdf. [26] E. J. Walsh, T. J. Breen, J. Punch, A. J. Shah, and C. E. Bash, â€œFrom chip to cooling tower data center modeling: Part II influence of chip temperature control philosophy,â€ in Proc. 2010 12th IEEE Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems , 2010, pp. 1â€“7. [27] A. C. Kheirabadi and D. Groulx, â€œCooling of server electronics: A design review of existing technology,â€ Applied Thermal Engineering , vol. 105, pp. 622â€“638, 2016. [28] Y. Taniguchi, K. Suganuma, T. Deguchi, G. Hasegawa, Y. Nakamura, N. Ukita, N. Aizawa, K. Shibata, K. Matsuda, and M. Matsuoka, â€œTandem equipment arranged architecture with exhaust heat reuse system for software-defined data center infrastructure,â€ IEEE Transactions on Cloud Computing , vol. 5, no. 2, pp. 182â€“192, 2015. [29] J. Wan, X. Gui, S. Kasahara, Y. Zhang, and R. Zhang, â€œAir flow measurement and management for improving cooling and energy efficiency in raised-floor data centers: A survey,â€ IEEE Access , vol. 6, pp. 48 867â€“48 901, 2018. [30] C.-H. Wang, Y.-Y. Tsui, and C.-C. Wang, â€œAirflow management on the efficiency index of a container data center having overhead air supply,â€ 

Journal of Electronic Packaging , vol. 139, no. 4, 2017. [31] P. Lin and V. Avelar, â€œHow row-based data center cooling works,â€ Schneider Electric, White Paper, 2007. [32] ASHRAE, Thermal Guidelines for Data Processing Environments , 4th ed. Atlanta, GA, USA: ASHRAE, 2015. [33] A. Afram and F. Janabi-Sharifi, â€œTheory and applications of HVAC control systemsâ€”a review of model predictive control (MPC),â€ Building and Environment , vol. 72, pp. 343â€“355, 2014. [34] J. B. Petersen, J. D. Bendtsen, and J. Stoustrup, â€œNonlinear model predictive control for energy efficient cooling in shopping center HVAC,â€ in Proc. 2019 IEEE Conference on Control Technology and Applications (CCTA) , 2019, pp. 611â€“616. [35] Q. Fang, J. Wang, Q. Gong, and M. Song, â€œThermal-aware energy management of an HPC data center via two-time-scale control,â€ IEEE Transactions on Industrial Informatics , vol. 13, no. 5, pp. 2260â€“2269, 2017. [36] M. Kheradmandi, D. G. Down, and H. Moazamigoodarzi, â€œEnergy-efficient data-based zonal control of temperature for data centers,â€ in 

Proc. 10th Int. Green and Sustainable Comput. Conf. (IGSC) , 2019, pp. 1â€“7. [37] Z. Li, H. Wang, Q. Fang, and Y. Wang, â€œA data-driven subspace predictive control method for air-cooled data center thermal modelling and optimization,â€ J. Franklin Inst. , vol. 360, no. 5, pp. 3657â€“3676, 2023. [38] Q. Fang, Q. Gong, J. Wang, and Y. Wang, â€œOptimization based resource and cooling management for a high performance computing data center,â€ 

ISA Trans. , vol. 90, pp. 202â€“212, 2019. [39] Y. J. Choi, B. R. Park, J. Y. Hyun, and J. W. Moon, â€œDevelopment of an adaptive artificial neural network model and optimal control algorithm for a data center cyberâ€“physical system,â€ Build. Environ. , vol. 210, p. 108704, 2022. [40] A. Qiao, S. K. Choe, S. J. Subramanya, W. Neiswanger, Q. Ho, H. Zhang, G. R. Ganger, and E. P. Xing, â€œPollux: Co-adaptive cluster scheduling for goodput-optimized deep learning,â€ in Proc. 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21) , 2021. [41] NVIDIA, â€œNVLink and NVLink switch,â€ 2024, [Online]. Available: https://www.nvidia.com/en-us/data-center/nvlink/. [42] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al. , â€œLLaMA 2: Open foundation and fine-tuned chat models,â€ arXiv preprint arXiv:2307.09288 ,2023. [43] H. Huang, W. Lin, J. Lin, and K. Li, â€œPower management optimization for data centers: A power supply perspective,â€ IEEE Transactions on Sustainable Computing , 2025. [44] T. Patel and D. Tiwari, â€œCLITE: Efficient and QoS-aware co-location of multiple latency-critical jobs for warehouse scale computers,â€ in Proc. IEEE Int. Symp. on High Performance Computer Architecture (HPCA) ,2020, pp. 193â€“206. [45] P. Patel, E. Choukse, C. Zhang, A. Shah, I. Goiri, S. Maleki, and R. Bianchini, â€œSplitwise: Efficient generative LLM inference using phase splitting,â€ in Proc. 51st Annu. Int. Symp. on Computer Architecture (ISCA) , 2024, pp. 412â€“425. [46] T. Gao, E. Kumar, M. Sahini, C. Ingalz, A. Heydari, W. Lu, and X. Sun, â€œInnovative server rack design with bottom located cooling unit,â€ in Proc. IEEE TRANSACTIONS ON CLOUD COMPUTING 12 

15th IEEE Intersoc. Conf. Thermal Thermomech. Phenom. Electron. Syst. (ITherm) , 2016, pp. 1172â€“1181. [47] X. Tong, J. Wang, W. Liu, H.-A. Samah, Q. Zhang, and L. Zhang, â€œA time-varying state-space model for real-time temperature predictions in rack-based cooling data centers,â€ Appl. Therm. Eng. , vol. 230, p. 120737, 2023. [48] W. Liu, X. Tong, J. Wang, C. Yue, and Q. Zhang, â€œReal-time temperature predictions via state-space model and parameters identification within rack-based cooling data centers,â€ J. Build. Eng. , vol. 58, p. 105013, 2022. [49] H. Moazamigoodarzi, S. Pal, S. Ghosh, and I. K. Puri, â€œReal-time temperature predictions in IT server enclosures,â€ Int. J. Heat Mass Transf. , vol. 127, pp. 890â€“900, 2018. [50] Z. Li, H. Wang, Q. Fang, and Y. Wang, â€œA data-driven subspace predictive control method for air-cooled data center thermal modelling and optimization,â€ J. Franklin Inst. , vol. 360, no. 5, pp. 3657â€“3676, 2023. [51] J. Guerreiro, A. Ilic, N. Roma, and P. Tom Â´as, â€œModeling and decoupling the GPU power consumption for cross-domain DVFS,â€ IEEE Trans. Parallel Distrib. Syst. , vol. 30, no. 11, pp. 2494â€“2506, 2019. [52] Y. Wang, M. Hao, H. He, W. Zhang, Q. Tang, X. Sun, and Z. Wang, â€œDRLCAP: Runtime GPU frequency capping with deep reinforcement learning,â€ IEEE Transactions on Sustainable Computing , vol. 9, no. 5, pp. 712â€“726, 2024. [53] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, â€œDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,â€ arXiv preprint arXiv:1910.01108 , 2019. [54] Python PuLP, â€œOptimization with PuLP,â€ 2024. [55] Python SciPy, â€œSciPy library,â€ 2024, [Online]. Available: https://scipy. org/. [56] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, â€œEfficient memory management for large language model serving with PagedAttention,â€ in Proc. 29th Symp. Operating Syst. Principles (SOSP) , 2023, pp. 611â€“626. 

APPENDIX 

A. Spatial Distribution of Server Exhaust Temperature 

Figure 6 shows the spatial change in server tempera-ture across the rack in the 10-server operation window 

[t06:30 , t 06:59 ]. As illustrated in Fig 2, the server close to the cooling unit has smaller index and receives cooler air. Therefore, it exhibits a lower server exhaust temperature. The server temperature difference can be attributed to the relative distance of the server to the cooling unit, resulting in the non-uniform airflow and uneven workload allocation, which may produce more heat in servers with high workload. Collectively, these contributions lead to an increase in server temperatures at the upstream end of the rack. 1 2 3 4 5 6 7 8 9 10     

> Server Index
> 0
> 10
> 20
> 30
> 40
> 50
> 60
> s, i (Â°C)
> 22.4
> 33.1 35.5 33.3 32.2 30.8
> 34.0
> 37.6
> 51.4
> 63.1
> [Time Window 06:30 06:59]

Fig. 6: Mean serverâ€™s exhaust temperature Î¸s,i (t) during the first 30-minute interval [t06:30 , t 06:59 ] after the system scaled to ten active servers. 

B. GPU Frequency Sensitivity 

We test on real LLM query data using LlaMA-2-7B model. Figure 7 illustrates the relationship between GPU frequency, power, latency, and temperature under different token-length categories. It is noteworthy the trained classifer is compatible with such token-length classification. As the frequency of the GPU increases, per-job latency decreases, but power and temperature generally increase. We utilize these profiling data in the DVFS control problem (15) . This shows the need of class-aware frequency selection to satisfy latency targets while limiting power and temperature. 1000 1200 1400 1600 1800         

> Frequency (MHz)
> 60
> 90
> 120
> 150
> 180
> Power (W)
> (a)
> Power (W)
> Latency (s)
> Temp (Â°C)
> 1000 1200 1400 1600 1800
> Frequency (MHz)
> 140
> 160
> 180
> 200
> Power (W)
> (b)
> Power (W)
> Latency (s)
> Temp (Â°C)
> 1000 1200 1400 1600 1800
> Frequency (MHz)
> 75
> 100
> 125
> 150
> Power (W)
> (c)
> Power (W)
> Latency (s)
> Temp (Â°C)
> 3.60
> 3.62
> 3.64
> 3.66
> 3.68
> Latency (s)
> 41.6
> 42.4
> 43.2
> 44.0
> 44.8
> Temp (Â°C)
> 3.64
> 3.68
> 3.72
> 3.76
> 3.80
> Latency (s)
> 40.5
> 42.0
> 43.5
> 45.0
> 46.5
> Temp (Â°C)
> 3.45
> 3.48
> 3.50
> 3.53
> 3.55
> Latency (s)
> 40.0
> 40.8
> 41.6
> 42.4
> 43.2
> Temp (Â°C)

Fig. 7: GPU frequency sensitivity for LL AMA-2-7B inference on a real 8 Ã—Tesla (16 GB) GPU server. We report latency, GPU temperature, GPU power, and cooling power versus frequency for different token-length classes. Subfigures correspond to token-length classes: (a) Medium, (b) Large, and (c) Small. 

C. Performance Comparison 

Fig. 8: Comparison of energy and temperature metrics under proposed and baseline control strategies. In Fig. 8 the left panel shows the energy consumption of computing and cooling, while the right panel illustrates the average and maximum temperatures of the GPU. The percentages indicate relative differences between the proposed hierarchical control and the baseline configuration, highlighting the energy savings and the thermal reduction achieved. 

D. PID-Based Datacenters Cooling 

To provide a comparison with the proposed MPC-based cooling controller, we implemented PID controller at the cooling layer. The PID loop uses the tracking error between the measured return-air temperature and its set point to adjust the RCU supply-air temperature command, which is constrained within its physical limits. This setup represents a purely feedback-based, non-predictive controller that is commonly used in practice. For a fair comparison, both MPC and PID controllers were evaluated under the same workload and IT power profiles. The results show that the proposed MPC controller reduces the cooling-energy consumption by approximately 28% while maintaining the return-air temperature closer to its set point than the PID baseline.