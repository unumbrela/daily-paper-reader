---
title: "Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization"
title_zh: 前向与后向：直接偏好优化中推理目标的比较
authors: "Murtaza Nikzad, Raghuram Ramanujan"
date: 2026-01-12
pdf: "https://arxiv.org/pdf/2601.07199v1"
tags: ["query:sr-llm"]
score: 10.0
evidence: 比较直接偏好优化中的推理目标
tldr: 本研究探讨了直接偏好优化（DPO）中前向思维链生成与后向验证两种推理目标对模型可靠性的影响。通过在GSM8K数据集上的实验，发现前向训练能显著提升解题准确率，而后向训练则在减少虚假正例和提升验证校准方面表现优异。研究揭示了两者在提升推理能力与减少幻觉之间的权衡关系，为优化大模型推理性能提供了新视角。
motivation: 旨在解决大语言模型在推理过程中经常产生看似合理但错误的“幻觉”问题，并探究不同训练目标对推理可靠性的影响。
method: 对比研究了DPO框架下的两种目标：训练模型生成正确推理轨迹的前向生成，以及训练模型识别并纠正候选方案错误的后向验证。
result: "前向训练将准确率从83.1%提升至86.6%，而后向训练虽准确率增幅较小，但将虚假正例率从13.4%大幅降至4.3%。"
conclusion: 前向和后向推理目标提供互补的信号，前者增强解题能力，后者优化验证校准，两者之间存在基础性的性能权衡。
---

## 摘要
大语言模型展现出令人印象深刻的推理能力，但经常生成看似合理但错误的解决方案，这种现象通常被称为幻觉。本文通过直接偏好优化（Direct Preference Optimization）研究了训练目标构成对推理可靠性的影响。研究考察了两种互补的训练信号：前向思维链（chain-of-thought）生成，即训练模型产生正确的推理轨迹；以及后向验证，即训练模型验证并承认候选方案中的错误。在 GSM8K 上的实验揭示了这些目标之间存在根本性的权衡。仅进行前向 DPO 训练实现了最高的准确率提升，从 83.1% 增加到 86.6%（+3.5 个百分点），而仅进行后向训练带来的准确率提升极小，但显著将误报率（false positive rate）从 13.4% 降低到 4.3%。值得注意的是，与基准模型相比，两种训练变体都降低了承认率（acknowledgement rate），这表明偏好优化增加了模型对其输出的置信度。这些发现表明，前向和后向推理目标提供了独特且互补的学习信号：前向训练提高了问题解决能力，而后向训练改善了验证校准。完整的训练和评估流水线通过低秩自适应（Low-Rank Adaptation）高效实现，并已发布以促进进一步研究。

## Abstract
Large language models exhibit impressive reasoning capabilities yet frequently generate plausible but incorrect solutions, a phenomenon commonly termed hallucination. This paper investigates the effect of training objective composition on reasoning reliability through Direct Preference Optimization. Two complementary training signals are examined: forward chain-of-thought generation, which trains the model to produce correct reasoning traces, and backward verification, which trains the model to verify and acknowledge errors in candidate solutions. Experiments on GSM8K reveal a fundamental trade-off between these objectives. Forward-only DPO training achieves the highest accuracy improvement, increasing from 83.1% to 86.6% (+3.5 percentage points), while backward-only training yields minimal accuracy gains but substantially reduces the false positive rate from 13.4% to 4.3%. Notably, both training variants reduce acknowledgement rate compared to the baseline, suggesting that preference optimization increases model confidence in its outputs. These findings indicate that forward and backward reasoning objectives provide distinct and complementary learning signals: forward training improves problem-solving capability, while backward training improves verification calibration. The complete training and evaluation pipeline, implemented efficiently through Low-Rank Adaptation, is released to facilitate further research.

---

## 论文详细总结（自动生成）

### 论文结构化深度分析总结

#### 1. 核心问题与研究动机
论文探讨了大语言模型（LLM）在复杂推理任务中的“幻觉”问题——即模型能生成看似逻辑连贯但结果错误的推理链。研究的核心在于：**不同的训练目标（前向推理 vs. 后向验证）如何影响模型的推理准确性与错误识别能力？** 作者试图通过直接偏好优化（DPO）来量化这两种信号对模型可靠性的不同贡献。

#### 2. 方法论：核心思想与技术细节
研究采用了 **直接偏好优化 (DPO)** 框架，并对比了两种不同的训练目标：
*   **前向推理 (Forward Reasoning):** 训练模型在给定问题下，偏好正确的推理轨迹（Chain-of-Thought）而非错误的轨迹。目标是提升模型从问题到答案的直接求解能力。
*   **后向验证 (Backward Verification):** 训练模型对给定的“问题+候选答案”进行校验，偏好正确的判断（PASS/FAIL）轨迹。
*   **关键技术细节:**
    *   **数据生成:** 使用 LLaMA 3.1 8B-Instruct 作为教师模型，通过**拒绝采样 (Rejection Sampling)** 获取同一问题的正确和错误解法，构建高质量的负样本。
    *   **样本加权:** 对包含真实错误推理的负样本对赋予更高的权重 ($\alpha = 1.2$)。
    *   **参数高效微调:** 使用 **LoRA (Low-Rank Adaptation)** 作用于注意力机制的投影矩阵（Q, K, V, O），仅增加约 0.25% 的可训练参数。
    *   **推理策略:** 前向生成使用较高温度 ($\tau=0.7$) 以保持多样性，后向验证使用较低温度 ($\tau=0.3$) 以保证判断的保守性和一致性。

#### 3. 实验设计
*   **数据集:** 使用小学数学竞赛数据集 **GSM8K**（8,500个问题）。
*   **基准 (Benchmark):** 以未经微调的 **LLaMA 3.1 8B-Instruct** 作为 Baseline。
*   **对比实验组:** 
    1.  **Forward-Only:** 仅使用前向推理偏好数据训练。
    2.  **Backward-Only:** 仅使用后向验证偏好数据训练。
*   **评估指标:** 准确率 (Accuracy)、承认率 (Ack. Rate，识别自身错误的能力)、虚假正例率 (FPR，误判正确答案为错误的比例) 以及验证校准 F1 值。

#### 4. 资源与算力
*   **硬件:** 使用单张 **NVIDIA RTX A6000 GPU (48GB 显存)**。
*   **时长:** 每个实验条件的训练耗时约 **1 小时**。
*   **总算力消耗:** 包含消融实验在内，总计约 **20 GPU-hours**。
*   **配置:** 采用 bfloat16 混合精度训练和梯度检查点技术。

#### 5. 实验数量与充分性
*   **实验规模:** 针对基准、前向、后向三种条件进行了系统对比。测试集样本量在 250 到 350 之间。
*   **充分性评价:** 实验设计较为**客观且聚焦**。通过隔离变量（Forward vs. Backward）清晰地展示了不同目标的增益方向。
*   **局限性:** 实验仅限于 GSM8K 一个数据集，且模型规模固定在 8B。虽然对于验证核心假设是充分的，但在跨领域（如逻辑、科学推理）和跨模型规模上的泛化性仍有待进一步验证。

#### 6. 主要结论与发现
*   **性能权衡:** 前向训练显著提升准确率（+3.5%），但对验证能力提升有限；后向训练显著降低误报率（从 13.4% 降至 4.3%），但对解题准确率几乎没有帮助。
*   **置信度陷阱:** 这是一个意外发现——**DPO 训练会降低模型的“承认率”**。经过优化后的模型变得更加“自信”，即使在回答错误时也倾向于认为自己是正确的，这表明偏好优化可能损害模型的自我校准能力。
*   **技能独立性:** 前向解题和后向验证是两种截然不同的技能，需要针对性的信号进行训练。

#### 7. 优点与亮点
*   **指标创新:** 提出了“承认率 (Acknowledgement Rate)”指标，量化了模型识别自身错误的能力，这对安全部署至关重要。
*   **高效实现:** 证明了在消费级 GPU 上通过 LoRA 和 DPO 就能实现显著的推理性能提升。
*   **开源贡献:** 提供了从数据生成、拒绝采样到评估的完整复现流水线。

#### 8. 不足与局限
*   **任务单一:** 仅在数学推理（GSM8K）上验证，未覆盖常识或代码推理。
*   **承认率下降风险:** 论文揭示了 DPO 可能导致模型过度自信，但未能在本文中提出有效的缓解方案。
*   **验证框架简化:** 采用二元（PASS/FAIL）验证，无法处理部分正确或带有不确定性的复杂推理场景。

（完）
