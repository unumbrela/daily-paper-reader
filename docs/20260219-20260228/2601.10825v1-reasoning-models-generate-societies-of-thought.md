---
title: Reasoning Models Generate Societies of Thought
title_zh: 推理模型生成思维社会
authors: "Junsol Kim, Shiyang Lai, Nino Scherrer, Blaise Agüera y Arcas, James Evans"
date: 2026-01-15
pdf: "https://arxiv.org/pdf/2601.10825v1"
tags: ["query:sr-llm"]
score: 10.0
evidence: LLM中思维链与推理的机制
tldr: 本研究探讨了推理模型（如DeepSeek-R1）高性能的内在机制，发现其卓越的推理能力并非仅源于计算量的增加，而是通过模拟“思想社会”实现。模型在推理链中展现出多智能体交互特征，通过不同人格和专业视角的冲突与辩论来优化决策。研究通过量化分析和强化学习实验证明，这种社会化组织结构是提升复杂任务准确性的关键，为理解大模型推理本质提供了新视角。
motivation: 旨在揭示推理模型在复杂认知任务中超越普通指令微调模型的底层机制与计算原理。
method: 采用机械可解释性方法和量化分析研究推理轨迹，并结合受控强化学习实验验证视角多样性与准确性的关系。
result: 推理模型表现出显著的视角多样性和内部对话行为，通过模拟异构人格与专业知识的冲突与和解来提升推理精度。
conclusion: 推理能力的提升源于思想的社会化组织，这种模拟集体智能的机制能更有效地探索解空间，为多智能体系统设计提供了新启示。
---

## 摘要
大型语言模型在各领域已取得显著能力，但复杂推理的底层机制仍不明确。近期，推理模型在复杂认知任务上优于同类指令微调模型，这通常归功于通过更长思维链实现的扩展计算。本研究表明，增强的推理能力并非仅源于扩展计算，而是源于模拟多智能体交互——即“思维社会”。这种机制实现了具有不同人格特质和领域专业知识的内部认知视角之间的多样化与辩论。通过对推理轨迹进行定量分析和机械可解释性研究，我们发现 DeepSeek-R1 和 QwQ-32B 等推理模型表现出比指令微调模型更高的视角多样性，在推理过程中激活了异质人格与专业特征之间更广泛的冲突。这种多智能体结构体现在对话行为中（包括问答、视角转换和冲突观点调和），以及刻画激烈交锋的社会情感角色中，共同构成了推理任务中的准确性优势。受控强化学习实验显示，仅针对推理准确性进行奖励时，基座模型的对话行为也会增加；而使用对话支架对模型进行微调，比基座模型能更快提升推理能力。这些发现表明，思维的社会化组织实现了对解空间的有效探索。我们认为，推理模型在计算上平行于人类群体的集体智慧，即当多样性被系统化组织时，能够实现卓越的问题解决能力，这为利用“群智”进行智能体组织提供了新机遇。

## Abstract
Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.