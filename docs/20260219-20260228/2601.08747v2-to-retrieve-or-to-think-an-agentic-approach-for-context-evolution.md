---
title: To Retrieve or To Think? An Agentic Approach for Context Evolution
title_zh: 检索还是思考？一种用于上下文演进的智能体方法
authors: "Rubing Chen, Jian Wang, Wenjie Li, Xiao-Yong Wei, Qing Li"
date: 2026-01-13
pdf: "https://arxiv.org/pdf/2601.08747v2"
tags: ["query:sr-llm"]
score: 10.0
evidence: 检索增强生成的智能体方法
tldr: 针对检索增强生成（RAG）中频繁检索导致的计算冗余和噪声干扰问题，本文提出Agentic Context Evolution (ACE) 框架。该框架受人类元认知启发，通过中央协调代理动态决策是进行外部检索还是内部推理。ACE在多跳问答任务中显著提升了准确率并降低了Token消耗，为复杂知识密集型任务提供了一种更高效、智能的上下文演进方案。
motivation: 传统的检索增强生成方法采用固定的检索策略，容易引入无关噪声并造成不必要的计算开销。
method: 提出ACE框架，利用中央协调代理通过多数投票机制，动态调度检索代理获取新证据或推理代理进行内部分析。
result: 在多跳问答基准测试中，ACE在保持上下文简洁的同时，准确率显著优于现有基准模型，且Token使用效率更高。
conclusion: 通过模拟人类元认知过程动态平衡检索与推理，可以有效提升大模型处理复杂知识任务的性能与效率。
---

## 摘要
当前的上下文增强方法（如检索增强生成）对于解决知识密集型推理任务至关重要。然而，这些方法通常遵循一种僵化的暴力策略，即在每一步都执行检索。这种不加区分的方法不仅产生了不必要的计算开销，还因无关噪声使上下文饱和而降低了性能。为了解决这些局限性，我们引入了智能体上下文演进（ACE）框架。该框架受人类元认知启发，能够动态决定是寻求新证据还是利用现有知识进行推理。ACE 采用中央编排智能体，通过多数投票进行战略决策，旨在交替激活用于外部检索的检索智能体和用于内部分析与优化的推理智能体。通过消除冗余的检索步骤，ACE 保持了简洁且不断演进的上下文。在具有挑战性的多跳问答基准测试上的广泛实验表明，ACE 在准确率上显著优于竞争基准，同时实现了高效的 Token 消耗。我们的工作为推进复杂知识密集型任务的上下文演进生成提供了宝贵的见解。

## Abstract
Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks. However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting. It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption. Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.

---

## 论文详细总结（自动生成）

### 论文分析总结：To Retrieve or To Think? An Agentic Approach for Context Evolution

#### 1. 核心问题与研究动机
*   **核心问题**：传统的检索增强生成（RAG）和迭代式 RAG 往往采用“暴力检索”策略，即在每一步推理中都强制执行检索。
*   **研究背景**：这种不加区分的检索会导致两个主要问题：
    1.  **上下文饱和与噪声干扰**：引入大量无关信息，干扰模型推理，甚至导致幻觉。
    2.  **计算开销冗余**：增加了不必要的 Token 消耗和推理延迟。
*   **研究动机**：受人类“元认知”启发——人类在解决问题时会动态评估知识缺口，交替进行“外部搜寻”和“内部思考”，而非盲目搜索。

#### 2. 方法论：Agentic Context Evolution (ACE)
ACE 将上下文管理从静态流水线转变为自主的、状态感知的多智能体演进过程。
*   **核心思想**：引入一个中央协调智能体（Orchestrator），动态决定当前步骤是应该执行 **RETRIEVE（检索）** 还是 **THINK（思考）**。
*   **关键技术细节**：
    *   **多智能体投票机制**：由 $k$ 个智能体组成委员会，针对当前工作记忆（Working Memory）和原始问题进行独立判断，通过**多数投票**决定下一步行动。
    *   **RETRIEVE 动作**：当现有信息不足时，激活检索智能体从外部数据库获取新片段，更新工作记忆。
    *   **THINK 动作**：当现有信息足以进行下一步推理时，激活推理智能体生成子查询（Sub-query）并进行内部解答，提炼中间见解，避免上下文膨胀。
    *   **上下文演进**：通过 $N$ 轮迭代，工作记忆不断精炼和深化，最终由生成函数合成最终答案。

#### 3. 实验设计
*   **数据集**：选取了三个具有挑战性的多跳问答（Multi-hop QA）基准：**MultiHop-RAG**、**HotpotQA** 和 **2WikiQA**。
*   **对比方法（Baselines）**：
    1.  **Vanilla**：无检索的原始 LLM。
    2.  **RAG**：标准的单步检索增强生成。
    3.  **IterDRAG**：一种典型的迭代式检索方法（每步必检索）。
*   **评估指标**：准确率（Accuracy）和平均 Token 消耗量（Avg. Tokens）。

#### 4. 资源与算力
*   **模型底座**：所有实验均基于 **LLaMA-3.1-8B-Instruct**。
*   **算力说明**：论文**未明确说明**具体的 GPU 型号、数量及训练/推理总时长。由于该方法主要关注推理侧的智能体调度，不涉及模型重训练，其开销主要体现在推理时的 API 调用或本地推理次数。

#### 5. 实验数量与充分性
*   **实验规模**：
    *   在 3 个主流数据集上进行了主实验对比。
    *   进行了关于**迭代深度 $N$** 的参数敏感性实验（$N$ 从 1 取到 8）。
    *   统计了 **Think %**（思考动作占比）随迭代次数的变化趋势。
*   **充分性与客观性**：实验设计较为客观，通过对比 IterDRAG 证明了 ACE 在性能提升的同时能有效控制 Token 成本。通过 $N=1$ 的消融实验证明了 ACE 退化后等同于标准 RAG，验证了其动态决策的有效性。

#### 6. 主要结论与发现
*   **性能卓越**：ACE 在所有数据集上均刷新了 SOTA。在 HotpotQA 上，准确率比标准 RAG 高出 23% 以上。
*   **效率提升**：相比于暴力迭代的 IterDRAG，ACE 在 MultiHop-RAG 上节省了约 42% 的 Token，证明了“思考”动作能有效替代不必要的检索。
*   **动态平衡**：随着迭代轮数 $N$ 增加，智能体选择 THINK 的比例显著上升（最高超过 80%），说明模型学会了先检索基础信息，再通过内部推理演进上下文。
*   **边际效应**：迭代次数并非越多越好，过多的步骤可能引入干扰，每个数据集存在特定的最优迭代深度。

#### 7. 优点与亮点
*   **策略灵活性**：打破了 RAG 固有的“检索-生成”僵化模式，引入了类似人类的决策逻辑。
*   **上下文质量高**：通过 THINK 动作提炼中间逻辑，保持了上下文的简洁性和高相关性，有效缓解了长文本干扰问题。
*   **成本可控**：在复杂任务中通过减少无效检索，实现了精度与成本的良好平衡。

#### 8. 不足与局限
*   **计算延迟**：虽然 Token 数少于 IterDRAG，但多智能体投票和多轮迭代本身会带来多次 LLM 调用，可能增加端到端的响应延迟。
*   **超参数依赖**：最优迭代轮数 $N$ 在不同数据集上表现不一，缺乏一种自动停止（Early Stopping）的机制来动态确定何时结束演进。
*   **模型依赖**：实验仅基于 LLaMA-3.1-8B，未探讨更小规模模型或闭源模型（如 GPT-4）在 ACE 框架下的决策表现差异。

（完）
