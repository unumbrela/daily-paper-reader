---
title: "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models"
title_zh: SGHA-Attack：面向视觉语言模型可迁移定向攻击的语义引导层次化对齐
authors: "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao"
date: 2026-02-02
pdf: "https://arxiv.org/pdf/2602.01574v1"
tags: ["query:sr-llm"]
score: 6.0
evidence: 视觉语言模型的对齐与对抗鲁棒性
tldr: 针对视觉语言模型（VLM）在目标迁移攻击中容易过拟合替代模型且忽略中间层语义的问题，本文提出了SGHA-Attack框架。该方法利用文本生成图像模型构建语义参考池，并通过多层级对齐策略，在全局、空间及跨模态子空间上同步中间特征，显著提升了对抗样本在不同黑盒模型间的迁移成功率和鲁棒性。
motivation: 现有的目标迁移攻击过度依赖单一参考点且仅关注顶层对齐，导致模型在异构VLM间的迁移效果受限。
method: 通过预训练文生图模型生成多样化参考池，并实施涵盖中间层全局、空间特征以及跨模态子空间的语义引导层级对齐。
result: 在开源及商业黑盒VLM上的实验证明，SGHA-Attack的攻击迁移性优于现有方法，且能有效对抗多种防御手段。
conclusion: 强化中间层语义一致性和利用多样化参考样本是提升视觉语言模型目标迁移攻击性能的关键。
---

## 摘要
大型视觉语言模型（VLMs）容易受到基于迁移的对抗扰动的影响，攻击者可以在替代模型上进行优化，从而操纵黑盒 VLM 的输出。先前的定向迁移攻击通常依赖单一参考物并强调最终层对齐，这导致模型过度拟合于特定替代模型的嵌入空间，未能充分利用中间语义，并降低了在异构 VLM 之间的迁移效果。为了解决这一问题，我们提出了 SGHA-Attack，这是一个语义引导的层次化对齐框架，它采用多个目标参考物并强制执行中间层的一致性。具体而言，我们通过对以目标提示词为条件的冻结文本生成图像模型进行采样，生成一个具有视觉基础的参考池，然后精心选择替代模型下语义相关性最高的 Top-K 个锚点，形成加权混合以提供稳定的优化引导。基于这些锚点，SGHA-Attack 通过在多个深度上对齐全局和空间粒度的中间视觉表示，并在共享潜子空间中同步中间视觉和文本特征，从而在最终投影前提供早期跨模态监督，将目标语义注入整个特征层次结构。在开源和商业黑盒 VLM 上的广泛实验表明，SGHA-Attack 比以往的方法具有更强的定向迁移性，并且在预处理和净化防御下保持鲁棒。

## Abstract
Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.