---
title: Long-Chain Reasoning Distillation via Adaptive Prefix Alignment
title_zh: 通过自适应前缀对齐实现长链推理蒸馏
authors: "Zhenghao Liu, Zhuoyang Wu, Xinze Li, Yukun Yan, Shuo Wang, Zulong Chen, Yu Gu, Ge Yu, Maosong Sun"
date: 2026-01-15
pdf: "https://arxiv.org/pdf/2601.10064v1"
tags: ["query:sr-llm"]
score: 10.0
evidence: 蒸馏长链条推理轨迹
tldr: 针对大模型长链推理蒸馏中，教师模型生成的推理路径过于冗长复杂导致学生模型难以吸收的问题，本文提出了P-ALIGN框架。该框架通过自适应前缀对齐技术，动态截断教师推理路径中冗余或不确定的后缀，仅保留简洁且关键的前缀信息来监督学生模型。实验证明，该方法在多个数学推理基准测试中显著优于现有基准，有效提升了小规模模型的推理性能。
motivation: 教师模型生成的长推理链条往往包含冗余和复杂结构，超出了学生模型的学习能力，导致监督信号与学习容量不匹配。
method: 提出P-ALIGN框架，通过自适应判断后缀的简洁性与充分性来截断推理路径，利用高质量前缀引导学生模型进行对齐学习。
result: "在多个数学推理基准测试中，P-ALIGN的效果比所有基准方法提升了3%以上。"
conclusion: 通过自适应前缀对齐可以提供更有效的监督信号，避免冗余信息对蒸馏过程的负面影响，显著增强小模型的推理能力。
---

## 摘要
大语言模型（LLMs）展现出了卓越的推理能力，尤其是在解决复杂的数学问题方面。最近的研究表明，蒸馏长推理轨迹可以有效增强小规模学生模型的推理性能。然而，教师模型生成的推理轨迹往往过长且结构复杂，导致学生模型难以学习。这种不匹配造成了所提供的监督信号与学生模型学习能力之间的差距。为了应对这一挑战，我们提出了前缀对齐蒸馏（P-ALIGN）框架，通过自适应前缀对齐充分利用教师模型的思维链（CoT）进行蒸馏。具体而言，P-ALIGN 通过判断剩余后缀是否简洁且足以引导学生模型，从而自适应地截断教师生成的推理轨迹。随后，P-ALIGN 利用教师生成的前缀来监督学生模型，促进有效的前缀对齐。在多个数学推理基准测试上的实验表明，P-ALIGN 的表现优于所有基线模型 3% 以上。进一步的分析表明，P-ALIGN 构建的前缀提供了更有效的监督信号，同时避免了冗余和不确定推理组件的负面影响。所有代码均可在 https://github.com/NEUIR/P-ALIGN 获取。

## Abstract
Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.