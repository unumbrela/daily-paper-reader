Title: Long-Chain Reasoning Distillation via Adaptive Prefix Alignment

URL Source: https://arxiv.org/pdf/2601.10064v1

Published Time: Fri, 16 Jan 2026 01:36:34 GMT

Number of Pages: 18

Markdown Content:
# Long-Chain Reasoning Distillation via Adaptive Prefix Alignment 

Zhenghao Liu 1*â€  , Zhuoyang Wu 1*, Xinze Li 1, Yukun Yan 2â€ ,Shuo Wang 2, Zulong Chen 3, Yu Gu 1, Ge Yu 1, Maosong Sun 21School of Computer Science and Engineering, Northeastern University, China 

> 2

Department of Computer Science and Technology, Institute for AI, Tsinghua University, China 

> 3

Alibaba Group, China 

Abstract 

Large Language Models (LLMs) have demon-strated remarkable reasoning capabilities, par-ticularly in solving complex mathematical prob-lems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale stu-dent models. However, teacher-generated rea-soning trajectories are often excessively long and structurally complex, making them diffi-cult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose 

Prefix-ALIGN ment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix align-ment. Specifically, P-ALIGN adaptively trun-cates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Ex-periments on multiple mathematical reason-ing benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Fur-ther analysis indicates that the prefixes con-structed by P-ALIGN provide more effective supervision signals, while avoiding the neg-ative impact of redundant and uncertain rea-soning components. All code is available at 

https://github.com/NEUIR/P-ALIGN .

1 introduction 

Large Language Models (LLMs) have exhibited impressive reasoning capabilities across complex tasks, especially in solving mathematical prob-lems (Brown et al., 2020; Zhang et al., 2022). By adopting the long-form chain-of-thoughts (CoTs) paradigm (DeepSeek-AI et al., 2025), LLMs can  

> *

indicates equal contribution.  

> â€ 

indicates corresponding author. Sufficient                                                                              

> & Concise
> Verbose
> Content
> Insufficient
> Prefix
> Question: How many non -negative integer pairs (ð‘¥ +
> ð‘¦ ) satisfy ð‘¥ 2+ð‘¦ 2+ð‘¥ +ð‘¦ =2ð‘¥ð‘¦ ?
> Reasoning:
> First, we analyze the questionâ€¦ Then,
> rewrite the equation as ð± âˆ’ð² ðŸ +ð± +
> ð² =ðŸ” ,ð =ð± âˆ’ð² â€¦convert to: ð± +ð² =
> ðŸ” âˆ’ð ðŸ â‰¥ðŸŽ ð âˆˆ{âˆ’ðŸ ,âˆ’ðŸ ,ðŸŽ ,ðŸ ,ðŸ }â€¦I will
> check each answer againâ€¦
> Prefix (Fix -Ratio):
> First, we analyze the question and
> to find the pairs. [Core Missing]
> Adaptive Truncation
> Prefix (P -ALIGN):
> ðŒ ð­ðžðšðœð¡ðžð«
> First, we analyzeâ€¦ Then, rewrite
> as ð± âˆ’ð² ðŸ +ð± +ð² =ðŸ” ,ð =ð± âˆ’ð² â€¦
> ð± +ð² =ðŸ” âˆ’ð ðŸ â‰¥ðŸŽ ð âˆˆ{âˆ’ðŸ ,
> âˆ’ðŸ ,ðŸŽ ,ðŸ ,ðŸ }.
> ðŒ ð¬ð­ð®ððžð§ð­
> Fix -Ratio Truncation
> ðŒ ð¬ð­ð®ððžð§ð­
> Prefix Alignment
> To continue solving the question
> based on the retained prefixâ€¦
> Continuation:
> [Core Retained]

P-ALIGN 

> Self -
> Judging
> ï¼ˆOurs ï¼‰
> â‘ 
> â‘¡

Figure 1: The Framework of Our P-ALIGN Model. Later steps in traces are often more uncertain and harder for the student to learn. P-ALIGN therefore performs adaptively selecting a sufficient prefix and aligning stu-dent supervision to it for more effective distillation. 

engage in deeper, iterative thinking, which sub-stantially improves their performance on challeng-ing reasoning tasks (Yang et al., 2025; Chen et al., 2025a). Despite these successes, enabling small-scale models to generate outputs that exhibit such reflective and structured reasoning behavior re-mains a significant challenge when tackling com-plex mathematical problems (Luo et al., 2025a). Recent work has explored distilling reasoning abilities from long-form CoTs to improve the per-formance of small-scale models on mathematical reasoning benchmarks (Ye et al., 2025; DeepSeek-AI et al., 2025). In these approaches, small-scale models are treated as student models and trained via Supervised Fine-Tuning (SFT) on full long-

> arXiv:2601.10064v1 [cs.CL] 15 Jan 2026

form reasoning trajectories generated by Reason-ing Language Models (RLMs) (Hsieh et al., 2023). However, such intricate and reflective reasoning trajectories often exceed the learning capacity of student models (Li et al., 2025; Anonymous, 2025), making it difficult for them to internalize the under-lying reasoning patterns and, in some cases, even leading to degraded reasoning performance (Luo et al., 2025a). To address this issue, several re-cent studies attempt to refine teacher-generated tra-jectories using LLM-based refinement techniques to reduce redundancy and improve the compat-ibility of distilled reasoning with student mod-els (Wu et al., 2025; Zeng et al., 2025b). Neverthe-less, refinement-based methods introduce an addi-tional and weakly constrained transformation stage, where the refined trajectories are not explicitly op-timized to match the capacity of the student model. To better exploit teacher-generated reasoning tra-jectories, existing works (Ji et al., 2025; Chen et al., 2025b) incorporate prefixes into the SFT process, as these prefixes help preserve structural knowl-edge while mitigating uncertainty and redundancy that tend to emerge in later reasoning steps. How-ever, as shown in Figure 1, using a fixed ratio for prefix truncation renders this strategy suboptimal: prefixes may still fail to retain sufficient informa-tion for complex tasks. In this paper, we introduce the Adaptive Prefix-

ALIGN ment reasoning distillation method (P-ALIGN), which adaptively truncates prefixes from teacher-generated reasoning trajectories and opti-mizes student models to align with the retained teacher prefixes. Specifically, P-ALIGN enables the student model to adaptively assess the suffi-ciency of a truncated prefix and identify a minimal sufficient prefix boundary via binary search, ensur-ing that the retained prefix is both effective and concise. Building on this adaptive prefix truncation strategy, P-ALIGN further performs prefix-based alignment by treating the retained teacher prefix as prior reasoning context and guiding the student to generate a complete reasoning trajectory, which serves as the supervision signal for SFT. Our experimental results demonstrate that P-ALIGN consistently outperforms all baselines across a range of mathematical tasks, highlight-ing its effectiveness in distilling long-form reason-ing into student models. Further analysis reveals that reasoning content from later steps introduces increased uncertainty into student models, which in turn degrades distillation performance. There-fore, to fully exploit the teacher-generated reason-ing trajectories, P-ALIGN adaptively applies prefix truncation to the reasoning trajectories, retaining more informative content while reducing unneces-sary uncertainty and bias when guiding the student model on problems of varying difficulty. In addi-tion, the prefix alignment strategy in P-ALIGN fur-ther demonstrates its effectiveness under the SFT setting by enabling the student model to generate complete reasoning chains autonomously. This de-sign avoids overfitting to incomplete or overly short reasoning patterns, a common issue in prefix-based SFT methods (Ji et al., 2025). 

2 Reated Work 

Large Language Models (LLMs) have demon-strated strong capabilities in mathematical reason-ing (Cobbe et al., 2021; Hendrycks et al., 2021). Chain-of-Thought (CoT) (Wei et al., 2022) elic-its step-by-step reasoning and has been shown to substantially improve performance on mathemat-ical reasoning benchmarks (Li et al., 2023; Qin et al., 2023; Luo et al., 2023). To enhance small-scale student models, prior work commonly lever-ages a stronger LLM as a teacher to generate long-form reasoning trajectories and uses these trajec-tories as supervision signals to fine-tune student models (DeepSeek-AI et al., 2025; Yang et al., 2025). However, the uncertainty and redundancy of teacher-generated long-form CoTs often exceed the capacity of student models, making it challeng-ing to faithfully distill such reasoning capabilities into them (Chen et al., 2025a; Li et al., 2025). To mitigate the challenges of distilling long-form reasoning trajectories, recent work has increasingly focused on constructing more efficient and higher-quality SFT data. One line of research investi-gates quality-based data selection, where sophis-ticated criteria are employed to identify a small set of high-quality examples for training (Ye et al., 2025; Muennighoff et al., 2025). Another line of work refines teacher-generated reasoning trajec-tories through prompting strategies or structural editing, producing shorter yet still effective trajec-tories (Xu et al., 2025; Wu et al., 2025; Jin et al., 2025). However, these selection and refinement approaches typically involve multiple sequential stages, such as iterative improvements (Zelikman et al., 2022) and verification, to refine or select high-quality long-form reasoning trajectories. Unlike these sophisticated CoT selection or re-finement methods, recent studies have proposed unsupervised approaches to fully exploit the ef-fectiveness of teacher-generated reasoning trajec-tories. Specifically, Ji et al. (2025) observe that the prefixes of reasoning trajectories are typically consistent; they then extract such prefixes and mix them with full CoTs as supervision during SFT. Subsequent studies (Chen et al., 2025b; Sun et al., 2025) further explore the use of randomly truncated prefixes from teacher-generated reasoning trajec-tories to either enhance reasoning performance or improve trajectory sampling efficiency in RL train-ing. However, employing a fixed truncation ratio lacks adaptability across problem difficulties, often failing to preserve reasoning information that is both sufficient for complex tasks and concise for simpler ones. 

3 Methodology 

In this section, we present Prefix-ALIGN ment dis-tillation (P-ALIGN), a framework designed to dis-till long-chain reasoning capabilities into small-scaled models efficiently. As shown in Figure 2, we first describe the standard supervised fine-tuning (SFT) for reasoning distillation, along with its truncated-prefix SFT variant (Sec. 3.1). We then introduce our P-ALIGN model (Sec. 3.2), in which the student model evaluates reasoning trajectories and uses binary search to truncate them into min-imal, sufficient prefixes aligned with its needs to solve the given problem. Then P-ALIGN treats the retained prefixes as prior reasoning context to guide prefix-aligned generation of full reasoning sequences for effective supervision. 

3.1 Distilling Long-Chain Reasoning via Supervised Fine-Tuning 

Given a mathematical problem q, a Large Lan-guage Model (LLM) is typically prompted with a problem-solving instruction ( Instruct QA ) to gen-erate a complete solution. In reasoning distilla-tion, we treat the small-scale model as the student model Mstudent and employ different Supervised Fine-Tuning (SFT) strategies to optimize it using signals derived from a stronger teacher model. 

Distillation from Long-Form CoT. To dis-till the reasoning capability of a teacher model 

Mteacher into a student model Mstudent , existing approaches (Wang et al., 2023; Yin et al., 2025; DeepSeek-AI et al., 2025) typically use the reason-ing trajectories generated by Mteacher as supervi-sion signals for SFT. Specifically, given a question 

q, we adopt a Reasoning Language Model (RLM), such as DeepSeek-R1 (DeepSeek-AI et al., 2025), as the teacher model and prompt it to produce a long-form reasoning response R:   

> R=Mteacher (Instruct QA (q)) .(1)

We then collect the queryâ€“response pairs as the training dataset D = {(q1, R 1), . . . , (qn, R n)},which consists of n examples. The training ob-jective minimizes the negative log-likelihood of the teacher-generated reasoning trajectory Ri:      

> J=âˆ’
> nX
> i=1
> |Ri|
> X
> t=1
> log P(Rit|Ri<t ,Instruct QA (qi); Mstudent ),
> (2)

where |Ri| denotes the token number of the gen-erated reasoning trajectory Ri. Although dis-tilling long-form CoTs from the teacher model substantially improves LLM problem-solving ac-curacy (Hsieh et al., 2023), the student model may struggle to effectively learn from these long-form reasoning trajectories (Luo et al., 2025b; Gudibande et al., 2023). This limitation has mo-tivated recent efforts toward constructing higher-quality SFT datasets (Wettig et al., 2024). 

SFT with Truncated Prefixes. To further im-prove the quality of supervision signals, recent stud-ies (Ji et al., 2025; Chen et al., 2025b) demonstrate that incorporating truncated prefixes during SFT can enhance model performance, since different solution paths often share a common initial rea-soning trajectory. Accordingly, they construct a prefix-truncated dataset DPrefix to encourage the student model to better learn from the early stages of reasoning trajectories, which is then mixed with the original training dataset D for SFT. To construct the prefix-truncated dataset DPrefix ,the Mteacher -generated reasoning trajectory Ri is truncated using the function Truncate (Â·):    

> ËœRi=Truncate (Ri, Î» Â· | Ri|),(3)

where Î» âˆˆ (0 , 1) controls the truncation ratio and Truncate (Â·) retains the first Î» Â· | Ri| tokens of Ri.The resulting prefix ËœRi is directly used as the super-vision signal for training. We then construct the pre-fix SFT dataset DPrefix = {(q1, ËœR1), . . . , (qn, ËœRn)}

by pairing each input query qi with its correspond-ing truncated reasoning prefix. However, such ratio-based truncation strategies may still discard nec-essary information from the reasoning trajectories, potentially leading to incomplete or incorrect rea-soning outcomes. Self -Judging 

> Question ï¼š
> A train leaves
> City A at 8:00
> AM, traveling
> at a constant
> speed of 60
> km/h.
> Another train
> leaves City B
> (180 km away)
> at 9:00 AM,
> heading
> toward City
> what time will
> the two trains
> meet?

Task Definition  

> Question Excess Reflection
> Skewed Thinking
> SFT

Prefix Alignment        

> Answer:
> 9:48 AM
> CoT ðŒ ð¬ð­ð®ððžð§ð­
> ðŒ ð¬ð­ð®ððžð§ð­
> ðŒ ð­ðžðšðœð¡ðžð«
> Concatenate
> Long -CoT : <think> First, Train A starts
> at 8:00 AM â€¥â€¥â€¥ Second, Relative speed
> = 60 + 90 = 150 km/h â€¥â€¥â€¥ Emm , Let me
> check â€¥Aha , I will revise my previous
> answer â€¥â€¥â€¥â€¥ Wait, these steps maybe
> </think> The final answer is : 9:48 AM.
> ...  â€¦...
> ...
> ð‘† 1
> ð‘† 1ð‘† 2ð‘† 3ð‘† ð‘ ð‘† ð‘ +1ð‘† ð‘š

â€¦ â€¦ ð‘† ð‘š âˆ’1

> ð‘† ð‘š
> ð‘† ð‘
> ð‘† 1
> ð‘† 2
> Binary Search
> Judge Prefix
> ENOUGH or NOT
> ð‘† 2
> ð‘† ð‘
> ð‘† ð‘š
> Minimal &
> Sufficient

Adaptive Truncation â‘  â‘¡ 

> ...
> Search More Supervision Fine -Tuning
> Prefix
> Missing Information
> Limited Learning
> Prefix Completion
> Supervision Signal :
> <Prefix > â€¦ </ Prefix> + CoT

Figure 2: Illustration of P-ALIGN Model. 

3.2 Adaptive Prefix Alignment based Reasoning Capability Distillation 

To further enhance the distillation of the reason-ing capability from Mteacher , P-ALIGN introduces an adaptive prefix alignment strategy, which en-courages the student model Mstudent to dynami-cally truncate prefixes from reasoning trajectories. Specifically, P-ALIGN prompts Mstudent to assess the sufficiency of a prefix for truncation, while en-suring minimality so as to avoid the introduction of redundant or noisy information. Subsequently, the student model Mstudent is optimized via SFT to align with the truncated prefixes generated by the teacher model Mteacher .

3.2.1 Efficient Minimal Prefix Truncation via Binary Search 

To adaptively truncate prefixes from reasoning tra-jectories, P-ALIGN prompts the student model 

Mstudent to progressively read prefix sentences from a teacher-generated reasoning trajectory Ri

and determine whether the truncated prefix con-tains sufficient information to solve the problem, while maintaining a minimal length. To enable efficient identification of such a prefix, we also em-ploy a binary search algorithm to locate a tailored truncation point. 

Prefix Evaluation by Self-Judging. For the i-th reasoning trajectory Ri generated by the teacher model Mteacher , P-ALIGN first segments Ri into 

m sentences: 

Ri = {ri

> 1

, r i

> 2

, . . . , r im}. (4) 

These sentences serve as the basic units for pre-fix truncation, rather than individual tokens (Chen et al., 2025b), in order to preserve the semantic completeness of the truncated prefixes. We then prompt the student model Mstudent to self-evaluate whether the current truncated prefix ËœRi contains sufficient information to solve the problem qi:

L = Mstudent (Instruct Eval (qi, ËœRi)) , (5) 

where Instruct Eval denotes an evaluation in-struction that guides Mstudent to make a suffi-ciency judgement. The judgment label L âˆˆ{ENOUGH , NOT_ENOUGH } represents the predicted sufficiency of the current prefix ËœRi.

Binary Search for Efficient Truncation. To efficiently identify the minimal sufficient trunca-tion point for qi, we perform binary search over the sentence sequence Ri = {ri

> 1

, r i

> 2

, . . . , r im} to locate the shortest prefix that satisfies the sufficiency cri-terion. In the binary search procedure, the search boundaries â„“ and r are initialized as â„“ = 1 and 

r = m. The truncation point p is selected as the midpoint of the current interval: 

p =

 â„“ + r

2



. (6) 

We truncate Ri to its first p sentences to obtain the candidate prefix ËœRi = Ri

> 1: p

. If the evaluation result is ENOUGH , the current truncation point p is deemed feasible, and we continue searching for a shorter sufficient prefix by updating the right boundary to 

r = p. Conversely, if the label is NOT_ENOUGH , the prefix lacks sufficient information, and we extend the search to the right half by setting â„“ = p + 1 .Formally, the boundary update rule is defined as:           

> (â„“, r ) =
> (
> (â„“, p ),if L=ENOUGH ,
> (p+ 1 , r ),if L=NOT_ENOUGH .(7)

This binary search process is repeated on the up-dated interval [â„“, r ] by recomputing the midpoint in Eq. 6 and adjusting the boundaries according to Eq. 7. The search terminates when â„“ = r, yielding the final truncation point pâˆ— that is equal to â„“. The resulting minimal prefix ËœRi = Ri 

> 1: pâˆ—

thus consists of the first pâˆ— sentences of Ri. For clarity, we pro-vide a complete procedure of this binary search in Appendix A.7. 

3.2.2 Prefix-based Alignment for Effective Supervised Fine-Tuning 

To better align the student model with the re-tained prefixes ËœRi introduced in Sec. 3.2.1, we first collect all query-prefix pairs into a dataset 

DPrefix = {(q1, ËœR1), . . . , (qn, ËœRn)}. Based on this dataset, we further complete each retained prefix into a full reasoning trajectory, thereby construct-ing the alignment dataset Dalign for training the student model Mstudent .Specifically, for each pair (qi, ËœRi), an instruction Instruct Align prompts the student model Mstudent to condition on ËœRi as prior reasoning context and then continue the reasoning process to generate a complete reasoning trajectory that is consistent with the given prefix:     

> yi=Mstudent (Instruct Align (qi,ËœRi)) .(8)

To enforce prefix-based alignment during SFT, we concatenate the retained prefix ËœRi with the student-generated continuation yi as the supervision signal. To ensure the correctness of the constructed super-vision signals, we conduct the DAlign dataset by retaining only those samples whose final answers match the ground-truth answer ai

> âˆ—

:              

> Dalign ={(qi,ËœRiâŠ•yi)|1â‰¤iâ‰¤n, Ans (yi) = ai
> âˆ—},(9)

where Ans (Â·) denotes an answer extraction func-tion. The resulting dataset Dalign is then used for fine-tuning the student model Mstudent using Eq. 2, enabling the student model to align its reasoning prefixes with those of the teacher model. 

4 Experimental Methodology 

This section first introduces the datasets, baselines, and evaluation metrics, and then details the imple-mentation settings used in our experiments. 

Dataset. We construct the training data using the training split of the s1K-1.1 dataset (Muen-nighoff et al., 2025), which contains high-quality solution traces generated by the DeepSeek-R1 model (DeepSeek-AI et al., 2025). For evalu-ation, we consider four mathematical problem datasets that span a wide range of difficulty lev-els. MATH500 (Godahewa et al., 2021) is a bench-mark composed of competition-level mathemat-ics problems with varying degrees of complexity. AIME (AIM, 2025) and AMC (AMC, 2025) assess mathematical problem-solving capabilities across arithmetic, algebra, counting, geometry, number theory, probability, and other topics in secondary school mathematics. 

Baselines. We compare P-ALIGN against both zero-shot and SFT-based baselines. In the zero-shot setting, the model is prompted with the question and directly generates a solution with-out task-specific fine-tuning. We further include three SFT baselines: SFT (Label), SFT (Long-CoT) (DeepSeek-AI et al., 2025), and UPFT (Ji et al., 2025). SFT (Label) trains the student model using ground-truth labels provided in the dataset. Following prior work (Wang et al., 2024), SFT (Long-CoT) fine-tunes the student model using full-length reasoning traces produced by the teacher model. In addition, UPFT selects prefixes based on self-consistency and conducts training on truncated prefixes, using the first 32 tokens in accordance with the original experimental setup. 

Evaluation Metric. Following prior work (Re-ich et al., 2010; Zhang et al., 2024), we evaluate model performance using Pass@1 and Pass@3. Pass@1 measures the accuracy of a single gener-ated answer, while Pass@3 denotes the proportion of problems for which at least one of three sampled outputs matches the ground-truth answer. 

Implementation Details. All experiments em-ploy Qwen2.5-7B-Instruct (Yang et al., 2024) and Qwen3-8B (Yang et al., 2025) as student mod-els, with DeepSeek-R1 (DeepSeek-AI et al., 2025) serving as the teacher model. We train the mod-els for 3 epochs with a learning rate of 5 Ã— 10 âˆ’5

and adopt LoRA (Hu et al., 2022) for parameter-efficient fine-tuning. Our P-ALIGN is implemented AIME25 AIME24 AMC12 MATH500 Avg.                                                                                                              

> Pass@1 Pass@3 Pass@1 Pass@3 Pass@1 Pass@3 Pass@1 Pass@3 Pass@1 Pass@3
> Qwen2.5-7B-Instruct
> Zero-Shot 6.67 10.00 10.00 20.00 40.76 49.70 69.80 84.80 31.81 41.13 SFT (Label) 3.33 6.67 3.33 10.00 32.92 34.62 62.20 71.00 25.45 30.57 SFT (Long-CoT) 10.00 20.00 10.00 26.67 48.19 56.63 75.60 86.60 35.95 47.68 UPFT (2025) 13.33 20.00 13.33 23.33 47.58 59.04 74.80 84.60 37.26 46.74 P-ALIGN 16.67 26.67 16.67 26.67 49.40 63.86 75.80 85.20 39.64 50.60 Qwen3-8B
> Zero-Shot 20.00 23.33 26.67 36.67 59.84 71.08 83.60 91.20 47.53 55.57 SFT (Label) 10.00 13.33 10.00 10.00 48.51 59.04 73.20 84.60 35.43 41.74 SFT (Long-CoT) 23.33 30.00 26.67 40.00 58.81 67.04 84.40 90.80 48.30 56.96 UPFT (2025) 23.33 26.67 23.33 36.67 61.74 75.38 85.60 92.40 48.50 57.78 P-ALIGN 23.33 33.33 30.00 36.67 66.27 77.11 84.80 92.80 51.10 59.98

Table 1: Overall Performance. 

based on TRL 1 and LLaMA Factory 2. Additional experimental details are provided in Appendix A.2, and the prompt templates are presented in Ap-pendix A.3. 

5 Evaluation Result 

In this section, we first evaluate the overall per-formance of P-ALIGN through the main experi-ence. Next, we conduct ablation studies to exam-ine the contribution of different components in P-ALIGN. Furthermore, we analyze the effectiveness of distilled models under different prefix trunca-tion strategies. Finally, we investigate positional effects in long-form reasoning trajectories, aiming to analyse how prefix-based designs contribute to improved model performance. The case study is conducted in Appendix A.8. 

5.1 Overall Performance 

As shown in Table 1, we compare P-ALIGN with several baseline models across different mathemat-ical reasoning tasks. The evaluation results indicate that P-ALIGN consistently outperforms all baselines, highlighting the effectiveness of our prefix-alignment frame-work for reasoning distillation. Notably, P-ALIGN achieves consistent improvements across different backbone models, demonstrating its generalization ability. Among different SFT strategies, SFT (La-bel) performs substantially worse than the zero-shot model, suggesting that supervising the student model solely with annotated ground-truth targets may lead to overfitting and consequently degrade its reasoning capability. In contrast, SFT (Long-CoT) consistently improves the reasoning perfor-mance of student models, as teacher-generated 

> 1https://github.com/huggingface/trl
> 2https://github.com/hiyouga/LLaMA-Factory

long-form reasoning trajectories provide richer and more informative patterns for imitation. Build-ing upon the distillation of teacher-generated long-form reasoning trajectories, P-ALIGN further con-structs higher-quality supervision signals by adap-tively truncating a minimal sufficient prefix through student self-judging and performing prefix-based alignment for distillation, yielding an additional 3% improvement over SFT (Long-CoT). Compared with UPFT, P-ALIGN achieves an improvement of more than 2%, demonstrating the crucial role of adaptive prefix alignment, rather than relying on brute-force prefix truncation with a fixed token budget for SFT. In addition, we report results for student models at different scales in Appendix A.6. 

5.2 Ablation Study 

This subsection conducts ablation studies to inves-tigate the contributions of different components in our P-ALIGN model. As shown in Table 2, we first evaluate two vari-ants, SFT w/ Teacher Prefix and SFT w/ Student CoT, to analyze the impact of different supervision signals. Specifically, SFT w/ Teacher Prefix only leverages the teacher-generated prefix, while SFT w/ Student CoT relies solely on student-generated reasoning conditioned on the teacher-generated pre-fix. We then conduct three additional variants to examine the effectiveness of the prefix truncation strategy in P-ALIGN. The P-ALIGN (InfoGain) model adopts InfoGain (Wang et al., 2025) as the criterion for prefix truncation. P-ALIGN w/o Bi-nary Search removes the binary search procedure for identifying the truncation point. In contrast, P-ALIGN w/o Adaptive Read directly feeds the entire teacher-generated trajectory to the student model without adaptive truncation. Compared with P-ALIGN, both SFT w/ Teacher Prefix and SFT w/ Student CoT decrease the an-AIME25 AIME24 AMC12 MATH500 Avg.                                                                                                                                                      

> Pass@1 Pass@3 Pass@1 Pass@3 Pass@1 Pass@3 Pass@1 Pass@3 Pass@1 Pass@3
> Qwen2.5-7B-Instruct
> SFT (Long-CoT) 10.00 20.00 10.00 26.67 48.19 56.63 75.60 86.60 35.95 47.68 P-ALIGN 16.67 26.67 16.67 26.67 49.40 63.86 75.80 85.20 39.64 50.60
> SFT w/ Teacher Prefix 13.33 20.00 13.33 23.33 46.71 54.22 70.20 82.80 35.89 45.09 SFT w/ Student CoT 13.33 16.67 13.33 20.00 40.55 56.63 71.60 83.80 34.70 44.28 P-ALIGN (InfoGain) 13.33 20.00 10.00 20.00 42.17 55.90 73.80 85.40 34.83 45.33 P-ALIGN w/o Adaptive Read 13.33 16.67 16.67 23.33 47.53 60.19 73.60 84.80 37.78 46.25 P-ALIGN w/o Binary Search 16.67 23.33 16.67 26.67 49.26 64.07 75.00 85.20 39.40 49.82
> Qwen3-8B
> SFT (Long-CoT) 23.33 30.00 26.67 40.00 58.81 67.04 84.40 90.80 48.30 56.96 P-ALIGN 23.33 33.33 30.00 36.67 66.27 77.11 84.80 92.80 51.10 59.98
> SFT w/ Teacher Prefix 16.67 30.00 26.67 33.33 60.24 72.29 83.60 92.20 46.80 56.96 SFT w/ Student CoT 16.67 30.00 23.33 30.00 60.04 71.08 84.60 91.20 46.16 55.57 P-ALIGN (InfoGain) 23.33 26.67 23.33 33.33 63.58 73.21 84.20 88.40 48.61 55.40 P-ALIGN w/o Adaptive Read 23.33 26.67 26.67 40.00 62.75 75.64 82.20 92.80 48.74 58.36 P-ALIGN w/o Binary Search 26.67 26.67 26.67 40.00 62.63 76.34 85.00 91.40 50.24 58.60

Table 2: Ablation Study. 

swering accuracy by more than 3%, highlighting the effectiveness of incorporating both supervision components. Each component contributes to con-structing higher-quality supervision, which helps distill the teacherâ€™s reasoning patterns into the stu-dent model. The main reason may lie in that the teacher prefix guides the student model to align with the teacherâ€™s reasoning process, while the student-generated CoT provides more complete and prefix-guided reasoning trajectories for SFT. We then ablate different prefix truncation strategies to further verify the effectiveness of our truncation procedure. The InfoGain-based variant determines prefix boundaries based on changes in information entropy (Wang et al., 2025). Although it improves over the vanilla LLM, it underperforms SFT (Long-CoT), demonstrating the advantage of using stu-dent self-judging to assess prefix sufficiency. Next, when directly truncating teacher-generated reason-ing trajectories (P-ALIGN w/o Adaptive Read), the performance of P-ALIGN decreases, indicating that reading the entire reasoning trajectories makes it difficult to localize a concise yet sufficient bound-ary. Finally, although P-ALIGN w/o Binary Search achieves performance comparable to P-ALIGN, it incurs substantially higher computational cost, re-sulting in more than 20 times higher latency than P-ALIGN when searching for the truncation point (More details are provided in Appendix A.5). 

5.3 Distillation Performance with Different Prefix Truncation Strategies 

In this section, we investigate the distillation perfor-mance under different prefix truncation strategies. As shown in Figure 3, we first compare fixed-ratio truncation to motivate the necessity of adaptive pre-fix selection, and then analyze the length and qual-ity of responses generated by models optimized using different prefix truncation strategies. 

Optimization with Fixed Prefix Ratios. To evaluate the effectiveness of our adaptive prefix truncation method in P-ALIGN, we compare P-ALIGN with fixed ratio-based truncation strategies, where the truncation ratio ranges from 10% to 90%. As shown in Figure 3(a), on competition-level benchmarks (AIME24&25), the performance of optimized LLMs generally improves as longer pre-fixes are retained. In contrast, on the relatively easier dataset MATH500, performance degrades when longer prefixes are used (Figure 3(b)). This suggests that, for difficult problems, longer rea-soning trajectories provide informative and effec-tive supervision signals for student model learn-ing. Conversely, for easier problems, extended reasoning trajectories may introduce the overthink-ing issue (Chen et al., 2024), which can mislead the student model and degrade its reasoning capa-bility (Luo et al., 2025a). Benefiting from adaptive prefix truncation, P-ALIGN consistently outper-forms models trained with fixed-ratio truncation strategies across different settings, highlighting the critical role of adaptive truncation in balancing in-sufficient and excessive prefixes to deliver more effective supervision signals. 

Distillated Models with Different Truncation Strategies. To more thoroughly evaluate the effec-tiveness of optimized student models under differ-ent truncation strategies, we further analyze both the response lengths and the quality of the gener-ated reasoning trajectories produced by these opti-mized student models. We first present the average lengths of reason-ing trajectories in Figure 3(c). Among all models, UPFT produces significantly shorter trajectories, 10 30 50 70 90 

> Truncation Ratio (%)
> 10
> 20
> 30
> Accuracy (%)
> P-ALIGN(Qwen2.5)
> P-ALIGN(Qwen3)
> Qwen2.5
> Qwen3

(a) AIME24&25. 10 30 50 70 90  

> Truncation Ratio (%)
> 65
> 75
> 85
> 95
> Accuracy (%)
> P-ALIGN(Qwen2.5)
> P-ALIGN(Qwen3)

(b) MATH500. Qwen2.5 Qwen3    

> 500
> 1500
> 2500
> 3500
> Output #Tokens
> 2670 2469
> 1458 1284
> 2258 2102
> SFT(Long-CoT)
> UPFT
> P-ALIGN

(c) Response Length. Qwen2.5 Qwen3  

> 20
> 30
> 40
> Win Rate (%)
> 24
> 27
> 29
> 33
> 47
> 41

(d) GLM Preference Rate. 

Figure 3: Performance of Distilled Student Models un-der Different Prefix Truncation Strategies. We com-pare models distilled using fixed prefix truncation ratios and P-ALIGN (Figures 3(a) and 3(b)), analyze their re-sponse lengths (Figure 3(c)), and evaluate CoT quality with GLM-4.5 as the judge (Figure 3(d)). 

indicating that naively using prefixes as supervi-sion signals may cause the student model to overfit superficial patterns. Compared with SFT (Long-CoT), P-ALIGN generates more concise reason-ing trajectories, demonstrating its effectiveness in mitigating the influence of redundant or uncertain reasoning content. We then employ GLM-4.5 (Fig-ure 3(d)) to evaluate the quality of reasoning trajec-tories generated by different models, with detailed evaluation prompts provided in Appendix A.4. The evaluation results show that P-ALIGN achieves the highest win rate, further confirming its effective-ness in providing higher-quality supervision that enables the student model to produce more concise and higher-quality reasoning trajectories. 

5.4 Effectiveness of Reasoning Trajectory Chunks at Different Positions 

As illustrated in Figure 4, we analyze the effective-ness of teacher-generated reasoning trajectories in supervising student models, with a particular fo-cus on evaluating the contributions of trajectory prefixes at different positions. As shown in Figure 4(a), we assess the uncer-tainty of teacher-generated reasoning regions at different positions using the student model. Specif-ically, we evenly divide the teacher-generated rea-soning trajectories into ten sequential chunks and compute the average token-level entropy score for 2 4 6 8 10 

> Chunk Position
> 0.4
> 0.8
> 1.2
> 1.6
> Token Entropy  Qwen2.5
> Qwen3

(a) Chunk Uncertainty at Dif-ferent Positions. Qwen2.5 Qwen3 

> 30
> 40
> 50
> Accuracy (%)
> 38.2
> 50.1
> 35.0
> 47.6
> 33.7
> 44.7
> Prefix
> Middle
> Suffix

(b) P-ALIGN Using Fixed Teacher-generated Prefix. 

Figure 4: Effectiveness of Chunks in Long-Form CoTs at Different Positions. Figure 4(a) shows the entropy scores of student models across different chunks at vary-ing positions. Figure 4(b) compares the performance of student models when using prefix, middle, or suffix segments as supervision. 

chunks at each position. A higher entropy score in-dicates greater uncertainty (Kendall and Gal, 2017). The results show that entropy increases as the chunk position moves toward the rear of the trajec-tory, indicating that later reasoning steps contain more uncertain content and potential exploratory trails. This observation is consistent with previous work (Ji et al., 2025). Furthermore, as shown in Fig-ure 4(b), we evenly partition the teacher-generated CoT into three segments: prefix, middle, and suffix, and directly use each segment to replace the adap-tively truncated prefix in P-ALIGN for optimizing the student model. The results demonstrate that conditioning on prefixes yields substantially better performance than using either the middle or suffix segments, suggesting that prefixes provide more stable and informative context than later parts of the reasoning trajectory. This finding further vali-dates the motivation of P-ALIGN to fully exploit prefixes for synthesizing higher-quality supervi-sion, thereby more effectively guiding the student model through SFT. 

6 Conclusion 

This paper proposes the Adaptive Prefix-

ALIGN ment reasoning distillation method (P-ALIGN), which adaptively truncates prefixes from teacher-generated reasoning trajectories and optimizes student models to align with the retained teacher prefixes. Our experimental results demon-strate that P-ALIGN consistently outperforms all baselines across multiple mathematical reasoning benchmarks, highlighting its effectiveness and robustness in distillating long-form reasoning into student models. Limitation 

Although P-ALIGN effectively constructs higher-quality SFT data, it still relies on powerful closed-source reasoning models to generate long-form reasoning chains, which incurs substantial com-putational overhead. Moreover, while adaptive prefix truncation proves effective, the self-judge process heavily depends on the judgment capabil-ity of the student model. This dependency may cause smaller-scale student models to become con-fused during self-judgment. Therefore, beyond self-information requirements, explicitly account-ing for prefix quality is also crucial for adaptive prefix truncation. 

References 

2025. AIME. aime problems and solutions. Accessed: September 23, 2025. 2025. AMC12. amc 12 problems and solutions. Ac-cessed: September 23, 2025. Anonymous. 2025. MORALE: Segment-guided dis-tillation framework for small reasoning models. In 

Submitted to ACL Rolling Review - May 2025 . Under review. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In 

Proceedings of NeurIPS .Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025a. Towards reasoning era: A survey of long chain-of-thought for reasoning large language models. ArXiv preprint .Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. ArXiv preprint .Xinjie Chen, Minpeng Liao, Guoxin Chen, Chengxi Li, Biao Fu, Kai Fan, and Xinggao Liu. 2025b. From data-centric to sample-centric: Enhancing llm rea-soning via progressive optimization. ArXiv preprint .Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word prob-lems. ArXiv preprint .DeepSeek-AI, Daya Guo, Dejian Yang, and Haowei Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv preprint .Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob J. Hyndman, and Pablo Montero-Manso. 2021. Monash time series forecasting archive. ArXiv preprint .Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, and Abhishek Kadian. 2024. The llama 3 herd of models. ArXiv preprint .Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary llms. ArXiv preprint .Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Ja-cob Steinhardt. 2021. Measuring mathematical prob-lem solving with the math dataset. ArXiv preprint .Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Dis-tilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Proceedings of ACL Findings , pages 8003â€“ 8017. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In Proceedings of ICLR .Ke Ji, Jiahao Xu, Tian Liang, Qiuzhi Liu, Zhiwei He, Xingyu Chen, Xiaoyuan Liu, Zhijie Wang, Junying Chen, Benyou Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025. The first few tokens are all you need: An efficient and effective unsupervised prefix fine-tuning method for reasoning models. Zhensheng Jin, Xinze Li, Yifan Ji, Chunyi Peng, Zheng-hao Liu, Qi Shi, Yukun Yan, Shuo Wang, Furong Peng, and Ge Yu. 2025. Recut: Balancing reasoning length and accuracy in llms via stepwise trails and preference optimization. ArXiv preprint .Alex Kendall and Yarin Gal. 2017. What uncertainties do we need in bayesian deep learning for computer vision? In Proceedings of NeurIPS , pages 5574â€“ 5584. Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. 2023. Query and response augmenta-tion cannot help out-of-domain math reasoning gen-eralization. ArXiv preprint .Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubra-manian, and Radha Poovendran. 2025. Small mod-els struggle to learn from strong reasoners. ArXiv preprint .Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-guang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-ardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 

ArXiv preprint .Renjie Luo, Jiaxi Li, Chen Huang, and Wei Lu. 2025a. Through the valley: Path to effective long CoT train-ing for small language models. In Proceedings of EMNLP , pages 4972â€“4992. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2025b. An empirical study of catas-trophic forgetting in large language models during continual fine-tuning. IEEE Transactions on Audio, Speech and Language Processing .Niklas Muennighoff, Zitong Yang, Weijia Shi, Xi-ang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori B Hashimoto. 2025. s1: Simple test-time scaling. In Proceedings of EMNLP , pages 20286â€“ 20332. Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. 2023. Cross-lingual prompt-ing: Improving zero-shot chain-of-thought reasoning across languages. In Proceedings of EMNLP , pages 2695â€“2709. Taly Reich, Alex Kaju, and Sam J Maglio. 2010. How to overcome algorithm aversion: Learning from mis-takes. ArXiv preprint .Yiliu Sun, Zicheng Zhao, Yang Wei, Yanfang Zhang, and Chen Gong. 2025. Well begun, half done: Rein-forcement learning with prefix optimization for llm reasoning. ArXiv preprint .Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. 2023. Making large language models better reasoners with alignment. ArXiv preprint .Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy Baldwin. 2024. Learning from failure: Integrating negative examples when fine-tuning large language models as agents. ArXiv preprint .Zihan Wang, Zihan Liang, Zhou Shao, Yufei Ma, Huangyu Dai, Ben Chen, Lingtao Mao, Chenyi Lei, Yuqing Ding, and Han Li. 2025. Infogain-rag: Boost-ing retrieval-augmented generation through docu-ment information gain-based reranking and filtering. In Proceedings of EMNLP .Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. In 

Proceedings of NeurIPS .Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. Qurating: Selecting high-quality data for training language models. ArXiv preprint .Zhuoyang Wu, Xinze Li, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Minghe Yu, Cheng Yang, Yu Gu, Ge Yu, and Maosong Sun. 2025. Enhancing long-chain rea-soning distillation through error-aware self-reflection. 

ArXiv preprint .Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of draft: Thinking faster by writing less. ArXiv preprint .An Yang, Anfeng Li, Baosong Yang, and et al. Be-ichen Zhang. 2025. Qwen3 technical report. ArXiv preprint .An Yang, Baosong Yang, Beichen Zhang, and Binyuan Hui. 2024. Qwen2.5 technical report. ArXiv preprint .Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. ArXiv preprint .Huifeng Yin, Yu Zhao, Minghao Wu, Xuanfan Ni, Bo Zeng, Hao Wang, Tianqi Shi, Liangying Shao, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2025. Marco-o1 v2: Towards widen-ing the distillation bottleneck for reasoning models. 

ArXiv preprint .Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-man. 2022. Star: Bootstrapping reasoning with rea-soning. Advances in Neural Information Processing Systems , pages 15476â€“15488. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. 2025a. Glm-4.5: Agen-tic, reasoning, and coding (arc) foundation models. 

ArXiv preprint .Wenhao Zeng, Yaoning Wang, Chao Hu, Yuling Shi, Chengcheng Wan, Hongyu Zhang, and Xiaodong Gu. 2025b. Pruning the unsurprising: Efficient code reasoning via first-token surprisal. ArXiv preprint .Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, and Xi Victoria Lin. 2022. Opt: Open pre-trained transformer language models. 

ArXiv preprint .Yongheng Zhang, Qiguang Chen, Jingxuan Zhou, Peng Wang, Jiasheng Si, Jin Wang, Wenpeng Lu, and Libo Qin. 2024. Wrong-of-thought: An integrated rea-soning framework with multi-perspective verification and wrong information. In Proceedings of EMNLP Findings , pages 6644â€“6653. Dataset Train Test           

> s1K-1.1 (2025) 1,000 -AIME25 (2025) -30 AIME24 (2025) -30 AMC23 (2025) -83 MATH500 (2021) -500

Table 3: Data Statistics.             

> Model Long-CoT Prefix P-ALIGN
> Qwen2.5-7B 9,291 4,531 5,453 Qwen3-8B 9,291 4,402 5,732 Llama3.2-3B 8,755 4,780 6,035

Table 4: Average Token Length Statistics of Training Dataset Across Different Student Models. We evalu-ate the average token lengths of the original long-form CoTs, the truncated prefixes, and the final trajectories used by P-ALIGN. 

A Appendix 

A.1 License 

We provide the licenses for the datasets used in P-ALIGN. s1K-1.1 is licensed under the MIT License, while MATH500, AIME24, AIME25, and AMC12 are licensed under the Apache License 2.0. All of these licenses permit the use of their data for academic purposes. 

A.2 Additional Implementation Details 

In this section, we provide the detailed data statis-tics in P-ALIGN. We build our training set us-ing 1,000 examples sampled from s1K-1.1 (Muen-nighoff et al., 2025) as the training dataset. For evaluation, we use the MATH500 with 500 exam-ples, AIME25 with 30 examples, AIME24 with 30 examples, and AMC12 with 83 examples. All statistics are shown in Table 3. To clarify the impact of P-ALIGN on supervision efficiency, we report average token length statistics across different student models in Table 4. Specif-ically, Long-CoT denotes the token length of the original teacher-generated reasoning traces, Prefix corresponds to the adaptive prefixes truncated by P-ALIGN, and P-ALIGN reports the token length of the final trajectories used for SFT. The results show that P-ALIGN substantially reduces the length of training data by retaining only concise and suf-ficient prefixes, which lowers training cost while maintaining or even improving model performance. 

A.3 Prompt Templates Used in P-ALIGN 

We detail the instruction prompts employed at dif-ferent stages of P-ALIGN. As shown in Figure 5,                                     

> Methods AIME24 AMC12 MATH500 Avg.
> Llama3.2-3B
> Zero-Shot 6.67 18.81 39.80 21.76 SFT (Long-CoT) 6.67 20.19 43.60 23.49 UPFT (2025) 10.00 26.50 44.60 27.03 P-ALIGN 16.67 30.12 48.40 31.73
> Qwen3-14B
> Zero-Shot 33.33 72.29 87.40 64.34 SFT 36.67 73.26 88.60 66.18 UPFT (2025) 36.67 74.69 89.60 66.99 P-ALIGN 43.33 73.26 89.20 68.60

Table 5: Performance of P-ALIGN on Models of Differ-ent Sizes. We provide additional results on Llama3.2-3B and Qwen3-14B to evaluate the generalization of P-ALIGN across different parameter scales.         

> Backbone Model P-ALIGN w/o Binary Search
> Qwen2.5-7B 7.4 144.6 Qwen3-8B 7.4 137.1 Llama3.2-3B 7.4 152.9

Table 6: Average Search Count for Binary Search Usage in Adaptive Prefix Truncation. 

the Instruct QA prompts the model to directly an-swer the given question, serving as the standard question-answering setting and providing the initial reasoning. Building on this, the prefix evaluation stage utilizes the Instruct Eval prompt, as illustrated in Figure 6, which guides the student model in assessing whether a given reasoning prefix con-tains sufficient information to solve the problem. Based on the retained prefix, the prefix-based align-ment stage applies the Instruct Align prompt shown in Figure 7, guiding the student model to continue generating a complete reasoning trajectory. 

A.4 Prompt Templates Used for Evaluating the Quality of CoTs 

As shown in Figure 8, we present the prompt tem-plates used to evaluate the quality of the CoTs synthesized by three methods: SFT (Long-CoT), UPFT, and P-ALIGN. The evaluation is conducted using GLM-4.5 (Zeng et al., 2025a) as the evalua-tor, which assesses each generated reasoning trace based on its logical completeness, reflective rea-soning behavior, conciseness, and overall organi-zation. We employ consistent prompt templates across all methods to ensure fair comparison and reproducibility. A.5 Verification of Binary Search Effectiveness in P-ALIGN 

To further verify the effectiveness of binary search in improving the efficiency of adaptive prefix trun-cation, we measure the average search count for determining whether binary search is used in prefix truncation. When binary search is not used, the sufficient condition is checked in a sentence-level sequential manner. As shown in Table 6, the aver-age search count without binary search is about 20 times higher than when binary search is used. This clearly demonstrates the effectiveness of binary search in improving efficiency. Notably, the total number of iterations for binary search is fixed at 

O(log 2 n), independent of both the student model and the results of the ENOUGH judgment. 

A.6 Additional Results with Different Student Models 

To further evaluate the scalability and robustness of P-ALIGN, we conduct additional experiments on models of different sizes, including the smaller Llama3.2-3B-Instruct (Grattafiori et al., 2024) and the larger Qwen3-14B (Yang et al., 2025). All training settings remain consistent with those used for the 7B-scale experiments. As shown in Ta-ble 5, P-ALIGN yields consistent improvements across model scales, demonstrating that the pro-posed framework generalizes effectively to both lower-capacity and higher-capacity models. 

A.7 The Case of Binary Search for Minimal Sufficient Prefix Selection 

As shown in Figure 9, P-ALIGN employs the bi-nary search procedure for adaptive prefix evalua-tion by self-judging, which improves both the ef-ficiency and precision of locating a minimal suf-ficient prefix. Specifically, when the self-judging evaluation returns ENOUGH , the current prefix al-ready contains sufficient information to solve the problem, and we continue searching for a shorter prefix to obtain a more concise and learnable su-pervision signal. Otherwise, if the evaluation re-turns NOT_ENOUGH , the current prefix is still insuffi-cient, and a longer prefix with richer information is required to support solving the problem. In this case, P-ALIGN identifies the minimal sufficient prefix with only 6 self-evaluation iterations. In contrast, the sentence-by-sentence sequential scan (P-ALIGN w/o Binary Search) requires 12 evalu-ations to reach the same boundary. This compari-son demonstrates that binary search substantially reduces the prefix-selection cost and avoids unnec-essary evaluation overhead. 

A.8 Case Study 

In this section, we present a detailed case study using two tables to illustrate the key steps and the qualitative benefits of P-ALIGN. As shown in Ta-ble 7, we illustrate the full pipeline of P-ALIGN, including the prefix truncation and prefix-based alignment process. Building on this, Table 8 com-pares the final response generated by P-ALIGN and baseline methods, highlighting the improved quality and clarity of P-ALIGN outputs. First, we examine a representative mathematical problem to analyze the behavior of P-ALIGN. As shown in Table 7, the key reasoning that directly supports the correct answer(â€œTherefore, P â‰¡ 109 (mod 125) â€) appears early in the teacherâ€™s reason-ing trajectory, within the first 30% of the full rea-soning length. The subsequent steps often contain more reflective and exploratory reasoning, which can be less learnable for the student model. In this case, the vanilla student model initially fails and produces an incorrect result (â€œthus the remainder is 999â€). In contrast, when conditioned on the re-tained prefix, the student can follow the preserved reasoning structure and complete the remaining rea-soning steps, ultimately arriving at the correct solu-tion (â€œTherefore, the remainder is boxed {109}.â€). This comparison highlights the advantage of prefix-aligned supervision in stabilizing the studentâ€™s rea-soning and helping it recover correct solutions that are otherwise difficult to obtain. Then, we compare student outputs under differ-ent baselines on the same question. As shown in Table 8, SFT (Label) provides little explicit reason-ing and directly outputs an incorrect answer. The vanilla model attempts step-by-step derivations, but fails to identify the key insight of applying Vietaâ€™s formula and instead attempts to solve the cubic equation explicitly, which is error-prone due to heavy algebraic computation. Furthermore, SFT (Long-CoT) performs a deeper analysis and recog-nizes the crucial insight early (â€œThat reminds me of Vietaâ€™s formulaâ€), realizing that the problem can be solved without computing each root. However, excessive reflection in later steps still leads to a cal-culation error, despite having found the correct ap-proach. Compared with these baselines, P-ALIGN distills the reasoning patterns through prefix-based supervision, enabling the student to follow the cor-rect high-level structure without overthinking. As a result, it produces a cleaner solution trajectory and reaches the correct final answer (â€œTherefore, the new volume is boxed {30}.â€). Please reason step by step, and put your final answer within boxed{}. 

*Problem*: {Problem} 

Instruction  for  Direct Question Answering Figure 5: The Prompt Templates Used for Direct Question Answering. Instruction  for  Prefix Evaluation by  Self -Judging 

You are a reasoning evaluator. 

Your task is to judge whether the  prefix already  includes the essential logical and computational steps required to 

confidently reach the correct answer easily. 

- Reply "[ENOUGH]" if the reasoning already contains the key logic and main transformations, and the remaining 

work is mostly routine. 

- Reply "[NOT_ENOUGH]" if any essential step or reasoning link is still missing, making it uncertain to reach the 

correct answer. 

Reply only with one word: either [ENOUGH] or [NOT_ENOUGH]. 

Do not add any explanation. 

Question: {question} 

Prefix: {prefix} 

Figure 6: The Prompt Templates Used for Prefix Evaluation. Instruction  for  Prefix -based Alignment 

Please continue from the prior knowledge and solve the problem step by step, and put your final answer within 

\\boxed{}. 

I will provide you with the prefix as the prior knowledge to assist you in solving the question. 

Question: {question} 

Prefix: {prefix} 

Figure 7: The Prompt Templates Used for Prefix-based Alignment. Instruction  for  GLM -based Evaluation 

You will be given one question and four reasoning chains A / B / C  .

All of them reach a correct answer, but the reasoning quality differs. Your task is to evaluate the  three  reasoning 

processes and select the best one. 

Evaluation Criteria: 

1.The reasoning should be complete and logically structured. 

2.It should include reflective or self -checking reasoning. 

3.No redundant or repetitive thinking steps. 

4.Reasoning should be concise, efficient, and well -organized. 

Question: {question} 

Chain A: {a}  Chain B: {b}  Chain C:{c} 

Only output the final choice without explanations, in the following format: 

Best Reasoning Chain: {A/B/C /} Figure 8: The Prompt Templates Used for GLM-based Evaluation. The Case of Binary Search for Adaptive Prefix Truncation 

[Starting Binary Search for Minimal Sufficient Prefix] 

Total sentences: 48 

Initial search interval: [1, 48] 

================================================================================ 

[Round 1 Evaluation] 

Current search interval: left=1, right=48 

Prefix sentences checked: 24/48 

Model output: [ENOUGH] 

Action : Current prefix is sufficient â†’ Try shorter prefix 

[Round 2 Evaluation] 

Current search interval: left=1, right=23 

Prefix sentences checked: 12/48 

Model output: [ENOUGH] 

Action : Current prefix is sufficient â†’ Try shorter prefix 

[Round 3 Evaluation] 

Current search interval: left=1, right=11 

Prefix sentences checked: 6/48 

Model output: [NOT_ENOUGH ]

Action: Current prefix is insufficient â†’ Increase prefix length 

[Round 4 Evaluation] 

Current search interval: left=7, right=11 

Prefix sentences checked: 9/48 

Model output: [NOT_ENOUGH] 

Action : Current prefix is insufficient â†’ Increase prefix length 

[Round 5 Evaluation] 

Current search interval: left=10, right=11 

Prefix sentences checked: 10/48 

Model output: [NOT_ENOUGH] 

Action : Current prefix is insufficient â†’ Increase prefix length 

[Round 6 Evaluation] 

Current search interval: left=11, right=11 

Prefix sentences checked: 11/48 

Model output: [NOT_ENOUGH] 

Action : Current prefix is insufficient â†’ Increase prefix length 

--------------------------------------------------------------------------------

[Binary Search Completed] 

Minimal sufficient prefix length: 12/48 

Prefix proportion: 0.2500 Figure 9: The Case of Adaptive Prefix Truncation via Binary Search. Case: The Case of Prefix-Alignment Distillation in P-ALIGN. 

Question: Find the remainder when 9 Ã— 99 Ã— 999 Ã— Â· Â· Â· Ã— 99 Â· Â· Â· 9| {z }

> 999 9â€™s

is divided by 1000 .

Ground-Truth: 109 

Vanilla Generation: To find the remainder of the question. . . we can use properties of modular arithmetic and the Chinese Remainder Theorem. . . Each number in the product is of the form 10 k âˆ’ 1. For any n â‰¥ 3, 10 n â‰¡ 0(mod 8) , hence 10 n âˆ’ 1 â‰¡ âˆ’ 1 (mod 8) . Thus the product satisfies 9 Ã— 99 Ã— 999 Ã— Â· Â· Â· â‰¡ (âˆ’1) 999 â‰¡ âˆ’ 1(mod 8) . . . Similarly, since each 10 k âˆ’ 1 for k â‰¥ 3 ends in 999, the product modulo 125 is claimed to be 

9 Ã— 99 Ã— 999 Ã— Â· Â· Â· â‰¡ (âˆ’1) 999 â‰¡ âˆ’ 1 (mod 125) . . . . and thus the remainder is 999 .

Original Long-Form Reasoning: Alright, so I have this problem here: I need to find the remainder when the product of numbers like 9, 99 , 999 , and so on, up to a number with 999 nines, is divided by 1000. Hmm, okay, letâ€™s. . . Therefore, the product is Q999 

> k=1

(10 k âˆ’ 1) . . . Since 10 â‰¡ 2 (mod 8) , so 10 k (mod 8) is 2k

(mod 8) . . . All terms from k = 3 onward are âˆ’1 (mod 8) .Therefore, P â‰¡ 5 (mod 8) . . . Therefore, (10 k âˆ’

1) â‰¡ âˆ’ 1 (mod 125) for k â‰¥ 3 . . . Therefore, P â‰¡ 109 (mod 125) . . . 

First, the problem is asking for the remainder of a very large product divided by 1000. When dealing with remainders, especially modulo 1000, it often helps to look at the problem modulo 8 and modulo 125 separately, since 1000 is 8 * 125 and 8 and 125 are coprime. Then, using the Chinese Remainder Theorem, combine the results. . . A number consisting of n 9s can be written as 10Ë†n - 1. . . . The number with k 9s is 10Ë†k - 1. Therefore, the product is Î 999 

> k=1

(10 k âˆ’ 1) . . . . We need to compute P mod 1000 . . . Compute P mod 8 . Since 10 â‰¡ 2 (mod 8), 10Ë†k (mod 8) is 2Ë†k (mod 8), so each term (10Ë†k - 1) (mod 8) is (2Ë†k - 1) (mod 8). . . . For k=1: (2Ë†1 - 1) = 1 (mod 8). For k=2: (4 - 1) = 3 (mod 8). For k â‰¥ 3: 2Ë†k â‰¡ 0 (mod 8), so (2Ë†k - 1) â‰¡ -1 (mod 8). . . . Therefore, 

P â‰¡ 5 (mod 8) . . . Alright, now onto modulo 125. . . . 10Ë†3 = 1000 â‰¡ 0 (mod 125). Therefore, for all k â‰¥ 3, 10Ë†k â‰¡ 0 (mod 125), so (10Ë†k - 1) â‰¡ -1 (mod 125). . . . Thus, P â‰¡ 9 Â· 99 Â· (âˆ’1) 997 (mod 125) . . . . 9*99 = 891, and 891 (mod 125) = 16, hence P â‰¡ âˆ’ 16 â‰¡ 109 (mod 125) . . . So summary so far: P â‰¡ 5 (mod 8) and P â‰¡ 109 (mod 125) .Now need to find x such that x â‰¡ 5 (mod 8) and x â‰¡ 109 (mod 125) . . . . Let x = 125m + 109. Then 125m + 109 â‰¡ 5 (mod 8). . . . This gives 5m â‰¡ 0 (mod 8), so m â‰¡ 0 (mod 8). Therefore, x = 1000n + 109, hence x â‰¡ 109 (mod 1000) . . . . . . 

Therefore, the boxed answer is boxed{109} .

Truncation Prefix: Alright, so I have this problem here: I need to find the remainder when the product of numbers like 9, 99 , 999 , and so on, up to a number with 999 nines, is divided by 1000. Hmm, okay, letâ€™s. . . Therefore, the product is Q999 

> k=1

(10 k âˆ’ 1) . . . Since 10 â‰¡ 2 (mod 8) , so 10 k (mod 8) is 2k (mod 8) . . . All terms from k = 3 

onward are âˆ’1 (mod 8) .Therefore, P â‰¡ 5 (mod 8) . . . Therefore, (10 k âˆ’ 1) â‰¡ âˆ’ 1 (mod 125) for k â‰¥ 3

. . . Therefore, P â‰¡ 109 (mod 125) .

Prefix-based Alignment: To continue answering the question based on existing prior knowledge . . . First, consider the product modulo 8. . . the product modulo 8 is 1Â·3Â·(âˆ’1) 997 â‰¡ âˆ’ 3 â‰¡ 5 (mod 8) .. . . Next, consider the product modulo 125 . . . the product modulo 125 is 9 Â· 99 Â· (âˆ’1) 997 â‰¡ âˆ’ 891 â‰¡ 16 Â· (âˆ’1) â‰¡ 109 (mod 125) . . . Finally, by the Chinese Remainder Theorem, solving x â‰¡ 5 (mod 8) and x â‰¡ 109 (mod 125) yields x = 125 m + 109 

with m â‰¡ 0 (mod 8) , hence x â‰¡ 109 (mod 1000) . Therefore, the remainder is boxed{109} .

Truncation Ratio: 0.27 

Table 7: Case Study. Text in red highlights erroneous parts in the reasoning and their underlying causes, text in 

green denotes correct reasoning content, and text in blue indicates the truncation point of the informative prefix. Case: The Case of Outputs Under Different Distillation Methods. 

Question: The roots of the polynomial 10 x3 âˆ’ 39 x2 + 29 x âˆ’ 6 are the height, length, and width of a rectangular box (right rectangular prism). A new rectangular box is formed by lengthening each edge of the original box by 2.What is the volume of the new box? 

Ground-Truth: 30 

Vanilla Response: To solve the problem, we will follow these steps: first, find the roots of the polynomial 

10 x3 âˆ’ 39 x2 + 29 x âˆ’ 6, then determine the dimensions of the original box, and finally compute the volume of the new box. Using the Rational Root Theorem, it is claimed that x = 2 is a root, and the polynomial is factored as (x âˆ’ 2)(10 x2 âˆ’ 19 x + 3) . . . The resulting roots are x = 2 , x = 19+ âˆš241 20 , and x = 19 âˆ’âˆš241 20 . Each dimension is increased by 2, leading to a new volume computed as 4 Â· 59+ âˆš241 20 Â· 59 âˆ’âˆš241 20 = 32 .4 . . . Therefore, the final answer is boxed{32.4} .

SFT (Label): Thus, the volume of the new box is {37.5} .

SFT (Long-CoT): Alright, so I have this problem here where I need to find the volume of a new rectangular box formed by increasing each edge of an original box by 2 units. The original boxâ€™s dimensions are the roots of the polynomial 10 x3 âˆ’ 39 x2 + 29 x âˆ’ 6. Hmm, okay. Let me break this down step by step . . . That reminds me of Vietaâ€™s formulas. For 10 x3 âˆ’ 39 x2 + 29 x âˆ’ 6, we have h + l + w = 39 10 , hl + hw + lw = 29 10 , and 

hlw = 35 . . . Now expand the volume: (h + 2)( l + 2)( w + 2) = hlw + 2( hl + hw + lw ) + 4( h + l + w) + 8 

. . . At this point, I want to double-check the arithmetic to avoid mistakes. Converting everything to a common denominator, I rewrite 8 as 80 10 and sum the terms, obtaining 58+156+80 10 . . . Wait, let me verify this again to be safe.. . . After rechecking, the total is interpreted as 294 10 , which simplifies to 29 .4. The steps seem consistent after revision. . . So the final answer is boxed{29.4} .

P-ALIGN: <Begin_of_prefix> Okay, so I need to find the volume of a new rectangular box formed by increasing each edge of an original box by 2 units. The original boxâ€™s dimensions (length, width, height) are the roots of the polynomial 10 x3 âˆ’ 39 x2 + 29 x âˆ’ 6. First, I need to find those roots, or at least use their relationships. 

I remember Vietaâ€™s formulas: for a cubic equation ax 3 + bx 2 + cx + d = 0 , the sum of the roots is âˆ’b/a ,the sum of pairwise products is c/a , and the product of the roots is âˆ’d/a . . . <End_of_prefix> 

To continue solving the question based on the existing prior knowledge . . . Therefore, the roots (L, W, H ) satisfy 

L + W + H = 39 10 , LW + W H + HL = 29 10 , and LW H = 35 . The new volume is (L + 2)( W + 2)( H + 2) ,which expands to LW H + 2( LW + W H + HL ) + 4( L + W + H) + 8 . Substituting the values gives  

> 35

+ 2 Â· 29 10 + 4 Â· 39 10 + 8 = 30 . . . . Therefore, the new volume is boxed{30} .

Table 8: Case Study. Text in red highlights erroneous parts in the reasoning and their underlying causes, text in 

green denotes correct reasoning content.