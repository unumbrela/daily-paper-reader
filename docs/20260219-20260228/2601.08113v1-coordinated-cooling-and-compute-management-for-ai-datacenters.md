---
title: Coordinated Cooling and Compute Management for AI Datacenters
title_zh: AI 数据中心的协同冷却与计算管理
authors: "Nardos Belay Abera, Yize Chen"
date: 2026-01-13
pdf: "https://arxiv.org/pdf/2601.08113v1"
tags: ["query:sr-llm"]
score: 7.0
evidence: 大模型推理服务的冷却与计算协同管理
tldr: 针对AI数据中心在大规模部署大语言模型（LLM）时面临的高能耗与散热挑战，本文提出了一种协同冷却与计算管理的层次化控制框架。研究通过分析GPU服务器在不同负载下的热力学特性，综合优化GPU并行度、频率（DVFS）及冷却控制参数。该方法在满足LLM推理延迟和热约束的前提下，显著提升了数据中心的能源效率，为绿色AI基础设施建设提供了新思路。
motivation: 现有研究多侧重于计算侧调度，忽视了GPU密集型推理产生的巨大热量对能耗和系统性能的负面影响。
method: 建立了一套联合冷却与计算的动力学模型，并开发了一个层次化框架来协同优化GPU配置与冷却系统控制变量。
result: 基于Azure真实推理轨迹的实验表明，该方案在保证服务延迟和热安全的同时，大幅降低了AI数据中心的总能耗。
conclusion: 通过计算与冷却系统的深度协同管理，可以有效平衡AI推理性能与能源可持续性。
---

## 摘要
目前，AI 数据中心正处于大规模部署阶段，以支持功耗密集型大语言模型（LLM）的训练与部署。数据中心所需的大量计算和冷却需求引发了人们对 AI 数据中心能源消耗和碳排放的担忧。尽管目前的最前沿研究已经探讨了 LLM 推理的能源效率，但大多数先前的研究都集中在优化计算侧调度，而未考虑热目标或约束。由于 GPU 密集型推理会产生大量热量，从而降低数据中心性能，忽视热效应会增加总能耗并降低 LLM 服务的效率。为了填补这一空白，我们分析了 GPU 服务器在不同冷却条件和 AI 任务下的特性，并为 AI 数据中心开发了一种联合冷却与计算建模方法。基于此类工作负载和热动力学模型，本文提出了一种新型分层控制框架，通过确定最佳 GPU 并行度、频率（DVFS）和冷却控制参数，协同优化计算与热管理。利用真实的 Azure 推理轨迹和详细的 GPU 分析，我们的模型在平衡 AI 数据中心服务延迟和热约束的同时，显著提高了 AI 数据中心的能源效率。

## Abstract
The AI datacenters are currently being deployed on a large scale to support the training and deployment of power-intensive large-language models (LLMs). Extensive amount of computation and cooling required in datacenters increase concerns about the energy use and carbon emissions of AI datacenters. Although current state-of-the-art has examined the energy efficiency of LLM inference, most prior research focused on optimizing compute-side scheduling without considering thermal objectives or constraints. Since GPU-intensive inference generates substantial heat that can degrade datacenter performance, ignoring thermal effects can increase total energy consumption and reduce the efficiency of LLM serving. To fill this gap, we profile the characteristics of GPU servers under varying cooling and AI jobs, and develop a joint cooling and computing modeling approach for AI datacenters. Built upon such workload and thermal dynamics models, a novel hierarchical control framework is proposed to co-optimize computing and thermal management by identifying the optimal GPU parallelism, frequency (DVFS), and cooling control knobs. Using real Azure inference traces and detailed GPU profiling, our model balances serving latency and thermal constraints in AI datacenters while significantly improving AI datacenters' energy efficiency.