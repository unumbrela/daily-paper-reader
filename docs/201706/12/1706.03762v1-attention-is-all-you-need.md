---
title: Attention Is All You Need
title_zh: 注意力即一切
authors: "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
date: 20170612
pdf: "https://arxiv.org/pdf/1706.03762v1"
tags: ["query:transformer", "query:attention"]
tldr: 本研究针对传统序列转导模型依赖复杂循环或卷积神经网络、计算效率低且难以并行化的问题，提出了 Transformer 架构。该架构完全摒弃了循环和卷积，仅依靠自注意力机制实现编码器-解码器连接。实验证明，Transformer 在机器翻译任务上不仅取得了 SOTA 性能，还显著提升了训练效率和并行化能力，并能良好推广至语法解析等其他任务。
motivation: 传统的循环和卷积神经网络在处理长序列时存在计算效率低和难以并行化的问题。
method: 提出了一种名为 Transformer 的新架构，完全基于注意力机制，彻底取代了循环和卷积结构。
result: 在 WMT 2014 英德和英法翻译任务中均刷新了 BLEU 得分纪录，且训练成本大幅降低。
conclusion: Transformer 证明了仅靠注意力机制即可实现卓越的序列建模能力，并具有极强的通用性和高效性。
---

## 摘要
主流的序列转导模型基于编码器-解码器配置中的复杂循环或卷积神经网络。性能最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构，即 Transformer，它完全基于注意力机制，彻底抛弃了循环和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优，同时具有更好的可并行性，且训练所需时间显著减少。我们的模型在 WMT 2014 英德翻译任务中实现了 28.4 BLEU 分数，比现有最佳结果（包括集成模型）提高了 2 个 BLEU 以上。在 WMT 2014 英法翻译任务中，我们的模型在 8 个 GPU 上训练 3.5 天后，创下了 41.0 的单模型 SOTA（当前最佳）BLEU 分数，这仅是文献中最佳模型训练成本的一小部分。我们通过将 Transformer 成功应用于具有大量和有限训练数据的英语成分句法分析，证明了该模型能够很好地泛化到其他任务。

## 速览
**TLDR**：本研究针对传统序列转导模型依赖复杂循环或卷积神经网络、计算效率低且难以并行化的问题，提出了 Transformer 架构。该架构完全摒弃了循环和卷积，仅依靠注意力机制实现编码器-解码器连接。实验证明，Transformer 在机器翻译任务上不仅取得了 SOTA 性能，还显著提升了训练效率和并行化能力，并能良好推广至语法解析等其他任务。 \
**Motivation**：传统的循环和卷积神经网络在处理长序列时存在计算效率低和难以并行化的问题。 \
**Method**：提出了一种名为 Transformer 的新架构，完全基于注意力机制，彻底取代了循环和卷积结构。 \
**Result**：在 WMT 2014 英德和英法翻译任务中均刷新了 BLEU 得分纪录，且训练成本大幅降低。 \
**Conclusion**：Transformer 证明了仅靠注意力机制即可实现卓越的序列建模能力，并具有极强的通用性和高效性。

---

## Abstract
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

---

## 论文详细总结（自动生成）

这篇论文是深度学习领域的里程碑之作，它提出了 **Transformer** 架构，彻底改变了自然语言处理（NLP）及后续多模态学习的发展方向。以下是对该论文的结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **核心问题**：传统的序列转导模型（如机器翻译）主要依赖循环神经网络（RNN/LSTM）或卷积神经网络（CNN）。
    *   **RNN 的局限**：本质上是顺序计算，无法并行化，导致长序列训练效率低下；且难以捕捉远距离的依赖关系。
    *   **CNN 的局限**：虽然可以并行，但要建立长距离依赖需要堆叠多层，计算开销随距离增加。
*   **整体含义**：论文提出了一种完全摒弃循环和卷积、仅依赖**自注意力机制（Self-Attention）**的新架构。其目标是在提高翻译质量的同时，实现高度的并行化并显著缩短训练时间。

### 2. 方法论：核心思想与关键技术
Transformer 采用经典的编码器-解码器（Encoder-Decoder）结构，核心组件包括：
*   **缩放点积注意力（Scaled Dot-Product Attention）**：
    *   通过计算查询（Query）与键（Key）的点积来确定权重，并除以 $\sqrt{d_k}$ 进行缩放以防止梯度消失，最后作用于值（Value）上。
*   **多头注意力（Multi-Head Attention）**：
    *   核心思想是将模型分为多个“头”，在不同的子空间并行学习信息，最后将结果拼接。这允许模型同时关注来自不同位置和表示维度的信息。
*   **位置编码（Positional Encoding）**：
    *   由于模型没有循环结构，为了引入序列的顺序信息，作者使用正弦和余弦函数的不同频率来对位置进行编码，并将其与输入嵌入相加。
*   **前馈网络（FFN）**：
    *   每个层包含一个全连接的前馈网络，独立且相同地应用于每个位置。
*   **残差连接与层归一化**：
    *   每个子层周围都使用了残差连接（Residual Connection），随后进行层归一化（Layer Normalization），以确保深层网络的训练稳定性。

### 3. 实验设计
*   **数据集**：
    *   **WMT 2014 英德翻译**：约 450 万个句子对。
    *   **WMT 2014 英法翻译**：约 3600 万个句子对。
    *   **英语成分句法分析**：使用 Penn Treebank 数据集测试模型的泛化能力。
*   **Benchmark（基准）**：主要使用 **BLEU 分数** 衡量翻译质量。
*   **对比方法**：对比了当时最先进的模型，包括 ByteNet、Deep-Att、GNMT（谷歌神经机器翻译）、ConvS2S（卷积序列到序列）以及各类集成模型。

### 4. 资源与算力
*   **硬件**：在一台配备 **8 个 NVIDIA P100 GPU** 的机器上进行训练。
*   **训练时长**：
    *   **基础模型（Base Model）**：训练 10 万步，耗时约 **12 小时**。
    *   **大模型（Big Model）**：训练 30 万步，耗时约 **3.5 天**。
*   **效率对比**：相比于当时的 SOTA 模型（如 GNMT），Transformer 的训练成本（FLOPs）降低了数个数量级。

### 5. 实验数量与充分性
*   **实验规模**：作者进行了大规模的机器翻译实验，并针对模型超参数进行了详尽的**消融实验（Ablation Study）**（见表 3）。
*   **消融变量**：包括注意力头的数量（$h$）、Key 的维度（$d_k$）、模型深度、Dropout 比例、位置编码方式等。
*   **充分性评价**：实验设计非常充分且客观。通过在不同规模的数据集（英德 vs 英法）和不同任务（翻译 vs 句法分析）上进行验证，证明了架构的鲁棒性和通用性。

### 6. 主要结论与发现
*   **性能突破**：Transformer (Big) 在英德翻译上达到 28.4 BLEU，在英法翻译上达到 41.0 BLEU，均刷新了当时的单模型 SOTA 纪录。
*   **训练高效**：在达到更高精度的同时，训练速度比循环或卷积模型快得多。
*   **泛化能力强**：在几乎没有针对性调优的情况下，Transformer 在句法分析任务上也表现出色，证明其不仅限于翻译，而是通用的序列建模工具。

### 7. 优点
*   **高度并行化**：打破了 RNN 的顺序限制，极大地提升了计算效率。
*   **长距离依赖建模**：自注意力机制使得任意两个位置之间的“路径长度”为常数 $O(1)$，有效解决了长文本建模难题。
*   **可解释性**：通过可视化注意力权重，可以直观地看到模型在处理某个词时关注了哪些上下文（如代词指代关系）。

### 8. 不足与局限
*   **计算复杂度**：自注意力的计算复杂度随序列长度 $n$ 呈平方增长 $O(n^2)$，这使得处理超长文本（如整本书或高分辨率图像）时内存压力巨大。
*   **局部信息丢失风险**：虽然多头注意力有所缓解，但相比 CNN，它对局部结构（如相邻词组）的归纳偏置（Inductive Bias）较弱。
*   **推理开销**：在生成阶段（Decoder），由于是自回归生成，推理速度仍受限于序列长度。

（完）
