Title: On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking

URL Source: https://arxiv.org/pdf/2602.16849v1

Published Time: Fri, 20 Feb 2026 01:10:38 GMT

Number of Pages: 60

Markdown Content:
# On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking 

Jianliang He Leda Wang Siyu Chen Zhuoran Yang 

Department of Statistics and Data Science, Yale University 

{jianliang.he, leda.wang, siyu.chen.sc3226, zhuoran.yang}@yale.edu 

Abstract 

We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification . We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism . Our gradient flow analysis proves that frequencies compete within each neuron, with the “winner” determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay. 1

# Contents 

1 Introduction 3

1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

2 Preliminaries 53 Empirical Findings 6

3.1 Mechanistic Pattern: Experimental Observations on Learned Weights . . . . . . . . . 63.2 Dynamical Perspective: Phase Alignment and Feature Emergence . . . . . . . . . . . 93.3 Grokking: From Memorization to Generalization . . . . . . . . . . . . . . . . . . . . . 11 

> 1

Our code is available at GitHub. For interactive visualizations and further experimental results, see our Hugging Face Space at Hugging Face. 

1

> arXiv:2602.16849v1 [cs.LG] 18 Feb 2026

4 Mechanistic Interpretation of Learned Model 13 5 Training Dynamics for Feature Emergence 14 

5.1 Background: Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . 14 5.2 A Dynamical Perspective on Feature Emergence . . . . . . . . . . . . . . . . . . . . . 15 5.3 Properties at the Initial Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5.4 Preservation of Single-Frequency Pattern . . . . . . . . . . . . . . . . . . . . . . . . . 17 5.5 Neuron-Wise Phase Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 

6 Theoretical Extensions 20 

6.1 Theoretical Underpinning of Lottery Ticket Mechanism . . . . . . . . . . . . . . . . . 20 6.2 Dynamics Beyond Quadratic Activation . . . . . . . . . . . . . . . . . . . . . . . . . . 21 

7 Conclusion 22 A Additional Experimental Details and Results 26 

A.1 Detailed Interpretation of Grokking Dynamics in Section 3.3 . . . . . . . . . . . . . . 26 A.2 Ablations Studies for Fully-Diversified Parametrization . . . . . . . . . . . . . . . . . 28 A.3 Training Dynamics with Quadratic Activation . . . . . . . . . . . . . . . . . . . . . . 30 

B Proof of Results in Section 4 and 5 30 

B.1 Proof of Proposition 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 B.2 Preliminary: Gradient Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 B.3 Main Flow Approximation under Small Parameter Scaling . . . . . . . . . . . . . . . 33 B.3.1 Proof Overview: Simplified Dynamics under Approximation . . . . . . . . . 34 B.3.2 Proof of Lemma B.3: Main Flow of Decoupled Neurons . . . . . . . . . . . . . 35 B.4 Proof of Theorem 5.2: Single-Frequency Preservation . . . . . . . . . . . . . . . . . . 38 B.4.1 Proof of Auxiliary Lemma B.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 B.5 Proof of Theorem 5.3: Phase Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 44 B.5.1 Proof of Auxiliary Lemma B.8, B.9 and B.10 . . . . . . . . . . . . . . . . . . . . 48 

C Proof of Results for Theoretical Extensions in Section 6 51 

C.1 Proof of Corollary 6.1: Phase Lottery Ticket . . . . . . . . . . . . . . . . . . . . . . . . 51 C.1.1 Proof of Auxiliary Lemma C.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 C.2 Proof of Proposition 6.3: Dynamics of ReLU Activation . . . . . . . . . . . . . . . . . 56 

D Comparison with Existing Results 59 

21 Introduction 

A central mystery in deep learning is how neural networks learn to generalize. While these models are trained to find patterns in data, the precise way they build internal representations through gradient-based training and make predictions on new, unseen data is not fully understood. The sheer complexity of modern networks often obscures the fundamental principles at work. To gain a clearer view, researchers often simplify the problem by studying how networks solve simple but rich tasks that can be precisely analyzed. By meticulously analyzing the learning process in these controlled "toy" settings, we can uncover basic mechanisms that may apply more broadly. The modular addition task, (x, y ) 7 → (x + y) mod p has emerged as a canonical problem for this approach, as it is simple to define yet reveals surprisingly complex and insightful learning dynamics. +

σ

> Input Dimension
> Neuron Dimension
> Output Dimension
> Neuron Dimension

or       

> p×M
> M×p
> p×1
> Discrete Fourier Basis

× 

> Original Parameters of Neuron m

or 

> Discrete Fourier Transform (DFT)

=×                      

> Frequency Basis
> w/sin & cos
> Fourier Coef fi cients of Neuron m
> p×1p×1
> p×p
> p×1p×1
> Magnitude
> Phase
> - Sparsity with a Shared Frequency
> -Comparable Magnitudes
> - Output Phase Double the Input
> Training Dynamics
> (a)
> (b)
> (c)
> Input/Output
> Value
> For large , frequencies and phases are fully
> diversi fi ed across neurons.
> M
> Steps
> Magnitude
> Output Phase
> 2Input Phase ×
> Winning
> Frequency

Figure 1: An illustration of the primary analytical technique and results. Discrete Fourier Transform (DFT) is utilized to quantitatively interpret the mechanism of learned models within the feature space, revealing the training dynamics that result in consistent feature learning. Figure (a) shows the neural network architecture — we adopt a two-layer fully connected neural network to learn the modular addition task. The inputs x and y are represented as one-hot vectors in Rp, σ(·) denotes the activation function, and the width of the neural network is denoted by M . Figure (b) illustrates the technique of DFT. We apply DFT to the weights at the input and output layers, respectively. Each neuron involves two weight vectors, which lead to two magnitudes and phases. (See Observation 1 in §3.) Figure (c) illustrates some of our key empirical observations — phase alignment (Observation 2), phase symmetry (Observation 3), and lottery ticket mechanism (Observation 6). Prior work has established that neural networks trained on modular arithmetic discover a Fourier feature representation, embedding inputs onto a circle to transform addition into geometric rotation (Nanda et al., 2023; Zhong et al., 2023). These studies have also highlighted the intriguing grokking 

phenomenon, where a model suddenly generalizes long after it has memorized the training data (Power et al., 2022; Liu et al., 2022). While these observations are foundational, prior work has not yet offered a conclusive, end-to-end explanation of the learning process. Existing theoretical accounts 3often rely on mean-field approximations (Wang and Wang, 2025) or analyze non-standard loss functions (Morwani et al., 2023; Tian, 2024), leaving a gap in our understanding of the finite-neuron dynamics under standard training. This leaves fundamental questions unanswered: (Q1 ) Mechanistic Interpretability: How does the trained network leverage its learned Fourier features to implement the modular addition algorithm precisely? (Q2 ) Training Dynamics: How do these specific Fourier features reliably emerge from gradient-based training with random initialization? In this paper, we provide comprehensive answers to these questions via systematic experiments and a rigorous theoretical analysis of two-layer networks. For ( Q1 ), while prior work has identified that neurons learn single-frequency features and exhibit phase alignment , we quantitatively characterize how these local features are synthesized into a global mechanism. Specifically, we demonstrate that the network develops a collective diversification condition (see Observation 3 and 4, formalized in Definition 4.1) characterized by two key properties: (i) frequency diversification : The network ensures that the full spectrum of necessary Fourier components is represented across the neuron population. (ii) phase symmetry : Within each frequency group, neurons exhibit high-order symmetry to ensure the balance required for noise cancellation. We rigorously prove that this dual condition allows the network to aggregate the noisy, biased signals of individual neurons into a collective approximation of a flawed indicator function (see Theorem 4.2) and how these patterns emerge from gradient training from a mean-field perspective driven by the layer-wise phase coupling dynamics (see Theorem 5.2, 5.3 and Proposition 6.3 with proof sketch). To address ( Q2 ), we explain the emergence of these features via lottery ticket mechanism (see Observation 6). Our analysis of the gradient flow reveals a competitive dynamic in which multiple frequency components compete within each individual neuron during training. Specifically, by applying the ODE comparison lemma , we prove that the frequency component with the largest initial magnitude and the smallest phase misalignment grows exponentially faster than its competitors, eventually becoming the single dominant “winner” (see Corollary 6.1). This provides a rigorous, neuron-wise explanation for the learned single-frequency structure, demonstrating how random initialization determines which specific Fourier features the network ultimately adopts. Finally, having established the underlying mechanism and training dynamics, we can address the final bonus question regarding the grokking phenomenon: (Q3 ) Memorization to Generalization: How do these mechanisms and dynamics explain the full timeline of grokking, from memorization to delayed generalization? We characterize it as a three-stage process driven by the competition between loss minimization and weight decay. We demonstrate that the model first memorizes training data through a “perturbed” version of the lottery ticket mechanism, followed by two generalization stages where weight decay prunes residual noise and refines the learned features into the sparse Fourier representation required for generalization. By providing a complete, end-to-end theoretical and empirical account of this learning problem, our work offers a concrete foundation for understanding the interplay between feature learning, training dynamics, and generalization in neural networks. 

1.1 Related Work 

Modular Addition and Grokking Phenomenon. Studying simple tasks like modular addition has revealed deep insights into neural network mechanisms (e.g., Power et al., 2022). Reverse-engineering has shown models learn a Fourier feature, converting addition into a geometric rotation by embedding numbers on a circle (Nanda et al., 2023; Zhong et al., 2023; Gromov, 2023; Doshi 4et al., 2024; Yip et al., 2024; McCracken et al., 2025). This discovery is central to understanding grokking, a phenomenon where generalization suddenly emerges long after overfitting, which these papers study using specific train-test data splits (e.g., Liu et al., 2022; Doshi et al., 2023; Yip et al., 2024; Mallinar et al., 2024; Wu et al., 2025). Theoretical understanding of this modular addition task, however, remains incomplete. Morwani et al. (2023) characterize the loss landscape under the max-margin framework using a non-standard 

ℓ2,3-regularization. The work Tian (2024) further analyzes the landscape of a modified ℓ2-loss within the Fourier space, generalized these results to data with semi-ring structures on Abelian groups, and provided a heuristic derivation for the mean-field dynamics of frequencies. Recently, Wang and Wang (2025) formalize and extende these mean-field results by analyzing the Wasserstein gradient flow under a geometric equivariance constraint, and Kunin et al. (2025) characterize the Fourier feature emergence as a trade-off between maximizing a utility function over the dormant neurons and minimizing a cost function over active ones. While Tian (2024) and Wang and Wang (2025) provide a characterization of a simpler, mean-field dynamics, a full analytical result explaining the alignment and competition dynamics at the finite, neuron-wise level remains an open problem. A different approach studies grokking modular arithmetic via the average gradient outer product for backpropagation-free models (Mallinar et al., 2024). Another line of research focuses on grokking dynamics and frames it as a two-phase process, transitioning from an initial lazy (kernel) regime to a later rich (feature) regime (Kumar et al., 2024; Lyu et al., 2023; Mohamadi et al., 2024; Ding et al., 2024), which are broadly related to our work. Recently, the work of Tian (2025) proposes a three-stage theoretical framework for grokking dynamics that includes lazy learning, independent feature learning, and interactive feature learning. This three-stage process echoes our own observations for modular addition in §3.3, §A.1. A more detailed comparison with related work is provided in §D. 

Training Dynamics of Neural Networks. To understand how neural networks perform feature learning, a significant body of work has analyzed the training dynamics of neural networks under gradient-based optimization. This research typically focuses on settings where the target function exhibits a low-dimensional structure, such as single-index (Ba et al., 2022; Lee et al., 2024; Berthier et al., 2024; Chen et al., 2025) and multi-index models (Damian et al., 2022; Arnaboldi et al., 2024; Ren et al., 2025). Taking a step further, Allen-Zhu and Li (2019); Shi et al. (2022, 2023) have considered more general cases, analyzing function classes that encode latent features rather than relying on the explicit structure of index models. While insightful, these works assume well-structured target functions and clearly defined features, leaving the feature learning from natural data largely unclear. 

Notation. For any positive integer n ∈ N+, let [n] = {i ∈ Z : 1 ≤ i ≤ n}. Let Zp denote the set of integers modulo p. The ℓp-norm is denoted by ∥ · ∥ p. For a vector ν ∈ Rd, its i-th entry is denoted by υ[i]. The softmax operator, smax (·), maps a vector to a probability distribution, where the i-th component is given by smax (υ)i = exp( υi)/ P 

> j

exp( υj ). For two non-negative functions f (x) and 

g(x) defined on x ∈ R+, we write f (x) ≲ g(x) or f (x) as O(g(x)) if there exists two constants c > 0

such that f (x) ≤ c · g(x), and write f (x) ≳ g(x) or f (x) if there exists two constants c > 0 such that 

f (x) ≥ c · g(x). We write f (x) ≍ g(x) or f (x) = Θ( g(x)) if f (x) ≲ g(x) and g(x) ≲ f (x).

# 2 Preliminaries 

Modular Addition. In a modular addition task, we aim to learn whose form is given by (x, y ) 7 →

(x + y) mod p for (x, y ) ∈ Z2

> p

. The complete dataset is given by Dfull = {(x, y, z ) | x, y ∈ Zp, z =(x + y) mod p} which consists of all possible input pairs (x, y ) and their corresponding modular 5sums z. This dataset is then partitioned into a training set for learning and a disjoint test set for evaluation. The performance of learned model is assessed on the test set by evaluating how accurately it predicts (x + y) mod p for unseen input pairs. Such a training setup is widely used in the literature to study phase transition phenomena, such as grokking (e.g., Nanda et al., 2023), and feature learning (e.g., Morwani et al., 2023) in modular arithmetic tasks. 

Two-Layer Neural Network. We consider a two-layer neural network with M hidden neurons and no bias terms. Each input x is assigned to embedding vectors hx ∈ Rd, where h : Zp 7 → Rd

is an embedding function of dimension d ∈ N. The embedding can be either the canonical embedding ex ∈ Rp in which case d = p or a trainable one {hx}x∈Zp ⊆ Rd. Let θ = {θm}m∈[M ] and 

ξ = {ξm}m∈[M ] denote the parameters, where θm ∈ Rd is the parameter vector of the m-th hidden neuron and ξm ∈ Rp is its corresponding output-layer weight. The network output is then given by 

f (x, y ; ξ, θ ) = 

> M

X

> m=1

ξm · σ(⟨hx + hy, θ m⟩) ∈ Rp, (2.1) where σ(·) is a nonlinear activation. In this paper, we primarily focus on the ReLU activation σ(x) = max {x, 0} for experiments and the quadratic activation σ(x) = x2 for theoretical interpretations. Since the modular addition is essentially a classification problem, we apply the softmax function 

smax : Rd 7 → Rd to the network output and consider the cross-entropy (CE) loss: 

ℓD(ξ, θ ) = − X

> (x,y )∈D

log ◦smax ◦ f (x, y ; ξ, θ ), e (x+y) mod p . (2.2) Here, log( ·) is applied entrywise and e(x+y) mod p is the one-hot vector that corresponds to the correct label. Intuitively, each input pair (x, y ) is mapped to a hidden representation by σ(⟨hx + hy, θ m⟩)

for each neuron m, then linearly combined by ξm’s to produce the logits f (x, y ; ξ, θ ), and finally processed via the softmax function to yield a categorical distribution for classification. 

# 3 Empirical Findings 

In this section, we present the empirical findings. We set p = 23 without loss of generality, and use a two-layer neural network with width M = 512 and ReLU activation. The network is trained using the AdamW optimizer with a constant step size of η = 10 −4. We initialize all parameters using PyTorch’s default method (Paszke et al., 2019). For stable training, we then normalize these initial values and use the average loss over the dataset. We note that all of our empirical findings below are robust to the choice of p, and they appear as long as M is sufficiently large and the neural network is properly optimized. Following prior work (Morwani et al., 2023; Tian, 2024), we primarily focus on training the model with the complete dataset Dfull (without train-test splitting), as this yields more stable training dynamics and enhances model interpretability. While the train-test split setup exhibits the intriguing grokking behavior (e.g., Nanda et al., 2023; Doshi et al., 2023; Gromov, 2023), wherein models suddenly achieve generalization after extensive training despite initial overfitting, we defer this analysis to §3.3, building upon the foundational results presented in subsequent sections. 

3.1 Mechanistic Pattern: Experimental Observations on Learned Weights 

We first summarize the main empirical findings of our experiments using ReLU activation (see Figures 2 and 3), formalized as four key observations. The first two — trigonometric parameterization 

6(a) Heatmap of Learned Parameters. (b) Actual Learned and Fitted Parameters of Each Neuron. 

Figure 2: Learned parameters under the full random initialization with p = 23 and ReLU activation using AdamW. Figure (a) plots a heatmap of the learned parameters for the first 10 neurons after Discrete Fourier Transform (DFT, see §5.1) grouped with frequency. Each row in the heatmap corresponds to the Fourier components of a single neuron’s parameters. The plot clearly reveals a single-frequency pattern: each neuron exhibits a large, non-zero value focused on only one specific frequency component, confirming a highly sparse and specialized frequency encoding. We remark that since only 10 out of 512 neurons are shown, not all (p − 1) /2 = 11 frequencies appear in this sample. The same single-frequency pattern holds across all 512 neurons, which collectively cover all 11 frequencies (see Observation 3). Figure (b) further examines the periodicity by plotting line plots of the learned parameters for three neurons, each overlaid with a trigonometric curve fitted via DFT. The fitted curve aligns almost perfectly with the actual one. and phase alignment — have been previously explored in the literature (Gromov, 2023; Nanda et al., 2023; Yip et al., 2024), and are included for completeness. For clarity, we focus on the case where inputs are one-hot embedded. We begin with the most striking observation: a global trigonometric pattern in parameters that consistently emerges across all training runs with random initialization. 

Observation 1 (Fourier Feature). There exists a frequency mapping φ : [ M ] → [ p−12 ], along with magnitudes αm, βm ∈ R+ and phases ϕm, ψm ∈ [−π, π ), such that 

θm[j] = αm · cos( ωφ(m)j + ϕm), ξm[j] = βm · cos( ωφ(m)j + ψm), ∀(m, j ) ∈ [M ] × [p], (3.1) where we denote ωk = 2 πk/p for all k ∈ [ p−12 ].This observation shows that the parameter vectors θm and ξm simplify during training into a clean trigonometric pattern. In the frequency domain, this corresponds to a sparse signal . After applying a Discrete Fourier Transform (DFT, see §5.1), each neuron is represented by a single active frequency φ(m). Given this single-frequency structure, we will henceforth refer to αm and ϕm as the 

input magnitude and phase , and to βm and ψm the output magnitude and phase for neuron m.This observation is illustrated in Figure 2. In Figure 2b, we zoom in on the learned parameters of the first three neurons, with each entry corresponding to the input or output value j ∈ Zp. The plots show that these parameters are well approximated by cosine curves, shifted by phases ϕm and ψm,and scaled by magnitudes αm and βm, respectively. This suggests that the trained neural network learns to solve modular addition by embedding a trigonometric structure into its parameters, where 7(a) Scatter of (2 ϕm, ψ m).               

> average value
> ιcos( ιϕ m) sin( ιϕ m)
> ×10.0123 -0.0500
> ×2-0.0234 0.0319
> ×3-0.0531 -0.0032
> ×4-0.0235 -0.0451
> ×5-0.0505 -0.0372
> (b) Phase Symmetry within Frequency Group Nk.(c) Distribution of αm,βm.

Figure 3: Visualizations of learned phases with M = 512 neurons. Figure (a) plots the relationship among the normalized 2ϕm and ψm, with all points lying around the line y = x. Figure (b) shows the uniformity of the learned phases within a specific group Nk. The left panel displays ιϕ m for 

ι ∈ { 1, 2, 3, 4} on unit circles, and the points are nearly uniformly distributed. The right panel quantifies this symmetry by computing the averages of cos( ιϕ m) and sin( ιϕ m), all of which are close to zero. Figure (c) presents violin plots of the magnitudes αm and βm. The tight distribution of these values around their mean suggests that the neurons learn nearly identical magnitudes. each dimension j ∈ [p] corresponds to the value of a cosine function at j. Next, we examine the local structure of individual neurons, and observe a highly structured phase alignment behavior. 

Observation 2 (Doubled Phase). For each neuron m ∈ [M ], the parameter exhibits a doubled phase relationship, where the output phase is twice the input phase, i.e., (2 ϕm − ψm) mod 2 π = 0 .We visualize the relationship between ϕm and ψm in Figure 3a. Specifically, the dots represent the pairs (2 ϕm, ψ m), which lie precisely on the line y = x, confirming the claim made in Observation 2. This indicates that the first-layer θm and second-layer ξm learns to couple in the feature space, specifically the Fourier space, through training. Having studied both global and neuron-wise local parameter patterns, we now examine how neurons coordinate their collective operation. Consider a network with a sufficiently large number of neurons, then the phases exhibit clear within-group uniformity and the magnitudes display nearly homogeneous scaling across neurons. 

Observation 3 (Model Symmetry). Let Nk be the set of neurons for frequency k, defined as 

Nk = {m ∈ [M ] : φ(m) = k}. For large M , (i) phases are approximately uniform over (−π, π )

within frequency group Nk, i.e., ϕm, ψ m i.i.d. 

∼ Unif( −π, π ), (ii) every frequency k is represented among the neurons, and (iii) the magnitudes αm’s and βm remains close across all neurons. Figure 3b illustrates the uniformity of phases within a specific frequency group Nk by examining the higher-order symmetry, i.e., the symmetry of ιϕ m for ι ∈ { 1, 2, 3, 4}. Both the visualizations and the quantitative averages of sine and cosine values support the within-group uniformity claim stated in Observation 3. In addition, the learned magnitudes are similar across all neurons, preventing any single neuron from becoming dominant (see Figure 3c). Although a large M is not required for successful model training, it significantly aids in interpreting the mechanism of the learned model (see §4 for details). While previous work (e.g., Kumar et al., 2024), has introduced the phase uniformity to provide a constructive model that solves modular addition, our findings significantly refine the understanding. Through empirical validations, we show that this phase uniformity is a consistent when M is large. Furthermore, in §4, we derive and utilize a substantially 8σ(x) max {x, 0} |x| x2 x4 x8 log(1 + e2x) ex x x3                          

> Loss 1.194 ×10 −80.000 0.000 3.1×10 −50.051 1.2×10 −36.5×10 −44.246 3.891 Accuracy 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.041 0.036

Table 1: Adaptivity of the learned parameterization. We evaluate the robustness of the trained model by replacing the ReLU with various alternative functions at test time. As shown in the table, the model maintains perfect prediction accuracy when using the absolute value function, even-order polynomials, or the exponential function. This demonstrates that the learned features are not strictly dependent on the original activation but rather on its underlying even-order components. weaker condition than strict uniformity to enable a more precise, joint analysis of noise cancellation across a diversified, finite set of neurons. Finally, we report a surprising adaptivity in the learned parametrization: the network continues to perform perfectly when ReLU is replaced by a broad class of alternative activations at the test time. See Table 1 for details. As shown in Table 1, when we replace the ReLU activation to other activation functions that has nonzero even-order components, e.g., |x|, x2, and x4, the resulting models still have perfect prediction accuracy. However, suppose we replace ReLU to an activation wihout any even-order component, e.g., x and x3, the prediction accuracy is close to zero. This suggests that the key property of ReLU activation is that it has even-order components. 2

Observation 4 (Robustness to Activation Swapping). A model trained with ReLU is robust to changes of activation function at inference time. This is because learning a good solution only relies on the activation’s dominant even-order components . Consequently, functions with strong even components, such as the absolute value and quadratic, can be used interchangeably after training, all while maintaining perfect accuracy with a negligible change in loss. 

Motivated by this key observation, in the sequel, we analyze the training dynamics of how two-layer neural networks solve modular addition using the more tractable quadratic activation. 

3.2 Dynamical Perspective: Phase Alignment and Feature Emergence 

We conduct an analysis of training dynamics in an analytically tractable setting, using quadratic activation with small random initialization, and focus on the early stages of training. Motivated by Observation 1, our analysis hinges on studying the training dynamics within the frequency domain. To do this, we use the Discrete Fourier Transform (DFT), which is formalized in §5.1, to decompose the model’s parameters. Without loss of generality, any random initial parameter vector can be exactly represented by its frequency components — magnitudes (αkm, β km)’s and phases (ϕkm, ψ km)’s. This allows us to express the parameters as: for each entry j ∈ [p],

θm[j] = α0 

> m

+

> (p−1) /2

X

> k=1

αkm · cos( ωkj + ϕkm), ξm[j] = β0 

> m

+

> (p−1) /2

X

> k=1

βkm · cos( ωkj + ψkm), (3.2) As we will show in §5, under small initialization, the neurons and frequencies are fully decoupled .That is, the evolution of each neuron’s Fourier frequency components (magnitudes and phases)         

> 2Due to phase symmetry, the output of a ReLU neural network is fundamentally determined by the 12|x|term. This follows from the identity ReLU (x) = 12(x+|x|). Under phase symmetry, the linear components cancel out across the network’s operations, leaving the absolute value term as the primary contribution to the output.

9(a) Evolution of misalignment level Dkm and magnitude βkm of a specific neuron under gradient flow under small random initialization.   

> (b) Leaned magnitude βkmunder different initializations.

Figure 4: Illustration of the lottery-ticket mechanism under the fully random initialization. Figure (a) plots the dynamics of every frequency k for a specific neuron, with the red curve tracing the trajectory of the frequency that eventually dominates. In the left-hand plot, misalignment levels 

Dkm are rescaled to [−π, π ) for clarity. Typically, the winning frequency is the one that starts with a comparatively larger initial magnitude and a smaller misalignment. Figure (b) plots the contour of the magnitude βkm with various (βkm(0) , Dkm(0)) after 10 , 000 steps. The contours are symmetric about π, and reproduce the trend seen in Figure (a): initial states with larger magnitude and lower phase misalignment yield higher final magnitudes after the same training duration. only depends on the dynamics of themselves. This results in the parallel growth of the magnitudes and phases for each neuron-frequency pair (m, k ). The central question is how the training process evolves this complex, multi-frequency initial state into the simple, single-frequency pattern observed at the end of training. Our finding is surprising: The final, dominant frequency learned by each neuron is entirely determined by a small subset of Fourier components in its initial parameters. 

It arises from a competitive dynamics among frequencies, as shown in Figure 4a. A frequency’s success is determined by its initial conditions, primarily two key factors: its initial magnitudes and its initial phase misalignment level . To gain a more detailed understanding of the dynamics, we begin by tracking the evolution of phases. Motivated by the double phase phenomenon in Observation 2, we monitor the normalized phase difference Dkm, defined as Dkm = (2 ϕkm − ψkm) mod 2 π ∈ [0 , 2π).This quantity plays a central role throughout our analysis: as we will show in §5, the magnitude growth rate is governed by cos( Dkm) and the phase rotation speed by sin( Dkm), making it the key variable that simultaneously controls both alignment and amplification. In the left-hand side of Figure 4a, we plot the dynamics of this phase difference, rescaling its range to (−π, π ] for visual clarity. This analysis leads to the following observation. 

Observation 5 (Dynamics of Phase-Aligning). The phase difference Dkm(t) for each frequency converges monotonically to “zero” without crossing the axis. Generally, frequencies that start with an initial phase difference Dkm(0) closer to zero converge faster. To formalize the closeness of phase difference to zero, we define the phase misalignment eDkm as 

eDkm = max {Dkm, 2π − Dkm}. In the following, we outline the core dynamics of the training process. It reveals that the single-frequency pattern in Observation 1 is the direct result of a frequency competition, a process governed by the interplay of phase misalignment and magnitude. 10 Observation 6 (Lottery Ticket Mechanism). Under small random initialization, neurons are decoupled. Each frequency k draws a “lottery ticket” specified by its initial magnitudes αkm(0) ,

βkm(0) and misalignment level eDkm(0) . All frequencies grow in parallel, and the one with the largest 

αkm(0) and βkm(0) and the smallest eDkm(0) ultimately wins — dominating the feature of specific neuron — due to the rapid acceleration once magnitudes become larger and eDkm(t) reaches zero. 

Figure 4a provides a clear empirical illustration of the mechanism. The winning frequency, highlighted in red, begins with a highly advantageous initialization: a competitively large magnitude and a misalignment value close to zero. While other frequencies exhibit slow growth, the holder of this winning ticket undergoes a distinct phase of rapid, exponential acceleration in its magnitude. Figure 4b plots the magnitude under different initializations after a fixed time t = 10 , verifying that frequencies with a larger magnitude and a smaller misalignment take advantage. 

3.3 Grokking: From Memorization to Generalization 

In this section, we provide empirical insights into grokking by analyzing the model’s training dynamics using a progress measure designed based on our prior observations. Prior work, such as Nanda et al. (2023), identifies two key factors for inducing grokking: a distinct train-test data split and the application of weight decay. Here, we randomly partition the entire dataset of p2 points, using a training fraction of 0.75, and apply a weight decay of 2.0. The experimental results with detailed progress measure is plotted in Figure 5 As shown in Figure 5a, this elicits a clear grokking phenomenon: the training loss drops quickly to zero. In contrast, the test loss initially remains high before gradually decreasing, signaling a delayed generalization. We track four key progress measures: (a) Train-Test Loss and Accuracy. Standard indicators used to differentiate between the memo-rization phase and the onset of generalization; (b) Phase Difference. We monitor | sin( D⋆m)|, where D⋆m := 2 ϕ⋆m − ψ⋆m mod 2 π, to evaluate the degree of layer-wise phase alignment; (c) Frequency Sparsity. Measured via the Inverse Participation Ratio (IPR), defined as IPR (ν) = (∥ν∥2r/∥ν∥2)2r with r = 2 , to capture the single-frequency emergence of Fourier coefficients; (d) ℓ2-norm of parameters. Utilized as a proxy to monitor the structural evolution of weights and the specific influence of weight decay on the model’s complexity. Building upon Figure 5, we identify two primary driving forces behind the dynamics: loss mini-mization and weight decay . These forces guide the training process through an initial memorization phase followed by two generalization stages. The memorization phase is dominated by loss minimization, causing the model to fit the training data with its parameter norms increasing rapidly. As a result, the model achieves perfect accuracy on the training data and their symmetric counterparts in the test set (due to the exchangability of the two input numbers), but completely fails to generalize to truly “unseen” test points (see Figure 10). At this phase, all the frequency components in one neuron keep growing but at different pace similar to the lottery ticket mechanism described previously, resulting in a perturbed Fourier solution 

that overfits the training data. Next, the model enters the first generalization stage, which is characterized by a precise interplay between the two forces. We conclude that both forces are active because the parameter norms continue to grow, which is a clear indicator of ongoing loss minimization. At the same time, weight decay induces a sparsification effect in the frequency domain. Specifically, the one frequency 11 (a) Train-test Loss. (b) Train-test Accuracy. (c) Phase Difference. (d) Norm & Freq. Sparsity. 

Figure 5: Progress measure of grokking behavior. The shaded regions mark three distinct phases: an initial memorization phase, followed by two generalization phases. Figures (a) and (b) plot the train-test loss and accuracy curve, where the network first overfits the training data to achieve a near-zero training loss while the test loss remains high. Figure (c) visualizes the dynamics of average phase alignment level, measured by m−1 PMm=1 | sin( D⋆m)|. Figure (d) tracks the evolution of the average neuron-wise frequency sparsity level, as measured by the inverse participation ratio (IPR) of the Fourier coefficients, alongside the ℓ2-norm of the parameter. component that dominates in the lottery ticket mechanism continues growing, while weight decay refines the learned sparse features by pruning the remaining components, making it closer to the clean single-frequency solution for each neuron and causing the test loss to drop sharply. Specifically, the weight decay refines the learned sparse features, making it closer to the clean single-frequency solution for each neuron, causing the test loss to drop sharply. This dynamic culminates in a turning point around step 10,000, which marks the onset of the second and final generalization stage. From this point, weight decay becomes the dominant force, slowly pushing the test accuracy toward a perfect score. Memorization  Generalization I  Generalization II  

> Loss Minimization
> Weight Decay Learn “Common” Learn “Rare”

Figure 6: An illustration of the three stages of grokking dynamics and their main driving force. 

Principle of Memorization: Common-to-Rare. Early in training, as training accuracy rises, test accuracy falls from an initial 5% (due to small random initialization) to 0% (see Figure 5b). By Step 1000, when training accuracy peaks, the first phase is evident: the model prioritizes memorizing common data, specifically symmetric pairs where both (i, j ) and its counterpart (j, i ) are in the training set. This intense focus comes at a cost, as the model actively suppresses performance on rare examples within the same training set, driving their accuracy to zero. Only after mastering the common data does the model shift its focus to the second phase: memorizing these rare examples that appear only once. Please refer to §A.1 for a more detailed interpretation of grokking dynamics. 12 4 Mechanistic Interpretation of Learned Model 

In this section, we first tackle the interpretability question in a slightly idealized setting, leveraging the trigonometric patterns in Observations 1-3 and, motivated by Observation 4, adopting a quadratic activation for analytical convenience. We show that the trained model effectively approximates an 

indicator function via a majority-voting scheme within the Fourier space. 

Single-Neuron Contribution and Majority Voting. Under the parametrization of (3.1) in Ob-servation 1 and the phase-alignment condition 2ϕm − ψm = 0 mod 2 π for all m in Observation 2, the contribution of each neuron m, i.e., f [m](x, y ) = ξm · σ(⟨ex + ey, θ m⟩), to the logit at dimension 

j ∈ [p] can be expressed as: 

f [m](x, y ; ξ, θ )[ j] ∝ cos( ωφ(m)(x − y)/2) 2 · { cos( ωφ(m)(x + y − j)) 

| {z }

> primary signal

+ 2 cos( ωφ(m)j + 2 ϕm) + cos( ωφ(m)(x + y + j) + 4 ϕm)}. (4.1) Here, cos( ωk(x+y −j)) provides the primary signal, whose value peaks exactly at j = ( x+y) mod p,while the remaining terms act as residual noise whose amplitude and sign depend on the chosen frequency k, phase ϕm, and input pair (x, y ). Similar results have also been reported in Gromov (2023); Zhong et al. (2023); Nanda et al. (2023); Doshi et al. (2023). Although each neuron’s contribution is biased by its own frequency-phase “view”, the network as a whole can attain perfect accuracy via a majority-voting mechanism: every neuron votes based on its individual view, the model then aggregates these biased yet diverse votes to distill the correct answer. Despite this intuitive diversification argument, two questions remain unanswered: (a) How should we define “diversification”? (b) To what extent can the residual noise be canceled by aggregating over a diverse set of frequency-phase pairs (φ(m), ϕ m)?

Figure 7: Heatmap of the output logits with quadratic activation, with boxes indicating predicted higher values. 

Majority-Voting Approximates Indicator via Overparam-eterization. Motivated by Observation 3, when M is suf-ficiently large, the model naturally learns completely diver-sified neurons: every frequency k is represented, and the phases exhibit uniform symmetry. We formalize this below. 

Definition 4.1 (Full Diversification) . Neurons is called fully diversified if the frequency-phase pairs {(φ(m), ϕ m)}m∈[M ] sat-isfy the following properties: (i) for every frequency k ∈ [ p−12 ],there are exactly N neurons m with φ(m) = k, (ii) there exists a constant a > 0 such that αmβ2 

> m

= a for all m ∈ [M ], and (iii) for each k and ι ∈ { 2, 4}, exp  i · ι P 

> m∈N k

ϕm

 = 0 .

Note that Definition 4.1 is primarily a formal restatement of Observation 3. In particular, Condition (ii) follows from the homogeneous scaling of magnitudes, and Condition (iii) captures the high-order phase symmetry implied by the uniformity within the frequency group. Condition (i) assumes an exact frequency balance — an idealization that holds approximately under random initialization (see §6.1). We are now ready to present the main results regarding the interpretation of the learned model. 13 Proposition 4.2. Suppose that the neurons are completely diversified as per Definition 4.1. Under the parametrization in (3.1) and the phase-alignment condition 2ϕm − ψm = 0 mod 2 π for all m ∈ [M ], the output logit at dimension j ∈ [p] takes the form: 

f (x, y ; ξ, θ )[ j] = aN/ 2 ·  − 1 + p/ 2 · 1(x + y mod p = j)

| {z }

> signal term

+p/ 4 · X

> z∈{ x,y }

1(2 z mod p = j)

| {z }

> noise terms

. (4.2) 

For any ϵ ∈ (0 , 1) , by taking a ≳ (N p )−1 · log( p/ϵ ), it holds that ∥smax ◦ f (·, ·; ξ, θ ) − emp(·,·)∥1,∞ ≤ ϵ.

Please refer to §B.1 for a detailed proof of Proposition 4.2. The proposition states that although each neuron individually implements a trigonometric mechanism as shown in (4.1) , the diversified neurons indeed collectively approximate the indicator function 1(x + y mod p = j). As noted in Zhong et al. (2023), the cos( ωφ(m)(x − y)/2) 2 term in (4.1) is the Achilles’ heel of this strategy. We show that even under complete diversification, it would still introduce spurious peaks at 2x mod p

and 2y mod p. However, from (4.2) , we see that the true-signal peak exceeds these noise peaks by 

aN p/ 8. Hence, after the softmax operation, the model’s output would concentrate on the correct sum x + y mod p as long as the magnitude grows large enough during the training. In §A.2, we present ablation studies on full diversification, evaluating the performance of neural network predictors with limited frequencies and non-uniformly distributed phases under the same neuron budget constraint. The results show that fully diversified parameterization is the most 

parameter-efficient approach, yielding the largest logit gap between the ground-truth index and incorrect labels. 

# 5 Training Dynamics for Feature Emergence 

In this section, we provide a theoretical understanding of how features emerge during standard gradient-based training. Unlike previous theoretical works that focused on loss landscape analysis (e.g., Morwani et al., 2023), we offer a more complete view from the perspective of training dynamics. To achieve this, we track the evolution of the model’s parameters directly in the Fourier space. 

5.1 Background: Discrete Fourier Transform 

Motivated by empirical observations in §3, it is natural to apply the Fourier transform to model parameters and to track the evolution of the Fourier coefficients throughout the training process. This allows us to investigate how these Fourier features are learned. We begin by defining the Fourier basis matrix over Zp by Bp = [ b1, . . . , b p] ∈ Rp×p, where each column is given by 

b1 = 1p

√p , b2k =

r 2

p · [cos( ωk), . . . , cos( ωkp)] , b2k+1 =

r 2

p · [sin( ωk), . . . , sin( ωkp)] ,

where wk = 2 kπ/p for all k ∈ [ p−12 ]3. We then project the model parameters, ξm’s and θm’s, onto this basis. This change of basis is equivalent to applying the Discrete Fourier Transform (DFT, Sundararajan, 2001), yielding the Fourier coefficients: 

gm = B⊤ 

> p

θm, rm = B⊤ 

> p

ξm, ∀m ∈ [M ].  

> 3We choose pas a prime number greater than 2 to simplify the analysis.

14 To better interpret these coefficients, we group the sine and cosine components for each frequency k

and reparameterize them by their magnitude and phase. Denote by gkm = ( gm[2 k], g m[2 k + 1]) and 

rkm = ( rm[2 k], r m[2 k + 1]) the coefficient vector in correspondence to frequency k. Their magnitudes and phases are defined as follows. For the input layer , αkm denotes the magnitude and ϕkm the phase of the k-th frequency component of θm. For the output layer , βkm and ψkm are the corresponding magnitude and phase of ξm. These can be formalized as 

αkm =

r 2

p · ∥ gkm∥, ϕkm = atan( gkm), βkm =

r 2

p · ∥ rkm∥, ψkm = atan( rkm).

Here, atan( x) = atan2( −x[2] , x [1]) where atan2 : R × R 7 → (−π, π ] is the 2-argument arc-tangent. This polar representation is intuitive, as it directly relates the coefficients to a phase-shifted cosine, e.g., gm[2 k] · b2k[j] + gm[2 k + 1] · b2k+1 [j] = αkm · cos( wkj + ϕkm). By setting constant coefficients as 

α0 

> m

= gm[1] /√p and β0 

> m

= rm[1] /√p, we can recover the expanded form in (3.2). 

5.2 A Dynamical Perspective on Feature Emergence 

In the following, we provide a theoretical explanation of how the features — single-frequency 

and phase alignment patterns, i.e, Observation 1 and 2, emerge during training. For theoretical convenience, we adopt the quadratic activation (Arous et al., 2025) and focus on the training over a complete dataset Dfull , a familiar setting in prior work (e.g., Morwani et al., 2023; Tian, 2024). To better understand the training dynamics using the gradient-based optimization methods, we analyze the continuous-time limit of gradient descent — gradient flow, which is introduced below. 

Gradient Flow. Consider training a two-layer neural network as defined in (2.1) with one-hot input embeddings, i.e., hx = ex ∈ Rp, parameterized by Θ = {ξ, θ }, and the loss ℓ is given by the cross-entropy (CE) loss in (2.2) , evaluated over the full dataset Dfull . When training the parameter Θ

using the gradient flow, the dynamics are governed by the following ODE: 

∂tΘt = ∇ℓ(Θ t), ℓ(Θ) = − X

> x∈Zp

X

> y∈Zp

log ◦smax ◦ f (x, y ; ξ, θ ), e (x+y) mod p .

We consider gradient flow under an initialization that satisfies the following conditions. 

Assumption 5.1 (Initialization) . For each neuron m ∈ [M ], the network parameters (ξm, θ m) are initialized as θm ∼ κinit · pp/ 2 · (ϱ1[1] · b2k + ϱ1[2] · b2k+1 ) and ξm ∼ κinit · pp/ 2 · (ϱ2[1] · b2k + ϱ2[2] · b2k+1 ) where 

ϱ1, ϱ 2 i.i.d. 

∼ Unif( S1), k ∼ Unif([ p−12 ]) and κinit > 0 denotes a sufficiently small initialization scale. 

Assumption 5.1 posits that each neuron m is initialized randomly but contains a single-frequency component, all at the same small scale, i.e., αkm(0) = βkm(0) = κinit . This specialized initialization is adopted for theoretical convenience, allowing us to sidestep the chaotic frequency competition induced by entirely random initialization and study the evolution of one specific frequency. Specifically, the single-frequency is sufficient to capture the overall behavior as each frequency component evolves within its own orthogonal subspace . In §6.1, we will extend to the case where each neuron is initialized with multiple frequencies. 

5.3 Properties at the Initial Stage 

Given a sufficiently small initialization in Assumption 5.1, a key property at the initial stage is that the parameter magnitudes remain small, resulting in the softmax output being nearly uniform over. 15 Formally, ∥θm∥∞ and ∥ξm∥∞ are small such that the following equality holds approximately: 

smax ◦ f (x, y ; ξ, θ ) ≈ 1

p · 1p. (5.1) While (5.1) suggests that the neural network behaves as a poorly performing uniform predictor at the initial stage due to the small parameter magnitudes, this does not imply that the model learns nothing. Instead, the model can learn the "feature direction" of the data under the guidance of the gradient. In what follows, we examine the key components of the gradient and define the time threshold tinit to ensure all parameters remain within a small scale. 

Neuron Decoupling. We first show that the neurons are decoupled at the initial stage, meaning the evolution of parameters θm and ξm depends solely on (θm, ξ m)—the parameters of neuron m

itself—by using the approximation in (5.1) . To establish this, we compute the gradient and simplify it using periodicity. We derive that the gradient flow for each neuron m ∈ [M ] at the initial stage admits the following simplified form: for each entry j ∈ [p], we have 

∂tθm[j]( t) ≈ 2p ·

> (p−1) /2

X

> k=1

αkm(t) · βkm(t) · cos  ωkj + ψkm(t) − ϕkm(t), (5.2a) 

∂tξm[j]( t) ≈ p ·

> (p−1) /2

X

> k=1

αkm(t)2 · cos( ωkj + 2 ϕkm(t)) . (5.2b) Here, we use the Fourier expansion of parameters θm(t) and ξm(t) as given in (3.2) . In words, the first equation states that θm evolves as a superposition of cosines, where each frequency k contributes with a rate proportional to the product of the input and output magnitudes αkm · βkm, modulated by the phase difference ψkm − ϕkm between the two layers. The second equation shows that ξm evolves similarly, but its rate depends only on the input magnitude αkm squared, with a phase of 2ϕkm.Crucially, the dynamics, i.e., ∂tθm(t) and ∂tξm(t), only depends on {(αkm, β km, ϕ km, ψ km)}k∈[( p−1) /2] 

and rm[1] that corresponds to neuron m. This demonstrates a decoupled evolution among neurons. Hence, in the remaining section, we can focus on a fixed neuron m. Similar decoupling technique with a similar small output scale is also seen in Lee et al. (2024); Chen et al. (2025) for ℓ2-loss. 

Remark 5.1 (Equivalence to Margin Maximization under Small Initialization) . Notice that the modular addition task is a multi-class classification problem. To understand the feature emergence, Morwani et al. (2023) considers an average margin maximization problem, where the margin is defined by 

max  

> ξ,θ

ℓAM (ξ, θ ) with ℓAM (ξ, θ ) = X

> x∈Zp

X

> y∈Zp



f (x, y ; ξ, θ )[( x + y) mod p] − 1

p

X

> j∈Zp

f (x, y ; ξ, θ )[ j]



.

In comparison, given the small scale of parameters during the initial stage, we can show that, similar to the approximation in (5.1) , the loss takes the approximate form: 

ℓ(ξ, θ ) = − X

> x∈Zp

X

> y∈Zp

f (x, y ; ξ, θ )[( x + y) mod p] + X

> x∈Zp

X

> y∈Zp

log 

 pX

> j=1

exp( f (x, y ; ξ, θ )[ j]) 



≈ − X

> x∈Zp

X

> y∈Zp

f (x, y ; ξ, θ )[( x + y) mod p] + 1

p

X

> x∈Zp

X

> y∈Zp
> p

X

> j=1

f (x, y ; ξ, θ )[ j]

| {z }

= −ℓAM (ξ, θ )+p2 log p, 

16 where we use the first-order approximations exp( x) ≈ 1 + x and log(1 + x) ≈ x for small x. Following this, we observe that during the initial stage, minimizing the loss in (2.2) is equivalent to optimizing the average margin. This connection underpins the theoretical insights in Morwani et al. (2023), which links the margin maximization problem to empirical observations. 

Section Roadmap. With slight abuse of notation, we let k⋆ denote the initial frequency of each neuron (see Assumption 5.1) and use the superscript ⋆ instead of k⋆ to simplify the notation further. In the following, we aim to show that (i) the single-frequency pattern, i.e., gm[j] = rm[j] = 0 for all 

j̸ = 2 k⋆, 2k⋆ + 1 , is preserved throughout the gradient flow (see §5.4), and (ii) the phases of the first and second layers will align such that 2ϕ⋆m(t) − ψ⋆m(t) mod 2 π converges to 0 (see §5.5). 

5.4 Preservation of Single-Frequency Pattern 

Recall that the dynamics of the parameters are approximately given by the entry-wise ODEs in (5.2a) and (5.2b) . Our goal is to lift these entry-wise dynamics into the Fourier domain and show that the single-frequency pattern is preserved. The argument proceeds in three steps: (i) project the entry-wise ODEs onto the Fourier basis Bp to obtain the dynamics of the Fourier coefficients gm and 

rm; (ii) convert to polar coordinates (αkm, ϕ km) and (βkm, ψ km) via the chain rule; and (iii) show that the orthogonality of the Fourier basis ensures different frequencies decouple, so that non-feature frequencies initialized at zero remain negligible. We begin with the constant component. Note the constant frequency, i.e., gm[1] and rm[1] , remains almost 0 due to the centralized dynamics: 

∂tθm[j]( t), ∂ tξm[j]( t) ∈ span( {bτ }pτ =2 ), ∀j ∈ [p]. (5.3) By definition, we can show that ∂tgm[1]( t) = ⟨b1, ∂ tθm(t)⟩ and ∂trm[1]( t) = ⟨b1, ∂ tξm(t)⟩. Given the zero-initialization gm[1] = rm[1] = 0 (see Assumption 5.1), and utilizing (5.3), it follows that 

∂tgm[1]( t) ≈ ∂trm[1]( t) ≈ 0 s.t. gm[1]( t) ≈ rm[1]( t) ≈ 0, (5.4) holds throughout the first stage. Moreover, to establish frequency preservation, we track the magnitudes of each frequency, i.e., {αkm}k∈[( p−1) /2] and {βkm}k∈[( p−1) /2] . Thanks to the orthogonality of the Fourier basis, by applying the chain rule, for each frequency k, it holds that 

∂tαkm(t) ≈ 2p · αkm(t) · βkm(t) · cos  2ϕkm(t) − ψkm(t),∂tβkm(t) ≈ p · αkm(t)2 · cos  2ϕkm(t) − ψkm(t),

where the evolution of the magnitudes for frequency k only depends on (αkm, β km, ϕ km, ψ km). Given the initial value αkm(0) = βkm(0) = 0 for k̸ = k⋆ (see Assumption 5.1), we have 

αkm(t) ≈ βkm(t) ≈ 0, ∀k̸ = k⋆. (5.5) Recall that we define αkm = p2/p · ∥ gkm∥ and βkm = p2/p · ∥ rkm∥. By combining (5.4) and (5.5) , we can establish the preservation of single-frequency pattern (see Figure 14 for experimental results): 

gm[j]( t) ≈ rm[j]( t) ≈ 0, ∀j̸ = 2 k⋆, 2k⋆ + 1 . (5.6) Based on (5.6), we can further simplify (5.2a) and (5.2b) as follows 

∂tθm[j]( t) ≈ 2p · α⋆m(t) · β⋆m(t) · cos( ω⋆j + ψ⋆m(t) − ϕ⋆m(t)) ,∂tξm[j]( t) ≈ p · α⋆m(t)2 · cos( ω⋆j + 2 ϕ⋆m(t)) . (5.7) 17 π 0          

> sin( D⋆m)≥0
> sin( D⋆m)<0
> D⋆m(t)
> +( D⋆m(t)−π
> 2)
> D⋆m(t)
> −(D⋆m(t)−π
> 2)

(a) Illustration of Phase Alignment Behavior. (b) Dynamics of Magnitudes and Phases for Neuron m.

Figure 8: Visualizations of the alignment behavior and neuron evolution dynamics with κinit = 0 .02 .Figure (a) illustrates the dynamics of the normalized phase difference D⋆m(t) given by (5.9) . Initialized randomly on the unit circle, the gradient flow will always drive D⋆m(t) to 0, regardless of the initial half-space. Figure (b) plots the dynamics of magnitudes and phases of the feature frequency for a specific neuron m during the initial stage of training. 2ϕ⋆m and ψ⋆m evolves to align, and magnitudes 

α⋆m and β⋆m starts growing rapidly once the phases are well-aligned. For each neuron, its evolution can be approximately characterized by a four-particle dynamical system consisting of magnitudes α⋆m(t) and β⋆m(t) and phases ϕ⋆m(t) and ψ⋆m(t). We formalize the result in (5.6) and the approximate arguments above into the following theorem. 

Theorem 5.2 (Informal) . Under the initialization in Assumption 5.1, for a given threshold Cend > 0, we define the initial stage as (0 , t init ], where tinit := inf {t ∈ R+ : max m∈[M ] ∥θm(t)∥∞ ∨ ∥ ξm(t)∥∞ ≤ Cend }.Suppose that log M/M ≲ c−1/2 · (1 + o(1)) , κinit = o(M −1/3) and Cend ≍ κinit , given sufficiently small 

κinit , we have max k̸ =k⋆ inf t∈(0 ,t init ] αkm(t) ∨ βkm(t) = o(κinit ).

The formal statement and proof of Theorem 5.2 is provided in §B.4. The theorem states that under a small random initialization, during the initial training stage where the feature magnitudes remain within a constant factor of their starting values, the non-feature frequencies, which are initialized at zero, will not grow beyond a negligible o(κinit ). We remark that the initial stage is sufficient to understand the dynamics of feature emergence. As we will show in the next section (§5.5), a constant-order growth of the parameter norms, i.e., max m∈[M ] ∥θm(t)∥∞ ∨ ∥ ξm(t)∥∞ ≲ κinit ,is sufficient to achieve the desired phase alignment. 

5.5 Neuron-Wise Phase Alignment 

We proceed to investigate the emergence of the phase alignment phenomenon. To build intuition, we first consider a special stationary point ψ⋆m = 2 ϕ⋆m. According to the dynamics given by (5.7) , it is straightforward to observe the stationarity, as: 

∂tθm[j]( t) ∝ cos( ω⋆j + ϕ⋆m(t)) , ∂tξm[j]( t) ∝ cos( ω⋆j + 2 ϕ⋆m(t)) = cos( ω⋆j + ψ⋆m(t)) .

This implies that at the stationary point where θm[j]( t) ∝ cos( ω⋆j + ϕ⋆m(t)) and ξm[j]( t) ∝ cos( ω⋆j +

ψ⋆m(t)) , θm[j]( t) and ξm[j]( t) evolve in the same direction as themselves. Hence, the phases cease to 18 rotate and remain stationary. Formally, by applying the chain rule over (5.7), we have 

∂t exp( iϕ ⋆m(t)) ≈ 2p · β⋆m(t) · sin  2ϕ⋆m(t) − ψ⋆m(t) · exp ( i {ϕ⋆m(t) − π/ 2}) ,∂t exp( iψ ⋆m(t)) ≈ p · α⋆m(t)2/β ⋆m(t) · sin  2ϕ⋆m(t) − ψ⋆m(t) · exp ( i {ψ⋆m(t) + π/ 2}) . (5.8) The first line tracks the input phase ϕ⋆m and the second tracks the output phase ψ⋆m. Both are driven by the shared misalignment factor sin(2 ϕ⋆m − ψ⋆m): when phases are misaligned this factor is nonzero and drives rotation, while at alignment it vanishes and both phases freeze. The −π/ 2

versus +π/ 2 in the exponential indicates that the two phases rotate in opposite directions on the unit circle, converging toward each other. See Figure 8a for an illustration. Thus, phases ϕ⋆m and ψ⋆m

evolve in the opposite directions, with rotation speed primarily determined by the magnitudes and misalignment level, quantified by | sin(2 ϕ⋆m(t) − ψ⋆m(t)) |. This suggests that 2ϕ⋆m will eventually “meet” ψ⋆m. To understand the dynamics of the alignment behavior, we track D⋆m(t) = 2 ϕ⋆m(t)−ψ⋆m(t)mod 2 π ∈ [0 , 2π). Using (5.8), the chain rule gives that 

∂t exp( iD⋆m(t)) ≈  4β⋆m(t) − α⋆m(t)2/β ⋆m(t) · p · sin  D⋆m(t) · exp ( i{D⋆m(t) − π/ 2})

| {z }

zero-attractor term 

. (5.9) Notably, though {0, π } are both stationary points of (5.9) , the evolution of D⋆m(t) is consistently directed toward 0. This is due to the sign of sin( D⋆m(t)) , which adaptively ensures ∂t exp( iD⋆m(t)) 

converges only to zero (see Figure 8a). Thus, we can establish the phase alignment behavior below: 

2ϕ⋆m(t) − ψ⋆m(t) mod 2 π → 0 when t → ∞ .

Magnitude Remains Small after Alignment. Note the above analysis hinges on the parameter scale being sufficiently small, ensuring that the dynamics can be fully decoupled neuron-wise and that the approximation error remains negligible, as discussed in §5.3. To complete the argument, it remains to show that α⋆m(t) and β⋆m(t) remain small even after the phase is well-aligned. Under the initialization specified in Assumption 5.1, we can establish the following relationship: 

sin( D⋆m(t)) = sin( D⋆m(0)) · { R⋆m(t) · (2 R⋆m(t)2 − 1) }−1, where R⋆m(t) := β⋆m(t)/κ init .

Here, R⋆m(t) measures how much the output magnitude β⋆m has grown relative to its initial value 

κinit . The identity is an exact conservation law that couples phase alignment to magnitude growth: the product R⋆m · (2 R⋆2 

> m

− 1) on the right-hand side is monotonically increasing in R⋆m, so a increase in magnitude must be accompanied by a proportional decrease in misalignment sin( D⋆m). Therefore, when misalignment level sin( D⋆m(t)) reaches a small threshold δ > 0, the ratio R⋆m(t) is bounded by {sin( D⋆m(0)) /δ }1/3. Since α⋆m(t) ≍ β⋆m(t), when the neuron is well-aligned, the parameter scales remain on the same order as at initialization. This aligns with experimental results in Figure 8b. We summarize these findings in the theorem below. 

Theorem 5.3. Consider the main flow dynamics under the initialization in Assumption 5.1. For any initial misalignment D⋆m(0) ∈ [0 , 2π) and small tolerance level δ ∈ (0 , 1) , the minimal time tδ required for the phase to align such that |D⋆m(t)| ≤ δ satisfies that 

tδ ≍ (pκ init )−1 ·  1 − { sin( D⋆m(0)) /δ }−1/3 + max {π/ 2 − | D⋆m(0) − π|, 0},

and the magnitude at this time is given by β⋆m(tδ) ≍ κinit · { sin( D⋆m(0)) /δ }1/3. Moreover, in the mean-field regime m → ∞ , let ρt = Law  ϕ⋆m(t), ψ ⋆m(t) for all t ∈ R+. Then, given ρ0 = λ⊗2 

> unif

, we have 

ρ∞ = T#λunif with T : φ 7 → (φ, 2φ) mod 2 π, 

where we let λunif denote the uniform law on (0 , 2π].

19 Theorem 5.3 provides two key insights into the learning dynamics. First, it establishes that the convergence time depends on three key factors: (i) the initial misalignment level, measured by | sin( D⋆m(0)) |, (ii) the extent to which D⋆m(0) deviates from the intermediate stage π 

> 2

or 3π 

> 2

for 

D⋆m(0) ∈   π 

> 2

, 3π

> 2

, and (iii) the initialization scale κinit and modulus p. Second, the theorem provides a theoretical justification for the emergence of phase symmetry (Observation 3) in the mean-field regime. For the formal theorem, a proof sketch, and the complete proof, see Theorem B.7, §B.3.1, and §B.5, respectively. This result is derived from an analysis of a simplified "main flow" of the dynamics, which neglects approximation errors and represents the limiting case as κinit 7 → 0. This simplified flow is compared visually to the full training dynamics in Figures 8b and 15b. 

# 6 Theoretical Extensions 

In this section, we extend the results from §5 to two more general scenarios: lottery mechanism under multi-frequency initialization in §6.1 and the dynamics with ReLU activation in §6.2. 

6.1 Theoretical Underpinning of Lottery Ticket Mechanism 

To understand why a single frequency pattern emerges from a random, multi-frequency initialization (Observation 1), we can analyze the training dynamics for each frequency within a specific neuron. The ODEs capture the dynamics of competition in (6.1), which are fully derived in §5.2. 

∂tαkm(t) ≈ 2p · αkm(t) · βkm(t) · cos( Dkm(t)) ,∂tβkm(t) ≈ p · αkm(t)2 · cos( Dkm(t)) ,∂tDkm(t) ≈ −  4βkm(t) − αkm(t)2/β km(t) · p · sin( Dkm(t)) , ∀k̸ = 0 ,

(6.1) and ∂tα0

> m

(t) ≈ ∂tβ0

> m

(t) ≈ 0. In words, the first two equations state that the magnitudes αkm and βkm

grow at rates proportional to cos( Dkm): when the phases are well-aligned ( Dkm ≈ 0), cos( Dkm) ≈ 1

and magnitudes grow rapidly, when misaligned ( Dkm < π/ 2), magnitudes decrease. The third equation governs the misalignment itself: Dkm decreases at a rate proportional to sin( Dkm), so it is attracted toward zero. Together, these form a self-reinforcing loop: 

Better alignment accelerates growth, and larger magnitudes speed up alignment. 

A key insight from (6.1) is that the dynamics are fully decoupled . The evolution of each frequency is self-contained , proceeding orthogonally without cross-frequency interaction. This structural independence establishes the competitive environment required for the lottery ticket mechanism. The ODEs also reveal a powerful reinforcing dynamic : the growth rate, proportional to the alignment term cos( Dkm(t)) , is amplified by the magnitudes This creates a “larger-grows-faster”positive feedback loop that drives the winner’s dominance. As introduced in §3.2, this process is not chaotic but is instead a predictable competition governed by a "Lottery Ticket Mechanism". Applying an ODE comparison lemma (Smith, 1995), we can compare the evolution of frequency magnitudes based on their initial conditions. This allows us to formally prove that the "lottery ticket" drawn at initialization determines which frequency will ultimately dominate. We formalize the results into the following corollary. 

Corollary 6.1. Consider a multi-frequency initialization akin to Assumption 5.1. For a given dominance level ε ∈ (0 , 1) and fixed neuron m, let tε be the minimal time required for the winning frequency k⋆ to dominate all others, such that max k̸ =k⋆ βkm(t)/β ⋆m(t) ≤ ε. Then, it holds that 

k⋆ = min 

> k

eDkm(0) , tε ≲ π2p−(2 c+3) 

κinit 

+ (c + 1) log p + log 11−ε

pκ init · { 1 − 2c2π2 · (log p/p )2} ,

20 (a) Heatmaps of parameters after discrete Fourier transform for the first 20 neurons with ReLU activation at the intial stage. 

(b) Dynamics of magnitude and phase for Neuron m with ReLU activation. 

Figure 9: Learned feature and dynamics of parameters initialized at Assumption 5.1 with p = 23 

and ReLU activation. Figure (a) shows heatmaps of the parameters after DFT at initialization and at the end of the initial stage. Similar to the quadratic activation (see Figure 14), the single-frequency pattern is approximately maintained, with small values emerging at frequencies “ 3k⋆”, “ 5k⋆” for 

θm, and “ 2k⋆”, “ 3k⋆” for ξm. Figure (b) plots the dynamics of a specific neuron m. Here, the phase quickly aligns, i.e., ψ⋆m ≈ 2ϕ⋆m, and the magnitudes α⋆m and β⋆m grow rapidly and synchronously. 

where the bound holds under mild conditions and with a high probability of at least 1 − eΘ( p−c).

The proof is deferred to §C.1. Corollary 6.1 formalizes our Lottery Ticket Mechanism in Observation 6. It states that under a multi-frequency random initialization where all frequencies start with identical magnitudes, the frequency with the smallest initial misalignment eD⋆m will inevitably dominate. This dominance occurs rapidly, on a timescale of eO  log p/ (pκ init ).

6.2 Dynamics Beyond Quadratic Activation 

So far, we have focused on quadratic activation for more precise interpretation. However, experi-mental results indicate that quadratic activation is not essential or can be even problematic . In practice, quadratic activation often leads to unstable training with highly imbalanced neurons. 4 In contrast, ReLU activation consistently leads to the emergence of desired features, as shown in §3. In this section, we investigate the training dynamics of ReLU activation. 

Training Dynamics of ReLU Activation. In parallel, we adopt an experimental setup identical to that of Figure 14 using the single-frequency initialization specified in Assumption 5.1, with the only   

> 4The failure of the quadratic activation stems from the significant disparity in growth rates among neurons due to the nature of the quadratic function. Specifically, a few neurons with more well-aligned initial phases grow faster in magnitude and come to dominate the output, leaving an insufficient growth of other neurons. This issue can be mitigated using techniques such as normalized GD (Cortés, 2006) or spherical GD.

21 modification being the replacement of quadratic activation with ReLU activation. The experimental results are shown in Figure 9, and the key observation is summarized below. 

Observation 7 (ReLU Leakage). For ReLU activation, although each neuron is initialized with a single frequency k⋆, such a pattern is preserved approximately with small leakage during the training, with small values emerging at other frequencies. For θm, the values emerges at frequencies “ 3k⋆”, “5k⋆” and higher odd multiples, with magnitudes decaying gradually. In contrast, for ξm, these appear at “ 2k⋆”, “ 3k⋆”, and others, which also exhibit decay with increasing multiplicative factors. As shown in Observation 5, ReLU mostly preserves the single-frequency pattern but still exhibits small leakage at other frequencies. For instance, in Figure 9a, Neuron 3 is initialized with dominant frequency 1. After 30,000 training steps, small values emerge at frequencies 3 and 5 in θm, and at 2 and 3 in ξm. In what follows, we first formalize the multiplicative relationship among frequencies. 

Definition 6.2 (Frequency Multiplication) . Given k, τ ∈ [ p−12 ], we say frequency τ is r-fold multiple of k

under modulo p if τ = rk mod p or p − τ = rk mod p for some r ∈ [ p−12 ], denoted by τ p

= rk .

Now we are ready to present the main result for training dynamics of ReLU activation. To state the result, we introduce ∆kυ, which measures the magnitude of the gradient component at frequency 

k for parameter υ ∈ { θm, ξ m}. In other words, ∆kυ captures how strongly a single gradient step pushes energy into frequency k. The proposition compares this “push” at a non-feature frequency 

k to that at the dominant frequency k⋆, where rk denotes the harmonic order of k relative to k⋆.

Proposition 6.3. Consider gradient update with respect to the decoupled loss ℓm and assume that (θm, ξ m)

satisfying (3.1) . Let ∆kυ = p⟨∇ υℓm, b 2k⟩2 + ⟨∇ υℓm, b 2k+1 ⟩2 denote the incremental scale for frequency 

k ∈ [ p−12 ]. Under the asymptotic regime where p → ∞ , it holds that (i) ∆kθm /∆⋆θm = Θ( r−2 

> k

) and ∆kξm /∆⋆ξm = Θ( r−2 

> k

) · 1(r is odd ), where k p

= rkk⋆;(ii) P∥ 

> k⋆

∇υℓm ∝ υ for υ ∈ { θm, ξ m} when ψm = 2 ϕm mod p, where P∥ 

> k

= I − P  

> j≥1,j ̸=2 k, 2k+1

bj b⊤ 

> j

.

See §C.2 for a detailed proof. In words, Part (i) states that the leakage to a non-feature frequency 

k, i.e., the rk-th harmonic of k⋆, decays as 1/r 2 

> k

relative to the dominant frequency. For the output layer ξm, an additional parity constraint holds: only odd harmonics of k⋆ receive any leakage, while even harmonics receive zero. Part (ii) states that the gradient component at the feature frequency itself is proportional to the parameter vector, so the feature direction is reinforced without phase rotation — consistent with the phase alignment observed for quadratic activation. Here, P∥ 

> k⋆

is the projection onto the Fourier subspace spanned by frequency k⋆ (i.e., it filters out all other frequencies from the gradient). This provide a quantitative explanation of the emergence dynamics of single frequency and phase alignment pattern in Observation 1 and 2. 

# 7 Conclusion 

In this paper, we provide an end-to-end reverse engineering of how two-layer neural networks learn modular addition, from training dynamics to the final learned model. First, we show that trained networks implement a majority-voting algorithm in the Fourier domain through phase alignment and model symmetry. Second, we explain how these features emerge from a lottery-like mechanism where frequencies compete within each neuron, with the winner determined by initial magnitude and phase misalignment. Third, we characterize grokking as a three-stage process where weight decay prunes non-feature frequencies, transforming a perturbed Fourier representation into a clean, 22 generalizable solution. These findings offer insights into the dynamics of feature learning in neural networks, a mechanism that may extend to more general tasks. 

# References 

Allen-Zhu, Z. and Li, Y. (2019). What can resnet learn efficiently, going beyond kernels? Advances in Neural Information Processing Systems , 32. 5 Arnaboldi, L., Dandi, Y., Krzakala, F., Pesce, L., and Stephan, L. (2024). Repetita iuvant: Data repeti-tion allows sgd to learn high-dimensional multi-index functions. arXiv preprint arXiv:2405.15459 .5Arous, G. B., Erdogdu, M. A., Vural, N. M., and Wu, D. (2025). Learning quadratic neural networks in high dimensions: Sgd dynamics and scaling laws. arXiv preprint arXiv:2508.03688 . 15 Ba, J., Erdogdu, M. A., Suzuki, T., Wang, Z., Wu, D., and Yang, G. (2022). High-dimensional asymptotics of feature learning: How one gradient step improves the representation. Advances in Neural Information Processing Systems , 35:37932–37946. 5 Berthier, R., Montanari, A., and Zhou, K. (2024). Learning time-scales in two-layers neural networks. 

Foundations of Computational Mathematics , pages 1–84. 5 Chen, S. and Li, Y. (2024). Provably learning a multi-head attention layer. arXiv preprint arXiv:2402.04084 . 30 Chen, S., Wu, B., Lu, M., Yang, Z., and Wang, T. (2025). Can neural networks achieve optimal computational-statistical tradeoff? an analysis on single-index model. In The Thirteenth International Conference on Learning Representations . 5, 16 Cortés, J. (2006). Finite-time convergent gradient flows with applications to network consensus. 

Automatica , 42(11):1993–2000. 21 Damian, A., Lee, J., and Soltanolkotabi, M. (2022). Neural networks can learn representations with gradient descent. In Conference on Learning Theory , pages 5413–5452. PMLR. 5 Ding, X. D., Guo, Z. C., Michaud, E. J., Liu, Z., and Tegmark, M. (2024). Survival of the fittest representation: A case study with modular addition. arXiv preprint arXiv:2405.17420 . 5 Doshi, D., Das, A., He, T., and Gromov, A. (2023). To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets. arXiv preprint arXiv:2310.13061 . 5, 6, 13, 26 Doshi, D., He, T., Das, A., and Gromov, A. (2024). Grokking modular polynomials. arXiv preprint arXiv:2406.03495 . 4 Gromov, A. (2023). Grokking modular arithmetic. arXiv preprint arXiv:2301.02679 . 4, 6, 7, 13 Hirsch, M. W. (1982). Systems of differential equations which are competitive or cooperative: I. limit sets. SIAM Journal on Mathematical Analysis , 13(2):167–179. 55 Kamke, E. (1932). Zur theorie der systeme gewöhnlicher differentialgleichungen. ii. Acta Mathematica ,58(1):57–85. 55 Kramer, B. and MacKinnon, A. (1993). Localization: theory and experiment. Reports on Progress in Physics , 56(12):1469. 26 23 Kumar, T., Bordelon, B., Gershman, S. J., and Pehlevan, C. (2024). Grokking as the transition from lazy to rich training dynamics. In The Twelfth International Conference on Learning Representations .5, 8 Kunin, D., Marchetti, G. L., Chen, F., Karkada, D., Simon, J. B., DeWeese, M. R., Ganguli, S., and Miolane, N. (2025). Alternating gradient flows: A theory of feature learning in two-layer neural networks. arXiv preprint arXiv:2506.06489 . 5 Lee, J. D., Oko, K., Suzuki, T., and Wu, D. (2024). Neural network learns low-dimensional polynomials with sgd near the information-theoretic limit. Advances in Neural Information Processing Systems ,37:58716–58756. 5, 16 Liu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., and Williams, M. (2022). Towards understanding grokking: An effective theory of representation learning. Advances in Neural Information Processing Systems , 35:34651–34663. 3, 5 Lyu, K., Jin, J., Li, Z., Du, S. S., Lee, J. D., and Hu, W. (2023). Dichotomy of early and late phase implicit biases can provably induce grokking. arXiv preprint arXiv:2311.18817 . 5 Mallinar, N., Beaglehole, D., Zhu, L., Radhakrishnan, A., Pandit, P., and Belkin, M. (2024). Emergence in non-neural models: grokking modular arithmetic via average gradient outer product. arXiv preprint arXiv:2407.20199 . 5 McCracken, G., Moisescu-Pareja, G., Letourneau, V., Precup, D., and Love, J. (2025). Uncover-ing a universal abstract algorithm for modular addition in neural networks. arXiv preprint arXiv:2505.18266 . 5 Mohamadi, M. A., Li, Z., Wu, L., and Sutherland, D. J. (2024). Why do you grok? a theoretical analysis of grokking modular addition. arXiv preprint arXiv:2407.12332 . 5 Morwani, D., Edelman, B. L., Oncescu, C.-A., Zhao, R., and Kakade, S. (2023). Feature emergence via margin maximization: case studies in algebraic tasks. arXiv preprint arXiv:2311.07568 . 4, 5, 6, 14, 15, 16, 17 Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. (2023). Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217 . 3, 4, 6, 7, 11, 13 Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. 

Advances in neural information processing systems , 32. 6 Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V. (2022). Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177 . 3, 4 Ren, Y., Nichani, E., Wu, D., and Lee, J. D. (2025). Emergence and scaling laws in sgd learning of shallow neural networks. arXiv preprint arXiv:2504.19983 . 5 Shi, Z., Wei, J., and Liang, Y. (2022). A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. arXiv preprint arXiv:2206.01717 . 5 Shi, Z., Wei, J., and Liang, Y. (2023). Provable guarantees for neural networks via gradient feature learning. Advances in Neural Information Processing Systems , 36:55848–55918. 5 24 Smith, H. L. (1995). Monotone dynamical systems: an introduction to the theory of competitive and cooperative systems: an introduction to the theory of competitive and cooperative systems . Number 41. American Mathematical Soc. 20, 55 Sundararajan, D. (2001). The discrete Fourier transform: theory, algorithms and applications . World Scientific. 14 Tian, Y. (2024). Composing global optimizers to reasoning tasks via algebraic objects in neural nets. 

arXiv preprint arXiv:2410.01779 . 4, 5, 6, 15, 59, 60 Tian, Y. (2025). A framework on dynamics of feature emergence and delayed generalization. arXiv preprint arXiv:2509.21519 . 5 Wang, P. and Wang, Z. (2025). Why neural network can discover symbolic structures with gradient-based training: An algebraic and geometric foundation for neurosymbolic reasoning. arXiv preprint arXiv:2506.21797 . 4, 5, 59, 60 Wu, W., Jaburi, L., jacob drori, and Gross, J. (2025). Towards a unified and verified understanding of group-operation networks. In The Thirteenth International Conference on Learning Representations . 5 Yip, C. H., Agrawal, R., Chan, L., and Gross, J. (2024). Modular addition without black-boxes: Com-pressing explanations of mlps that compute numerical integration. arXiv preprint arXiv:2412.03773 .5, 7 Zhong, Z., Liu, Z., Tegmark, M., and Andreas, J. (2023). The clock and the pizza: Two stories in mechanistic explanation of neural networks. Advances in neural information processing systems ,36:27223–27250. 3, 4, 13, 14 25 A Additional Experimental Details and Results 

A.1 Detailed Interpretation of Grokking Dynamics in Section 3.3 

Inverse Participation Ratio (IPR). To quantitatively characterize the concentration of Fourier coefficients at a specific frequency k, or equivalently, the sparsity level of the learned parameters in the Fourier domain, we introduce the inverse participation ratio (IPR). This metric, originally used in physics as a localization measure (Kramer and MacKinnon, 1993), was recently adopted in Doshi et al. (2023) as a progress measure to understand the generalization behavior in machine learning. Specifically, given ν ∈ Rd, the IPR is defined as IPR (ν) = ( ∥ν∥2r/∥ν∥2)2r for some integer r > 1. We calculate the IPR for all {θm}m∈[M ] and {ξm}m∈[M ], and take the average. 

Definition of Progress Measure. Here, we provide a formal definition of the progress measure for grokking used in Figure 5, which is defined over the model output and parameters θm’s and ξm’s. 

- Loss : ℓD = − X

> (x,y )∈D

log ◦smax ◦ f (x, y ; ξ, θ ), e (x+y) mod p ;

- Accuracy : Acc D = 1

|D| 

X

> (x,y )∈D

1 argmax  smax ◦ f (x, y ; ξ, θ ) = ( x + y) mod p ;

- IPR : IPR θ,ξ = 12M

> M

X

> m=1

∥B⊤ 

> p

θm∥4

∥B⊤ 

> p

θm∥2

!4

+ 12M

> M

X

> m=1

∥B⊤ 

> p

ξm∥4

∥B⊤ 

> p

ξm∥2

!4

;

- ℓ2-norm : ℓ2-norm θ,ξ = 12M

> M

X

> m=1

(∥θm∥2 + ∥ξm∥2).

Three-Phase Dynamics of Grokking. As discussed in §3.3, the grokking process is governed by the interplay between two primary forces: loss minimization and weight decay. The dynamics unfold across three major phases: an initial memorization stage dominated by the loss gradient, followed by two distinct generalization stages where the balance between these forces shifts. Below, we provide a more detailed account of each phase by examining our key progress measures. - Phase I: Memorization. Initially, the network quickly memorizes the training data, reaching 100% accuracy. Test accuracy also improves to around 70%, aided by the model’s symmetric architecture. Figure 10 provide clear empirical evidence for this perfect memorization. The model achieves flawless accuracy and high confidence on the training data (dark blue entries) and test data whose symmetric counterparts were part of the training set (light blue entries). Note that the model completely fails on the truly "unseen" held-out test data (white entries outlined in red), confirming it has learned to exploit symmetry rather than achieving true generalization at this stage. During this time, feature frequencies become roughly aligned (see Figure 5c) and their sparsity increases significantly (see Figure 5d). While these dynamics resemble a full-data setup, the incomplete data yields a perturbed Fourier solution that overfits the training set. - Phase II: Loss-Driven Norm Growth with Rapid Feature Cleanup. After reaching perfect training accuracy, the model’s parameters continue evolving to further reduce the loss. Instead of naively amplifying parameter magnitudes, weight decay actively steers their direction. As shown in Figure 5d, the dynamic is thus a balancing act: the loss gradient pushes to scale up parameters, while weight decay prunes unnecessary frequencies to decelerate the growth of norm. 26 Figure 10: Heatmaps of trained model from Figure 5 at the end of the memorization stage . The left panel displays the data distribution: dark blue entries represent training data, light blue entries are test data whose symmetric counterparts are in the training set, and white entries (outlined in red) are the remaining held-out test data. The middle panel shows the model’s accuracy, demonstrating that it has perfectly memorized all training data and their symmetric variants but completely fails to generalize to the held-out data. Finally, the right panel visualizes the model’s post-softmax output on the correct answer for each data point, further confirming the accuracy results. 

Figure 11: Data distribution during the memorization stage. The first panel illustrates the data partitioning, which, unlike in Figure 10, uses the following scheme: white entries denote test data, dark brown entries represent common (symmetric) training data, and light brown entries (outlined in red) denote rare (asymmetric) training data. The remaining three plots track the model’s accuracy, demonstrating a two-stage memorization scheme. At initialization, the model performs at a low, chance-level accuracy. However, after approximately 1000 steps, it masters the common symmetric training data, but its performance on rare asymmetric data drops to zero, overwriting any initially correct random predictions. By the end of the memorization stage, the model finally memorizes these rare data points, achieving 100% training accuracy - Phase III: Slow Cleanup Driven Solely by Weight Decay. By the end of Phase II, training loss is near-zero and test accuracy approaches 100%. Thus, in the final stage, the diminished loss gradient allows weight decay to dominate, causing the parameter norm to decrease (see Figure 5d). Without the main driving force of the loss, this final “cleanup" phase is extremely slow (see Figure 5b), during which test accuracy gradually converges to 100%. 27 Figure 12: Heatmaps of parameters after applying discrete Fourier transform along training epoches for the first 20 neurons with p = 23 under train-test split setup. At the end of the memorization stage (step 2200), a single-frequency pattern has started to emerge, accompanied by noisy perturbations in other frequencies. This initial "perturbed Fourier solution" is subsequently refined, as weight decay prunes the noisy, non-feature frequencies to reveal the final, clean pattern. 

A.2 Ablations Studies for Fully-Diversified Parametrization 

In this section, we present comprehensive ablation studies investigating the efficiency of the fully diversified parametrization as defined in Definition 4.1. We evaluate the models based on the CE loss defined in Equation 2.2 while maintaining a fixed, equivalent computational budget. All predictors share a fixed neuron constraint M = 128 and scale αmβ2 

> m

= 1 for all m ∈ [M ].The ablation is performed across two distinct dimensions of the diversification strategy: 

• Ablation of Frequency Diversification. We examine the impact of restricting the number of learned frequencies. We use only a subset of frequencies K ⊆ [ p−12 ] with |K| = {1, 2, 4, 8}.The phases for each selected frequency k are kept uniformly distributed over [0 , 2π).

• Ablation of Phase Uniformity. We investigate the effect of restricting the range of the phase distribution. The model utilizes the full set of frequencies, but the phase for each frequency is uniformly distributed over a restricted interval [0 , ιπ ) with ι ∈ { 0.4, 0.8, 1.2, 1.6}.The ablation study results in Table 2 confirm that full frequency and phase diversification is essential for maximizing parametrization efficiency under fixed constraints. Part I shows that the CE loss decreases rapidly as the number of frequencies increases, dropping from 1.64 at |K| = 1 

to 7.41 × 10 −15 for the full frequency set, underscoring the critical role of spectral richness. Part II reveals that restricting the phase distribution range significantly degrades performance. For instance, the loss is 4.82 for [0 , 0.4π) but achieves the minimum of 7.41 × 10 −15 only when the phases span the full [0 , 2π) interval. These findings collectively validate that the fully diversified parametrization achieves the maximum efficiency. Visually, this maximum efficiency is confirmed in Figure 13, where the fully diversified parametrization generates the highest confidence prediction 28 Part I: Frequency Diversity Ablation. 

Loss 1 Freqs 2 Freqs 4 Freqs 8 Freqs Full Freqs Average 1.64 6.02 × 10 −1 2.88 × 10 −2 2.99 × 10 −8 7.41 × 10 −15 

Standard Deviation 2.01 × 10 −2 8.79 × 10 −2 1.55 × 10 −2 1.07 × 10 −7 −

Part II: Phase Diversity Ablation. 

[0 , 0.4π) [0 , 0.8π) [0 , 1.2π) [0 , 1.6π) [0 , 2π)

Loss 4.82 2.00 × 10 −3 1.19 × 10 −9 3.54 × 10 −7 7.41 × 10 −15 

Table 2: Performance of the predictor under different ablation configurations. For the frequency ablation study, the average and standard deviation of the loss are reported across all possible combinations of frequencies of the specified size |K| . The results show that the fully diversified parametrization achieves the lowest CE loss, confirming its maximum efficiency under the fixed constraints of model scale αmβ2 

> m

= 1 and neuron budget M = 128 .

Figure 13: Output logits for the predictor under different ablation configurations, evaluated across four distinct query points (x, y ). The true prediction label is indicated by the dashed vertical line in each panel. The fully diversified parametrization yields the largest logit gap between the ground truth and incorrect labels, signifying maximal prediction confidence. by creating the largest logit gap between the ground truth label and all incorrect alternatives. Please refer to Figure 13 for visualizations of model outputs under different ablation configurations. 29 A.3 Training Dynamics with Quadratic Activation 

To under the training dynamics with quadratic activation, we set p = 23 and use a two-layer neural network with width M = 512 . The network is trained using SGD optimizer with step size η = 10 −4,initialized under Assumption 5.1 with initial scale κinit = 0 .02 .

Figure 14: Heatmaps of parameters after applying discrete Fourier transform along training epoches for the first 20 neurons initialized under Assumption 5.1 with p = 23 and quadratic activation. At the initial stage, these neurons preserve the single-frequency pattern by evolving only the Fourier coefficients corresponding to the initial frequency k⋆, while keeping the others 0 throughout. As shown in Figure 14, a single-frequency pattern is preserved throughout the training process. This empirical result aligns with our theoretical findings in Theorem 5.2, which states that under a sufficiently small initialization, the single-frequency structure will remain stable during the initial stage of training. In other words, the neurons are fully decoupled and the main flow dominates. 

# B Proof of Results in Section 4 and 5 

B.1 Proof of Proposition 4.2 

We first introduce a useful lemma about the softmax operation. 

Lemma B.1. Let ν ∈ Rd. If i∗ = argmax i νi and νi∗ − νi ≥ τ for all i̸ = i∗, then 

∥smax (ν) − ei∗ ∥1 ≤ d − 1exp( τ ) + ( d − 1) .

Proof of Lemma B.1. See Lemma 3.6 in Chen and Li (2024) for a detailed proof. Now we are ready to present the proof of Proposition 4.2. 30 Proof of Proposition 4.2. Let f [m] be the logit contributed by neuron m, and fix j ∈ [p]. Under the parametrization in (3.1) and the phase-alignment condition 2ϕm − ψm = 0 mod 2 π, we have 

f [m](x, y ; ξ, θ )[ j]= αmβ2 

> m

· cos( ωφ(m)j + 2 ϕm) ·  cos( ωφ(m)x + ϕm) + cos( ωφ(m)y + ϕm)2

= 2 a · cos( ωφ(m)(x − y)/2) 2 · cos( ωφ(m)j + 2 ϕm) · { 1 + cos( ωφ(m)(x + y) + 2 ϕm)}

= a · cos( ωφ(m)(x − y)/2) 2 · { 2 cos( ωφ(m)j + 2 ϕm)+ cos( ωφ(m)(x + y − j)) + cos( ωφ(m)(x + y + j) + 4 ϕm)},

where the second equality uses the homogeneous scaling, i.e., condition (ii) in Definition 4.1. Next, summing over all neurons in the frequency-group Nk, gives 

X

> m∈N k

f [m](x, y ; ξ, θ )[ j]= a · cos( ωk(x − y)/2) 2 · N · cos( ωk(x + y − j)) 

| {z }

condition (i): |N k| = N

+ a · cos( ωk(x − y)/2) 2 · X

> m∈N k

2 cos( ωkj + 2 ϕm) + cos( ωk(x + y + j) + 4 ϕm)

| {z }

= 0 due to condition (iii) 

= aN/ 2 · cos( ωk(x + y − j)) + aN/ 4 · { cos( ωk(2 x − j)) + cos( ωk(2 y − j)) }, (B.1) where the second equality follows from the balanced-frequency and the high-order phase-symmetry conditions (i) and (iii) in Definition 4.1. Summing (B.1) over all frequency k yields 

f (x, y ; ξ, θ )[ j] = 

> (p−1) /2

X

> k=1

X

> m∈N k

f [m](x, y ; ξ, θ )[ j]= aN/ 2 ·

> (p−1) /2

X

> k=1

cos( ωk(x + y − j)) + aN/ 4 ·

 (p−1) /2

X

> k=1

cos( ωk(2 x − j)) + 

> (p−1) /2

X

> k=1

cos( ωk(2 y − j)) 



. (B.2) By symmetry, for any fixed z ∈ N, P(p−1) /2 

> k=1

cos( ωkz) = ( p − 1) /2 if z = 0 mod p else −1/2. Then, 

> (p−1) /2

X

> k=1

cos( ωkz) = − 12 + p

2 · 1(z mod p = 0) . (B.3) Thus, by combining (B.2) and (B.3), we can conclude that 

f (x, y ; ξ, θ )[ j] = aN/ 2 ·  − 1 + p/ 2 · 1(x + y mod p = j) + p/ 4 · X

> z∈{ x,y }

1(2 z mod p = j) , ∀j ∈ [p].

Note that when x̸ = y, the true-signal logit at j = ( x + y) mod p exceeds all others by aN p/ 8, and when x = y, the margin is even larger. Applying Lemma B.1 yields 

∥smax ◦ f (x, y ; ξ, θ ) − e(x+y) mod p∥1 ≤ p − 1exp( aN p/ 8) + p − 1 ≤ p · exp( −aN p/ 8) .

Hence, to achieve error ϵ, it suffices to choose a ≳ (N p )−1 · log( p/ϵ ), which completes the proof. 31 B.2 Preliminary: Gradient Computation 

Recall the logit of the two-layer neural network in (2.1) takes the form: 

f (x, y ) := f (x, y ; ξ, θ ) = 

> M

X

> m=1

ξm · σ(⟨ex + ey, θ m⟩) ∈ Rp. (B.4) For theoretical analysis, we consider the training dynamics over the full dataset Dfull = {(x, y, z ) |

x, y ∈ Zp, z = ( x + y) mod p} and the corresponding CE loss, defined in (2.2), can be written as 

ℓ := ℓ(ξ, θ ; D∗) = − X

> x∈Zp

X

> y∈Zp

log ◦smax ◦ f (x, y ; ξ, θ ), e (x+y) mod p

= − X

> x∈Zp

X

> y∈Zp

log exp( f (x, y )[( x + y) mod p]) 

Ppj=1 exp( f (x, y )[ j]) 

!

= − X

> x∈Zp

X

> y∈Zp

f (x, y )[( x + y) mod p]

| {z }

:= eℓ

+ X

> x∈Zp

X

> y∈Zp

log 

 pX

> j=1

exp( f (x, y )[ j]) 

| {z }

:= ¯ℓ. (B.5) Following the loss decomposition in (B.5) , we compute the gradients of these two parts respectively. Recall that the two-layer neural network is parametrized by ξ = {ξm}m∈[M ] and θ = {θm}m∈[M ]

with ξm, θ m ∈ Rp. By substituting the form of f in (B.4) into eℓ and ¯ℓ, we have 

eℓ = − X

> x∈Zp

X

> y∈Zp
> M

X

> m=1

ξm[( x + y) mod p] · σ(⟨ex + ey, θ m⟩),

¯ℓ = X

> x∈Zp

X

> y∈Zp

log 

> p

X

> j=1

exp 

 MX

> m=1

ξm[j] · σ(⟨ex + ey, θ m⟩)

!

.

Fix a neuron m ∈ [M ]. First, we calculate the gradients for eℓ. By direct calculation, we have 

∇ξm eℓ = − X

> x∈Zp

X

> y∈Zp

e(x+y) mod p · σ(⟨ex + ey, θ m⟩).

Following this, the entry-wise derivative with respect to ξm[j] satisfies that 

∂eℓ∂ξ m[j] = − X 

> x,y ∈Zp:( x+y) mod p=j

σ(⟨ex + ey, θ m⟩) := − X

> (x,y )∈S pj

σ(⟨ex + ey, θ m⟩). (B.6) Here, we define Spj = {x, y ∈ Zp : ( x + y) mod p = j} for notational simplicity. Similarly, we can compute the gradient with respect to θm, following that 

∇θm eℓ = − X

> x∈Zp

X

> y∈Zp

ξm[( x + y) mod p] · (ex + ey) · σ′(⟨ex + ey, θ m⟩)= −2 X

> x∈Zp

ex · X

> y∈Zp

ξm[( x + y) mod p] · σ′(⟨ex + ey, θ m⟩),

32 where the last equality uses the symmetry of x and y. Hence, the entry-wise derivative follows 

∂eℓ∂θ m[j] = −2 X

> x∈Zp

ξm[mp(x, j )] · σ′(⟨ex + ej , θ m⟩), (B.7) where we re-index x = j and y → x to simplify the form. Next, we compute the gradients for ¯ℓ.Following a similar argument in (B.7) and (B.6), based on the chain rule, it holds that 

∂ ¯ℓ∂ξ m[j] = X

> x∈Zp

X

> y∈Zp

exp   P Mm=1 ξm[j] · σ(⟨ex + ey, θ m⟩)Ppi=1 exp   P Mm=1 ξm[i] · σ(⟨ex + ey, θ m⟩) · σ(⟨ex + ey, θ m⟩). (B.8) In addition, by direct calculation, we can obtain that 

∂ ¯ℓ∂θ m[j] = 2 X

> x∈Zp
> p

X 

> τ=1

exp   P Mm=1 ξm[τ ] · σ(⟨ex + ej , θ m⟩)Ppi=1 exp   P Mm=1 ξm[i] · σ(⟨ex + ej , θ m⟩) · ξm[τ ]

· σ′(⟨ex + ej , θ m⟩), (B.9) where the last equality results from re-indexing x = j, y → x, and j → i. Throughout the section, we consider quadratic activation σ(x) = x2 for theoretical convenience. 

B.3 Main Flow Approximation under Small Parameter Scaling 

The key property used in Stage I is that the scale of parameters is relatively small due to the small initialization and sufficiently small constant a. Following this, we have the approximation below: 

 smax ◦ f (x, y ; ξ, θ )[j] = exp   P Mm=1 ξm[j] · ⟨ ex + ey, θ m⟩2Ppi=1 exp   P Mm=1 ξm[i] · ⟨ ex + ey, θ m⟩2 ≈ 1

p , ∀j ∈ [p]. (B.10) To formalize the approximation above, we introduce the following approximation error terms: 

Err (1)  

> m,j

= X

> x∈Zp

X

> y∈Zp

exp   P Mm=1 ξm[j] · ⟨ ex + ey, θ m⟩2Ppi=1 exp   P Mm=1 ξm[i] · ⟨ ex + ey, θ m⟩2 − 1

p

!

· ⟨ ex + ey, θ m⟩2,

Err (2)  

> m,j

= 2 X

> x∈Zp
> p

X 

> τ=1

exp   P Mm=1 ξm[τ ] · ⟨ ex + ej , θ m⟩2Ppi=1 exp   P Mm=1 ξm[i] · ⟨ ex + ej , θ m⟩2 − 1

p

!

· ξm[τ ] · ⟨ ex + ey, θ m⟩,

for all (j, m ) ∈ [p] × [M ]. The approximation result is formalized in the following lemma. 

Lemma B.2. Denote ∥θ∥∞ = max m ∥θm∥∞ and ∥ξ∥∞ = max m] ∥ξm∥∞. For all (j, m ) ∈ [p] × [M ], the approximation error is upper bounded by 

|Err (1)  

> m,j

| ∨ | Err (2)  

> m,j

| ≤ 8p · ∥ θm∥∞ · max {∥ ξm∥∞, ∥θm∥∞} · (exp(8 M · ∥ ξ∥∞ · ∥ θ∥2

> ∞

) − 1) .

Proof of Lemma B.2. Let sj (x, y ) = PMm=1 ξm[j] · ⟨ ex + ey, θ m⟩2 denote the score given by the neural network for the j-th entry. Then, for fixed (x, y ), the softmax vector for j-th entry is given by 

p(x, y )[ j] = exp( s(x, y )[ j]) / Ppi=1 exp( s(x, y )[ i]) . Note that, for any (m, j ) ∈ [M ] × [p], we have 

|Err (1)  

> m,j

| = X

> x∈Zp

X

> y∈Zp



p(x, y )[ j] − 1

p



·⟨ ex + ey, θ m⟩2 ≤ 4p2 · max 

> (x,y )∈Z2
> p

p(x, y )[ j] − 1

p · ∥ θm∥2

> ∞

. (B.11) 33 Let ∆x,y = max j∈[p] s(x, y )[ j] − min j∈[p] s(x, y )[ j] > 0 for any (x, y ) ∈ Z2

> p

. It is straightforward to see that ∆x,y can be effectively bounded by the scales of θm’s and ξm’s, following that 

∆x,y ≤ 2

> M

X

> m=1

ξm[j] · ⟨ ex + ey, θ m⟩2

> ∞

≤ 8M · ∥ ξ∥∞ · ∥ θ∥2

> ∞

, ∀(x, y ) ∈ Z2

> p

. (B.12) Following this, we upper bound the difference between the softmax-induced distribution and the uniform distribution using the small-scale score vector. By simple algebra, we can show that 

max  

> j∈[p]

p(x, y )[ j] − 1

p ≤ max  

> j∈[p]

p(x, y )[ j] − 1

p

_ min  

> j∈[p]

p(x, y )[ j] − 1

p

≤ 11 + ( p − 1) · exp( −∆x,y ) − 1

p

_ 11 + ( p − 1) · exp(∆ x,y ) − 1

p

= p − 1

p ·

 exp(∆ x,y ) − 1exp(∆ x,y ) + p − 1

_ 1 − exp( −∆x,y )exp( −∆x,y ) + p − 1



≤ 1

p · (exp(∆ x,y ) − 1) · max {exp( −∆x,y ), 1} ≤ 1

p · (exp(∆ x,y ) − 1) . (B.13) By combining (B.11), (B.12) and (B.13), we can reach the conclusion that 

|Err (1)  

> m,j

| ≤ 4p · ∥ θm∥2 

> ∞

· (exp(8 M · ∥ ξ∥∞ · ∥ θ∥2

> ∞

) − 1) .

Building upon a similar argument, it holds that 

|Err (2)  

> m,j

| = 2 X

> x∈Zp
> p

X 

> τ=1

p(x, j )[ τ ] − 1

p · ξm[τ ] · ⟨ ex + ey, θ m⟩≤ 8p · ∥ θm∥∞ · ∥ ξm∥∞ · (exp(8 M · ∥ ξ∥∞ · ∥ θ∥2

> ∞

) − 1) .

Hence, we complete the proof of bounded approximation error. Lemma B.2 formalizes a key technical tool for analyzing the dynamics during the initial stage: given small-scale parameters θm’s and ξm’s, and a specified small constant a ∈ R (introduced for technical convenience), the softmax components in the gradient can be effectively approximated by a uniform vector, with a controllable and small approximation error. In the following sections, we denote Err (1)  

> m

= ( Err (1)  

> m,j

)j∈[p] ∈ Rp and Err (2)  

> m

= ( Err (2)  

> m,j

)j∈[p] ∈ Rp

for notational simplicity and we remark that the error vectors would vary along the grdient flow. 

B.3.1 Proof Overview: Simplified Dynamics under Approximation 

Before delving into the technical details, we provide a brief summary of the approximate dynamics of parameters and their transformations along gradient flow in Table 3. This overview characterizes the training during the initial phase, when parameter magnitudes are small. We use ≈ to highlight the central flow, omitting the perturbations introduced by approximation errors as defined in (B.10) .The simplification of the approximate dynamics leverages two key features that arise under the specialized initialization in Assumption 5.1: neuron-wise decoupled loss landscape—meaning the evolution of each neuron depends only on itself—and preservation of a single-frequency structure— i.e., the parameters exhibit only one frequency component in the Fourier domain. These properties hold during the early stage of training. Refer to §5.2 for a detailed illustration and proof sketch. With slight abuse of notation, we let k⋆ denote the initial frequency of each neuron and we use the superscript ⋆ instead of k⋆ to simplify the notation in Table 3. 34 Roadmap. In Part I, we present the dynamics of original parameters— {θm}m∈[M ] and {ξm}m∈[M ]

with calculation details provided in §B.3.2 and §B.4. In Part II, building on the results from §B.4, we shift focus to the dynamics of the discrete Fourier coefficients, defined in §5.1, to better understand the evolution of parameters in the Fourier domain. Finally, based on the results in Part I and Part II, we analyze the dynamics of the magnitudes and phases of the Fourier signals (see §5.1 for definitions), to interpret the alignment behavior between θm and ξm, and the detailed derivations are provided in §B.5. The auxiliary equalities naturally arise from the definition of discrete Fourier coefficients and their transformations. 

Part I: Dynamics of Original Parameters. 

θm[j]( t) ∂tθm[j]( t) ≈ 2p · α⋆m(t) · β⋆m(t) · cos( ωkj + ψ⋆m(t) − ϕ⋆m(t)) , ∀j ∈ [p]

ξm[j]( t) ∂tξm[j]( t) ≈ p · α⋆m(t)2 · cos( ωk⋆ j + 2 ϕ⋆m(t)) , ∀j ∈ [p]

Part II: Dynamics of Dicrete Fourier Coefficients. 

gm[2 k⋆]( t) ∂tgm[2 k⋆]( t) ≈ √2 · p3/2 · α⋆m(t) · β⋆m(t) · cos  ψ⋆m(t) − ϕ⋆m(t)

gm[2 k⋆ + 1]( t) ∂tgm[2 k⋆ + 1]( t) ≈ − √2 · p3/2 · α⋆m(t) · β⋆m(t) · sin  ψ⋆m(t) − ϕ⋆m(t)

rm[2 k⋆]( t) ∂trm[2 k⋆]( t) ≈ p3/2/√2 · α⋆m(t)2 · cos  2ϕ⋆m(t)

rm[2 k⋆ + 1]( t) ∂trm[2 k⋆ + 1]( t) ≈ − p3/2/√2 · α⋆m(t)2 · sin  2ϕ⋆m(t)

Part III: Dynamics of Magnitudes and Phases. 

α⋆m(t) ∂tα⋆m(t) ≈ 2p · α⋆m(t) · β⋆m(t) · cos  2ϕ⋆m(t) − ψ⋆m(t)

β⋆m(t) ∂tβ⋆m(t) ≈ p · α⋆m(t)2 · cos  2ϕ⋆m(t) − ψ⋆m(t)

ϕ⋆m(t) ∂t exp( iϕ ⋆m(t)) ≈ 2p · β⋆m(t) · sin  2ϕ⋆m(t) − ψ⋆m(t) · exp ( i {ϕ⋆m(t) − π/ 2})

ψ⋆m(t) ∂t exp( iψ ⋆m(t)) ≈ p · α⋆m(t)2 

> β⋆m(t)

· sin  2ϕ⋆m(t) − ψ⋆m(t) · exp ( i {ψ⋆m(t) + π/ 2})

D⋆m(t)1 ∂t exp( iD⋆m(t)) ≈ p ·



4β⋆m(t) + α⋆m(t)2

> β⋆m(t)



· sin( D⋆m(t)) · exp ( i {D⋆m(t) − π/ 2})

Part IV: Auxiliary Equalities. 

cos( ϕ⋆m(t)) = p2/p · gm[2 k⋆]( t)/α ∗

> m

(t), sin( ϕ⋆m(t)) = −p2/p · gm[2 k⋆ + 1]( t)/α ∗

> m

(t),

cos( ψ⋆m(t)) = p2/p · rm[2 k⋆]( t)/β ∗

> m

(t), sin( ψ⋆m(t)) = −p2/p · rm[2 k⋆ + 1]( t)/β ∗

> m

(t).       

> 1We use D⋆m(t)denote the phase misalignment level defined as D⋆m(t) = 2 ϕ⋆m(t)−ψ⋆m(t)mod 2 π.

Table 3: Summarization of the approximate dynamics during the initial stage. Please refer to §B.3.2, §B.4 and §B.5 for formalized arguments and detailed derivations. 

B.3.2 Proof of Lemma B.3: Main Flow of Decoupled Neurons Lemma B.3 (Main Flow) . Consider the discrete Fourier coefficients, as well as the signal magnitudes and phases, defined over {θm}m∈[M ] and {ξm}m∈[M ] (see §5.1 for definitions). Then, at each time t ∈ R+ and 

m ∈ [M ], the gradient dynamics takes the following form: 

∂tξm[j]( t) = p ·

> (p−1) /2

X

> k=1

αkm(t)2 · cos( ωkj + 2 ϕkm(t)) − Err (1)  

> m,j

(t),

35 ∂tθm[j]( t) = 2 p ·

> (p−1) /2

X

> k=1

αkm(t) · βkm(t) · cos( ωkj + ψkm(t) − ϕkm(t)) − Err (2)  

> m,j

(t),

where the approximation errors Err (1)  

> m,j

(t) and Err (2)  

> m,j

(t) are defined in §B.3 

Lemma B.3 indicates that the dynamics of θm(t)’s and ξm(t)’s only depend on θm(t) and ξm(t)

such that the neurons are almost fully decoupled with small approximation errors. 

Proof of Lemma B.3. Consider a fixed neuron m. By combining the gradient computations in (B.6) and (B.8), we can write the complete form of derivative of loss ℓ with respect to ξm[j] as 

∂ℓ ∂ξ m[j] = ∂eℓ∂ξ m[j] + ∂ ¯ℓ∂ξ m[j]= − X

> (x,y )∈S pj

σ(⟨ex + ey, θ m⟩) + 1

p · X

> x∈Zp

X

> y∈Zp

σ(⟨ex + ey, θ m⟩) + Err (1)  

> m,j

. (B.14) Similarly, by combining (B.7) and (B.9), we have the derivative of ℓ with respect to θm[j]:

∂ℓ ∂θ m[j] = ∂eℓ∂θ m[j] + ∂ ¯ℓ∂θ m[j] = −2 X

> x∈Zp

ξm[mp(x, j )] · σ′(⟨ex + ej , θ m⟩)+ 2

p · X

> x∈Zp
> p

X 

> τ=1

ξm[τ ] · σ′(⟨ex + ej , θ m⟩) + Err (2)  

> m,j

. (B.15) Motivated by Lemma B.3, we focus on the dominant terms of the gradient and carefully manage the error terms to characterize the central flow that determines the main dynamics in the initial stage. 

Step 1: Deriving Gradient of ξm. By switching from the standard canonical basis to the Fourier basis, we can write θm using a form of discrete Fourier expansion, as shown in (3.2) . Then, we have 

X

> (x,y )∈S pj

σ(⟨ex + ey, θ m⟩) = X

> (x,y )∈S pj

2α0 

> m

+

> (p−1) /2

X

> k=1

αkm

X

> z∈{ x,y }

cos( ωkz + ϕkm)

!2

= 4 p · (α0

> m

)2 +

> (p−1) /2

X

> k=1

(αkm)2 · (i) + X  

> 1≤k̸=τ≤(p−1) /2

αkmατm · (ii) 

+ 2 α0 

> m

·

> (p−1) /2

X

> k=1

αkm · (iii) . (B.16) where we denote each term as 

(i) = X

> (x,y )∈S pj

X

> z∈{ x,y }

cos( ωkz + ϕkm)

!2

,

(ii) = X

> (x,y )∈S pj

X

> z∈{ x,y }

cos( ωkz + ϕkm) · X

> z∈{ x,y }

cos( ωτ z + ϕτm),

(iii) = X

> (x,y )∈S pj

X

> z∈{ x,y }

cos( ωkz + ϕkm).

36 In the following, we compute (i) , (ii) and (iii) respectively using trigonometric identities and the periodicity of the module addition task over the full space Z2

> p

. First, note that 

(i) = 2 X

> x∈Zp

cos( ωkx + ϕkm)2 + 2 X

> (x,y )∈S pj

cos( ωkx + ϕkm) · cos( ωky + ϕkm)= p + X

> x∈Zp

cos( ω2kx + 2 ϕkm) + X

> (x,y )∈S pj

cos( ωk(x + y) + 2 ϕkm) + X

> (x,y )∈S pj

cos( ωk(x − y)) = p · (1 + cos( ωkj + 2 ϕkm)) , (B.17) where the last equality uses the fact that P 

> (x,y )∈S pj

cos( ωk(x − y)) = P 

> x∈Zp

cos( ωkx) = 0 and 

cos( ωk(x + y) + 2 ϕkm) = cos( ωkj + 2 ϕkm) for all (x, y ) ∈ S pj . Following a similar argument, we have 

(ii) = 2 X

> x∈Zp

cos( ωkx + ϕkm) · cos( ωτ x + ϕτm) + 2 X

> (x,y )∈S pj

cos( ωkx + ϕkm) · cos( ωτ y + ϕτm)= X

> (x,y )∈S pj

cos(( ωkx + ωτ y) + ϕkm + ϕτm) + X

> (x,y )∈S pj

cos(( ωkx − ωτ y) + ϕkm − ϕτm)+ X

> x∈Zp

cos(( ωk + ωτ )x + ϕkm + ϕτm) + X

> x∈Zp

cos(( ωk − ωτ )x + ϕkm − ϕτm) = 0 , (B.18) where we use P 

> (x,y )∈S pj

cos(( ωkx + ωτ y) + ϕkm + ϕτm) = P 

> x∈Zp

cos(( ωk − ωτ ) + ωτ j + ϕkm + ϕτm) in the last inequality for the first term and a similar arguent for the second one. In addition, it is easy to show that (iii) = 2 P 

> x∈Zp

cos( ωkx + ϕkm) = 0 . By combining (B.16), (B.17) and (B.18), we have 

X

> (x,y )∈S pj

σ(⟨ex + ey, θ m⟩) = 4 p · (α0

> m

)2 + p ·

> (p−1) /2

X

> k=1

(αkm)2 · (1 + cos( ωkj + 2 ϕkm)) .

Following this, based on (B.14), the simplified derivative of each entry takes the form 

∂ℓ ∂ξ m[j] − Err (1)  

> m,j

= − X

> (x,y )∈S pj

σ(⟨ex + ey, θ m⟩) + 1

p ·

> p

X

> j=1

X

> (x,y )∈S pj

σ(⟨ex + ey, θ m⟩)= −p ·

> (p−1) /2

X

> k=1

(αkm)2 · cos( ωkj + 2 ϕkm), ∀j ∈ [p].

Step 2: Deriving Gradient of θm. Next, we calculate the gradient of θm, following a procedure analogous to the one in Step 1. To begin, we consider the expression: 

X

> x∈Zp

ξm[mp(x, j )] · σ′(⟨ex + ej , θ m⟩) = 2 X

> x∈Zp

ξm[mp(x, j )] · θm[x]

| {z }

> (iv)

+2 θm[j] · X

> x∈Zp

ξm[x]

| {z }

> (v)

. (B.19) Term (iv) can be decomposed and simplified using the fourier expansions of ξm and θm in (3.2) . By carefully applying cosine product identities and rearranging the terms, we have 

(iv) = X

> x∈Zp

β0 

> m

+

> (p−1) /2

X

> k=1

βkm · cos( ωk · mp(x, j ) + ψkm)

!

· α0 

> m

+

> (p−1) /2

X

> k=1

αkm · cos( ωkx + ϕkm)

!

37 = p · α0 

> m

· β0 

> m

+

> (p−1) /2

X

> k=1

αkmβkm · (iv.1) + α0 

> m

·

> (p−1) /2

X

> k=1

βkm · (iv.2) 

+ X  

> 1≤k̸=τ≤(p−1) /2

αkmβτm · (iv.3) + β0 

> m

·

> (p−1) /2

X

> k=1

αkm · (iv.4) .

where we denote each term as 

(iv.1) = X

> x∈Zp

cos( ωk · mp(x, j ) + ψkm) · cos( ωkx + ϕkm), (iv.2) = X

> x∈Zp

cos( ωk · mp(x, j ) + ψkm),

(iv.3) = X

> x∈Zp

cos( ωτ · mp(x, j ) + ϕτm) · cos( ωkx + ψkm), (iv.4) = X

> x∈Zp

cos( ωk · mp(x, j ) + ϕkm).

Analogous to (B.17) and (B.18) , using the trigonometric identities and periodicity of the module addition task, we have (iv.2) = (iv.3) = (iv.4) = 0 , and for the first term we can show that 

(iv.1) = X

> x∈Zp

cos( ωk · mp(x, j ) + ψkm) · cos( ωkx + ϕkm) = p · cos( ωkj + ψkm − ϕkm).

By combining the arguments above, we can conclude that 

(iv) = p · α0 

> m

· β0 

> m

+ p

> (p−1) /2

X

> k=1

αkmβkm · cos( ωkj + ψkm − ϕkm). (B.20) Besides, by substituting the fourier expansions of ξm into (v) , it holds that 

(v) = θm[j] · X

> x∈Zp

β0 

> m

+

> (p−1) /2

X

> k=1

βkm · cos( ωkx + ψkm)

!

= p · θm[j] · β0 

> m

= p · β0 

> m

· α0 

> m

+

> (p−1) /2

X

> k=1

αkm · cos( ωkj + ϕkm)

!

. (B.21) By combining (B.19) , (B.20) , (B.21) and substituting them back into (B.15) , by simple calculation, we can show that constant frequencies are cancelled and we have 

∂ℓ ∂θ m[j] − Err (2)  

> m,j

= −2 X

> x∈Zp

ξm[mp(x, j )] · σ′(⟨ex + ej , θ m⟩) + 2

p · X

> x∈Zp
> p

X 

> τ=1

ξm[τ ] · σ′(⟨ex + ej , θ m⟩)= −2p ·

> (p−1) /2

X

> k=1

αkmβkm · cos( ωkj + ψkm − ϕkm), ∀j ∈ [p].

Recall that the gradient flow is defined as ∂tΘ( t) = −∇ ℓ(Θ( t)) . Following this, we have ∂tθm(t) = 

−∇ θm ℓ and ∂tξm(t) = −∇ ξm ℓ for all m ∈ [M ]. Then, by combining Step 1 and Step 2 and using the definition of gradient flow, we complete the proof. 

B.4 Proof of Theorem 5.2: Single-Frequency Preservation 

Theorem B.4 (Formal Statement of Theorem 5.2) . Let the model be initialized according to Assumption 5.1 with a scale κinit > 0. For a given threshold Cend > 0, we define the initial stage as the time interval 

(0 , t init ], where tinit is the first hit time: 

tinit := inf {t ∈ R+ : max   

> m∈[M]

∥θm(t)∥∞ ∨ ∥ ξm(t)∥∞ ≤ Cend }. (B.22) 38 Suppose the following conditions hold: (i) log M/M ≲ c−1/2 · (1 + o(1)) , κinit = o(M −1/3) and Cend =Θ( κinit ), and (ii) scale κinit is sufficiently small such that the event Ephase = {∃ m ∈ [M ] s.t. cos(2 ϕ⋆m(t) −

ψ⋆m(t)) ≥ 1 − c · (M −1 log M )2, ∀t ∈ (0 , t init ]} holds with probability greater than 1 − M −c for some constant c > 0. Then, we have max k̸ =k⋆ inf t∈(0 ,t init ] αkm(t) ∨ βkm(t) = o(κinit ).

In Theorem B.4, the initial time interval (0 , t init ) is defined by imposing that the parameters remain substantially small, upper bounded by Cend as stated in (B.22) . Ephase assumes during the initial stage, there exists at least one well-aligned neuron whose phase difference 2ϕ⋆m(t) − ψ⋆m(t)

has a uniformly lower-bounded cosine value. This should hold with high probability under the random initialization in Assumption 5.1, jointly resulting from the concentration (see Lemma B.6) and the consistent decrease of phase difference for well-initialized neurons when κinit 7 → 0 (see Lemma B.9). Since the difference between the real dynamics for θm(t), ξ m(t) and the central flow can be bounded by some error uniformly over t ∈ (0 , t init ], where the error is a monotone function with respect to κinit , and the real dynamics for ϕ⋆m(t), ψ ⋆m(t) is a continuous function of the real dynamics for θm(t), ξ m(t), this claim holds. 

Proof of Theorem B.4. Based on Lemma B.2 and (B.22) , throughout the training, we can uniformly upper bound the approximation errors by 

sup 

> t∈(0 ,t init )

max  

> m,j

|Err (1)  

> m,j

(t)| ∨ | Err (2)  

> m,j

(t)|≤ 8p · sup 

> t∈(0 ,t init )

max  

> m

∥θm(t)∥∞ · max {∥ ξm(t)∥∞, ∥θm(t)∥∞} · (exp(8 M · ∥ ξ(t)∥∞ · ∥ θ(t)∥2

> ∞

) − 1) 

≲ M p · C5

> end

, (B.23) where the last inequality uses exp( x)−1 ≲ x for x ∈ [0 , 1] and (B.22) implies 8M ·∥ ξ(t)∥∞·∥ θ(t)∥2 

> ∞

≤ 1

for all t ∈ (0 , t init ) under the scaling that M C 3 

> end

≍ M κ 3 

> init

≪ 1. In the following, we show that the evolution of non-feature frequencies is governed by the bounded error terms, and the feature coefficient can grow rapidly even when perturbed by noise. 

Step 1: Derive the Dynamics with Approximation Errors. Consider a fixed neuron m. By applying the chain rule, we have ∂tgm(t) = B⊤ 

> p

∂tθm(t) and ∂trm(t) = B⊤ 

> p

∂tξm(t) such that 

∂tgm[j]( t) = ⟨bj , ∂ tθm(t)⟩, ∂trm[j]( t) = ⟨bj , ∂ tξm(t)⟩, ∀j ∈ [p].

Hence, the time derivatives of constant frequency, based on Lemma B.3, satisfy that 

∂trm[1]( t) = −⟨ Err (1)  

> m

(t), b 1⟩, ∂tgm[1]( t) = −⟨ Err (2)  

> m

(t), b 1⟩, (B.24) where the the RHS of (B.24) can be controlled by 

|⟨ Err (1)  

> m

(t), b 1⟩| ≤ ∥ Err (1)  

> m

(t)∥2 · ∥ b1∥2 ≤ √p · ∥ Err (1)  

> m

(t)∥∞, |⟨ Err (2)  

> m

(t), b 1⟩| ≤ √p · ∥ Err (2)  

> m

(t)∥∞.

Based on Lemma B.3 and the orthogonality of the Fourier basis, by simple calculation, it holds that 

∂trm[2 k]( t) = p ·

> p

X

> j=1

r 2

p · cos( ωkj) ·

> (p−1) /2

X

> k=1

αkm(t)2 · cos( ωkj + 2 ϕkm(t)) −

> p

X

> j=1

b2k[j] · Err (1)  

> m,j

(t)= p2p · αkm(t)2 ·

> p

X

> j=1

· cos( ωkj) · cos( ωkj + 2 ϕkm(t)) − ⟨ Err (1)  

> m

(t), b 2k⟩

39 = p3/2

√2 · αkm(t)2 · cos  2ϕ⋆m(t) − ⟨ Err (1)  

> m

(t), b 2k⟩,

and similarly, we have 

∂trm[2 k + 1]( t) = − p3/2

√2 · αkm(t)2 · sin(2 ϕkm(t)) − ⟨ Err (1)  

> m

(t), b 2k+1 ⟩.

Following this, by applying the chain rule, we have 

∂tβkm(t) = 

r 2

p · ∂t

prm[2 k]( t)2 + rm[2 k + 1]( t)2

= 2

p ·

 rm[2 k]( t)

βkm(t) · ∂trm[2 k]( t) + rm[2 k + 1]( t)

βkm(t) · ∂trm[2 k + 1]( t)



= 2

p · p3/2

√2 ·

r p

2 · αkm(t)2 ·  cos( ψkm(t)) · cos(2 ϕkm(t)) + sin( ψkm(t)) · sin(2 ϕkm(t)) + fErr (1)  

> m

(t)= p · αkm(t)2 · cos  2ϕkm(t) − ψkm(t) + fErr (1)  

> m

(t), (B.25) where we define the approximation-induced error term as: 

fErr (1)  

> m

(t) := − 2

p ·

 rm[2 k]( t)

βkm(t) · ⟨ Err (1)  

> m

(t), b 2k⟩ − rm[2 k + 1]( t)

βkm(t) · ⟨ Err (1)  

> m

(t), b 2k+1 ⟩



.

Here, notice that the error terms can be upper bounded by 

| fErr (1)  

> m

(t)| ≤ 

r 2

p ·

q

⟨Err (1)  

> m

(t), b 2k⟩2 + ⟨Err (1)  

> m

(t), b 2k+1 ⟩2

≤

r 2

p · ∥ Err (1)  

> m

(t)∥2 ·

q

∥b2k∥22 + ∥b2k+1 ∥22 ≤ 2∥Err (1)  

> m

(t)∥∞,

where the first inequality uses the Cauchy-Schwarz inequality and the fact that rm[2 k]( t)2 + rm[2 k +1]( t)2 = p/ 2 · βkm(t)2 by definition. Moreover, following a similar argument above, we have 

∂tgm[2 k]( t) = √2p3/2 · αkm(t) · βkm(t) · cos( ψkm(t) − ϕkm(t)) + ⟨Err (2)  

> m

(t), b 2k⟩,

and also 

∂tgm[2 k + 1]( t) = −√2p3/2 · αkm(t) · βkm(t) · sin( ψkm(t) − ϕkm(t)) + ⟨Err (2)  

> m

(t), b 2k+1 ⟩.

Thus, by applying the chain rule, we can reach that 

∂tαkm(t) = 

r 2

p · ∂t

pgm[2 k]( t)2 + gm[2 k + 1]( t)2

= 2 p · αkm(t) · βkm(t) · cos  2ϕkm(t) − ψkm(t) + fErr (2)  

> m

(t), (B.26) where the approximation error satisfies that 

| fErr (2)  

> m

(t)| = 2

p · gm[2 k]( t)

αkm(t) · ⟨ Err (2)  

> m

(t), b 2k⟩ − gm[2 k + 1]( t)

αkm(t) · ⟨ Err (2)  

> m

(t), b 2k+1 ⟩ ≤ 2∥Err (2)  

> m

(t)∥∞.

40 Step 2.1: Bound the Growth of Non-feature Frequency. By combining (B.24) , (B.25) and (B.26) ,since cos  2ϕkm(t) − ψkm(t), we can upper bound the growth of non-feature frequencies as 

∂tαkm(t) ≤ 2p · αkm(t) · βkm(t) + fErr (2)  

> m

(t), (B.27a) 

∂tβkm(t) ≤ p · αkm(t)2 + fErr (1)  

> m

(t), (B.27b) 

∂trm[1]( t) ≤ √p · ∥ Err (1)  

> m

(t)∥∞, ∂tgm[1]( t) ≤ √p · ∥ Err (2)  

> m

(t)∥∞, (B.27c) 

| fErr (i) 

> m

(t)| ≲ ∥Err (i) 

> m

(t)∥∞, ∀i ∈ { 0, 1}. (B.27d) for all k̸ = k⋆ and m ∈ [M ]. For the growth of constant coefficients, (B.27c) indicates that 

|α0

> m

(t)| ∨ | β0

> m

(t)| = 1 /√p · | gm[1]( t)| ∨ | rm[1]( t)|≤ max  

> t∈(0 ,t init ]

∥Err (1)  

> m

(t)∥∞ ∨ ∥ Err (2)  

> m

(t)∥∞ · t ≲ M p · C5 

> end

· t, (B.28) where the inequality results from (B.23) . Following this, by combining (B.27a) , (B.27b) , (B.27c) and (B.27d), it holds that 

∂t{αkm(t)/√2 + βkm(t)} ≤ p · αkm(t) · { αkm(t) + √2βkm(t)} + fErr (1)  

> m

(t) + fErr (2)  

> m

(t)/√2

≤ √2p · Cend · { αkm(t) + √2βkm(t)} + fErr (1)  

> m

(t) + fErr (2)  

> m

(t)/√2,

where the last inequality uses (B.22) and ∥θm(t)∥22 = p · α0

> m

(t)2 + p 

> 2

· P(p−1) /2 

> k=1

αkm(t)2 such that 

αkm(t) ≤ p2/p · ∥ θm(t)∥2 ≤ √2 · ∥ θm(t)∥∞ ≤ √2 · Cend , ∀t ∈ (0 , t init ), (B.29) for all frequency k and similarly we have βkm(t) ≤ √2 · Cend . For k̸ = k⋆, Lemma B.5 shows that 

αkm(t)/√2 + βkm(t) ≤ { αkm(0) /√2 + βkm(0) } · exp( √2p · Cend · t)+

Z t

> 0

{ fErr (1)  

> m

(s) + fErr (2)  

> m

(s)/√2} · exp( √2p · Cend · (t − s))d s

| {z }

> (i)

, (B.30) where the first term can be eliminated due to the zero initialization for non-feature frequencies as specified in Assumption 5.1. To upper bound (B.30), we can show that 

(i) ≤

Z t

> 0

{2∥Err (1)  

> m

(s)∥∞ + √2∥Err (1)  

> m

(s)∥∞} · exp( √2p · Cend · (t − s))d s

≤ 4 sup 

> t∈(0 ,t init )

max  

> m

∥Err (1)  

> m

(t)∥∞ ∨ ∥ Err (2)  

> m

(t)∥∞ ·

Z t

> 0

exp( √2p · Cend · (t − s))d s

≲ M p · C5 

> end

·

Z t

> 0

exp( √2p · Cend · (t − s))d s ≲ M p · C5 

> end

· t, (B.31) where the first inequality follows (B.27d) and the last inequality results from exp( x) − 1 ≤ 2x for 

x ∈ (0 , 1) . By combining (B.30) and (B.31), we can conclude that 

αkm(t) ∨ βkm(t) ≲ M p · C5 

> end

· t · max {p · Cend · t, 1} ≤ M p · C5 

> end

· t, (B.32) for all non-feature frequencies k̸ = k⋆ if we consider time t ≤ (√2p · Cend )−1 ∧ tinit . For the remainder of this analysis, we will adhere to this interval, and we will later show that tinit ≲ (p · Cend )−1.41 Step 2.2: Bound the Time of Initial Stage. Based on (B.25) and (B.26) , we first show that during the initial stage, the change in the quantity α⋆m(t)2 − 2β⋆m(t)2 remains small. Note that 

∂t{α⋆m(t)2 − 2β⋆m(t)2} = 2 α⋆m(t) · ∂tα⋆m(t) − 4β⋆m(t) · ∂tβ⋆m(t)= 4 p · α⋆m(t)2 · β⋆m(t) · cos  2ϕ⋆m(t) − ψ⋆m(t) + 2 α⋆m(t) · fErr (2)  

> m

(t)

− 4p · α⋆m(t)2 · β⋆m(t) · cos  2ϕ⋆m(t) − ψ⋆m(t) − 4β⋆m(t) · fErr (1)  

> m

(t)= 2 α⋆m(t) · fErr (2)  

> m

(t) − 4β⋆m(t) · fErr (1)  

> m

(t).

Following this, by integrating on both sides, we can show that 

α⋆m(t)2 − 2β⋆m(t)2 ≥ α⋆m(0) 2 − 2β⋆m(0) 2 −

Z t

> 0

|∂t{α⋆m(s)2 − 2β⋆m(s)2}| ds

≥ − κ2 

> init

− 6√2 · Cend · sup 

> t∈(0 ,t init )

| fErr (1)  

> m

(t)| ∨ | fErr (2)  

> m

(t)| · t

≥ − κ2 

> init

− O(M p · C6

> end

) · t, (B.33) where the second inequality uses (B.29) . Recall that we choose a sufficiently small κinit such that Ephase holds. Thus, there exists a neuron m such that inf t∈(0 ,t init ) cos(2 ϕ⋆m(t) − ψ⋆m(t)) ≥ CD.Leveraging this result along with (B.25) and (B.33), it follows that: 

∂tβ⋆m(t) ≥ p · CD · α⋆m(t)2 + fErr (1)  

> m

(t)= 2 p · CD · β⋆m(t)2 + p · CD · { α⋆m(t)2 − 2β⋆m(t)2} + fErr (1)  

> m

(t)

≥ 2p · CD · β⋆m(t)2 − p · CD · κ2 

> init

− O(M p 2 · C6

> end

) · CD · t − O(M p · C5

> end

)

≥ 2p · CD · β⋆m(t)2 − p · { κ2 

> init

+ O(M · C5

> end

)}≥ 2p · CD · β⋆m(t)2 − p · (1 + o(1)) · κ2

> init

, (B.34) where the second inequality results from (B.27d) , the third is guaranteed by the time interval constraint t ≤ (√2p · Cend )−1 ∧ tinit , and the last one uses M κ 3 

> init

= o(1) and Cend = Θ( κinit ).Given the Riccati ODE in (B.34) and the initialization β⋆m(0) = κinit , β⋆m(t) is monotone increasing as long as 2CD ≥ 1 + o(1) , which can be guaranteed by choosing a sufficiently large M such that 

log M/M ≲ c−1/2 · (1 + o(1)) . Following this, we can further show that 

∂tβ⋆m(t) ≥ 2pκ init · CD · β⋆m(t) − p · (1 + o(1)) · κ2

> init

, ∀t ≤ (√2p · Cend )−1 ∧ tinit . (B.35) By combining (B.35) and Lemma B.5, we can get 

β⋆m(t) ≥ κinit · exp(2 pκ init · CD · t) − (1 + o(1)) · κinit /(2 CD) · { exp(2 pκ init · CD · t) − 1}.

Recall that, by definition β⋆m(tinit ) ≲ Cend ≍ κinit . Thus, we can upper bound the hitting time tinit by 

tinit ≲ 12pκ init · CD

· log 

 Cend /κ init − (1 + o(1)) /(2 CD)1 − (1 + o(1)) /(2 CD)



≲ (pκ init )−1. (B.36) 

Step 3: Conclude the Proof. Based on (B.28), (B.32) and (B.36), it holds that 

max   

> k̸=k⋆

inf  

> t∈(0 ,t init ]

αkm(t) ∨ βkm(t) ≲ M p · C5 

> end

· tinit ≤ o(κinit ),

which completes the proof. 42 B.4.1 Proof of Auxiliary Lemma B.5 Lemma B.5. Let ι̸ = 0 denote a non-zero constant and ζ : [0 , ∞) 7 → Rn denote a continuous function. For any initial condition x(0) ∈ Rn, the unique solution of ∂tx(t) = ιx (t) + ζ(t) is given by 

x(t) = x(0) · exp( ιt ) + 

Z t

> 0

ζ(s) · exp( ι(t − s))d s. 

In particular, if ζ(t) ≡ ζ ∈ R is constant, then x(t) = x(0) · exp( ιt ) + ζ/ι · (exp( ιt ) − 1) .Proof of Lemma B.5. Note that, by chain rule, we have 

∂t{xt · exp( −ιt )} = −ιx (t) · exp( −ιt ) + ∂tx(t) · exp( −ιt ) = ζ(t) · exp( −ιt ).

By integrating both sides from 0 to t, we can obtain the desired result. 

Lemma B.6. Under the initialization in Assumption 5.1, with probability greater that 1 − M −c, it holds that max m∈[M ] cos( D⋆m) > 1 − c2π2 · M −2(log M )2, where c > 0 is a constant. Proof of Lemma B.6. Throughout the proof, we drop the initial time (0) for simplicity. Recall that, as specified in Assumption 5.1, the parameters are initialized as below 

θm ∼ κinit · pp/ 2 · (ϱ1[1] · b2k⋆ + ϱ1[2] · b2k⋆+1 ), ξm ∼ κinit · pp/ 2 · (ϱ2[1] · b2k⋆ + ϱ2[2] · b2k⋆+1 ).

By definition, we have cos( ϕ⋆m) = ϱ1[1] and sin( ϕ⋆m) = −ϱ1[2] . Thus, it holds that 

(cos( ϕ⋆m), sin( ϕ⋆m)) = ( ϱ1[1] , −ϱ1[2]) d

= ( ϱ1[1] , ϱ 1[2]) ,

following the symmetry of the uniform distribution on the unit circle. Hence, ϕ⋆m(0) ∼ Unif( −π, π ).Similarly, we have ψ⋆m ∼ Unif( −π, π ) such that D⋆m = 2 ϕ⋆m − ψ⋆m mod 2 π ∼ Unif(0 , 2π). Following this, the tail probability takes the form: 

P



max   

> m∈[M]

cos( D⋆m) > 1 − c2π2 · M −2(log M )2

= 1 − P



∀m ∈ [M ], cos( D⋆m) ≤ 1 − c2π2 · M −2(log M )2

= 1 −  1 − arccos  1 − c2π2 · M −2(log M )2/π M . (B.37) Suppose M > cπ log M such that cπ · M −1 log M ∈ (0 , 1) , then we have 

arccos  1 − c2π2 · M −2(log M )2) ≥ arccos   cos( cπ · M −1 log M ) = cπ · M −1 log M, (B.38) where the inequality follows from cos( x) ≥ 1 − x2 for all x ∈ R and fact that arccos( ·) is monotonely decreasing on [−1, 1] . By combining (B.37) and (B.38), we obtain 

P



max   

> m∈[M]

cos( D⋆m) > 1 − c2π2 · M −2(log M )2

≥ 1 −  1 − c · M −1 log M M ≥ 1 − exp( −c log M ) = 1 − M −c.

Here, we use (1 − x)M ≤ exp( −xM ) for all x ∈ [0 , 1] and then complete the proof. 43 B.5 Proof of Theorem 5.3: Phase Alignment 

In this section, due to the inherent difficulty of tracking a multi-particle dynamical system with error terms—even when the approximation errors are provably small—we focus on the central flow dynamics presented in Lemma B.3, directly omitting the error terms caused by unpredictable drift. In summary, the resulting dynamical system can be described by the following ODEs: 

∂tθm[j]( t) = −2p ·

> (p−1) /2

X

> k=1

αkm(t) · βkm(t) · cos( ωkj + ψkm(t) − ϕkm(t)) , (B.39a) 

∂tξm[j]( t) = p ·

> (p−1) /2

X

> k=1

αkm(t)2 · cos( ωkj + 2 ϕkm(t)) , (B.39b) for a fixed neuron m and all j ∈ [p]. We formalize the phase alignment in the following theorem. 

Theorem B.7 (Formal Statement of Theorem 5.3) . Consider the main flow dynamics defined in (B.39a) 

and (B.39b) , under the initialization in Assumption 5.1. Let δ = o(1) be a sufficiently small tolerance. For any D⋆m(0) ∈ (0 , 2π], define the convergence time tδ = inf {t ∈ R+ : |D⋆m(t)| ≤ δ}. Then, tδ satisfies 

tδ ≍ (pκ init )−1 · 1 − (sin( D⋆m(0)) /δ }−1/3 + max {π/ 2 − | D⋆m(0) − π|, 0},

Furthermore, the magnitude at this time is given by β⋆m(tδ) ≍ κinit · { sin( D⋆m(0)) /δ }1/3. Moreover, in the mean-field regime m → ∞ , let ρt = Law  ϕ⋆m(t), ψ ⋆m(t) for all t ∈ R+ and let λunif denote the uniform law on (−π, π ]. Then, ρ0 = λ⊗2 

> unif

and ρ∞ = T#λunif , where T : φ 7 → (φ, 2φ) mod 2 π.

Before presenting the proof of Theorem B.7, we first introduce several key intermediate results that help elucidate the dynamics. We begin with a lemma that characterizes the simplified dynamics of the system, leveraging the Fourier domain and the single-frequency initialization. 

Lemma B.8 (Main Flow under Fourier Domain) . Under the initialization in Assumption 5.1, let k⋆

denote the initial frequency of each neuron, and we use the superscript ⋆ for notational simplicity. We define 

D⋆m(t) = 2 ϕ⋆m(t) − ψ⋆m(t) mod 2 π, then the main flow can be equivalently described as 

∂tα⋆m(t) = 2 p · α⋆m(t) · β⋆m(t) · cos( D⋆m(t)) , ∂tβ⋆m(t) = p · α⋆m(t)2 · cos( D⋆m(t)) ,∂t exp( iD⋆m(t)) = p ·



4β⋆m(t) + α⋆m(t)2

β⋆m(t)



· sin  D⋆m(t) · exp ( i{D⋆m(t) − π/ 2}) . (B.40) This lemma allows us to largely simplify the analysis, reducing it from tracking a 2p-dimensional system to a three-particle dynamical system of α⋆m(t), β⋆m(t) and D⋆m(t)) . Building on this, the next two lemmas further show that the dynamics is indeed one-dimensional, and the trajectory exhibits a symmetry property that aids in understanding the evolutions under different initializations. 

Lemma B.9. Consider the ODE in (B.40) , the following quantities remain constant: 

α⋆m(t)2 − 2β⋆m(t)2 = Cdiff , sin( D⋆m(t)) · β⋆m(t) · α⋆m(t)2 = Cprod , ∀t ∈ R+.

Building upon this, we can further simplify the dynamics of D⋆m(t)) in as 

∂tD⋆m(t) = −p ·  4β⋆m(t) + α⋆m(t)2/β ⋆m(t) · sin ( D⋆m(t)) , (B.41) 

due to its well-regularized behavior ensured by the constant relationship. 

44 (a) Simplified Dynamics with D⋆m(0) ∈ (π/ 2, π ). (b) Simplified Dynamics with D⋆m(0) ∈ (0 , π/ 2) .

Figure 15: Training dynamics of a specific decoupled neuron characterized by (B.47a) and (B.47b) with identical initial scales α⋆m(0) = β⋆m(0) and different phase difference D⋆m(0) . Figure (a) plots the dynamics of phases, phase difference, and the magnitudes with D⋆m(0) ∈ (π/ 2, π ), whose behavior is detailedly characterized in Theorem B.7. The difference decreases monotonically to 0, while the magnitudes first decay slightly when D⋆m(t) ∈ (π/ 2, π ) and then increase rapidly when D⋆m(t) falls below π/ 2. Figure (b) plots the dynamics under D⋆m(0) ∈ (0 , π/ 2) where D⋆m is initialized closer to the convergence point, resulting in a shorter convergence time compared to the case in Figure (a). Moreover, the simplified dynamics shown in Figure (b) align well with the full dynamics in Figure 8a with the same initialization, indicating the effectiveness of the approximation. We highlight that (B.41) is not a direct corollary from (B.40) due to the potential jump from 0 to 

2π in the discontinuous definition of mod 2 π. However, thanks to the constant relationship revealed in Lemma B.9, we can show that D⋆m(t) is “well-behaved" by staying in the half-space where it is initialized, and consistently approaching zero throughout the gradient flow. 

Lemma B.10. Consider the ODE given in (B.40) with initial condition D⋆m(0) ∈ (π/ 2, π ). Let tπ/ 2 denote the hit time that D⋆m(tπ/ 2) = π/ 2, then for any ∆t ∈ (0 , t π/ 2), we have 

β⋆m(tπ/ 2 − ∆t) = β⋆m(tπ/ 2 + ∆ t), D⋆m(tπ/ 2 − ∆t) + D⋆m(tπ/ 2 + ∆ t) = π. 

Proof of Lemma B.8, B.9 and B.10. Please refer to §B.5.1 for a detailed proof. Now we are ready to present the proof of Theorem B.7. 

Proof of Theorem B.7. Without loss of generality, we focus on the case where D⋆m(0) ∈ (0 , π ). The case D⋆m(0) ∈ (−π, 0) can be extended identically owing to the symmetry of dynamics in (B.40) as established in Lemmas B.8 and B.9. Specifically, the trajectories of α⋆m(t) and β⋆m(t) are invariant 45 under a sign flip of D⋆m(t) such that the entire dynamics evolves symmetrically, with D⋆m(t) mirrored from (0 , π ) to (−π, 0) at each time t.

Roadmap. In the following, we establish the convergence time by further dividing into two cases— D⋆m(0) ∈ (0 , π/ 2) and D⋆m(0) ∈ (π/ 2, π ). Notably, thanks to the symmetry established in Lemma B.10, we only need to characterize two time intervals (i) the traveling time from D⋆m(0) to π/ 2

for any D⋆m(0) ∈ (π/ 2, π ), denoted by ∆t

> π/ 2

, both initialized at β⋆m(0) = κinit , (ii) the convergence time from D⋆m(0) to 0 for an arbitrary initial phase D⋆m(0) ∈ (0 , π/ 2) , denoted by ∆t 

> δ

This is because, 

• For D⋆m(0) ∈ (0 , π/ 2) , the convergence time can be captured by ∆t

> δ

• For D⋆m(0) ∈ (π/ 2, π ), the time is given by 2∆ t 

> π/ 2

+ ∆ t 

> δ

, where with slight abuse of notation we let ∆t 

> δ

denote the time traveling from π − D⋆m(0) to 0. Such argument is supported by Lemma B.10, as it takes equal time for D⋆m(t) to travel from π − D⋆m(0) to π/ 2 and from π/ 2 to 

π − D⋆m(0) . Also, when D⋆m(t) reaches π − D⋆m(0) , we have β⋆m(t) = κinit due to the symmetry, such that the remaining convergence time is equal to ∆t 

> δ

.Below are some useful properties. Under the initialization in Assumption 5.1, Lemma B.9 ensures 

α⋆m(t)2 = 2 β⋆m(t)2 − κ2

> init

, ∀t ∈ R+. (B.42) Following this, we can characterize the dynamics as follows: 

∂tβ⋆m(t) = p · (2 β⋆m(t)2 − κ2

> init

) · cos( D⋆m(t)) , (B.43a) 

∂tD⋆m(t) = −p ·  6β⋆m(t) − κ2

> init

/β ⋆m(t) · sin ( D⋆m(t)) . (B.43b) Hence, we have D⋆m(t) is monotonely decreasing, and β⋆m(t) first decreases when D⋆m(t) ∈ (π/ 2, π )

and increases thereafter. Besides, it follows from (B.42) that β⋆m(t) ≥ κinit /√2 for all t ∈ R+.

Part I: Travelling time from D⋆m(0) to π/ 2 with D⋆m(0) ∈ (π/ 2, π ). We consider t ∈ (0 , ∆t

> π/ 2

]

where we define ∆t 

> π/ 2

= min {t ∈ R+ : D⋆m(t) ≤ π/ 2}. Based on (B.43b), by definition, we have 

∂tD⋆m(t) ≥ − p ·  6β⋆m(t) − κ2

> init

/β ⋆m(t) ≥ − 5p · κinit ,

where the last inequality uses 6β⋆m(t) − κ2

> init

/β ⋆m(t) is monotonically increasing on R+ and β⋆m(t) ∈

[κinit /√2, κ init ] since β⋆m(t) is monotonically decreasing throughout the stage. Following this, we can lower bound D⋆m(t) by D⋆m(t) ≥ D⋆m(0) − 5p · κinit · t for all t ≤ tϵ

> 1

. Thus, we have 

∆t 

> π/ 2

≥ D⋆m(0) − D⋆m(∆ t

> π/ 2

)5p · κinit 

= D⋆m(0) − π/ 25p · κinit 

,

On the other side, (B.43b) implies that ∂tD⋆m(t) ≤ 0 such that D⋆m(t) ≤ D⋆m(0) . Then, we have 

∂tD⋆m(t) ≤ − p ·  6β⋆m(t) − κ2

> init

/β ⋆m(t) · sin( D⋆m(0)) ≤ − 2√2p · κinit · sin( D⋆m(0)) .

Similarly, we can upper bound ∆t

> π/ 2

. By combining the arguments above, we have 

∆t 

> π/ 2

≍ (p · κinit )−1 · { D⋆m(0) − π/ 2}.

46 Part II: Convergence time from D⋆m(0) to 0 with D⋆m(0) ∈ (0 , π/ 2) . Consider a small error level 

δ > 0, and the convergence time is formalized as ∆t 

> δ

= min {t ∈ R+ : sin( D⋆m(t)) ≤ δ}. Note that 

D⋆m(t) is monotonically decreasing and β⋆m(t) is monotonically increasing in this stage. Also, 

sin( D⋆m(t)) · β⋆m(t) · α⋆m(t)2 = sin( D⋆m(t)) · β⋆m(t) · (2 β⋆m(t)2 − κ2

> init

) = sin( D⋆m(0)) · κ3

> init

,

following (B.42), Lemma B.9 and β⋆m(0) = κinit as specified in Assumption 5.1. By definition, 

sin( D⋆m(0)) /δ · κ3 

> init

= β⋆m(∆ t 

> δ

) · (2 β⋆m(∆ t 

> δ

)2 − κ2

> init

) ≍ β⋆m(∆ t 

> δ

)3.

Hence, we have β⋆m(∆ t 

> δ

)/κ init ≍ 3

psin( D⋆m(0)) /δ . Following (B.43a), it holds that 

∂t log β⋆m(t) − κinit /√2

β⋆m(t) + κinit /√2

!

= 2√2 · κinit · ∂tβ⋆m(t)2β⋆m(t)2 − κ2

> init

= 2 √2 · κinit · p · cos( D⋆m(t)) ≍ κinit · p, 

since cos( D⋆m(t)) ∈ [cos( D⋆m(0)) , 1] . Hence, by integrating over time (0 , ∆t 

> δ

], we can show that 

log β⋆m(∆ t 

> δ

) − κinit /√2

β⋆m(∆ t 

> δ

) + κinit /√2

!

+ log(3 + 2 √2) ≍ κinit · p · ∆t 

> δ

. (B.44) Next, we bound the scale of the term within the logarithm. For a small tolerance δ = o(1) , we have 

β⋆m(∆ t 

> δ

) − κinit /√2

β⋆m(∆ t 

> δ

) + κinit /√2 = 1 − 2 √2 · β⋆m(∆ t 

> δ

)/κ init + 1 −1 = 1 − Θ  3

pδ/ sin( D⋆m(0)) . (B.45) Thus, by combing the arguments in (B.44) and (B.45), we can conclude that 

∆t 

> δ

≍ (p · κinit )−1 · 1 − 3

pδ/ sin( D⋆m(0)) ,

where we use the fact that log(1 − x) ≍ x for small x > 0.Based on the results in Part I and Part II, for any initial phase difference D⋆m(0) ∈ (0 , π ) and sufficiently small error tolerance δ ∈ (0 , 1) , by symmetry, the convergence time is of level 

tδ ≍ (pκ init )−1 · 1 − (sin( D⋆m(0)) /δ }−1/3 + max {π/ 2 − | D⋆m(0) − π|, 0},

where we let (x)+ = max {x, 0} denote the ReLU function. 

Part III: Preservation of Uniform Phase Distribution and Double-Phase Convergence. Recall that Lemma B.9 gives there exists constant Cprod ∈ R such that 

sin( D⋆m(t)) · β⋆m(t) · α⋆m(t)2 = Cprod .

Following this, we can write the dynamics of ϕ⋆m(t) and ψ⋆m(t) as 

∂t exp( iϕ ⋆m(t)) = 2 p · Cprod · α⋆m(t)−2 · exp ( i {ϕ⋆m(t) − π/ 2}) ,∂t exp( iψ ⋆m(t)) = p · Cprod · β⋆m(t)−2 · exp ( i {ψ⋆m(t) + π/ 2}) . (B.46) As established previously, the magnitudes of the learned parameters, α⋆m(t) and β⋆m(t), tend to infinity as t → ∞ . This divergence drives the convergence of the corresponding phases to fixed values, ϕ⋆m(∞) and ψ⋆m(∞), which are determined by the initialization. Furthermore, Theorem 5.3 47 proves that the misalignment term D⋆m(t) converges to zero. This directly implies that the limiting phases must satisfy the phase alignment condition: 2ϕ⋆m(∞) = ψ⋆m(∞).Let exp( iϕ ⋆m(t)) = z(t). By (B.46) , z(t) is continuously differentiable with respect to t. Consider 

Φ⋆m(t) = ϕ⋆m(0) + 

Z t

> 0

ℑ(¯ z(s) · ∂sz(s))d s, 

then we can check that Φ⋆m(t) is continuously differentiable. By differentiating both sides, we can also check that it satisfies exp( iΦ⋆m(t)) = z(t) since 

∂t exp( iΦ⋆m(t)) = exp( iΦ⋆m(t)) · ∂t

Z t

> 0

ℑ(¯ z(s) · ∂sz(s))d s · i

= exp( i(Φ ⋆m(t) − π/ 2)) · ℑ (¯ z(t) · ∂tz(t)) = 2 p · Cprod · α⋆m(t)−2 · exp( i(Φ ⋆m(t) − π/ 2)) · ℑ (¯ z(t) · z(t) · (−i)) = 2 p · Cprod · α⋆m(t)−2 · exp( i(Φ ⋆m(t) − π/ 2)) ,

where the third equality results from (B.46) and the last line we use the fact that |z(t)| = 1 by definition. Using the uniqueness of ODE and initial condition exp( iΦ⋆m(0)) = z(0) , we can conclude that exp( iΦ⋆m(t)) = z(t) and thus ϕ⋆m(t) mod 2 π = Φ ⋆m(t) mod 2 π. By direct calculation, we have 

∂tΦ⋆m(t) = −2p · Cprod · α⋆m(t)−2,

which indicates that 

Φ⋆m(t) = ϕ⋆m(0) − 2p · Cprod ·

Z t

> 0

α⋆m(s)−2ds. 

Recall that the dynamics of α⋆m(t) is jointly given by (B.42) , (B.43a) and (B.43b) . Following this, given {α⋆m(0) , β ⋆m(0) , D⋆m(0) }, we can write 

ϕ⋆m(t) mod 2 π = Φ ⋆m(t) mod 2 π := ϕ⋆m(0) mod 2 π + G(α⋆m(0) , β ⋆m(0) , D⋆m(0)) mod 2 π. 

By simple calculation, we can show that ϕ⋆m(0) ⊥⊥ D⋆m(0) and ϕ⋆m(0) i.i.d. 

∼ (−π, π ] for all m. Combining these arguments and applying a similar one to ψ⋆m(t) establishes that 

ϕ⋆m(t) i.i.d. 

∼ Unif( −π, π ), ψ⋆m(t) i.i.d. 

∼ Unif( −π, π ), ∀(t, m ) ∈ R+ × [M ].

Thus, ϕ⋆m(∞) and ψ⋆m(∞) are both uniformly distributed over [0 , 2π). Recall that 2ϕ⋆m(∞) = ψ⋆m(∞)

for any given initialization, then the joint measure of (ϕ⋆m(∞), ψ ⋆m(∞)) degenerates on the (periodic) line 2ϕ = ψ inside the support. Since the marginals of them are both uniform, the joint limiting measure is given by ρ∞ = T#λunif with T : φ 7 → (φ, 2φ) mod 2 π, which completes the proof. 

B.5.1 Proof of Auxiliary Lemma B.8, B.9 and B.10 

Proof of Lemma B.8. Following the same argument in the proof of Theorem 5.2, by pushing the approximation error to 0, we can show an exact single-frequency pattern: 

αkm(t) = βkm(t) ≡ 0, ∀t ∈ R+, k ̸ = k⋆.

Formally, this result holds under the initialization in Assumption 5.1, which can be justified using a matrix ODE argument over ukm(t) = ( αkm(t), β km(t)) ⊤ with zero initial value. Then, the dynamics of the original parameter can be simplified to a coefficient only related to k⋆. For all j ∈ [p], we have 

∂tθm[j]( t) = 2 p · α⋆m(t) · β⋆m(t) · cos( ωkj + ψ⋆m(t) − ϕ⋆m(t)) , (B.47a) 48 ∂tξm[j]( t) = p · α⋆m(t)2 · cos( ωk⋆ j + 2 ϕ⋆m(t)) . (B.47b) Recall ∂tgm[j]( t) = ⟨bj , ∂ tθm(t)⟩, by simple calculation, it holds that 

∂tgm[2 k⋆]( t) = √2 · p3/2 · α⋆m(t) · β⋆m(t) · cos  ψ⋆m(t) − ϕ⋆m(t),∂tgm[2 k⋆ + 1]( t) = −√2 · p3/2 · α⋆m(t) · β⋆m(t) · sin  ψ⋆m(t) − ϕ⋆m(t),

and similarly, by using ∂trm[j]( t) = ⟨bj , ∂ tξm(t)⟩, we can obtain that 

∂trm[2 k⋆]( t) = p3/2/√2 · α⋆m(t)2 · cos  2ϕ⋆m(t),∂trm[2 k⋆ + 1]( t) = −p3/2/√2 · α⋆m(t)2 · sin  2ϕ⋆m(t),

where the additional p2/p arises from the normalization factor in bj ’s (see §5.1). Since the magnitudes follows α⋆m = p2/p · ∥ g⋆m∥ and β⋆m = p2/p · ∥ r⋆m∥, by applying the chain rule, then 

∂tα⋆m(t) = 2 p · α⋆m(t) · β⋆m(t) · cos  2ϕ⋆m(t) − ψ⋆m(t), (B.48a) 

∂tβ⋆m(t) = p · α⋆m(t)2 · cos  2ϕ⋆m(t) − ψ⋆m(t). (B.48b) Next, we understand the evolution of phases by tracking the dynamics of exp( iϕ ⋆m(t)) and exp( iψ ⋆m(t)) 

via Euler’s formula. Note that ϕ⋆m(t) and ψ⋆m(t) cannot be directly tracked via ODEs due to abrupt jumps from −π to π, which arise from the use of atan2( ·) function in definitions (see §5.1). By definition and the chain rule, it follows that 

∂t cos( ϕ⋆m(t)) = 

r 2

p · ∂t

 gm[2 k⋆]( t)

α∗

> m

(t)



=

r 2

p ·

 ∂tgm[2 k⋆]( t)

α∗

> m

(t) − ∂tα∗

> m

(t)

α∗

> m

(t) · gm[2 k⋆]( t)

α∗

> m

(t)



= 2 p · β⋆m(t) · cos  ψ⋆m(t) − ϕ⋆m(t)

− 2p · β⋆m(t) · cos( ϕ⋆m(t)) · cos  2ϕ⋆m(t) − ψ⋆m(t)

= 2 p · β⋆m(t) · sin( ϕ⋆m(t)) · sin  2ϕ⋆m(t) − ψ⋆m(t),

where the second equality uses cos( ϕ⋆m(t)) = p2/p · gm[2 k⋆]( t)/α ∗

> m

(t) and the last one results from the trigonometric indentity. Similarly, we have 

∂t sin( ϕ⋆m(t)) = −2p · β⋆m(t) · cos( ϕ⋆m(t)) · sin  2ϕ⋆m(t) − ψ⋆m(t),

which gives that 

∂t exp( iϕ ⋆m(t)) = 2 p · β⋆m(t) · sin  2ϕ⋆m(t) − ψ⋆m(t) · exp ( i {ϕ⋆m(t) − π/ 2}) . (B.49) Following a similar argument, we can show that 

∂t exp( iψ ⋆m(t)) = p · α⋆m(t)2

β⋆m(t) · sin  2ϕ⋆m(t) − ψ⋆m(t) · exp ( i {ψ⋆m(t) + π/ 2}) . (B.50) Thanks to the initialization and preservation of the single-frequency, the 2p-dimensional dynamical system can be tracked via a four-particle system with α⋆m, β⋆m, ϕ⋆m, and ψ⋆m, whose dynamics are given by (B.48a), (B.48b), (B.49) and (B.50). Furthermore, note that 

∂t exp(2 iϕ ⋆m(t)) = 2 exp( iϕ ⋆m(t)) · ∂t exp( iϕ ⋆m(t)) 

49 = 4 p · β⋆m(t) · sin  2ϕ⋆m(t) − ψ⋆m(t) · exp ( i {2ϕ⋆m(t) − π/ 2}) . (B.51) Based on (B.50) and (B.51), by denoting D⋆m(t) = 2 ϕ⋆m(t) − ψ⋆m(t) mod 2 π, we obtain that 

∂t exp( iD⋆m(t)) = ∂t exp(2 iϕ ⋆m(t)) exp( iψ ⋆m(t)) − exp(2 iϕ ⋆m(t)) · ∂t exp( iψ ⋆m(t)) exp(2 iψ ⋆m(t)) = 4 p · β⋆m(t) · sin  D⋆m(t) · exp ( i {D⋆m(t) − π/ 2})

− p · α⋆m(t)2

β⋆m(t) · sin  D⋆m(t) · exp ( i {D⋆m(t) + π/ 2})= p ·



4β⋆m(t) + α⋆m(t)2

β⋆m(t)



· sin  D⋆m(t) · exp ( i{D⋆m(t) − π/ 2}) . (B.52) By combining (B.48a), (B.48b) and (B.52), we complete the proof. 

Proof of Lemma B.9. Following the simplified main flow in the Fourier domain (see Lemma B.8), it is easy to show that α⋆m(t)2 − 2β⋆m(t) is a constant throughout the gradient flow since 

∂t{α⋆m(t)2 − 2β⋆m(t)2} = 2 α⋆m(t) · ∂tα⋆m(t) − 4β⋆m(t) · ∂tβ⋆m(t) = 0 .

Hence, there exists an initialization-dependent constant Cdiff such that 

α⋆m(t)2 = 2 β⋆m(t)2 + Cdiff , ∀t ∈ R+.

Moreover, by applying the chain rule, we can deduce that 

∂t{α⋆m(t)2 · β⋆m(t)} = α⋆m(t)2 · ∂tβ⋆m(t) + ∂t{α⋆m(t)2} · β⋆m(t)= α⋆m(t)2 · ∂tβ⋆m(t) + 2 ∂t{β⋆m(t)2} · β⋆m(t)= p · α⋆m(t)4 · cos( D⋆m(t)) + 4 β⋆m(t)2 · p · α⋆m(t)2 · cos( D⋆m(t)) = p · α⋆m(t)2 · { α⋆m(t)2 + 4 β⋆m(t)2} · cos( D⋆m(t)) .

Following this, we can compute the time derivative of sin( D⋆m(t)) · β⋆m(t) · α⋆m(t)2, following that 

∂t{sin( D⋆m(t)) · β⋆m(t) · α⋆m(t)2}

= ∂t sin( D⋆m(t)) · β⋆m(t) · α⋆m(t)2 + sin( D⋆m(t)) · ∂t{α⋆m(t)2 · β⋆m(t)}

= − cos( D⋆m(t)) · α⋆m(t)2 · p ·  4β⋆m(t)2 + α⋆m(t)2 · sin ( D⋆m(t)) + sin( D⋆m(t)) · p · α⋆m(t)2 · { α⋆m(t)2 + 4 β⋆m(t)2} · cos( D⋆m(t)) = 0 ,

where the second equality uses (B.40) in Lemma B.8. Therefore, there exists constant Cprod such that sin( D⋆m(t)) · β⋆m(t) · α⋆m(t)2 = Cprod for all t ∈ R+.Finally, we show that D⋆m(t) remains within the half-space where it is initialized, which means 

D⋆m(t) ∈ (ιπ, (ι + 1) π) for ι ∈ {− 1, 0} determined by the initial state D⋆m(0) ∈ (ιπ, (ι + 1) π). By Lemma B.9, we always have sin( D⋆m(t)) ̸ = 0 , so D⋆m(t) will never reach ιπ for any ι. This ensures no jump behavior occurs for D⋆m(t), allowing us to directly track its dynamics. Following this, by applying chain rule over (B.52), we can reach that 

∂tD⋆m(t) = −p ·  4β⋆m(t) + α⋆m(t)2/β ⋆m(t) · sin ( D⋆m(t)) ,

which completes the proof. 50 Proof of Lemma B.10. Based on the results in Lemma B.8 and B.9, we reduce the main flow into a one-dimensional dynamical system characterized by β⋆m(t). Specifically, we have 

∂tβ⋆m(t) = p · α⋆m(t)2 · cos( D⋆m(t)) = p · (2 β⋆m(t)2 + Cdiff ) · sign {cos( D⋆m(t)) } · 

s

1 − C2

> prod

β⋆m(t)2 · (2 β⋆m(t)2 + Cdiff )2

:= ς(β⋆m(t)) · sign {cos( D⋆m(t)) }.

As given in (B.41) , due to the nonnegativity of the magnitudes, we can show that D⋆m(t) is monotonely decreasing if D⋆m(0) ∈ (π/ 2, π ). We consider s = t − tπ/ 2 for t ∈ [tπ/ 2, 2tπ/ 2) and r = tπ/ 2 − t for 

t ∈ (0 , t π/ 2], where tπ/ 2 denote the hit time that D⋆m(tπ/ 2) = π/ 2. Following this, we have 

∂sβ⋆m(s) = ∂tβ⋆m(t − tπ/ 2) = −ς(β⋆m(s)) , ∂rβ⋆m(r) = −∂tβ⋆m(tπ/ 2 − t) = −ς(β⋆m(r)) .

Here, we decompose ∂tβ⋆m(t) within time [0 , 2tπ/ 2] into a backward process within time (0 , t π/ 2] and a forward process within time [tπ/ 2, 2tπ/ 2] respectively. Starting from time s = r = 0 , where the initial value is both given by β⋆m(tπ/ 2), since ς is locally Lipschitz, by the uniqueness of the ODE solution, for s = r, we have β⋆m(s) = β⋆m(r), i.e., β⋆m(tπ/ 2 + ∆ t) = β⋆m(tπ/ 2 − ∆t) for all ∆t ∈ [0 , t π/ 2).Furthermore, by combining Lemma B.9, the monotonicity of D⋆m(t) and the arguments above, we can show that D⋆m(tπ/ 2 − ∆t) + D⋆m(tπ/ 2 + ∆ t) = π, which completes the proof. 

# C Proof of Results for Theoretical Extensions in Section 6 

C.1 Proof of Corollary 6.1: Phase Lottery Ticket 

We first formalize the random multiple frequency initialization as follows. 

Assumption C.1. For each neuron m ∈ [M ], the parameters (ξm, θ m) are initialized as 

θm(0) ∼ κinit · pp/ 2 ·

> (p−1) /2

X

> k=1

(ϱ1,k [1] · b2k + ϱ1,k [2] · b2k+1 ) ,ξm(0) ∼ κinit · pp/ 2 ·

> (p−1) /2

X

> k=1

(ϱ2,k [1] · b2k + ϱ2,k [2] · b2k+1 ) ,

where ϱr,k i.i.d. 

∼ Unif( S1) for all k and r ∈ { 1, 2}, and κinit > 0 denotes a small initialization scale. 

This is the natural extension of Assumption 5.1 to multiple frequencies, and the arguments in §B, i.e., Lemma B.8, B.9 and B.10, go through with only routine modifications thanks to the neuron decoupling and the orthogonality of frequencies. We first state the formal version of Corollary C.2. 

Corollary C.2 (Formal Statement of Corollary 6.1) . Consider a random initialization following Assump-tion C.1, and let k⋆ denote the winning frequency given by k⋆ = min k eDkm(0) . For a given ε ∈ (0 , 1) , define the dominance time tε as 

tε := inf {t ∈ R+ : max   

> k̸=k⋆

βkm(t)/β ⋆m(t) ≤ ε}.

Then, with probability at least 1 − eΘ( p−c), where c > 0 satisfying p ≳ c4π2e−2(1 −c), it holds that 

tε ≲ π2p−(2 c+3) 

κinit 

+ (c + 1) log p + log 11−ε

pκ init · { 1 − 2c2π2 · (log p/p )2} .

51 Before delving into the proof, we first establish a key property of the decoupled dynamics under this initialization—order preservation—under the initialization specified in C.1. 

Lemma C.3. Let σ be the permutation that sorts the initial phase differences in non-decreasing order: 

eDσ(1)  

> m

(0) ≤ eDσ(2)  

> m

(0) ≤ · · · ≤ eDσ( p−12 ) 

> m

(0) ,

where eDkm(0) = min {Dkm(0) , 2π − Dkm(0) } represents the shortest circular distance for the initial phase. Under the initialization in Assumption C.1, the rank-ordering of the corresponding magnitudes βkm(t) is inverted and preserved for all time t ≥ 0:

βσ(1)  

> m

(t) ≥ βσ(2)  

> m

(t) ≥ · · · ≥ βσ( p−12 ) 

> m

(t).

Proof of Lemma C.3. Please refer to §C.1.1 for a detailed proof. Lemma C.3 states that, when neurons are decoupled and each frequency is initialized at the same scale κinit > 0, the ordering of frequencies by magnitude βkm’s within each neuron remains fixed throughout the gradient flow, with larger magnitudes corresponding to smaller initial phase difference. Now we are ready to present the proof of Corollary C.2. 

Proof of Corollary C.2. As specified in Assumption C.1, for all m ∈ [M ], we initialize ϱr,k i.i.d. 

∼ Unif( S1)

for all r ∈ { 1, 2} and k ∈ [ p−12 ]. Thanks to the orthogonality among frequencies, each frequency evolves independently, so Lemmas B.8, B.9 and B.10 apply to every frequency k, not just the feature frequency k⋆. For fixed neuron m, by defining eDkm(0) = min {Dkm(0) , 2π − Dkm(0) }, we have 

∂tβkm(t) = p · (2 βkm(t)2 − κ2

> init

) · cos( eDkm(t)) , (C.1a) 

∂t eDkm(t) = −p ·  6βkm(t) − κ2

> init

/β km(t) · sin   eDkm(t). (C.1b) 

Step 1: Deriving Winning Frequency and Initial Phase Gap. By Lemma C.3, the dynamics preserves the ordering of eDkm’s and βkm’s throughout the gradient flow. Specifically, at any time 

t ∈ R+, the ordering remains unchanged. Thus, the lottery ticket winner, i.e., frequency k such that 

βkm(t) ≥ βτm(t) for all τ̸ = k, is given by k⋆ = argmin k eDkm(0) .To demystify the dominance phenomenon, it suffices to focus on the growth of the magnitude of the winning frequency k⋆ and the second-dominant frequency k♯ = argmin k̸ =k⋆ eDkm(0) . Under the initialization as specified in Assumption C.1, with probability greater than 1 − eΘ( p−c) for some constant c ∈ (0 , 1) , we have the following good initialization: 

Einit = E1 

> init

∩ E 2 

> init

∩ E 3

> init

:=  eD♯m(0) < π/ 2 ∩  cos( eD⋆m(0)) ≥ cos( eD♯m(0)) + π2p−2( c+1) }∩ { cos( eD⋆m(0)) ≤ 1 − 2c2π2 · (log p/p )2}. (C.2) This is because eDkm(0) i.i.d. 

∼ Unif(0 , π ) based on a similar argument in Lemma B.6, and thus eD⋆m(0) 

and eD♯m(0) are respectively the first- and the second- order statistics of p−12 i.i.d copies of Unif(0 , π ),denoted by U(i)’s. Notice that 

P E1,c 

> init

 = P ∀i, U (i) ≥ π/ 2 + P ∀i > 1, U (i) ≥ π/ 2, U (1) ≤ π/ 2 = ( p + 1) · 2− p+1 2 ≲ p−c. (C.3) 52 Furthermore, if p ≳ c4π2e−2(1 −c), it holds that 

P E2,c 

> init

 ≤ P {cos( U(1) ) ≤ cos( U(2) ) + π2p−2( c+1) } ∩ E 1

> init

 + P E1,c 

> init



≲ P {U 2(2) − U 2(1) − U 4(2) /12 ≤ 2π2p−2( c+1) } ∩ E 1

> init

 + p−c

≤ P {U 2(2) − U 2(1) ≤ 2π2p−2( c+1) + 2( cπ/p · log p)4} ∩ E 1

> init



+ P {U 4(2) ≥ 24 · (cπ/p · log p)4} ∩ E 1

> init

 + p−c

≤ P(U 2(2) − U 2(1) ≤ 8π2p−2( c+1) ) + P(U(2) ≥ 2cπ/p · log p) + p−c, (C.4) where the second inequality uses 1 − x2/2 ≤ cos( x) ≤ 1 − x2/2 + x4/24 for x ∈ (0 , π/ 2) . Moreover, to bound the RHS of (C.4), we can show that 

P(U 2(2) − U 2(1) ≤ 8π2p−2( c+1) ) ≤ P(U(1) · (U(2) − U(1) ) ≤ 4π2p−2( c+1) )

≤ P(U(1) ≤ 2πp −(c+1) ) + P(U(2) − U(1) ≤ 2πp −(c+1) )= 2 − 2(1 − 2p−(c+1) ) p−12 ≲ p−c, (C.5) where the second inequality follows U(1) 

> d

= U(2) − U(1) . Furthermore, it holds that 

P(U(2) ≥ 2cπ/p · log p) = (1 − 2c/p · log p) p−12 + c(p − 1) /p · log p · (1 − 2c/p · log p) p−32

≤ (1 + c log p) · (1 − 2c/p · log p) p−32 ≲ p−c log p. (C.6) By combining (C.4), (C.5) and (C.6), we have P E2,c 

> init

 ≲ p−c log p. Similarly, we can derive that 

P E3,c 

> init

 = P  cos( U(1) ) ≤ 1 − 2c2π2 · (log p/p )2

≤ P(U(1) ≥ 2cπ/p · log p) = (1 − 2c/p · log p) p−12 ≲ p−c, (C.7) where the inequality also uses cos( x) ≥ 1 − x2/2 for x ∈ (0 , π/ 2) Based on (C.3) , (C.4) and (C.7) , the good initialization event Einit holds with a probability of at least 1 − Θ( p−c log p). In the subsequent analysis, we assume that this event occurs. 

Step 2: Growth of Gap between Winning Frequency and Others. Based on (C.1a) , the dynamics for the log-magnitude follows 

∂t log βkm(t) = ∂tβkm(t)

βkm(t) = p · (2 βkm(t) − κ2

> init

/β km(t)) · cos( eDkm(t)) .

To compare the winning frequency ( ⋆) against the runner-up ( ♯), we examine the dynamics of their log-ratio ∂t log β⋆m(t) 

> β♯m(t)

, which measures the exponential rate at which the winner pulls ahead: 

∂t log β⋆m(t)

β♯m(t) = p · (2 β⋆m(t) − κ2

> init

/β ⋆m(t)) · cos( eD⋆m(t)) − p · (2 β♯m(t) − κ2

> init

/β ♯m(t)) · cos( eD♯m(t)) = p · (β⋆m(t) − β♯m(t)) · { 2 + κ2

> init

/(β⋆m(t) · β♯m(t)) } · cos( eD⋆m(t)) + p · (2 β♯m(t) − κ2

> init

/β ♯m(t)) · { cos( eD⋆m(t)) − cos( eD♯m(t)) }≥ 2p · cos(( eD⋆m(0)) · (β⋆m(t) − β♯m(t)) + p · κinit · { cos( eD⋆m(t)) − cos( eD♯m(t)) }. (C.8) 53 Here, we use (i) β⋆m(t) ≥ β♯m(t) and eD⋆m(t) ≤ eD♯m(t) for all t ∈ R+ based on the order preservation property in Lemma C.3, and (ii) under the good initialization Einit where eD⋆m(0) , eD♯m(0) ≤ π 

> 2

, we have 

∂t eD⋄

> m

(t) < 0 and ∂tβ⋄

> m

(t) > 0 for all (⋄, t ) ∈ { ⋆, ♯ }∪ R+. Therefore, we have cos( eD⋆m(t)) ≥ cos( eD⋆m(0)) ,

β♯m(t) ≥ β♯m(0) = κinit and 2β♯m(t)−κ2

> init

/β ♯m(t) ≥ 2β♯m(0) −κ2

> init

/β ♯m(0) = κinit under the initialization in Assumption C.1. Let ρm(t) = β⋆m(t)/β ♯m(t). Following (C.8), we have 

∂t log ρm(t) ≥ 2p · κinit · cos( eD⋆m(0)) · (ρm(t) − 1) ∨ p · κinit · { cos( eD⋆m(t)) − cos( eD♯m(t)) },

Based on the first term in the right-hand side, a simple calculation shows that the dynamics satisfy: 

∂t log 

 ρm(t) − 1

ρm(t)



≥ 2p · κinit · cos( eD⋆m(0)) > 0.

Thus, we can integrate this result over any interval [s, t ] to obtain a lower bound: 

ρm(t) ≥ { 1 + (1 /ρ m(s) − 1) · exp(2 p · cos( eD⋆m(0)) · κinit · (t − s)) }−1, ∀s ∈ (0 , t ]. (C.9) Following this, once the ratio ρm(t) is larger than 1, the ratio ρm(t) surpasses 1, it begins to grow super-exponentially, accelerating rapidly towards infinity. Motivated by this dynamics, our analysis proceeds in two stages: first, we show that ρm(t) does not get stuck at the initial stationary point 

ρm(t) ≡ 1, and second, we quantify its rate of growth using (C.9). 

Step 2.1. Initial Growth of the Ratio Beyond Unity. Consider a short initial time interval (0 , t 1],during which the model parameters remain close to their initial values while the ratio ρm(t) quickly exceeds 1. Based on (C.1b), we have 

| cos( eD⋆m(t)) − cos( eD♯m(t)) − cos( eD⋆m(0)) + cos( eD♯m(0)) |≤ 2 max  

> ⋄∈{ ⋆,♯ }

| cos( eD⋄

> m

(t)) − cos( eD⋄

> m

(0)) |≤ 2 max  

> ⋄∈{ ⋆,♯ }

cos( eD⋄

> m

(t)) = 2 max 

> ⋄∈{ ⋆,♯ }

Z t

> 0

∂s cos( eD⋄

> m

(s))d s

= 2 p · max 

> ⋄∈{ ⋆,♯ }

Z t

> 0

 6β⋄

> m

(s) − κ2

> init

/β ⋄

> m

(s) · sin( eD⋄

> m

(s)) 2ds

≤ 6p · max 

> ⋄∈{ ⋆,♯ }

Z t

> 0

β⋄

> m

(s)d s ≤ 6pt · max  

> ⋄∈{ ⋆,♯ }

max  

> 0≤s≤t

β⋄

> m

(s) = 6 pt · β⋆m(t), (C.10) where the last inequality results from β⋆m(s) ≤ β⋆m(t) for all s ∈ (0 , t ] and the rank preservation property, i.e., β⋄

> m

(t) ≤ β⋆m(t) at any time t, as shown in Lemma C.3. Following (C.1a), we get 

∂tβ⋆m(t) ≤ p · (2 β⋆m(t)2 − κ2

> init

) = ⇒ β⋆m(t) ≤ κinit /√2 · coth( −√2pκ init · t − ι1), ∀t ∈ R+, (C.11) where we denote ι1 = arccoth( √2) . By choosing cg ∈ (0 , 1) , we define 

t1 := inf s ∈ (0 , t ] : 3 √2pκ init · s · coth( −√2pκ init · s − ι1) > c g · π2p−2( c+1) .

Here, we choose a sufficiently small cg to ensure that t1 is well-defined and finite before the system explodes. This choice makes t1 correspondingly small and the following asymptotic result holds: 

coth( −√2pκ init · t1 − ι1) ≍ √2 + pκ init · t1 =⇒ t1 ≍ cg · π2p−(2 c+3) /κ init . (C.12) 54 Recall from (C.2) that under the good initialization Einit , the initial cosine gap cos( D⋆m(0)) −cos( D♯m(0)) 

is lower bounded by π2p−2( c+1) . By combining (C.10), (C.11) and definition of t1, we have 

cos( eD⋆m(t)) − cos( eD♯m(t)) 

≥ cos( eD⋆m(0)) − cos( eD♯m(0)) − | cos( eD⋆m(t)) − cos( eD♯m(t)) − cos( eD⋆m(0)) + cos( eD♯m(0)) |≥ π2p−2( c+1) − 6p · sup 

> t∈(0 ,t 1]

t · β⋆m(t) ≥ (1 − cg) · π2p−2( c+1) , (C.13) for all t ∈ (0 , t 1]. Building upon (C.12) and (C.13), we can show that 

log ρm(t1) = log ρm(0) + 

Z t1

> 0

cos( D⋆m(s)) − cos( D♯m(s))d s

≳ cg(1 − cg) · pκ init · t1 · π2p−2( c+1) ≍ π4p−4( c+1) ,

and thus ρm(t1) ≳ exp(1 + π4p−4( c+1) ) ≍ 1 + π4p−4( c+1) for sufficiently large p.

Step 2.2. Super-exponential Growth. Let ε > 0 be the dominance threshold. We now derive the time t2 required for the lower bound of the ratio to exceed this threshold, i.e., ρm(t2) > 1/ε , such that tε ≤ t2b. Our starting point is the state at time t1, after which we have ρm(t1) ≍ 1 + π4p−4( c+1) .Following (C.9), we have 

ρm(t)−1 ≤ 1 + (1 /ρ m(t1) − 1) · exp(2 p · cos( eD⋆m(0)) · κinit · (t − t1)) 

≲ 1 − π4p−4( c+1) · exp(2 p · { 1 − 2c2π2 · (log p/p )2} · κinit · (t − t1)) ,

where the last inequality results from 1/ρ m(t1) − 1 ≍ 1 − ρm(t1) given ρm(t1) is close to 1, and the good initialization cos( eD⋆m(0)) ≥ 1 − 2c2π2 · (log p/p )2 in (C.2). By choosing 

t2 = t1 + 4( c + 1) log p + log 11−ε − 4 log π

2pκ init · { 1 − 2c2π2 · (log p/p )2} ≍ π2p−(2 c+3) 

κinit 

+ (c + 1) log p + log 11−ε

pκ init · { 1 − 2c2π2 · (log p/p )2} ,

we can guarantee that ρm(tε)−1 < ε , which completes the proof. 

C.1.1 Proof of Auxiliary Lemma C.3 

We begin by recalling the foundational results for a celebrated class of dynamical systems–known as cooperative systems–which enjoy a useful rank-preservation property (e.g., Smith, 1995). Before stating this formally, let us give a precise definition. 

Definition C.4 (Cooperative System) . Consider a p-convex set S ⊂ Rd such that tx + (1 − t)xy ∈ S 

for all t ∈ [0 , 1] whenever x, y ∈ S and x− ≤ x+. Suppose f : S 7 → S is continuously differentiable. The dynamical system, defined by ∂txt = f (xt), is called cooperative if ∂f i 

> ∂x j

(x) ≥ 0 for all i̸ = j.

In other words, a cooperative system’s Jacobian has nonnegative off-diagonal entries , so increasing any coordinate of the state cannot decrease another in the next iteration. With this definition in hand, we can now state the key monotonicity property of cooperative systems. 

Lemma C.5. Consider a cooperative system ∂txt = f (xt), and write x ≤ y for x, y ∈ Rd if xi ≤ yi for all 

i ∈ Rd. Given two initial values x10 ≤ x20, then we have x1 

> t

≤ x2 

> t

at all times t ∈ R+.Proof of Lemma C.5. Please refer to Kamke (1932); Hirsch (1982) for a detailed proof. 55 In what follows, we prove Lemma C.3, which is a direct application of Lemma C.5. 

Proof of Lemma C.3. Recall that, by Lemmas B.9 and B.10, together with the orthogonality of the frequency basis, for every k ∈ [ p−12 ], the dynamical system is given by (C.1a) and (C.1b) with initial condition βkm(0) = κinit for every frequency k.We first show that the evolution of Dkm(t) consistently shares the symmetric trajectory at any time t if initialized symmetrically. Let x(t) = ( βkm(t), Dkm(t)) and denote by ς(x(t)) right-hand side of (C.1a) , (C.1b) , such that ∂tx(t) = ς(x(t)) . Define the involution I(β, D) = ( β, 2π − D) with its Jacobian following dI ≡ diag(1 , −1) . A direct calculation shows that 

ς ◦ I(βkm(t), Dkm(t)) − dI · ς(βkm(t), Dkm(t)) ≡ 0,

i.e., the system is equivariant under I. By uniqueness of solutions, the solution with initial x(0) = (βkm(0) , 2π − Dkm(0)) satisfies x(t) = I βkm(t), Dkm(t), so the two trajectories remain symmetric. Hence, it suffices to consider the dynamics with standardized initialization min {Dkm(0) , 2π −

Dkm(0) } ∈ (0 , π ]. Following a similar argument in Lemma B.9, under the standardized initialization, we have Dkm(t) ∈ (0 , π ) at all time t. To verify cooperativeness, we introduce eβkm = −βkm and rewrite the dynamics in the new coordinates ( ¯βkm, Dkm). From (C.1a) and (C.1b) one obtains 

∂t eβkm(t) = −p · (2 eβkm(t)2 − κ2

> init

) · cos( Dkm(t)) := ς1(βkm(t), Dkm(t)) ,∂tDkm(t) = p ·  6 eβkm(t) − κ2

> init

/ eβkm(t) · sin  Dkm(t) := ς2(βkm(t), Dkm(t)) ,

and it is easy to check that the vector field is cooperative by 

∂ς 1

∂Dkm

= sin( Dkm(t)) > 0, − ∂ς 2

∂ eβkm

= p ·  6 + κ2

> init

/ eβkm(t)2 > 0.

Thus, (−βkm, Dkm) is cooperative, and by Lemma C.5, it preserves the initial ordering. Since 

βkm(0) = κinit for all k and phase difference Dkm(0) ’s are distinct, it follows that 

Dkm(0) ≤ Dτm(0) = ⇒ ∀ t ∈ R+, eβkm(t) ≤ eβτm(t) = ⇒ ∀ t ∈ R+, β km(t) ≥ βτm(t),

for every pair k, τ ∈ [ p−12 ], which completes the proof. 

C.2 Proof of Proposition 6.3: Dynamics of ReLU Activation 

Proof of Proposition 6.3. We begin by recalling from §B.2 that, for each fixed index m, the gradient with respect to the decoupled loss ℓm takes the form 

∂ℓ ∂θ m[j] = −2 X

> x∈Zp

ξm[mp(x, j )] · 1(⟨ex + ej , θ m⟩ ≥ 0) + 2

p

X

> x∈Zp
> p

X 

> τ=1

ξm[τ ] · 1(⟨ex + ej , θ m⟩ ≥ 0) , (C.14a) 

∂ℓ m

∂ξ m[j] = − X

> (x,y )∈S pj

max {⟨ ex + ey, θ m⟩, 0} + 1

p

> p

X

> j=1

X

> (x,y )∈S pj

max {⟨ ex + ey, θ m⟩, 0}, (C.14b) for all j ∈ [p]. We first evaluate these gradients at the single-frequency θm[j] = α⋆m · cos( ωk⋆ j + ϕ⋆m)

and ξm[j] = β⋆m · cos( ωk⋆ j + ψ⋆m) for all j, and then to extract the DFT coefficients. 56 Step 1: Gradient of ξm. First observe that max {x, 0} = ( x + |x|)/2. Then, we have 

X

> (x,y )∈S pj

σ(⟨ex + ey, θ m⟩) = 12

X

> (x,y )∈S pj

⟨ex + ey, θ m⟩ + 12

X

> (x,y )∈S pj

|⟨ ex + ey, θ m⟩| 

= α∗

> m

2

X

> (x,y )∈S pj

| cos( ωk⋆ x + ϕ⋆m) + cos( ωk⋆ y + ϕ⋆m)|, (C.15) Moreover, by applying the sum-to-product trigonometric identities, we can show that 

12

X

> (x,y )∈S pj

| cos( ωk⋆ x + ϕ⋆m) + cos( ωk⋆ y + ϕ⋆m)|

= X

> (x,y )∈S pj

| cos( ωk(x + y)/2 + ϕ⋆m)| · | cos( ωk(x − y)/2) |

= | cos( ωkj/ 2 + ϕ⋆m)| · X

> x∈Zp

| cos( ωkx/ 2) | p→∞ 

= 2pπ · | cos( ωkj/ 2 + ϕ⋆m)|. (C.16) The last inequality uses the fact that for an odd prime p, {ωkx}x∈Zp = {2kxπ/p }x∈Zp = {2πx/p }x∈Zp ,which is a uniform sample of [0 , 1] . Thus, in the limit p → ∞ , we have 

1

p

X

> x∈Zp

| cos( ωkx/ 2) | p→∞ 

=

Z 10

| cos( πx )|dx = 1

π

Z π

> 0

| cos( u)|du = 2

π .

By putting these two asymptotic expressions (C.15) and (C.16) into (C.14b), we obtain that 

∂ℓ m

∂ξ m[j] = − pα ⋆m

π ·



| cos( ωkj/ 2 + ϕ⋆m)| − 1

p

> p

X

> i=1

| cos( ωki/ 2 + ϕ⋆m)|



, ∀j ∈ [p].

Next, we apply DFT with respect to ∇ξm ℓm in the asymptotic regime p → ∞ . Let rk ∈ [p] denote the multiplication factor in Definition 6.2, i.e., rkk⋆ = k mod p for k, k ⋆ ∈ [ p−12 ]. Then, we have 

12p

> p

X

> j=1

| cos( ωk⋆ j/ 2 + ϕ⋆m)| · exp( i · ωkj) p→∞ 

= (−1) rk +1 

π(4 r2 

> k

− 1) 

| {z }

:= ςrk

· exp( −2rkϕ⋆m · i). (C.17) A cosine derivation of (C.17) proceeds as follows: 

1

p

> p

X

> j=1

| cos( ωk⋆ j/ 2 + ϕ⋆m)| · cos( ωkj) p→∞ 

=

Z 10

| cos( πk ⋆x + ϕ⋆m)| · cos(2 rkπk ⋆x)d x

= 1

π

Z π

> 0

| cos( u)| · cos(2 rk · (u − ϕ⋆m))d u = cos(2 rkϕ⋆m)

π ·

Z π

> 0

| cos( u)| · cos(2 rku)d u

= cos(2 rkϕ⋆m)

π ·

Z π

> 2
> 0

cos((2 rk + 1) u)d u +

Z π

> 2
> 0

cos((2 rk − 1) u)d u

!

= 2( −1) rk +1 

π(4 r2 

> k

− 1) · cos(2 rkϕ⋆m),

where the third equality follows from trigonometric identities, evenness of sin(2 ru ), and periodicity. A similar calculation applies to the sine, and combining both real and imaginary parts yields (C.17) .Therefore, we have 

⟨∇ ξm ℓm, b 2k⟩ = 2 √2 · α⋆m/π · p3/2 · ςrk · cos(2 rkϕ⋆m),

57 ⟨∇ ξm ℓm, b 2k+1 ⟩ = −2√2 · α⋆m/π · p3/2 · ςrk · sin(2 rkϕ⋆m),

and thus ∆kξm /∆⋆ξm = |ςrk |/|ςrk⋆ | = Θ( r−2 

> k

). Moreover, it follows by simple calculation 

(P∥ 

> k⋆

∇ξm ℓm)[ j] = ⟨∇ ξm ℓm, b 2k⋆ ⟩ · b2k⋆ [j] + ⟨∇ ξm ℓm, b 2k⋆+1 ⟩ · b2k⋆+1 [j] ∝ cos(2 k⋆j + 2 ϕ⋆m),

for all j ∈ [p] such that we have P∥ 

> k⋆

∇ξm ℓm ∝ ξm.

Step 2: Gradient of θm. Following (C.14a), first notice that 

X

> x∈Zp

ξm[mp(x, j )] · 1(⟨ex + ej , θ m⟩)= β⋆m · X

> x∈Zp

cos( ωk⋆ (x + j) + ψ⋆m) · 1(cos( ωk⋆ x + ϕ⋆m) + cos( ωk⋆ j + ϕ⋆m) ≥ 0) 

> p→∞

= pβ ⋆m

π · | sin( ωk⋆ j + ϕ⋆m)| · cos( ωk⋆ j + ψ⋆m − ϕ⋆m),

where the last equality results from the following calculation under the asymptotic regime: 

1

p

X

> x∈Zp

cos( ωk⋆ (x + j) + ψ⋆m) · 1(cos( ωk⋆ x + ϕ⋆m) + cos( ωk⋆ j + ϕ⋆m) ≥ 0) 

> p→∞

=

Z 10

cos(2 πx + ωk⋆ j + ψ⋆m) · 1(cos(2 πx + ϕ⋆m) + cos( ωk⋆ j + ϕ⋆m))d x

= 12π

Z  

> ϕ⋆m≤u≤ϕ⋆m+2 π
> cos( u)≥− cos( ωk⋆j+ϕ⋆m)

cos( u + ωk⋆ j + ψ⋆m − ϕ⋆m)d u

= 12π · cos( ωk⋆ j + ψ⋆m − ϕ⋆m) ·

Z  

> 0≤u≤2π
> cos( u)≥− cos( ωk⋆j+ϕ⋆m)

cos( u)d u

= 1

π · sin(arccos( − cos( ωk⋆ j + ϕ⋆m))) 

| {z }

= | sin( ωk⋆ j + ϕ⋆m)|· cos( ωk⋆ j + ψ⋆m − ϕ⋆m).

By applying DFT over ∇θm ℓm in the asymptotic regime p → ∞ , we can show that 

1

p

> p

X

> j=1

| sin( ωk⋆ j + ϕ⋆m)| · cos( ωk⋆ j + ψ⋆m − ϕ⋆m) · exp( i · ωkj)

> p→∞

= − 1

π ·

 exp( {ψ∗ 

> m

− (rk + 2) ϕ∗

> m

} · i)

rk(rk + 2) + exp( −{ ψ∗ 

> m

+ ( rk − 2) ϕ∗

> m

} · i)

rk(rk − 2) 



· 1(rk is odd ),

(C.18) where rkk⋆ = k mod p. The above results follow the calculation below: 

1

p

> p

X

> j=1

| sin( ωk⋆ j + ϕ⋆m)| · cos( ωk⋆ j + ψ⋆m − ϕ⋆m) · cos( ωkj)

> p→∞

=

Z 10

| sin(2 πk ⋆x + ϕ⋆m)| · cos(2 πk ⋆x + ψ⋆m − ϕ⋆m) · cos(2 πr kk⋆x)d x

= 12π

Z 2π−ϕ⋆m

> −ϕ⋆m

| sin( u)| · cos( u + ψ⋆m − 2ϕ⋆m) · cos( rk(u − ϕ⋆m))d u

58 = 14π

Z 2π

> 0

| sin( u)| · cos(( rk + 1) u + ψ⋆m − (rk + 2) ϕ⋆m)d u

+ 14π

Z 2π

> 0

| sin( u)| · cos(( rk − 1) u − ψ⋆m − (rk − 2) ϕ⋆m)d u, (C.19) where for h1 = rk ± 1 and h2 = ψ⋆m − (rk + 2) ϕ⋆m/ − ψ⋆m − (rk − 2) ϕ⋆m, we can further show show 

Z 2π

> 0

| sin( u)| · cos( h1u + h2)d u = cos( h2) ·

Z 2π

> 0

| sin( u)| · cos( h1u)d u

= (1 + ( −1) h1 ) · cos( h2) ·

Z π

> 0

sin( u) cos( h1u)d u = 41 − h21

· cos( h2) · 1(h1 is even ). (C.20) By combining (C.19) and (C.20) , and performing a similar calculation for the sine component, we obtain the result in (C.18) This implies that for even rk, we have 

⟨∇ θm ℓm, b 2k⟩ = −√2β⋆m/π · p3/2 ·

n cos( ψ∗ 

> m

− (rk + 2) ϕ∗

> m

)

rk(rk + 2) + cos( ψ∗ 

> m

+ ( rk − 2) ϕ∗

> m

)

rk(rk − 2) 

o

,

⟨∇ θm ℓm, b 2k+1 ⟩ = −√2β⋆m/π · p3/2 ·

n sin( ψ∗ 

> m

− (rk + 2) ϕ∗

> m

)

rk(rk + 2) − sin( ψ∗ 

> m

+ ( rk − 2) ϕ∗

> m

)

rk(rk − 2) 

o

,

Hence, ∆k(θm)/∆⋆(θm) = Θ( r−2 

> k

) · 1(rk is even ) and for all j ∈ [p](P∥ 

> k⋆

∇θm ℓm)[ j] = ⟨∇ θm ℓm, b 2k⋆ ⟩ · b2k⋆ [j] + ⟨∇ θm ℓm, b 2k⋆+1 ⟩ · b2k⋆+1 [j] ∝ cos( wk⋆ j + ϕ⋆m),

which gives that P∥ 

> k⋆

∇θm ℓm ∝ θm and completes the proof. 

# D Comparison with Existing Results 

Our work is closely related to that of Tian (2024) and Wang and Wang (2025), who studied a two-layer network for learning group multiplication on an Abelian group, which is a generalization of the standard modular addition task. For theoretical convenience, they adopt a modified ℓ2-loss to mitigate noisy interactions induced by the constant frequency. Let P⊥ 

> 1

= I − 1 

> p

11 ⊤ denote the mean-zero projection, then the loss is defined as 

eℓ(ξ, θ ) = − X

> x∈Zp

X

> y∈Zp

P⊥

> 1



1/2p · f (x, y ; ξ, θ ) − e(x+y) mod p

 2

, (D.1) where the output of the network is normalized by 1/2p within loss calculation. Unlike (D.1) ,we show that minimizing a standard CE loss with a small initialization naturally decouples the dynamics of each frequency (see Theorem 5.2), with the constant frequency having a zero gradient throughout training and therefore remaining zero under zero-initialization (see Corollary 6.1). 

Notation Clarifications. We begin by explaining the notation used in Tian (2024). In their analysis, the (modified) complex Fourier coefficients of the weights are given by zqkm ∈ C, where the indices 

q ∈ { ξ, θ }, m ∈ [M ] and k ∈ [p − 1] ∪ { 0} correspond to the layer, neuron, and frequency, respectively. This complex representation is equivalent to the real-valued cosine-sine pairs used in our DFT definition in §5.1. Specifically, for all k ≤ (p − 1) /2, we can show that 

zθkm = αkm/√2 · exp( iϕ km), zξkm = βkm/√2 · exp( −iψ km).

59 By the conjugate symmetry of the DFT coefficients, our single real component at frequency k

determines the complex coefficients for both k and p − k. Therefore, for the higher frequencies 

(p + 1) /2 ≤ k ≤ p, the relationship is given by 

zθkm = ¯ zθ(p−k)m = αkm/√2 · exp( −iϕ km), zξkm = ¯ zξ(p−k)m = βkm/√2 · exp( iψ km),

which completes the one-to-one correspondence between our basis and the one used by Tian (2024). 

Loss Landscape within Fourier Domain. Tian (2024) expresses the loss eℓ from (D.1) in the Fourier domain using {zqkm }. In Theorem 1, they show that the loss eℓ decouples into per-frequency terms 

eℓ = p−1 · P  

> k̸=0

eℓk + ( p − 1) /p , where eℓk is a quadratic polynomial whose variables {ρk1k2k}k1,k 2∈[p−1] 

are third-order monomials of the Fourier coefficients. Formally, we have 

eℓk = poly  {ρk1k2k}k1,k 2∈[p−1] 

, where ρk1k2k =

> M

X

> m=1

zθk 1mzθk 2mzξkm . (D.2) 

Mean-Field Dynamics. Building on their analysis of the loss, Theorem 7 in Tian (2024) presents a heuristic result for the gradient dynamics. By considering a truncated loss polynomial from (D.2) , a symmetric Gaussian initialization, and the mean-field limit M → ∞ , they show that 

∂tρk1k2k(t) = 2 · ζk1k2k(t) · { 1(k1 = k2 = k) − ρk1k2k(t)}, (D.3) where ζk1k2k(t) is a term of constant order along the training. The solution to the ODE in (D.3) provides a more high-level theoretical basis for the emergence of the key structural properties we identified in our work. Consider the case k1 = k2 = k, we have 

ρkkk (t) = 

> M

X

> m=1

z2

> θkm

(t) · zξkm (t) ∝

> M

X

> m=1

αkm(t)2 · βkm(t) · exp( i{ψkm(t) − 2ϕkm(t)}) t→∞ 

−→ 1.

For this to hold, the imaginary part of ρkkk (t) should converge to 0:

ℑ(ρkkk (t)) ∝

> M

X

> m=1

αkm(t)2 · βkm(t) · sin( ψkm(t) − 2ϕkm(t)) t→∞ 

−→ 0.

This convergence is a direct consequence of the phase alignment dynamic (2 ϕkm(t)−ψkm(t)) mod 2 π →

0 as revealed in §5.5. Moreover, if we consider k1, k 2̸ = k, then we have 

ρk1k2k(t) ∝

> M

X

> m=1

αk1 

> m

(t) · αk2 

> m

(t) · βkm(t) · exp( i{ψkm(t) − ϕk1 

> m

(t) − ϕk2 

> m

(t)}) t→∞ 

−→ 0.

A sufficient condition for this is that the product of amplitudes αk1 

> m

(t) · αk2 

> m

(t) · βkm(t) goes to zero for all m ∈ [M ]. This corresponds precisely to the single-frequency sparsity we observed in §6.1. Beyond these, Tian (2024) also discussed data with a general algebraic structure and its relationship with properties of global optimizers. Recently, Wang and Wang (2025) formalized these mean-field dynamics by modeling the network’s parameters as a continuous distribution. This approach allows the training process to be rigorously described as a Wasserstein gradient flow on the measure space. 60