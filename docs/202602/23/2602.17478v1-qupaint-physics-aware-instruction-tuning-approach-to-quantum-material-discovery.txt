Title: QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery

URL Source: https://arxiv.org/pdf/2602.17478v1

Published Time: Fri, 20 Feb 2026 02:03:24 GMT

Number of Pages: 24

Markdown Content:
# QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery 

## Xuan-Bac Nguyen 1, Hoang-Quan Nguyen 1, Sankalp Pandey 1, Tim Faltermeier 2,Nicholas Borys 2, Hugh Churchill 3, Khoa Luu 11 CVIU Lab, University of Arkansas, USA 2 University of Utah, USA  

> 3

## Department of Physics, University of Arkansas, USA 

> 1

{xnguyen, hn016, sankalpp, khoaluu }@uark.edu, 

> 2

{timfaltermeier@montana.edu,nicholas.borys@utah.edu }, 3hchurch@uark.edu https://uark-cviu.github.io/projects/qupaint 

Figure 1. Overview of our proposed system. Synthia generates physics-based synthetic flakes, and QMat-Instruct provides multimodal, physics-aware supervision. Together, they enable the new Physics-Aware Instruction Tuning for Quantum Material Discovery ( QuPAINT )framework, to learn robust and interpretable representations for quantum material characterization. (Best view in colors) 

## Abstract 

Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the sub-tle layer-dependent contrast, limited labeled data, and sig-nificant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limita-tions from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum ma-terial flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the depen-dence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quan-tum materials, comprising multimodal, physics-informed question–answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offer-ing standardized protocols for fair and reproducible evalu-ation. 

## 1. Introduction 

Quantum materials, particularly atomically thin two-dimensional (2D) systems such as graphene, MoS 2, and WTe 2, have emerged as a new frontier for next-generation electronics, photonics, and quantum information technolo-gies. Their extraordinary physical properties, such as quantum confinement, spin–valley coupling, and layer-dependent band topology, arise from atomic-scale varia-tions that are invisible to conventional imaging and analy-sis pipelines [14]. Characterizing these materials precisely, especially determining the thickness, uniformity, and local defect structures of exfoliated flakes, is therefore a critical 1   

> arXiv:2602.17478v1 [cs.CV] 19 Feb 2026 Figure 2. Overview of the real data collection pipeline and challenges in quantum flake characterization. (a) Microscopy enables fast collection of thousands of flakes but provides no thickness information, while AFM offers accurate layer measurements but is slow and not scalable. (b) The required manual workflow is labor-intensive, involving repeated chip transfers and flake verification. (c) Real samples from multiple materials (MoS 2, h-BN, Graphene, WTe 2) show minimal visual differences between 1, 2, ‘ 3 layers, illustrating the difficulty of determining thickness from raw optical images. (Best view in colors)

step toward harnessing their quantum functionalities. Yet, this characterization process remains largely manual, sub-jective, and inconsistent across laboratories, limiting repro-ducibility and scalability. From a computer vision perspec-tive, quantum material characterization presents a uniquely challenging regime: the visual differences between mono-, bi-, and few-layer flakes can be subtler than illumination noise, and their optical contrast often entangles with back-ground interference patterns. 

Limitations of Object Detection Methods. Despite the impressive progress of foundational CNN architectures [13, 36] to recent transformer-based systems [2, 31, 35, 37, 39, 49], these models remain insufficient for quantum ma-terial characterization. Conventional object detectors are trained to localize and classify semantically distinct objects, where category boundaries are defined by strong texture, color, or shape cues. In contrast, 2D quantum materials ex-hibit sub-visual variations: two flakes may appear nearly identical to a detector but correspond to entirely different physical phases, e.g., monolayer vs. bilayer MoS 2 (Fig. 2 (c)). Their optical appearances are easily distorted by mi-croscope illumination, substrate thickness, or ambient hu-midity, breaking the visual assumptions of existing mod-els. Moreover, unlike natural images, where large anno-tated datasets enable domain generalization, quantum flake datasets are extremely scarce and exhibit high intra-class variability that makes end-to-end training of state-of-the-art architectures prone to overfitting. Finally, the task itself goes beyond mere detection: researchers require physically interpretable predictions, such as layer count or thickness gradient, quantities that standard vision models are not de-signed to infer. These challenges call for a new framework that couples physical priors with visual representations, en-abling models to reason jointly about material appearance and its underlying quantum properties. 

Limitations in Prior Work. While several recent works have demonstrated promising deep-learning pipelines for identifying exfoliated flakes under optical microscopes [24, 42, 43], their practical impact remains limited. Each study typically collects its own in-house dataset, focusing on spe-cific substrates, e.g., SiO 2/Si thickness, lighting conditions, or material types, and trains models that perform well only within that narrow domain. However, in real laboratory set-tings, imaging configurations vary substantially across insti-tutions: optical magnification, illumination spectra, camera sensors, and even environmental factors can alter color con-trast and scattering patterns. As a result, models overfit to their acquisition pipelines and fail to generalize to unseen microscopes or new materials of interest. Moreover, the community still lacks a standardized benchmark for quan-tum material vision, making it impossible to fairly compare methods or quantify their transferability in practice. With-out unified datasets or evaluation protocols, progress in this area remains fragmented and difficult to reproduce. This gap motivates the need for a physically grounded, domain-adaptive framework that can learn transferable representa-tions across labs, materials, and imaging modalities. to-ward reliable, real-world deployment of autonomous quan-tum material characterization systems. 

Contributions of this Work. In summary, this work has four main contributions. First, we introduce Synthia, a synthetic data generation framework that simulates real-istic quantum material flakes using physics-based optical modeling. Synthia produces diverse, high-quality sam-ples that help mitigate the limited availability of expert-labeled microscopy data. Second, we construct QMat-Instruct, the first large-scale dataset of instructions for quan-tum materials. It provides multimodal, physics-aware ques-tion–answer pairs that teach an MLLM to reason about flake appearance and thickness. Third, we propose the 2Physics-Aware Instruction Tuning (QuPAINT), a new mul-timodal framework that integrates a Physics-Informed At-tention module. It leverages optical priors and interference-aware cues to enable the model to learn more reliable and discriminative flake representations. Fig. 1 illustrates an overview of our proposed method. Finally, we establish QF-Bench, a large-scale benchmark for quantum material char-acterization, covering multiple materials, substrates, and imaging settings. This benchmark provides unified eval-uation protocols and allows fair comparison across future methods. 

## 2. Related Work 

Automated Two-Dimensional Material Characteriza-tion. Research on 2D materials has grown rapidly due to their importance in quantum technologies [6, 7, 9, 12, 15, 25, 27, 29, 30, 32, 40, 43, 44]. Early deep-learning approaches include UNet-based segmentation [11, 38] and Mask R-CNN pipelines for flake detection [13, 24]. Other work explores self-attention with soft-labeling [28] and false-negative mitigation [23]. Traditional machine-learning models such as Gaussian mixture models [42] have also been used but are sensitive to noise and perform poorly on real data. High-throughput microscopy systems aim to scale identification and enable statistical analysis [7]. Foun-dation models trained on 2D material data [43] and seg-mentation models like SAM [18, 35] provide strong priors for downstream tasks. Recent studies integrate detection with fabrication workflows for improving exfoliation, sam-ple handling, and labeling consistency [6, 15, 25, 29, 30]. 

Multimodal Large Language Models (MLLMs). 

MLLMs extend LLM reasoning to images and audio. Models such as CLIP [34] and ALIGN [16] learn joint embeddings, while instruction-tuned systems like BLIP-2 [19], LLaVA [21], and InstructBLIP [8] connect visual encoders to pretrained LLMs for open-ended reasoning. Follow-up work improves step-by-step reasoning [45], multi-agent collaboration [10], and task-specific alignment [41, 46]. Large-scale systems, e.g., GPT-4V [1], Qwen-VL [3], and InternVL [5] further expand capabilities to videos. Temporal modeling is another emerging direction. State Space Models aid long-sequence motion generation [48], while video understanding has improved through temporal reasoning of events [33]. Frame selection strategies also boost long-form video performance without retraining [22]. Despite these advances, vision-centric models still struggle to generalize across domains . Our work addresses this limitation by introducing a physics-informed multimodal framework that links visual cues with quantum material properties for more reliable characterization.              

> Table 1. Comparison of dataset synthesis techniques between MaskTerial [43] and our proposed method.
> Technique Aspect MaskTerial [43] Synthia (Ours)
> Flake shapes from multi-material sources ✓✓
> Physics-based optical simulation (TMM) ✓✓
> Personalized color calibration ✗✓
> Using CIE standard ✗✓
> Avoid overlapping with the existing flakes ✗✓

## 3. Challenges and Limitations of Prior Dataset 

3.1. Challenges In Data Collection 

The exfoliation of two-dimensional (2D) quantum ma-terials, such as graphene and transition-metal dichalco-genides (TMDs), typically relies on the mechanical cleav-age method using adhesive tape which is an approach rec-ognized by the 2010 Nobel Prize in Physics. In this process, bulk crystals are repeatedly peeled to obtain atomically thin flakes, which are then transferred onto SiO 2/Si substrates. Although this technique is simple and inexpensive, it is in-herently stochastic; the resulting flakes exhibit highly vari-able shapes, sizes, and thicknesses. Collecting large-scale data of exfoliated flakes is, in prin-ciple, straightforward since thousands of random flakes can be produced in a single exfoliation attempt. However, gen-erating datasets that contain flakes with specific layer num-bers or thicknesses, e.g., monolayer, bilayer, or few-layer regions, is extremely challenging. The randomness of the exfoliation process prevents direct control over the number of layers, meaning that flakes of interest must be “hunted” from among a vast background of irrelevant samples. This hunting process is extraordinarily time-consuming. The optical microscope images alone cannot determine layer thickness precisely because color contrast varies non-linearly with illumination and substrate conditions. Ac-curate thickness measurement requires atomic force mi-croscopy (AFM), which introduces substantial labor over-head: each candidate flake must be physically relocated to the AFM, measured at the nanometer scale, and then carefully remapped to its original microscopy coordinates. Such cross-modality alignment—between optical and AFM images—demands extensive manual effort and often limits the throughput of data collection. As a result, constructing quantum flake datasets suitable for AI training is both labor-intensive and data-scarce. The combination of random flake generation, uncertain thickness labeling, and slow AFM-based validation severely constrains the scalability of ex-isting datasets. Figure 2 demonstrates the overview of the data collection process for quantum flake identification. 

3.2. Limitations of Prior Dataset 

Although MaskTerial [43] pioneered the use of physics-based rendering for generating large-scale 2D material datasets, its synthesis process still suffers from several lim-itations that reduce the realism and generalization of the 3generated data. First, the optical simulation is applied un-der idealized conditions without personalized color calibra-tion, resulting in noticeable mismatches between the simu-lated and actual microscope images. Second, the previous method does not support the CIE color standard, which fur-ther limits the transferability of the dataset to diverse opti-cal setups. These simplifications collectively widen the do-main gap between synthetic and real data, constraining the practical usability of the generated dataset for robust model training. Lastly, prior work generates new flakes indepen-dently of the existing ones, which can result in unrealistic overlaps or physically inconsistent stacking between flakes in the same image. Tab. 1 demonstrates the pros and cons of the previous method compared to ours. 

## 4. Synthetic Materials Framework (Synthia) 

4.1. Multilayer Optical Model 

To synthesize realistic images of 2D material flakes on typ-ical substrates, we employ multilayer optical modeling via the Transfer Matrix Method (TMM) [4]. This physics-based model simulates thin-film interference effects, en-abling wavelength-dependent prediction of reflectance and optical contrast as a function of flake thickness. 

Optical model. Consider a multilayer stack with L thin lay-ers, each indexed by l = 1 , 2, . . . , L . The incident and sub-strate media are denoted by layers 0 and L+1 , respectively, both assumed semi-infinite. Each layer l is characterized by its complex refractive index nl(λ) + ik l(λ) and phys-ical thickness dl. When light of wavelength λ enters the stack at normal incidence, it undergoes multiple reflections and transmissions across layer interfaces. At each interface between layers l and l+1 , the reflection and transmission coefficients are determined by the Fresnel equations as in Eq. (1):             

> rl,l +1 =nl−nl+1
> nl+nl+1
> ,tl,l +1 =2nl
> nl+nl+1
> .(1)

Light propagation through each layer introduces a phase de-lay δl = 2πn ldl 

> λ

, which can be expressed by a propagation matrix Pl. The total electric field across the multilayer stack can then be written using the transfer-matrix formalism as in Eq. (2):     

> vu
> !
> =M0,1
> LY
> l=1
> PlMl,l +1
> !u0
> 0
> !
> ,(2)

where u0 = 1 is the incident field amplitude, v is the re-flected amplitude, and u is the transmitted amplitude into the substrate. The reflectance is formed as R(λ) = |v/u 0|2.

Reflectance computation for synthetic flakes. In our syn-thetic pipeline, each flake is modeled as a multilayer stack consisting of: (1) air (incident medium), (2) the 2D mate-rial flake of thickness dflake corresponding to number of lay-ers, (3) an oxide layer (e.g., SiO 2) of fixed thickness dSiO2 ,and a silicon substrate (semi-infinite). We sample wave-lengths λ ∈ [400 , 700] nm at D discrete intervals and com-pute R(λ) using Eq. (2). 

Color space conversion. To convert the simulated re-flectance spectra into RGB values, we integrate the re-flectance over the visible spectrum using the CIE 1931 color-matching functions S(λ) and the standard illuminant spectrum I(λ) as in Eq. (3).          

> x=
> Zλmax
> λmin
> S(λ)I(λ)R(λ)dλ ≈
> DX
> i=1
> S(λi)I(λi)R(λi),(3)

where S(λ) ∈ R3×1 produces the (R, G, B ) components. In matrix form, this can be expressed as x = S⊤(I ◦ R),where ◦ denotes the element-wise product. Fig. 3 (a) illus-trates what the optical model looks like. 

Implementation. We implement the above optical model using the Python package tmm [4], which provides effi-cient simulation of multilayer thin films. For each ma-terial (e.g., graphene, MoS 2, WSe 2, hBN), we define re-fractive index functions n(λ) and k(λ), and compute their reflectance spectra across thickness variations. The color-accurate RGB values are rendered as synthetic microscopy patches and combined with real microscope backgrounds. 

4.2. Details of Synthia 

In this section, we introduce Synthia , a new synthetic 2D material generation framework that can produce a large number of realistic flakes under diverse conditions, includ-ing different material types, substrate kinds, and layer thick-nesses. The core of Synthia is built upon the optical model described in Sec. 4.1, which simulates thin-film interfer-ence to reproduce the color appearance of 2D materials. To further enhance realism and reduce the domain gap be-tween synthetic and real data, we incorporate two addi-tional components: the White-Balance-Aware module and the Substrate-Aware module. These modules account for il-lumination variability and substrate-dependent color shifts, respectively. Specifically, Synthia leverages a Physics-Informed Attention (PIA) module, denoted as FPIA , to iden-tify physically meaningful substrate regions for flake place-ment. The details of the PIA module are presented later in Sec. 5.2. The complete pipeline of Synthia is detailed in Algorithm 1. Fig. 3 (b) compares the realism of our synthe-sized flakes with the other method. 

4.3. QMat-Instruct Dataset 

Lack of a 2D dataset to train MLLM. While previous studies in 2D material characterization mainly focused on image-based detection or segmentation, there is currently no existing dataset designed for training MLLMs to under-stand and reason about quantum flakes. 

Proposed QMat-Instruct Dataset. To support instruction tuning for quantum materials, we build QMat-Instruct , the 4first large-scale multimodal instruction dataset in this do-main. Our data is enabled by realistic synthetic images from Synthia, allowing us to generate diverse and physics-consistent supervision at scale. Each sample is formatted as a structured question-answer pair that embeds physics-aware descriptions of 2D flakes, e.g ., their contrast, color shift, and edge transparency. These cues come directly from thin-film interference and are essential for distinguish-ing mono-, bi-, and multi-layer flakes. Unlike prior meth-ods that rely on hand-crafted rules or specialized models, QMat-Instruct teaches the model to learn the physical re-lationships through both language and vision. The instruc-tions cover common tasks in quantum material characteri-zation, including counting, localization, and binary verifica-tion. Examples include: “How many monolayer flakes are in the image?”, “Locate the monolayer flakes,” and “Does this sample contain a monolayer flake?”. Overall, QMat-Instruct provides the first physics-informed instructional corpus for multimodal models in quantum materials , of-fering rich supervision that helps the model understand and reason about flake appearance across materials and imaging setups. 

Algorithm 1 Synthetic 2D Materials (Synthia)                                                                                                                   

> Require: Reference microscopy image Iref , Optical model T, Color pro-jector Φ, Material type s, Physics-Informed Attention function FPIA
> Ensure: Synthetic microscopy image Iout
> // — White-Balance-Aware Module —
> 1: cref sub ←MedianColor (Iref )
> 2: Rsub (λ)← T ({air ,SiO 2,Si }, t sub , λ )
> 3: c0
> sub ←Φ( Rsub (λ), I (λ))
> 4: g←cref sub ⊘c0
> sub ▷Compute personalized white-balance gain based on reference substrate color
> // — Substrate-Aware Module —
> 5: A← F PIA (Iref )
> 6: ˜A←Normalize (A)
> 7: Msub ←1{˜A<Perc 90 ( ˜A)}▷Detect clean substrate areas for valid flake placement
> // — Synthetic Flake Generation —
> 8: for i= 1 to Nflakes do
> 9: Mi←RandomFlakeMask() 10: ti←SampleThickness (s)
> 11: Find valid coordinates (ui, v i)such that Msub [vi:vi+hi, u i:
> ui+wi]⊙Mi≡0▷Avoid overlapping flakes 12: Ri(λ)← T ({air , s, SiO 2,Si }, t i, λ )
> 13: ci←Φ( Ri(λ), I (λ)) ▷Compute flake color via optical model 14: ci←g⊙ci▷Apply White-Balance-Aware correction 15: Iref [vi:vi+hi, u i:ui+wi]←ciMi+Iref [vi:vi+hi, u i:
> ui+wi](1 −Mi)
> 16: end for
> 17: Iout ←Iref
> 18: Return: Iout

## 5. The Proposed Method 

5.1. Physics-Aware Instruction Tuning (QuPAINT) 

Our proposed QuPAINT is illustrated in Fig. 4. Let I ∈

RH×W ×C denote a microscopy image of a 2D material, where H, W , and C are the image height, width, and num-ber of channels, respectively. Let T = {T1, . . . , T L} denote a physics-informed instruction or question sequence with L

tokens. The objective of our model is to enable a MLLM to reason about the optical properties of 2D flakes and gener-ate physically grounded textual answers. We adopt a multi-modal encoder–decoder architecture composed of (1) a Vi-sion Transformer (ViT) guided by a Physics-Informed At-tention (PIA) prior, (2) a text encoder that embeds physics-aware instructions, and (3) an MLLM decoder for multi-modal fusion and reasoning. 

Vision Encoder with Physics-Informed Attention. The vision encoder Enc v extracts k patch embeddings from the input image as in Eq. (4): 

zv = Enc v (I), zv = [ v1, . . . , vk ], vi ∈ Rd. (4) The PIA module, introduced in Sec. 5.2, provides patch-level physics-based contrast scores α = [ α1, . . . , α k] ∈

[0 , 1] k derived from optical cues such as thin-film interfer-ence and color contrast in CIELAB space. These scores re-flect physical relevance but may be affected by illumination variability, white-balance bias, or sensor noise. 

Motivation. While PIA encodes meaningful optical pri-ors, it is a purely physics-driven process and cannot adapt to dataset-specific factors such as lighting, substrate color, or camera spectral response. To enhance robustness while preserving physical interpretability, we introduce a learn-able linear correction layer that adjusts the scale and bias of PIA scores. This simple affine transformation enables the model to calibrate physics-based attention end-to-end through the language modeling loss, without requiring ad-ditional supervision. Formally, the corrected patch attention weights are com-puted as in Eq. (5): 

βi = σ(wα i + b), βi ∈ [0 , 1] , w, b ∈ R, (5) where σ(·) is the logistic sigmoid function. The corrected weights βi are then used to modulate the visual tokens as in Eq. (6): 

˜vi = βi · vi, ˜zv = [ ˜ v1, . . . , ˜vk ]. (6) This design retains the physically interpretable structure of PIA while allowing a small number of learnable parameters to adapt the attention distribution for better generalization. 

Text Encoder. The instruction T is tokenized and embed-ded using a pretrained text encoder as in Eq. (7): 

zt = Enc t(T), zt = [ t1, . . . , tL ], tj ∈ Rd. (7) 5Figure 3. (a) Optical thin-film model used in Synthia , capturing layer-dependent interference between the material (e.g., MoS 2), SiO 2,and Si substrate under microscope illumination. (b) Comparison of synthetic flake quality, showing that Synthia produces more realistic color contrast, edge appearance, and flake visibility across materials and thicknesses than previous approaches. (Best view in colors) 

These instructions incorporate physics-informed descrip-tions (e.g., “mono-layer flakes appear faint and semi-transparent”), which provide explicit physical guidance dur-ing multimodal training. 

Multimodal Fusion and Decoding. The physics-calibrated visual tokens ˜zv and textual tokens zt are concatenated and input into an MLLM decoder LLM , which performs cross-modal attention and autoregressive text generation as in Eq. (8): 

Y = LLM (˜ zv , z t), Y = [ y1, . . . , y M ], (8) where Y is the generated output sequence and M is the number of tokens. 

Training Objective. The model is trained using a standard autoregressive loss on the physics-informed Q&A corpus as   

> Figure 4. Overview of the proposed QuPAINTframework. APhysics-Informed Attention (PIA) module extracts optical cues from the microscopy image, which are fused with ViT visual em-beddings and text tokens in a multimodal large-language model for quantum material understanding.

in Eq. (9):            

> LLM =−1
> M
> MX
> j=1
> log Pθ(yj|y<j ,˜zv, z t).(9)

No explicit supervision is applied to PIA; the affine cor-rection parameters (w, b ) are optimized implicitly through backpropagation from LLM , enabling end-to-end calibration of the physics prior. During inference, given (I, T), the model outputs Y as physically grounded reasoning about flake count, thickness, and location. 

5.2. Physics-Informed Attention (PIA) 

Motivations. The optical contrast of two-dimensional (2D) materials observed under an optical microscope origi-nates from thin-film interference between the flake and the SiO 2/Si substrate. This phenomenon leads to characteristic color variations that correlate with the layer thickness and refractive index of the material. However, modeling this behavior using a full optical transfer-matrix simulation re-quires precise knowledge of wavelength-dependent refrac-tive indices, illumination spectra, and camera responses, which are often unavailable or vary across laboratories. To overcome these limitations, we adopt the CIE LAB color space as a perceptually uniform and device-independent representation that encodes color contrast similarly to hu-man visual perception. In the LAB space, the Euclidean dis-tance ∆E between a flake region and its surrounding sub-strate provides a robust and illumination-invariant measure of perceptual contrast, which approximates the reflectance difference caused by thin-film interference. Unlike RGB in-tensities that depend on hardware calibration and white bal-ance, LAB values separate luminance ( L∗) and chromatic components ( a∗, b∗), enabling more stable discrimination of flakes with subtle optical contrast variations. Therefore, the LAB representation offers a simple yet physically mean-ingful proxy to capture the qualitative optical characteristics of 2D materials without requiring full optical simulation. 

PIA Approximation of Optical Contrast. We pro-pose a simple, physics-aware proxy to approximate the 6Algorithm 2 Physics-Informed Attention Module                                                                             

> Require: Image I∈RH×W×3, image size (H, W ), patch size (h, w )
> Ensure: Perceptual attention map ALAB ∈RH×W
> 1: function PIA( I,(H, W ),(h, w ))2: Resize Ito (H, W )
> 3: Ibg ←median x∈I(I(x))
> 4: ILAB bg ←Φ( Ibg )
> 5: Initialize attention list A= [ ]
> 6: for each patch Pkin Iof size (h, w )do
> 7: bIk←median x∈P k(I(x))
> 8: bILAB
> k←Φ( bIk)
> 9: ∆Ek=bILAB
> k−ILAB bg 2
> 10: Append ∆Ekto A
> 11: end for
> 12: Reshape Ainto ALAB of size (H, W )
> 13: Normalize ALAB ←(ALAB −min) /(max −min)
> 14: return ALAB
> 15: end function

optical thin-film contrast of two-dimensional (2D) ma-terials using CIELAB distances. Let a pixel (or im-age patch) at position x have spectral reflectance R(x, λ )

and the bare substrate (background) have reflectance 

Rbg (λ). Under illumination spectrum E(λ) and cam-era sensitivities {Sc(λ)}c∈{ R,G,B }, the recorded RGB in-tensity is Ic(x) = R R(x, λ ) E(λ) Sc(λ) dλ and Ibg  

> c

=R Rbg (λ) E(λ) Sc(λ) dλ , where I(x) = ( IR, I G, I B ) and 

Ibg = ( Ibg  

> R

, I bg  

> G

, I bg  

> B

). We transform the RGB values into CIELAB color space through the nonlinear mapping Φ : 

R3 → R3, i.e., (L∗, a ∗, b ∗) = Φ( I). The perceptual color contrast (PIA score) at pixel x is defined as in Eq. (10)             

> ∆E(x) = Φ( I(x)) −Φ( Ibg )2
> =
> q
> (L∗(x)−L∗
> bg )2+ ( a∗(x)−a∗
> bg )2+ ( b∗(x)−b∗
> bg )2.
> (10)

Proof of Eq. (10) is in the appendix. In practice, the back-ground color Ibg is estimated as the median pixel value of the image (or a region of interest), and ∆E(x) is computed over non-overlapping patches to form an attention map. 

Implementation. Given an image divided into patches 

{P k}, we compute Ibg = median x∈image I(x), bIk =median x∈P k I(x) and define the patch-wise PIA scores as PIA-score [k] = Φ( bIk) − Φ( Ibg ) 2. (11) This produces a stable and illumination-robust attention map that highlights 2D flakes through their perceptual color contrast that is a direct image-space surrogate for the thin-film optical behavior predicted by physical reflectance mod-els. Algorithm 2 illustrates the PIA module, and Fig. 5 shows an example of the attention map. 

## 6. Experiments 

6.1. Implementation Details and Benchmarking 

Implementation Details. Our framework adopts the im-plementation of InternViT. Input microscopy images are re-sized to 448 ×448 and divided into non-overlapping 14 ×14 

patches. The text encoder is initialized from Qwen3 [47]. We use AdamW optimizer with a learning rate of 2 × 10 −5,weight decay 1 × 10 −4, and batch size 4. The model is trained for 2 epochs on 16 A100 GPUs. Data augmentation includes random cropping, horizontal flipping, and color jit-ter in CIELAB space to enhance robustness against illumi-nation and white-balance variations. During inference, the model generates answers in a temperature-controlled sam-pling manner ( T =0 .2), enabling physically consistent rea-soning on 2D flake characterization. 

Training Dataset. We train our models on a hybrid dataset combining real microscopy images with physics-based syn-thetic samples. Real flakes are collected under varied illu-mination, magnification, and 180 nm SiO 2 substrates, fol-lowing setups similar to prior work [24]. Each image con-tains about 30 annotated boxes. To compensate for the scarcity of thin-layer flakes, we synthesize data using thin-film optical simulation and color rendering that matches real microscope appearance. Our data includes both general scenes and focused regions with single-layer flakes across BN, Graphene, MoS 2, MoSe 2, MoWSe 2, WS 2, WSe 2, and WTe 2. Each image is paired with several physics-informed instruction–answer pairs describing optical cues and layer contrast, enabling the MLLM to align visual features with physics-grounded reasoning. In total, we generate 50,000 samples per material which yields 400,000 samples in total. 

Benchmark Dataset. Prior methods are typically evalu-ated on private datasets restricted to a single microscope, material, or imaging setup, which limits generalization. To provide a fair and comprehensive benchmark, we introduce 

QF-Bench , constructed from public datasets and our in-house collection. All samples are manually curated with bounding boxes and layer labels spanning mono-layer, few-layer (2-5 layers), and thick (5+ layers) flakes. The bench-mark covers eight common 2D materials and captures di-verse substrates and imaging conditions. Tab. 2 summarizes its statistics, including 280K annotated flakes. QF-Bench offers the first unified platform for evaluating flake charac-terization models under realistic variability. Input Image Attention Map    

> Input Image Attention Map
> Input Image Attention Map
> Input Image Attention Map

Figure 5. Physics-Informed Attention Map. Best view in colors .

7Table 2. Breakdown of annotated flake counts for each material by layer (Mono, Few, Thick).                                         

> Material Mono Few Thick Total
> BN 13 111 10,100 10,224 Graphene 1,856 2,081 4,270 8,207 MoS 2246 536 108,352 109,134 MoSe 224 32 1,327 1,383 MoWSe 2935 293 337 WS 243 52,144 2,192 WSe 2207 35 6,195 6,437 WTe 2148 582 141,882 142,612
> Total 2,546 3,417 274,563 280,526

6.2. Experimental Results 

General Flake Detection. As in Tab. 4, across all detec-tors, our method shows a consistent improvement in general flake detection. Traditional baselines, e.g ., MaskRCNN and ViTDet, achieve modest performance, with AP values rang-ing from 17–20%. YOLOv11 models provide stronger re-sults, improving AP to around 25–30%, but still struggle to capture fine-grained flake boundaries, especially at higher IoU thresholds. Similarly, Co–DETR and RT–DETRv2 per-form around 22–28%. A flake detection method, Mask-Terial [43], results in AP around 24%. Another physics– based flake detection approach [26] provides improved AP around 30%. In contrast, our models achieve substantial gains across all metrics. QuPAINT-1B already reaches 36.9 AP, outperforming YOLOv8-x by a large margin. As the model scale increases from 1B to 8B, the performance con-tinues to improve steadily, reaching 45.6 AP and 60.5 AP 50 .These results demonstrate that our multimodal architecture is more effective at modeling complex visual cues and ma-terial textures compared to conventional 2D detectors. The improvement at AP 75 is especially strong, showing that our method produces more accurate and tighter localization, which is critical for identifying thin flakes. 

Mono Flake Detection. As in Tab. 4, mono-layer flake detection presents a more challenging setting due to their extremely thin structures and low contrast. All baseline detectors show a significant performance drop when mov-ing from general flake detection to mono-layer detection. For example, YOLOv8-x drops from 28.3 AP to only 19.0 AP, confirming that single-layer flakes are difficult to lo-calize using standard detection. MaskRCNN and ViTDet perform even worse, falling below 12 AP. Meanwhile, our method maintains strong performance and shows a much smaller performance drop. QuPAINT-1B achieves 28.0 AP, already higher than all baseline models, and scaling up to QuPAINT-8B further boosts accuracy to 37.3 AP and 52.8 AP 50 . This shows that our model captures the subtle optical variations and boundary cues specific to mono-layer flakes. The improvements at AP 75 confirm sharper spatial reason-ing, indicating that our approach is better at distinguishing true mono flakes from visually similar multi-layer regions.         

> Table 3. Instruction Grounding Performance.
> Method Acc@50 Acc@75 InternVL-2B 43.3 30.8 Qwen3VL-2B 45.8 32.1 QuPAINT-2B 51.4 39.5

Instruction Grounding Evaluation. We produce an in-struction grounding evaluation. Specifically, we follow the standard protocols described in RefCOCO [17]. As described in Table 3, we compare QuPAINT-2B against the fine-tuned versions of InternVL-2B and Qwen3VL-2B. Overall, given a comparable number of parameters, our ap-proach outperforms the others on Acc@50 and Acc@75 due to the PIA module. 

6.3. Ablation Studies 

To understand the contributions of our approach, we con-duct ablation studies focusing on two key factors: Physics-Aware Description (Sec. 4.3) and Physical Interaction At-tention (Sec. 5.2) on the mono flake detection task. 

Effect of PAD. As in Tab. 5, the PAD module provides a physics-based prior that helps the model capture optical and material properties of thin flakes. Without PAD, the model obtains only 28.1 AP. Adding PAD alone significantly im-proves performance by +3 .6 AP. This improvement indi-cates that PAD helps the network learn more reliable repre-sentations and reduces confusion between mono-layer and multi-layer regions. PAD also improves localization qual-ity, as shown by the higher AP 50 and AP 75 scores. 

Effect of PIA. As in Tab. 5, the PIA module focuses the model on critical interaction patterns between the substrate, flake thickness, and lighting conditions. When used alone, PIA increases AP from 28.1 to 30.6. This gain demonstrates that PIA allows the network to attend to subtle cues that standard detectors tend to ignore. PIA strengthens mid- and high-IoU accuracy, leading to better spatial precision. 

Combined Effect of PAD and PIA. The full model, which integrates both PAD and PIA, achieves the best perfor-mance with 34.1 AP, 50.2 AP 50 , and 38.0 AP 75 (Tab. 5). These represent consistent improvements across all metrics. The combined gains show that PAD and PIA are comple-mentary. PAD improves the feature representation from a physics standpoint, while PIA helps the model attend to the correct visual cues. Together, they enable more accurate and stable mono flake detection, especially in cases with low contrast and fine-scale boundaries. 

## 7. Conclusion and Limitations 

Conclusions. Our paper introduced a complete pipeline for quantum material characterization that combines physics-based data generation, instruction tuning, and multi-modal reasoning. First, we proposed Synthia, a physics-grounded synthetic engine that simulates realistic optical responses of 2D materials. Second, we developed QMat-Instruct, a large-scale instruction dataset designed for mul-8Table 4. Performance Comparison on Flake Detection Tasks.                                                                                                                                  

> Detector General Flake Detection Mono Flake Detection #Param FLOPs AP AP 50 AP 75 AP AP 50 AP 75
> MaskRCNN-R50 [13] 18.7 36.5 17.8 9.8 28.9 9.0 44M 134.4B MaskRCNN-R101 [13] 20.3 38.1 18.9 11.1 30.6 10.3 63M 334.8B ViTDet (base) [20] 17.4 35.2 16.5 8.7 27.6 7.8 86M 1.3T ViTDet (large) [20] 18.9 37.0 17.3 9.9 29.8 8.5 300M 4.1T YOLOv11-m 24.8 41.9 21.1 17.0 27.2 16.4 20M 68.0B YOLOv11-l 26.9 43.0 23.5 18.9 29.8 17.3 25M 86.9B YOLOv11-x 29.6 44.5 24.6 19.8 31.3 18.7 57M 194.9B Co-DETR 22.7 35.9 23.3 12.2 25.3 11.4 304M 119.4B RT-DETRv2 27.5 37.2 27.0 17.5 37.8 14.7 76M 259.0B MaskTerial 23.8 41.0 25.4 16.8 35.5 17.2 45M 150.7B
> φ-Adapt 30.3 49.1 27.4 24.1 38.2 23.4 91M 368.3B QuPAINT-1B 36.9 50.7 38.6 28.0 42.9 29.5 1.1B 1.5T QuPAINT-2B 38.7 53.6 41.2 30.2 45.5 32.0 2.3B 3.4T QuPAINT-4B 42.4 58.2 46.8 34.1 50.2 38.0 4.7B 7.9T
> QuPAINT-8B 45.6 60.5 47.9 37.3 52.8 39.9 8.5B 14.5T

Table 5. Ablation study on Physics-Informed Reasoning (PAD) and PIA modules for Mono Flake Detection. 

PAD PIA AP AP 50 AP 75 

✗ ✗ 28.1 42.7 30.3 

✗ ✓ 30.6 44.8 32.6 

✓ ✗ 31.7 46.5 33.4 

✓ ✓ 34.1 50.2 38.0 

timodal models in quantum materials. Third, we presented QuPAINT, a Physics-Aware Instruction Tuning framework that integrates a Physics-Informed Attention to improve flake representation and thickness recognition. Finally, we establish a new benchmark covering multiple materials, substrates, and imaging settings for fair and reproducible evaluation. 

Limitations. Our approach still has a few limitations. First, while Synthia produces realistic physics-based im-ages, it cannot fully match all variations seen in real mi-croscopes or all types of flake defects. As a result, some domain gaps may remain when applying the model to new labs or imaging conditions. Second, QuPAINT is limited by the maximum token length of current multimodal mod-els. When an image contains too many flakes, the number of visual tokens can exceed this limit, making it difficult for QuPAINT to handle extremely dense scenes. 

## 8. Acknowledgment 

This work is partly supported by MonArk NSF Quan-tum Foundry, supported by the National Science Founda-tion QAMASE- i program under NSF award No. DMR-1906383. It acknowledges the Arkansas High-Performance Computing Center for providing GPUs. 

## References 

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-mad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 ,2023. 3 [2] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba, Komeili, Matthew Muckley, Am-mar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, and Nico-las Ballas. V-jepa 2: Self-supervised video models enable understanding, prediction and planning, 2025. 2 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023. 3 [4] Steven J. Byrnes. Multilayer optical calculations, 2020. 4 [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation mod-els and aligning for generic visual-linguistic tasks. In Pro-ceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 24185–24198, 2024. 3 [6] Elijah DS Courtney, Mihir Pendharkar, Nathan J Bittner, Aaron L Sharpe, and David Goldhaber-Gordon. Auto-mated tabletop exfoliation and identification of monolayer graphene flakes. Review of Scientific Instruments , 96(5), 2025. 3 [7] Juri G Crimmann, Moritz N Junker, Yannik M Glauser, Nolan Lassaline, Gabriel Nagamine, and David J Norris. High-throughput optical identification and statistical analysis of atomically thin semiconductors. Advanced Optical Mate-rials , 13(16):2500150, 2025. 3 [8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Sys-tems , 2023. 3 [9] Aditya Dendukuri, Blake Keeling, Arash Fereidouni, Joshua Burbridge, Khoa Luu, and Hugh Churchill. Defining quan-tum neural networks via quantum time evolution, 2020. 3 [10] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Ex-ploring long-chain visual reasoning with multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 9062–9072, 2025. 3 [11] Bingnan Han, Yuxuan Lin, Yafang Yang, Nannan Mao, Wenyue Li, Haozhe Wang, Kenji Yasuda, Xirui Wang, Valla Fatemi, Lin Zhou, et al. Deep-learning-enabled fast optical identification and characterization of 2d materials. Advanced Materials , 32(29):2000953, 2020. 3 [12] Yoshiaki Hattori, Takashi Taniguchi, Kenji Watanabe, and Masatoshi Kitamura. Identification of exfoliated monolayer hexagonal boron nitride films with a digital color camera under white light illumination. Nanotechnology , 35(37): 375704, 2024. 3 [13] Kaiming He, Georgia Gkioxari, Piotr Doll´ ar, and Ross Gir-shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision , pages 2961–2969, 2017. 2, 3, 9 [14] Md. Aminul Islam, Safiullah Khan, Juhi Jannat Mim, S M Maksudur Rahman, Md. Ahadul Islam Patwary, 

9Md. Safiul Islam, and Nayem Hossain. Recent advances of 2d materials in semiconductor application: A review. Ad-vanced Sensor and Energy Materials , 4(4):100161, 2025. 1 [15] Lina J¨ ackering, Konstantin G Wirth, Lukas Conrads, Jonas B Profe, Alexander Rothstein, Hristiyana Kyoseva, Kenji Watanabe, Takashi Taniguchi, Dante M Kennes, Christoph Stampfer, et al. Super-resolution imaging of nanoscale in-homogeneities in hbn-covered and encapsulated few-layer graphene. Advanced Science , 12(14):2409039, 2025. 3 [16] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representa-tion learning with noisy text supervision. In International conference on machine learning , pages 4904–4916. PMLR, 2021. 3 [17] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in pho-tographs of natural scenes. In Proceedings of the 2014 Con-ference on Empirical Methods in Natural Language Process-ing (EMNLP) , pages 787–798, Doha, Qatar, 2014. Associa-tion for Computational Linguistics. 8 [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. In Proceedings of the IEEE/CVF international confer-ence on computer vision , pages 4015–4026, 2023. 3 [19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In In-ternational conference on machine learning , pages 19730– 19742. PMLR, 2023. 3 [20] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object de-tection, 2022. 9 [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems , 36:34892–34916, 2023. 3 [22] Shuming Liu, Chen Zhao, Tianqi Xu, and Bernard Ghanem. Bolt: Boost large vision-language model without training for long-form video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 3318–3327, 2025. 3 [23] Khoa Luu, Xuan Bac NGUYEN, and Hugh Churchill. Auto-matically detecting false negative objects in 2d material de-tection data sets, 2024. US Patent App. 18/443,058. 3 [24] Satoru Masubuchi, Eisuke Watanabe, Yuta Seo, Shota Okazaki, Takao Sasagawa, Kenji Watanabe, Takashi Taniguchi, and Tomoki Machida. Deep-learning-based im-age segmentation integrated with optical microscopy for au-tomatically searching for two-dimensional materials. npj 2D Materials and Applications , 4(1):3, 2020. 2, 3, 7 [25] James McKenzie, Nileema Sharma, and Xiaolong Liu. Fab-rication of pristine 2d heterostructures for scanning probe microscopy. APL Materials , 12(7):070602, 2024. 3 [26] Hoang-Quan Nguyen, Xuan Bac Nguyen, Sankalp Pandey, Tim Faltermeier, Nicholas Borys, Hugh Churchill, and Khoa Luu. φ-adapt: A physics-informed adaptation learning ap-proach to 2d quantum material discovery, 2025. 8 [27] Xuan Bac Nguyen, Hugh Churchill, Khoa Luu, and Samee U Khan. Quantum vision clustering. arXiv preprint arXiv:2309.09907 , 2023. 3 [28] Xuan Bac Nguyen, Apoorva Bisht, Benjamin Thomp-son, Hugh Churchill, Khoa Luu, and Samee U Khan. Two-dimensional quantum material identification via self-attention and soft-labeling in deep learning. IEEE Access ,12:139683–139691, 2024. 3 [29] Taoufiq Ouaj, Leonard Kramme, Marvin Metzelaars, Ji-ahan Li, Kenji Watanabe, Takashi Taniguchi, James H Edgar, Bernd Beschoten, Paul K¨ ogerler, and Christoph Stampfer. Chemically detaching hbn crystals grown at atmo-spheric pressure and high temperature for high-performance graphene devices. Nanotechnology , 34(47):475703, 2023. 3 [30] Taoufiq Ouaj, Christophe Arnold, Jon Azpeitia, Sunaja Baltic, Julien Barjon, Jos´ e Cascales, Huanyao Cun, David Esteban, Mar Garcia-Hernandez, Vincent Garnier, Subodh K Gautam, Thomas Greber, Said Said Hassani, Adrian Hemmi, Ignacio Jim´ enez, Catherine Journet, Paul K¨ ogerler, An-nick Loiseau, Camille Maestre, Marvin Metzelaars, Philipp Schmidt, Christoph Stampfer, Ingrid Stenger, Philippe Steyer, Takashi Taniguchi, B´ erang` ere Toury, Kenji Watan-abe, and Bernd Beschoten. Benchmarking the integra-tion of hexagonal boron nitride crystals and thin films into graphene-based van der waals heterostructures. 2D Materi-als , 12(1):015017, 2024. 3 [31] Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, and Feng Wu. D-fine: Redefine regression task in detrs as fine-grained distribution refinement, 2024. 2 [32] Andr´ as P´ alink´ as, Kriszti´ an M´ arity, Konr´ ad Kandrai, Zolt´ an Tajkov, Martin Gmitra, P´ eter Vancs´ o, Levente Tapaszt´ o, and P´ eter Nemes-Incze. Identification of graphite with perfect rhombohedral stacking by electronic raman scattering. Car-bon , 230:119608, 2024. 3 [33] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, and Siliang Tang. Momen-tor: Advancing video large language model with fine-grained temporal reasoning. arXiv preprint arXiv:2402.11435 , 2024. 3[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi-sion. In International conference on machine learning , pages 8748–8763. PmLR, 2021. 3 [35] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R¨ adle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 , 2024. 2, 3 [36] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object de-tection. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 779–788, 2016. 2 [37] Isaac Robinson, Peter Robicheaux, Matvei Popov, Deva Ra-manan, and Neehar Peri. Rf-detr: Neural architecture search for real-time detection transformers, 2025. 2 

10 [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In International Conference on Medical image com-puting and computer-assisted intervention , pages 234–241. Springer, 2015. 3 [39] Oriane Sim´ eoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Micha¨ el Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timoth´ ee Darcet, Th´ eo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herv´ e J´ egou, Patrick La-batut, and Piotr Bojanowski. Dinov3, 2025. 2 [40] Corinne Steiner, Rebecca Rahmel, Frank Volmer, Rika Windisch, Lars H Janssen, Patricia Pesch, Kenji Watanabe, Takashi Taniguchi, Florian Libisch, Bernd Beschoten, et al. Current-induced brightening of vacancy-related emitters in hexagonal boron nitride. Physical Review Research , 7(3): L032037, 2025. 3 [41] Wentao Tan, Qiong Cao, Yibing Zhan, Chao Xue, and Changxing Ding. Beyond human data: Aligning multimodal large language models by iterative self-evolution. In Pro-ceedings of the AAAI Conference on Artificial Intelligence ,pages 7202–7210, 2025. 3 [42] Jan-Lucas Uslu, Taoufiq Ouaj, David Tebbe, Alexey Nekrasov, Jo Henri Bertram, Marc Sch¨ utte, Kenji Watan-abe, Takashi Taniguchi, Bernd Beschoten, Lutz Waldecker, et al. An open-source robust machine learning platform for real-time detection and classification of 2d material flakes. 

Machine Learning: Science and Technology , 5(1):015027, 2024. 2, 3 [43] Jan-Lucas Uslu, Alexey Nekrasov, Alexander Hermans, Bernd Beschoten, Bastian Leibe, Lutz Waldecker, and Christoph Stampfer. Maskterial: A foundation model for automated 2d material flake detection. Digital Discovery ,2025. 2, 3, 8 [44] Max Wegerhoff, Moritz Scharfst¨ adt, Stefan Linden, and An-drea Bergschneider. Coherent interaction of 2 s and 1 s ex-citon states in transition-metal dichalcogenide monolayers. 

Physical Review Letters , 134(23):236901, 2025. 3 [45] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 2087– 2098, 2025. 3 [46] Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, et al. Task preference optimization: Improving multi-modal large language models with vision task alignment. In 

Proceedings of the Computer Vision and Pattern Recognition Conference , pages 29880–29892, 2025. 3 [47] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025. 7 [48] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Motion mamba: Efficient and long sequence motion generation. In European Conference on Computer Vision , pages 265–282. Springer, 2024. 3 [49] Hongliang Zhou, Yongxiang Liu, Canyu Mo, Weijie Li, Bowen Peng, and Li Liu. When pixel difference patterns meet vit: Pidivit for few-shot object detection. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision (ICCV) , pages 24309–24318, 2025. 2 

11 QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery 

# Supplementary Material 

## 9. Proof of Eq. (10) 

Proof (First-order Link to Optical Model). Let ∆R(λ) = 

R(x, λ ) − Rbg (λ) denote the spectral difference caused by the 2D flake, which depends on the refractive index n(λ),thickness d, and interference phase ϕ = 2 πn (λ)d/λ . As-suming ∥∆R∥∞ is small (a valid approximation for mono-and few-layer flakes), we can linearize Φ ◦ I about Ibg using a first-order Taylor expansion:         

> ∆E(x)≈JΦ(Ibg )
> Z
> ∆R(λ)E(λ)S(λ)dλ
> |{z }
> ∆I
> 2,(12)

where JΦ is the Jacobian of the RGB-to-LAB transforma-tion at Ibg and S(λ) = ( SR(λ), S G(λ), S B (λ)) ⊤. There-fore, there exists a positive constant c (dependent on 

E, S, J Φ) such that            

> ∆E(x)≥c∆I2=c
> Z
> ∆R(λ)E(λ)S(λ)dλ 2.(13)

Since the thin-film interference model predicts that 

∆R(λ; d) varies systematically with the number of layers 

d, the perceptual contrast ∆E(x) is a monotonic proxy (lo-cally) for the reflectance deviation induced by optical inter-ference. Thus, maximizing ∆E over image patches identi-fies regions whose spectra differ from the substrate due to interference, without explicitly solving the transfer-matrix model. 

## 10. More Details of Synthia 

In this section, we further discuss in detail the Synthia framework. 

10.1. White Balance Calibration 

After the reflected light is captured by the microscope, users often perform a manual color calibration process, typically by applying white balance correction to the image. This step adjusts the gain factors of the red, green, and blue (RGB) channels to compensate for color casts introduced by the illumination source or optical path. In practice, this cali-bration alters the perceived color of the substrate and flakes. From the perspective of material scientists, such calibration serves two main purposes: (1) it enhances the visibility of the target flakes , making them more distinguishable from the background and other overlapping flakes; and (2) each user tends to develop personalized calibration preferences based on their experience and specific material systems, op-timizing contrast for their observation workflow. As a re-sult, microscopy images collected across different labora-tories often exhibit noticeable color variations, even when using the same materials, substrates, and hardware config-urations. This variability poses a significant challenge for AI models, which must remain robust to inconsistent color calibration settings. Therefore, in our synthetic pipeline, we explicitly model color calibration variability by simulating diverse white balance transformations. This step ensures that the generated synthetic flakes maintain realistic color contrast relative to their substrates and better reflect the di-versity of real-world microscopy datasets. 

Implementation of White-Balance Calibration. Let the reference microscopy image be Iref ∈ R3×H×W in linear RGB space, captured for a known material and substrate configuration (e.g., MoS 2 on Si/SiO 2 with known oxide thickness). We denote by Msub ∈ { 0, 1}H×W the binary mask identifying substrate regions, and by 1{·} the indica-tor function. The mean substrate color after user calibration (white balance already applied) is computed as: 

[cref sub ]k =

P 

> i,j

Iref [k, i, j ] Msub [i, j ]

P 

> i,j

Msub [i, j ] , k ∈ { 1, 2, 3}.

(14) Given the known optical stack (air / flake / SiO 2 / Si), the transfer-matrix module T computes the substrate re-flectance spectrum Rsub (λ) ∈ RD sampled across D dis-crete wavelengths. A colorimetric projection function Φ

(CIE 1931 color-matching with standard illuminant) maps spectra to linear RGB, yielding the pre-white-balance sub-strate color: 

c0sub = Φ  Rsub (λ) ∈ R3. (15) Assuming a per-channel diagonal white balance model, the user’s gain vector g ∈ R3+ satisfies: 

cref sub ≈ diag( g) c0sub =⇒ g = cref sub ⊘ c0sub , (16) where ⊘ denotes element-wise division (optionally normal-ized, e.g., 13 ∥g∥1 = 1 to fix global exposure). Finally, for any synthetic image before white balance 

I0syn ∈ R3×h×w, the personalized correction is applied as: 

Isyn [k, i, j ] = gk · I0syn [k, i, j ], k ∈ { 1, 2, 3}, (17) followed by clipping to [0 , 1] . This process aligns the syn-thetic color distribution to the user’s individual calibra-tion preferences while preserving the physically predicted flake–substrate contrast computed by T and Φ.110.2. Substrate-Aware Synthetic Placement 

Previous data synthesis pipelines often overlook the spa-tial context of existing flakes in the reference microscopy images. As a result, newly generated synthetic flakes are randomly overlaid on the image, frequently overlapping with real flakes or debris already present on the substrate. Such unawareness introduces unrealistic artifacts and de-grades the physical plausibility of the synthesized dataset. A straightforward solution would be to manually anno-tate bounding boxes for all visible flakes and restrict syn-thetic placement to the remaining background areas. How-ever, this approach is highly labor-intensive, prone to hu-man error, and does not scale to large microscopy collec-tions. To overcome these limitations, we propose an au-tomated substrate-detection algorithm that localizes clean substrate regions directly from the input image. By identify-ing substrate-only areas through reflectance consistency and color uniformity, our method ensures that synthetic flakes are placed exclusively on physically valid regions, avoid-ing overlaps with existing structures. This substrate-aware synthesis greatly improves dataset realism and scalability without requiring manual annotations. 

10.3. Substrate-Aware Synthetic Flake Generation 

Given an unlabeled microscopy image I ∈ [0 , 255] H×W ×3,our goal is to synthesize new flakes while avoiding overlap with existing ones. We first compute a LAB-based attention map A = FLAB (I) ∈ RH×W and normalize it to the range 

[0 , 1] :

˜A = A − min( A)max( A) − min( A) + ε . (18) High-attention regions correspond to existing flakes or high-texture areas. The substrate mask is then defined as 

Msub = 1

n ˜A < τ 

o

, τ = Perc 90 ( ˜A), (19) where τ denotes the 90th percentile threshold, preserving only low-attention (substrate) regions. For each synthetic flake fi with binary mask Mi and sampled thickness ti, we randomly select a valid substrate region (ui, v i) that satisfies 

Msub [vi : vi + hi, u i : ui + wi] ⊙ Mi ≡ 0, (20) ensuring no overlap with existing flakes or previously placed synthetic ones. 

Optical Color Generation. The color of each flake is physically computed using the transfer matrix method 

(TMM). Given a material s (e.g., MoS 2), a layer stack S =

{air , s, SiO 2, Si }, and a flake thickness ti, the wavelength-dependent reflectance spectrum is obtained by 

Ri(λ) = T (S, t i, λ ), (21) where T denotes the multilayer optical simulation. The re-sulting reflectance is converted into a perceptual RGB color via CIE 1931 color-matching functions S(λ) and illuminant spectrum I(λ):

ci = Φ  Ri(λ), I (λ) =

Z λmax 

> λmin

S(λ) I(λ) Ri(λ) dλ, (22) where ci ∈ [0 , 255] 3 represents the flake’s RGB color. 

Flake Composition. The synthetic flake is placed directly on the selected substrate area by replacing the correspond-ing region in I:

Iout [vi : vi + hi, u i : ui + wi] = ci Mi (23) 

+ I[vi : vi + hi, u i : ui + wi] (1 − Mi),

where Mi ∈ { 0, 1}hi×wi denotes the binary flake mask. This substrate-aware process ensures that synthetic flakes are placed only on physically valid substrate regions, avoid-ing overlaps and preserving realistic color consistency through the optical model. 

## 11. QMat-Instruct Dataset 

We present samples from the QMat-Instruct dataset in Fig. 6 through Fig. 15, showing the dialogue between a human and the model concerning material analysis using synthetic optical microscopy images. 

## 12. QF-Bench Dataset 

Due to the large size of the data, we currently release 50 samples from the QF-Bench dataset. We show some sam-ples from the benchmark dataset in Fig. 16. The benchmark release is structured in COCO JSON format. The JSON contains top-level keys such as images , annotations ,and categories . The dataset defines three distinct cate-gories based on flake thickness: Mono , Few , and Thick ,all grouped under the flake supercategory. Each individ-ual flake has a bounding box annotation. 2Figure 6. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

3Figure 7. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

4Figure 8. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

5Figure 9. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

6Figure 10. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

7Figure 11. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

8Figure 12. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

9Figure 13. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

10 Figure 14. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

11 Figure 15. Conversation between a human and model analyzing a synthetic optical microscopy image. The model output is highlighted in three parts: flake candidate enumeration (pink), physics-informed reasoning (blue), and final conclusion (green). 

12 Figure 16. Samples from the QF-Bench dataset. Each microscopy image displays annotations for 2D material flakes. Flake thickness categories are color-coded: green ( Mono ), blue ( Few ), and orange ( Thick ). 

13