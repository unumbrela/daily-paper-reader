---
title: Learning Data-Efficient and Generalizable Neural Operators via Fundamental Physics Knowledge
title_zh: 通过基础物理知识学习数据高效且具泛化性的神经算子
authors: "Siying Ma, Mehrdad M. Zadeh, Mauricio Soroco, Wuyang Chen, Jiguo Cao, Vijay Ganesh"
date: 2026-02-16
pdf: "https://arxiv.org/pdf/2602.15184v1"
tags: ["query:sr"]
score: 7.0
evidence: 用于物理定律提取和偏微分方程建模的科学机器学习。
tldr: 本研究针对神经算子在模拟物理系统时忽视基础物理原理的问题，提出了一种多物理场训练框架。该框架通过同时学习目标偏微分方程（PDE）及其简化的基础形式，显著提升了模型的数据效率和泛化能力。实验证明，该方法在处理物理参数偏移和虚实迁移等分布外场景时表现优异，且适用于多种1D/2D/3D PDE问题，为科学机器学习提供了更强的鲁棒性。
motivation: 现有的神经算子主要关注特定PDE的模拟，缺乏对底层基础物理规律的利用，导致在数据稀缺或分布外场景下泛化性不足。
method: 提出一种与架构无关的多物理场训练框架，通过将目标PDE与简化的基础物理方程进行联合训练来引入基础物理知识。
result: 在多种维度的PDE实验中，该方法显著降低了预测误差，并增强了模型在参数偏移和合成到真实迁移任务中的泛化表现。
conclusion: 显式引入基础物理知识能有效增强神经算子的泛化能力和数据效率，是提升科学机器学习模型性能的关键途径。
---

## 摘要
科学机器学习（SciML）的最新进展使得神经算子（NOs）能够作为强大的代理模型，用于模拟受偏微分方程（PDEs）支配的物理系统的动态演化。虽然现有方法主要侧重于从目标 PDE 中学习模拟，但它们往往忽略了这些方程背后更基础的物理原理。受数值求解器兼容不同设置下 PDE 模拟的启发，我们提出了一种多物理场训练框架，该框架同时从原始 PDE 及其简化的基本形式中进行联合学习。我们的框架提高了数据效率，降低了预测误差，并改善了分布外（OOD）泛化能力，特别是在涉及物理参数偏移和从合成到真实（synthetic-to-real）迁移的场景中。我们的方法与架构无关，并在广泛的一维/二维/三维 PDE 问题中展示了归一化均方根误差（nRMSE）的持续改进。通过广泛的实验，我们证明了显式引入基础物理知识能显著增强神经算子的泛化能力。我们将在 https://sites.google.com/view/sciml-fundemental-pde 发布模型和代码。

## Abstract
Recent advances in scientific machine learning (SciML) have enabled neural operators (NOs) to serve as powerful surrogates for modeling the dynamic evolution of physical systems governed by partial differential equations (PDEs). While existing approaches focus primarily on learning simulations from the target PDE, they often overlook more fundamental physical principles underlying these equations. Inspired by how numerical solvers are compatible with simulations of different settings of PDEs, we propose a multiphysics training framework that jointly learns from both the original PDEs and their simplified basic forms. Our framework enhances data efficiency, reduces predictive errors, and improves out-of-distribution (OOD) generalization, particularly in scenarios involving shifts of physical parameters and synthetic-to-real transfer. Our method is architecture-agnostic and demonstrates consistent improvements in normalized root mean square error (nRMSE) across a wide range of 1D/2D/3D PDE problems. Through extensive experiments, we show that explicit incorporation of fundamental physics knowledge significantly strengthens the generalization ability of neural operators. We will release models and codes at https://sites.google.com/view/sciml-fundemental-pde.