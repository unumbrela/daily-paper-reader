Title: Learning Data-Efficient and Generalizable Neural Operators via Fundamental Physics Knowledge

URL Source: https://arxiv.org/pdf/2602.15184v1

Published Time: Wed, 18 Feb 2026 01:18:14 GMT

Number of Pages: 24

Markdown Content:
Published as a conference paper at ICLR 2026 

# LEARNING DATA -E FFICIENT AND GENERALIZABLE 

# NEURAL OPERATORS VIA FUNDAMENTAL PHYSICS 

# KNOWLEDGE 

Siying Ma 1, Mehrdad M. Zadeh 1, Mauricio Soroco 1

Wuyang Chen 1, Jiguo Cao 1âˆ—, Vijay Ganesh 2âˆ—

> 1

Simon Fraser University, 2Georgia Institute of Technology 

## ABSTRACT 

Recent advances in scientific machine learning (SciML) have enabled neural opera-tors (NOs) to serve as powerful surrogates for modeling the dynamic evolution of physical systems governed by partial differential equations (PDEs). While existing approaches focus primarily on learning simulations from the target PDE, they often overlook more fundamental physical principles underlying these equations. Inspired by how numerical solvers are compatible with simulations of different settings of PDEs, we propose a multiphysics training framework that jointly learns from both the original PDEs and their simplified basic forms. Our framework en-hances data efficiency, reduces predictive errors, and improves out-of-distribution (OOD) generalization, particularly in scenarios involving shifts of physical pa-rameters and synthetic-to-real transfer. Our method is architecture-agnostic and demonstrates consistent improvements in normalized root mean square error (nRMSE) across a wide range of 1D/2D/3D PDE problems . Through extensive experiments, we show that explicit incorporation of fundamental physics knowl-edge significantly strengthens the generalization ability of neural operators. We will release models and codes at https://sites.google.com/view/sciml-fundemental-pde. 

## 1 INTRODUCTION Scientific  

> Machine
> Learning
> Fundamental
> Physics Knowledge
> Diffusion Reaction
> Navier Stokes
> Kuramoto -Sivashinsky
> Real -World Flow
> Diffusion
> Advection

Figure 1: Can SciML models (e.g., neural operators trained on advanced PDEs) also understand fundamental physics knowledge (basic terms like diffusion, advection)? 

Recent advances in scientific machine learning (SciML) have broadened traditional machine learning (ML) for mod-eling physical systems, using deep neural networks (DNNs) especially neural operators (NOs) (Li et al., 2021a; Pathak et al., 2022; Lam et al., 2023; Bi et al., 2023) as fast, ac-curate surrogates for solving partial differential equations (PDEs) (Raissi et al., 2019; Edwards, 2022; Kochkov et al., 2021; Pathak et al., 2022). However, compared to numer-ical methods, a key disadvantage of recent data-driven SciML models is their limited integration of fundamental physical knowledge .Numerical solvers, though tailored to specific PDEs or discretizations, inherently preserve physical laws (e.g., conservation, symmetry), ensuring consistent and plausible simulations across diverse conditions (physical parameters, boundaries, geometries, etc.) (Ketcheson et al., 2012; Hansen et al., 2023; Mouli et al., 2024; Holl & Thuerey, 2024). In contrast, data-driven models, despite learning across PDE types (e.g., via multiphysics pretraining in SciML foundation models (McCabe et al., 2023; Hao et al., 2024)), remain sensitive to training distributions, degrading under distribution shifts (Subramanian et al., 2023; Benitez et al., 2024) and requiring large, diverse datasets. This fragility is worsened by the absence of rigorous verification: Unlike classical solvers, SciML models are rarely evaluated against decomposed PDE components . This gap introduces three major challenges: 1) High data demands : Without physics priors, neural operators require large, diverse datasets for high precision, as seen in recent SciML foundation models(Hao et al., 2024; McCabe 

> âˆ—

Co-corresponding authors. 

1

> arXiv:2602.15184v1 [cs.LG] 16 Feb 2026

Published as a conference paper at ICLR 2026 et al., 2023) which focus on generalization without addressing training data efficiency. 2) Physical inconsistency : Lacking inductive biases, these models may violate conservation laws or produce unphysical outputs, particularly in long-term rollout predictions. 3) Poor generalization : Neural PDE solvers often struggle with unseen simulation settings and requires retraining. Motivated by the above challenges, we ask two scientific questions: 

Q1 : Can neural operators understand both original PDEs and fundamental physics knowledge? 

Q2 : Can neural operators benefit from explicit learning of fundamental physics knowledge? 

In this paper, we highlight the importance of enforcing the learning of fundamental physical knowl-edge in neural operators. The key idea is to identify physically plausible basic terms that can be 

decomposed from original PDEs , and incorporate their simulations during training . Although often overlooked in SciML, our experiments demonstrate that these fundamental physical terms encode rich physical knowledge. Not only can they be utilized without incurring additional computational costs, but they also widely offer substantial and multifaceted benefits . This opens up a new door to improve the comprehensive generalization of neural operators with data efficiency. We summarize our contributions below: 1. Through comprehensive evaluations of public SciML models, we observe a strong correlation between performance on original PDEs and basic PDE terms, highlighting the importance of fundamental physical knowledge in neural operators (Section 2.2). 2. We propose to explicitly incorporate fundamental physical knowledge into neural operators. Specifically, we design a simple and principled multiphysics strategy to train neural operators on simulations of both the original PDE and its basic form. (Section 3). 3. Our method exhibits three major benefits: 1) data efficiency (Section 4.2), 2) long-term physical consistency (Section 4.3), 3) strong generalization in OOD (Section 4.4) and real-world (Section 4.5) scenarios . We evaluate our method on a wide range of 1D/2D/3D PDEs, achieving consistent improvement in nRMSE (normalized root mean squared error, Section 4.2). 

## 2 BACKGROUND 

2.1 DEFINITION OF PDE L EARNING IN SCI ML For time-dependent PDEs, the solution is a vector-valued mapping v : T Ã— S Ã— Î˜ â†’ Rd, defined on the temporal domain T , the spatial domain S, and the parameter space Î˜, with d the number of dependent variables. Numerical solvers compute vÎ¸ (t, Â·) from â„“ â‰¥ 1 past steps, enabling finite-difference approximations: NÎ¸ = [ vÎ¸ (t âˆ’ â„“ Â· âˆ†t, Â·), . . . , vÎ¸ (t âˆ’ âˆ†t, Â·)] 7 â†’ vÎ¸ (t, Â·), where âˆ†t is the temporal resolution. SciML aims to learn a surrogate operator bNÎ¸,Ï• , parameterized by physical parameters Î¸ and learnable weights Ï•, that approximates this mapping. Given N simulations, 

D := v(i)([0 : tmax ], Â·) | i = 1 , . . . , N , the model is trained by minimizing a loss L, often the normalized root mean squared error: nRMSE = âˆ¥vpred âˆ’vâˆ¥2 

> âˆ¥vâˆ¥2

, where vpred is the model prediction. 2.2 MOTIVATION : N EURAL OPERATORS EXHIBIT CORRELATED YET WORSE PERFORMANCE ON FUNDAMENTAL PHYSICS 0.10 0.15 0.20 0.25 0.30 

> Decomposed Convection Term of
> 2D Navier Stokes (nRMSE)
> 0.00
> 0.01
> 0.02
> 0.03
> 0.04
> 0.05
> 0.06
> 2D Navier Stokes (nRMSE)
> MPP-L
> MPP-b
> MPP-S
> MPP-Ti
> DPOT-L
> DPOT-M
> DPOT-S
> DPOT-Ti
> Hyena

Figure 2: On 2D incompressible Navier-Stokes, neural opera-tors and SciML foundation models (MPP (McCabe et al., 2023), DPOT (Hao et al., 2024), Hyena (Patil et al., 2023)) exhibit cor-related yet worse performance on fundamental physics (x-axis). 

We begin with a motivating example to highlight the importance of incorpo-rating fundamental physical knowledge into neural operators. Specifically, we gather publicly released pretrained neu-ral operators, with a focus on SciML foundation models that are jointly pre-trained across multiple PDEs (â€œmulti-physicsâ€) (McCabe et al., 2023; Hao et al., 2024). We evaluate these models on their ability to capture fundamental physical knowledge (formally defined in Section 3.1), which none of them 2Published as a conference paper at ICLR 2026 were explicitly trained on, and compare their performance against the original PDE simulations. From Figure 2, we observe a strong Pearson correlation (0.9625) between errors on original PDEs and their basic terms. This suggests that stronger SciML models implicitly learn basic PDE components more effectively. However, because these terms are not explicitly included in their training data, their absolute errors (0.133â€“0.308 on the x-axis) remain much larger than those for the original PDEs (0.008â€“0.056 on the y-axis). This gap reveals a key limitation: while the models demonstrate transfer across multiple physics, they still fail to reliably capture the fundamental PDE components that underpin complex equations . This motivates us to explicitly enforce an understanding of fundamental physical knowledge within neural operators. 

## 3 METHODS 

Motivated by our observation in Section 2.2, we incorporate fundamental physics knowledge into learning neural operators via: 1) Defining and decomposing basic forms from the original PDE (Section 3.1); 2) Jointly training neural operators on simulations from both basic forms and the original PDE (Section 3.2). We provide an overview of our method in Figure 3. Basic Forms       

> (Sec. 3.1 )
> PDE
> Cheap Simulations
> (Table 1)
> Heavy Simulations
> Neural Operator
> Joint Training
> (mixed simulations)
> Sec. 3. 2
> Decompose
> Data Efficiency
> Long -term Consistency
> Generalization
> (OOD, Synthetic -to -Real)
> Benefits (Sec. 4.2~4.5)
> Figure 3: Overview of our method. Decomposed PDEs encode rich fundamental physical knowledge and introduce cheaper simulations. By jointly training on both the full PDE and its decomposed basic form, we bring multiple benefits to neural operators.

3.1 FUNDAMENTAL PHYSICAL KNOWLEDGE VIA DECOMPOSED BASIC PDE F ORMS 

3.1.1 DEFINING FUNDAMENTAL PHYSICAL KNOWLEDGE OF PDE S

Let us consider the second-order PDE as a general example: 

> n

X

> i,j =1

aij (x, u, âˆ‡u) âˆ‚2u

âˆ‚x iâˆ‚x j

+

> n

X

> i=1

bi(x, u, âˆ‡u) âˆ‚u

âˆ‚x i

+ c(x, u, âˆ‡u) = f (x), (1) where u is the target solution, with x âˆˆ Rn the physical space (e.g., n = 3 for 2D time-dependent PDEs). The coefficients aij , b i, c (â€œphysical parametersâ€) govern the dynamics; mismatches between training and testing values cause domain shifts, leading to out-of-distribution (OOD) simulations. Finally, f denotes an external forcing function (Nandakumaran & Datti, 2020). We establish a systematic process to define the fundamental physical knowledge of PDEs , namely 

basic PDE terms : 1) retain terms that govern the essential and dominant physical dynamics; 2) remove terms that induce solver stiffness, increase computational cost, or contribute little to the pattern formation of interest. This procedure typically yields a simplified PDE form that can be simulated far more efficiently, while still capturing the key physical dynamics of the original system. From a machine learning perspective, decomposing a PDE into its basic form acts as a data augmentation strategy that reduces data collection costs. Importantly, our approach of discovering and incorporating basic PDE forms differs fundamentally from the multiphysics training in recent SciML foundation models (McCabe et al., 2023; Hao et al., 2024), which simply aggregate diverse or even weakly related PDE systems. In contrast, we stress that neural operators must support basic PDE terms as a foundation while learning complex PDEs. Figure 4 illustrates the PDEs studied alongside their corresponding decomposed basic forms. An ablation study on different choices of terms is also provided in Appendix D.6. 3Published as a conference paper at ICLR 2026 PDE                                   

> Navier -Stokes (2D) Diffusion -Reaction
> Basic Form
> Navier -Stokes (3D)
> OOD Settings
> ðœˆ =0.001
> ScalarFlow
> ð· !
> ð· "
> "=5
> ð· !ð· "
> "=1
> ð· !
> ð· "
> "=100
> ðœˆ =0.01
> ðœˆ =0.05
> Kuramoto -Sivashinsky
> (a1)
> (a2)
> (a3)
> (a4)
> (b1)
> (b2)
> (b4)
> (b3)
> (c1)
> (c2)
> (c3)
> (d1 )
> (d2 )
> (d3 )
> Synthetic -to -Real
> (d4 )
> ð‘Ž ,ð‘ =1~10 ,ð‘˜ !,ð‘˜ "=64 ~128
> ð‘Ž ,ð‘ =0.05 ~2,ð‘˜ !,ð‘˜ "=1~32
> ð‘Ž ,ð‘ =1~10 ,ð‘˜ !,ð‘˜ "=1~32

Figure 4: Visualizations of simulations of PDEs and their decomposed basic forms (Section 3.1). From left to right: Diffusion-Reaction (activator concentration), 2D Navier-Stokes (fluid velocity), 3D Navier-Stokes (smoke density), and Kuramoto-Sivashinsky (perturbation amplitude). Basic PDE forms are used for training neural operators with fundamental physics knowledge (Section 3.2), and the OOD settings are used for evaluating the generalization of neural operators (Section 4.4). Dv , D u: diffusion coefficients (Equation 2). Î½: viscosity (Equation 4). a, b, k 1, k 2: magnitudes and wavenumbers (Equation 7). 

3.1.2 DIFFUSION -R EACTION 

The Diffusion-Reaction equation models an activator-inhibitor system, which typically happens in the dynamics of chemistry, biology, and ecology. The Diffusion-Reaction equation describes spatiotemporal dynamics where chemical species or biological agents diffuse through a medium and simultaneously undergo local reactions. These systems are often used to model pattern formation, such as Turing patterns, in domains ranging from chemistry to developmental biology. 

âˆ‚tu = Duâˆ‚xx u + Duâˆ‚yy u + Ru, âˆ‚tv = Dv âˆ‚xx v + Dv âˆ‚yy v + Rv . (2) In Equation 2, u and v represent the concentrations of activator and inhibitor, respectively, and Du, D v

are diffusion coefficients. The nonlinear reaction terms Ru, Rv model biochemical interactions. In our experiments, we adopt the FitzHughâ€“Nagumo variant with Ru(u, v ) = u âˆ’ u3 âˆ’ k âˆ’ v and 

Rv (u, v ) = u âˆ’ v, where k = 5 Ã— 10 âˆ’3, consistent with values used in models of excitable media such as neurons or cardiac tissue. 

Decomposed Basic Form. To isolate the fundamental transport behavior and reduce simulation cost, we consider a simplified form of Equation 2 by omitting the nonlinear reaction terms Ru and 

Rv , yielding pure diffusion equations: 

âˆ‚tu = Duâˆ‚xx u + Duâˆ‚yy u, âˆ‚tv = Dv âˆ‚xx v + Dv âˆ‚yy v. (3) â€¢ Why Drop Reaction Terms? This form retains the essential dispersal dynamics but eliminates the feedback coupling between u and v. Nonlinear reaction terms can vary rapidly, introducing stiffness into the PDE. This stiffness necessitates smaller time steps for stable numerical integration, increasing computational cost. By omitting these nonlinear terms, the system becomes linear and more amenable to efficient numerical solutions. â€¢ Why Prioritize the Diffusion Term? Pure diffusion, though simpler, encodes key properties such as isotropic spreading and mass conservation, providing inductive bias for learning. Unlike reaction terms, which act locally to update activator and inhibitor concentrations, the diffusion term governs spatial coupling, as the primary source of pattern formation and spatial dynamics, facilitating transport and stabilization, which explains the visually similar patterns in Figures 4 a1 and a2. 

Physical Scenarios. The emergence of spatial patterns in reaction-diffusion systems is governed by the ratio Dv 

> Du

, which affects the relative spreading rates of inhibitor and activator (Page et al., 4Published as a conference paper at ICLR 2026 2005; Asgari et al., 2011; Gambino et al., 2024). Classical Turing instability arises when Dv â‰« Du,leading to diffusion-driven pattern formation, and the inhibitor spreads out while the activator stays localized. Following previous works (Menou et al., 2023), we set Du = 1 Ã— 10 âˆ’3, and focus on learning simulations when Dv 

> Du

= 5 , and possible OOD scenarios when Dv 

> Du

âˆˆ { 1, 100 }.3.1.3 INCOMPRESSIBLE NAVIER -S TOKES 

The Navierâ€“Stokes equations govern the dynamics of fluid flow and serve as fundamentals for fluid simulations. It considers both the mass conservation and the momentum of fluid parcels. 

âˆ‚u

âˆ‚t = âˆ’(u Â· âˆ‡ )u + Î½âˆ‡2u âˆ’ 1

Ï âˆ‡p + f , (4) where u is the velocity field, Î½ is the dynamic viscosity, Ï is the fluid density, p is the fluid pressure, and f is the external force field. 

Decomposed Basic Form. To isolate fundamental nonlinear transport mechanisms and reduce computational complexity, we simplify Equation 4 by omitting the pressure term (incompressibility via projection) 1 

> Ï

âˆ‡p and diffusion term Î½âˆ‡2u:

âˆ‚u

âˆ‚t = âˆ’(u Â· âˆ‡ )u + f , (5) This form captures inertial advection with external forcing and approximates high Reynolds number flows, where viscous effects are negligible. Such simplifications are analytically meaningful and are often used in turbulence modeling (e.g., inviscid Euler equations). Learning this reduced dynamics can help models internalize convection-dominant regimes. â€¢ Why Drop Pressure and Diffusion Terms? The pressure term, which enforces fluid incompressibility, requires solving large linear systems and is difficult to parallelize and computationally expensive. Omitting it significantly accelerates the simulation. Similarly, the diffusion term in Navier-Stokes often uses explicit Euler integration with substeps, adding complexity. Removing it simplifies the simulation further. Moreover, for many visual effects like smoke or fire, viscosity is minimal, so the diffusion term has little visual impact and can often be omitted without noticeable loss in realism. â€¢ Why Prioritize the Convection Term? Computationally, the convection term is cheap as it describes the local transport of fluid and there is no need to iterate across the spatial domain. Meanwhile, convection is the main driver of motion in most fluid flows, as it transports vorticity and mass. Without it, the fluid would just sit still or respond passively to forces. It captures nonlinear self-interaction, which is critical for dynamic, complex-looking behavior. 

Physical Scenarios. In Navier-Stokes, the dynamic viscosity Î½ in Equation 4 (or the Reynolds number Re = ÏuL Î½ where Ï is the density of the fluid, u is the flow speed, L is the characteristic linear dimension) controls the fluid dynamics. It measures the balance between inertial forces (pushes the fluid particles in different directions, leading to chaotic flow patterns) and viscous forces (resists motion and smoothes out differences in velocity, promoting an orderly flow) of a fluid. Following previous works (Schlichting & Gersten, 2016; Kochkov et al., 2021; Page et al., 2024), we will mainly focus on learning simulations when Î½ = 0 .01 , and possible OOD scenarios when Î½ âˆˆ { 0.05 , 0.001 }.Note that a smaller Î½ will lead to more turbulent flows. 

3D Extension. In real-world scenarios such as atmospheric or smoke dynamics, buoyancy-driven flows provide additional complexity (Eckert et al., 2019). We extend our setting to simulate 3D incompressible Navierâ€“Stokes in a rising plume scenario (see Figure 4 c1 and c3). We simulate how a plume of smoke rises and spreads in a 3D box-shaped environment. Smoke is introduced from a small circular inflow region located at the bottom center of the domain, at a steady inflow rate of 0.2 units per timestep. The smoke is carried upward due to buoyancy. This setting tests the methodâ€™s robustness on complex spatiotemporal dynamics in three dimensions. 3.1.4 KURAMOTO -S IVASHINSKY 

The Kuramoto-Sivashinsky equation is a nonlinear PDE that models the interplay of instability, nonlinearity, and dissipation, making it a prototype for studying spatiotemporal chaos. It is used to 5Published as a conference paper at ICLR 2026 simulate phenomena such as wrinkled flame fronts, thin fluid film instabilities, and chaotic pattern formation. We consider the following one-dimensional Kuramoto-Sivashinsky equation: 

âˆ‚u âˆ‚t = âˆ’uâˆ‚ xu âˆ’ âˆ‚xx u âˆ’ âˆ‚xxxx u, on [0 , L ] Ã— [0 , T ] (6) The solution u generally represents the perturbation amplitude of a chaotic system. The spatial domain [0 , L ] is equipped with periodic boundary conditions. We impose the initial condition on 

x âˆˆ [0 , L ] as 

u0(x) = a cos 

 k1Ï€x L



+ b cos 

 k2Ï€x L



+ ÏƒÏµ (x), (7) where u0 is the superposition of two cosine modes and small mean-zero Gaussian perturbations. This initialization, common in KS studies (Papageorgiou & Smyrlis, 1991; Gudorf & Cvitanovic, 2019), combines deterministic cosine modes that inject controlled perturbations with small random noise. a, b, k 1, k 2 are tunable physical parameters controlling the strength and decay of the initial perturbation. Here, Ïµ(x) are i.i.d. standard normal samples, and we set Ïƒ = 0 .05 . Spatial derivatives are evaluated spectrally via discrete Fourier transforms, and time integration is performed with a fourthâ€“order Rungeâ€“Kutta method. 

Decomposed Basic Form. To isolate the linear stabilizing and destabilizing mechanisms and reduce computational complexity, we simplify the Kuramotoâ€“Sivashinsky equation by omitting the nonlinear advection term âˆ’uâˆ‚ xu. The reduced equation is 

âˆ‚u âˆ‚t = âˆ’âˆ‚xx u âˆ’ âˆ‚xxxx u, on [0 , L ] Ã— [0 , T ], (8) which captures the competition between the destabilizing anti-diffusion term ( âˆ’âˆ‚xx u) and the stabilizing diffusion term ( âˆ’âˆ‚xxxx u). â€¢ Why Drop the Nonlinear Advection Term? For small amplitudes or short times, âˆ’uâˆ‚xu is higher order in u, so the linearized dynamics govern the instabilities and pattern formation. Removing this term also eliminates costly Fourier transforms, yielding a significant speedup in simulation. â€¢ Why Prioritize High-Order Stabilizing/Destabilizing Terms? The balance between destabilizing 

âˆ’âˆ‚xx u and stabilizing âˆ’âˆ‚xxxx u determines how fast the chaotic model grows or decays. The fourth-order dissipation is particularly important for controlling stiffness and ensuring smoothness of solutions, making it critical for stable numerical integration. Finally, in practice, many inter-face/turbulence models reduce to precisely this â€œanti-diffusion + diffusionâ€ structure, so analyzing these terms provides broadly transferable insights. 

Physical Scenarios. In the initial condition (Equation 7), the amplitudes a, b control the strength of nonlinearity, while wavenumbers k1, k 2 control the growth or decay of chaos, yielding regimes 1) For large a, b and high k1, k 2, the dynamics are predominantly linear , and chaos decays over time. 2) For small a, b and small k1, k 2, the system develops weakly nonlinear chaos, with instabilities growing slowly. 3) For large a, b and small k1, k 2, the system exhibits strongly nonlinear chaotic growth. In our experiments, we focus on learning in the linear regime with a, b âˆˆ [1 , 10] and 

k1, k 2 âˆˆ [64 , 128] , and evaluate on two out-of-distribution (OOD) regimes: weak nonlinearity (a, b âˆˆ [0 .05 , 0.2] , k1, k 2 âˆˆ [1 , 32] ) and strong nonlinearity ( a, b âˆˆ [1 , 10] , k1, k 2 âˆˆ [1 , 32] ). 3.2 JOINT LEARNING WITH FUNDAMENTAL PHYSICAL KNOWLEDGE 

After defining our fundamental physics knowledge, we now explain how to integrate it into learning neural operators in principle from two perspectives: data composition and neural architecture. 

1) Data Composition. We jointly train neural operators on simulations of both our PDE and the decomposed basic form as a multiphysics training with a composite dataset. We summarize our simulations in Table 1. Since the simulation costs of decomposed basic forms are much cheaper than the original PDE, we can â€œtrade-inâ€ simulations of the original PDE for more simulations of basic forms under the same simulation costs . We define the â€Sample Mixture Ratioâ€ as the rate derived 

from the rate of simulation costs of original PDE with its decomposed basic form while making sure 6Published as a conference paper at ICLR 2026 

Table 1: Summary of simulations of PDEs and their decomposed basic forms. GPU: NVIDIA RTX 6000 Ada. 

PDE Spatial Resolution Temporal Steps Target Variables Simulation Costs (sec. per step) Sample Mixture Ratio (PDE : Basic Form) Diffusion-Reaction (Eq. 2) 128 Ã— 128 100 Activator u, Inhibitor v 1.864 Ã— 10 âˆ’2

1:3 Basic Form (Eq. 3) 6.610 Ã— 10 âˆ’3

Navier-Stokes (2D) (Eq. 4) 256 Ã— 256 1000 Velocity u, Density s 2.775 1:24 Basic Form (2D) (Eq. 5) 0.113 Navier-Stokes (3D) (Eq. 4) 50 Ã— 50 Ã— 89 150 Velocity u, Density s 1.047 1:3 Basic Form (3D) (Eq. 5) 0.300 Kuramoto-Sivashinsky (Eq. 6) 512 200 Perturbation Amplitude u 1.176 Ã— 10 âˆ’4

1:12 Basic Form (Eq. 8) 9.135 Ã— 10 âˆ’6

that â€œexchangeâ€ one primary data with the corresponding number of basic form data will maintain a comparable or reduced simulation budget when training baseline and our proposed model .We apply a multi-task formulation, where the model learns from both the original PDE and its simplified basic form. The idea is inspired from curriculum learning (Bengio et al., 2009; Pentina et al., 2014) and auxiliary task learning (Liu et al., 2019). In our method, the basic forms act as a simpler, physically motivated auxiliary task that can facilitate more efficient representation learning and accelerate the convergence on the primary task. 

2) Neural Architecture. We mainly consider the Fourier Neural Operator (FNO) (Li et al., 2021a) as our neural architecture. However, we make our method agnostic to specific architectures of neural operators: We generally share the backbone of the neural operator for learning both the main PDE and its basic terms, while employing separate final prediction layers for the two tasks. We will discuss the details on model structure in Appendix B, and will include more results using transformer (Dosovitskiy et al., 2020; Tong et al., 2022; McCabe et al., 2023; Chen et al., 2024) in Appendix D. 

## 4 EXPERIMENTS 

4.1 SETTINGS 

Our baseline is learning the original PDE problem. In general, our method reallocates half of the baselineâ€™s simulation budget to simulate the basic PDE terms, with the sample mixture ratio defined in Table 1. To fairly compare with the baseline, we adopt the same hyperparameters and optimization costs (number of gradient descent steps) . Since our goal is to evaluate performance on the original PDE, we use data from the basic PDE term only during training. All testing is conducted exclusively on the original PDE data with 100 samples (Takamoto et al., 2022). We use Adam optimizer, cosine annealing learning rate scheduler, and nRMSE defined in Section 2.1. We summarize our training details in Appendix C. 4.2 DATA EFFICIENCY 10 1 10 2     

> Simulation Costs (Seconds)
> 0.02
> 0.04
> 0.06
> 0.08
> 0.10
> 0.12
> Normalized RMSE
> 2D Diffusion-Reaction
> (FNO)
> Baseline
> Ours 10 410 5
> Simulation Costs (Seconds)
> 0.05
> 0.10
> 0.15
> Normalized RMSE
> 2D Incompressible Navier Stokes
> (FNO)
> Baseline
> Ours 10 310 4
> Simulation Costs (Seconds)
> 0.1
> 0.2
> 0.3
> Normalized RMSE
> 3D Incompressible Navier Stokes
> (FNO)
> Baseline
> Ours 10 230 40 60 200
> Simulation Costs (Seconds)
> 0.0035
> 0.0040
> 0.0045
> 0.0050
> Normalized RMSE
> 1D Kuramoto Sivashinsky
> (FNO)
> Baseline
> Ours

Figure 5: Joint training neural operators on data of the original PDE and the basic form improves performance and data efficiency. Y-axis: normalized RMSE. X-axis: simulation costs (seconds). 

We first study our method with dif-ferent numbers of training samples, and demonstrate that neural operators trained with our method can achieve stronger performance with less train-ing data. We consider the following methods: â€¢ â€œBaselineâ€: Neural operators that are only trained on simulations of the original PDE. â€¢ Ours: As described in Section 3.2, we can replace simulations of the original PDE with its decomposed basic form, allowing the total sim-7Published as a conference paper at ICLR 2026 ulation cost of the training data to be comparable or even reduced. See Table 1 for the sample mixture rate for fair comparison. In Figure 5, we study prediction errors of neural operators trained with different numbers of simula-tions (measured in their simulation costs). We can see that, across PDEs and neural architectures, our method (orange square) is to the lower left of the baseline (blue circle), which means that we can achieve improved prediction errors with reduced simulation costs .4.3 LONG -TERM PHYSICAL CONSISTENCY 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0                         

> Rollout
> 0.03
> 0.04
> 0.05
> 0.06
> 0.07
> Normalized RMSE
> 2D Reaction-Diffusion
> (FNO)
> Baseline
> Ours 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0
> Rollout
> 0.02
> 0.04
> 0.06
> 0.08
> 0.10
> Normalized RMSE
> 2D Incompressible Navier Stokes
> (FNO)
> Baseline
> Ours 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0
> Rollout
> 0.050
> 0.075
> 0.100
> 0.125
> 0.150
> 0.175
> 0.200
> 0.225
> Normalized RMSE
> 3D Incompressible Navier Stokes
> (FNO)
> Baseline
> Ours 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0
> Rollout
> 0.005
> 0.010
> 0.015
> 0.020
> 0.025
> 0.030
> Normalized RMSE
> 1D Kuramoto-Sivashinsky
> (FNO)
> Baseline
> Ours

Figure 6: Joint training neural operators on data of the original PDE and the basic form improves performance with autoregressive inference at different unrolled steps. Models are evaluated using the best-performing checkpoints from training, shown in Figure 5. 

Next-frame prediction (Takamoto et al., 2022; McCabe et al., 2023; Hao et al., 2024) is a widely adopted evaluation, where input frames are always ground truth. Meanwhile, autoregressive inference, where the model keeps rolling out to further temporal steps with its own output as inputs, is a meaningful and more challenging stress test. In autoregres-sive inference, a model forecasts fu-tures with its own (noisy) output as inputs, and thus prediction error will accumulate along rollout steps. We can see that our improvements in Fig-ure 5 further persist across five au-toregressive steps in Figure 6, leading to improved long-term consistency. 4.4 OUT -OF -D ISTRIBUTION (OOD) G ENERALIZATION 

We next show the benefits of our method towards the generalization of neural operators in out-of-distribution (OOD) settings, where the physical parameters used during simulation are significantly shifted. We consider physical scenarios described in Section 3.1, and show results in Table 2. We can see that our method not only improves in-distribution errors, but also generalizes better on unseen physical dynamics (simulations by unseen parameters), leading to more robust neural operators. 

Table 2: Comparisons of OOD generalization for different training methods. Models are evaluated using the best-performing checkpoints from training, as shown in Figure 5, under comparable simulation cost settings. 

PDE Model Source Target 1 Target 2 Setting nRMSE Setting nRMSE Setting nRMSE Diffusion-Reaction (2D) Baseline Dv 

> Du

= 5 0.0289 Dv 

> Du

= 1 0.0413 Dv 

> Du

= 100 0.0770 Ours 0.0231 0.0331 0.0538 

Navier-Stokes (2D) Baseline Î½ = 0 .01 0.0487 Î½ = 0 .05 0.0825 Î½ = 0 .0001 0.0369 Ours 0.0175 0.0222 0.0125 

Navier-Stokes (3D) Baseline Î½ = 0 .01 0.0675 Î½ = 0 .1 0.0393 Î½ = 0 .0001 0.0836 Ours 0.0481 0.0329 0.0602 

Kuramoto-Sivashinsky (1D) Baseline a, b = 1 âˆ¼ 10 

k1, k2 = 64 âˆ¼ 128 

0.0037 a, b = 0.05 âˆ¼ 2

k1, k2 = 1 âˆ¼ 32 

0.0021 a, b = 1 âˆ¼ 10 

k1, k2 = 1 âˆ¼ 32 

0.0200 Ours 0.0034 0.0018 0.0197 

4.5 SYNTHETIC -TO -R EAL GENERALIZATION 

Finally, we test neural operators trained on simulations of 3D Navier-Stokes in real-world scenarios. Essentially, transferring models trained on simulations to real observations is a synthetic-to-real generalization problem (Chen et al., 2020; 2021), as domain gaps between numerical simulations and real-world measurements always persist. We study the ScalarFlow dataset (Eckert et al., 2019), which is a reconstruction of real-world smoke plumes and assembles the first large-scale dataset of realistic turbulent flows. We provide visualizations of synthetic and ScalarFlow data in Figure 4. 8Published as a conference paper at ICLR 2026 Scalarflow Baseline Ours     

> nRMSE: 0.250 nRMSE: 0.213
> Figure 7: Visualizations of the last time step in the ScalarFlow and its predictions derived by baseline and our model.

We show the results and visualize the ground truth as well as the model predictions on smoke plumes from ScalarFlow in Figure 7. We can see that our method outperforms the baseline model and presents a qualita-tive comparison of scalar flow predictions on real data, illustrating that our jointly trained model exhibits improved synthetic-to-real generalization performance. Please read our Appendix D.2 for more results on 3D Navier-Stokes. 

## 5 RELATED WORKS 

Machine Learning for Scientific Modeling Learning-based methods have long been used to model physical phenomena (Lagaris et al., 1998; 2000; Chen & Chen, 1995b;a). Physics-informed neural networks (PINNs) (Raissi et al., 2019; Zhu et al., 2019; Geneva & Zabaras, 2020; Gao et al., 2021; Ren et al., 2022) incorporate PDEs into loss functions to enforce physical laws, but often struggle with generalization and optimization issues (Krishnapriyan et al., 2021; Edwards, 2022). Operator learning methods like Fourier Neural Operators (FNO) (Li et al., 2021a; 2020; Kovachki et al., 2023) and DeepONet (Lu et al., 2019) offer greater flexibility by learning mappings between function spaces but require extensive labeled data (Raissi et al., 2019; Brandstetter et al., 2022; Zhang et al., 2023; Chen et al., 2024; Liu et al., 2026). We adopt a unique approach by evaluating and enhancing SciML through the lens of its compatibility with fundamental physical principles. 

Data-Driven Neural PDE Solvers Machine learning has increasingly been used to approximate PDE solutions, with neural PDE solvers trained on diverse scenarios to mimic traditional simulators. Early models used CNNs (Guo et al., 2016; Zhu & Zabaras, 2018; Bhatnagar et al., 2019), while DeepONet (Lu et al., 2019) introduced a neural operator (NO) framework separating input and query encodings, inspiring many extensions (Wang et al., 2021; Hadorn, 2022; Wang et al., 2022; Lin et al., 2023; Venturi & Casey, 2023; Xu et al., 2023; McCabe et al., 2023; Hao et al., 2024). Advances like FNO (Li et al., 2021a), LNO (Cao et al., 2023), CNO (Raonic et al., 2024), and KNO (Xiong et al., 2024) have expanded the field, with FNO particularly impactful across applications (Li et al., 2021b; Guibas et al., 2021; Yang et al., 2021; Rahman et al., 2022b;a; Pathak et al., 2022; Liu et al., 2022). Compared with previous neural operator works, instead of naively swapping in cheaper simulations of simplified PDEs, the core merit of our work is to emphasize the multifaceted benefits of explicit learning of fundamental physics knowledge during operator learning. 

Out-of-Distribution Generalization in SciML Interest in out-of-distribution (OOD) generalization for scientific machine learning (SciML) has grown recently. Subramanian et al. (2023) showed that fine-tuning neural operators (NOs) on OOD PDEs often requires many OOD simulations, which may be impractical. Benitez et al. (2024) proposed a Helmholtz-specific FNO with strong OOD performance, supported by Rademacher complexity and a novel risk bound. Other work includes ensemble methods leveraging uncertainty (Hansen et al., 2023; Mouli et al., 2024), loss functions informed by numerical schemes (Kim & Kang, 2024), and meta-learning for varied geometries (Liu et al., 2023). However, varying PDE types and setups across studies hinder unified insights into OOD generalization for NOs. Our work demonstrates that neural operators explicitly trained with fundamental physics knowledge exhibit improved OOD and synthetic-to-real generalization. 

## 6 CONCLUSION 

We present a principle and architecture-agnostic approach to enhance neural operators by explicitly incorporating fundamental physical knowledge into their training. By decomposing complex PDEs into simpler, physically meaningful basic forms and using them as auxiliary training signals, our proposed method significantly improves data efficiency, long-term predictive consistency, and out-of-distribution generalization. These improvements are demonstrated across a variety of PDE systems and neural operator architectures. Our finding highlights the untapped potential of fundamental 9Published as a conference paper at ICLR 2026 physics as an inductive bias in scientific machine learning, offering a robust and cost-effective pathway to more reliable and generalizable surrogate models in real-world physical simulations. 

## THE USE OF LARGE LANGUAGE MODELS (LLM S)

LLMs did not play a significant role in either the research ideation or the writing of this paper. Their use was limited to correcting minor grammatical issues and typographical errors. 

## ACKNOWLEDGMENT 

This research used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231 using NERSC award NERSC DDR-ERCAP0034682 and ASCR-ERCAP0031463. J. Caoâ€™s research is partially supported by the Discovery Grants (RGPIN-2023-04057) of the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Canada Research Chair program. 

## REFERENCES 

Yazdan Asgari, Mehrdad Ghaemi, and Mohammad Ghasem Mahjani. Pattern formation of the fitzhugh-nagumo model: cellular automata approach. IRANIAN JOURNAL OF CHEMISTRY AND CHEMICAL ENGINEERING (IJCCE) , 2011. Yoshua Bengio, JÃ©rÃ´me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In 

Proceedings of the 26th Annual International Conference on Machine Learning , ICML â€™09, pp. 41â€“48, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585161. doi: 10.1145/1553374.1553380. URL https://doi.org/10.1145/1553374.1553380 .Jose Antonio Lara Benitez, Takashi Furuya, Florian Faucher, Anastasis Kratsios, Xavier Tricoche, and Maarten V de Hoop. Out-of-distributional risk bounds for neural operators with applications to the helmholtz equation. Journal of Computational Physics , pp. 113168, 2024. Saakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik. Predic-tion of aerodynamic flow fields using convolutional neural networks. Computational Mechanics ,64:525â€“545, 2019. Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range global weather forecasting with 3d neural networks. Nature , 619(7970):533â€“538, 2023. Johannes Brandstetter, Max Welling, and Daniel E Worrall. Lie point symmetry data augmentation for neural pde solvers. In International Conference on Machine Learning , pp. 2241â€“2256. PMLR, 2022. Qianying Cao, Somdatta Goswami, and George Em Karniadakis. Lno: Laplace neural operator for solving differential equations. arXiv preprint arXiv:2303.10528 , 2023. Tianping Chen and Hong Chen. Approximation capability to functions of several variables, nonlinear functionals, and operators by radial basis function neural networks. IEEE Transactions on Neural Networks , 6(4):904â€“910, 1995a. Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. IEEE transactions on neural networks , 6(4):911â€“917, 1995b. Wuyang Chen, Zhiding Yu, Zhangyang Wang, and Animashree Anandkumar. Automated synthetic-to-real generalization. In International conference on machine learning , pp. 1746â€“1756. PMLR, 2020. Wuyang Chen, Zhiding Yu, Shalini De Mello, Sifei Liu, Jose M Alvarez, Zhangyang Wang, and Anima Anandkumar. Contrastive syn-to-real generalization. arXiv preprint arXiv:2104.02290 ,2021. 10 Published as a conference paper at ICLR 2026 Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, and Michael W Mahoney. Data-efficient operator learning via unsupervised pretraining and in-context learning. 

arXiv preprint arXiv:2402.15734 , 2024. Michael Crawshaw. Multi-task learning with deep neural networks: A survey, 2020. URL https: //arxiv.org/abs/2009.09796 .Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020. Marie-Lena Eckert, Kiwon Um, and Nils Thuerey. Scalarflow: a large-scale volumetric data set of real-world scalar transport flows for computer animation and machine learning. ACM Transactions on Graphics (TOG) , 38(6):1â€“16, 2019. C. Edwards. Neural networks learn to speed up simulations. Communications of the ACM , 65(5): 27â€“29, 2022. G Gambino, MC Lombardo, R Rizzo, and M Sammartino. Excitable fitzhugh-nagumo model with cross-diffusion: close and far-from-equilibrium coherent structures. Ricerche di Matematica , 73 (Suppl 1):137â€“156, 2024. Han Gao, Luning Sun, and Jian-Xun Wang. Phygeonet: Physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state pdes on irregular domain. 

Journal of Computational Physics , 428:110079, 2021. Nicholas Geneva and Nicholas Zabaras. Modeling the dynamics of pde systems with physics-constrained deep auto-regressive networks. Journal of Computational Physics , 403:109056, 2020. Matthew Gudorf and Predrag Cvitanovic. Spatiotemporal tiling of the kuramoto-sivashinsky equation. In APS March Meeting Abstracts , volume 2019, pp. L70â€“263, 2019. John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catan-zaro. Adaptive fourier neural operators: Efficient token mixers for transformers. arXiv preprint arXiv:2111.13587 , 2021. Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady flow ap-proximation. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining , pp. 481â€“490, 2016. Patrik Simon Hadorn. Shift-deeponet: Extending deep operator networks for discontinuous output functions. ETH Zurich, Seminar for Applied Mathematics, 2022. Derek Hansen, Danielle C Maddix, Shima Alizadeh, Gaurav Gupta, and Michael W Mahoney. Learning physical models that can respect conservation laws. In International Conference on Machine Learning , pp. 12469â€“12510. PMLR, 2023. Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandku-mar, Jian Song, and Jun Zhu. Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training. arXiv preprint arXiv:2403.03542 , 2024. Philipp Holl and Nils Thuerey. Differentiable simulations for pytorch, tensorflow and jax. In 

Forty-first International Conference on Machine Learning , 2024. David I Ketcheson, Kyle Mandli, Aron J Ahmadia, Amal Alghamdi, Manuel Quezada De Luna, Matteo Parsani, Matthew G Knepley, and Matthew Emmett. Pyclaw: Accessible, extensible, scalable tools for wave propagation problems. SIAM Journal on Scientific Computing , 34(4): C210â€“C231, 2012. Taeyoung Kim and Myungjoo Kang. Approximating numerical fluxes using fourier neural operators for hyperbolic conservation laws. CoRR , 2024. 11 Published as a conference paper at ICLR 2026 Dmitrii Kochkov, Jamie A Smith, Ayya Alieva, Qing Wang, Michael P Brenner, and Stephan Hoyer. Machine learningâ€“accelerated computational fluid dynamics. Proceedings of the National Academy of Sciences , 118(21):e2101784118, 2021. Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to PDEs. Journal of Machine Learning Research , 24(89):1â€“97, 2023. Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Char-acterizing possible failure modes in physics-informed neural networks. Advances in Neural Information Processing Systems , 34:26548â€“26560, 2021. Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks , 9(5):987â€“1000, 1998. Isaac E Lagaris, Aristidis C Likas, and Dimitris G Papageorgiou. Neural-network methods for boundary value problems with irregular boundaries. IEEE Transactions on Neural Networks , 11 (5):1041â€“1049, 2000. Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Learning skillful medium-range global weather forecasting. Science , 382(6677):1416â€“1421, 2023. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations. Advances in Neural Information Processing Systems , 33:6755â€“6766, 2020. Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhat-tacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations , 2021a. Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations. arXiv preprint arXiv:2111.03794 , 2021b. Guang Lin, Christian Moya, and Zecheng Zhang. B-deeponet: An enhanced bayesian deeponet for solving noisy parametric pdes using accelerated replica exchange sgld. Journal of Computational Physics , 473:111713, 2023. Burigede Liu, Nikola Kovachki, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, An-drew M Stuart, and Kaushik Bhattacharya. A learning-based multiscale method and its application to inelastic impact problems. Journal of the Mechanics and Physics of Solids , 158:104668, 2022. Wenzhuo Liu, Mouadh Yagoubi, and Marc Schoenauer. Meta-learning for airflow simulations with graph neural networks. arXiv preprint arXiv:2306.10624 , 2023. Yifan Liu, Bohan Zhuang, Chunhua Shen, Hao Chen, and Wei Yin. Auxiliary learning for deep multi-task learning, 2019. URL https://arxiv.org/abs/1909.02214 .Yuqiu Liu, Jingxuan Xu, Mauricio Soroco, Yunchao Wei, and Wuyang Chen. Data-efficient inference of neural fluid fields via sciml foundation model. International Conference on 3D Vision , 2026. Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193 , 2019. Michael McCabe, Bruno RÃ©galdo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, et al. Multiple physics pretraining for physical surrogate models. arXiv preprint arXiv:2310.02994 ,2023. Lucas Menou, Chengjie Luo, and David Zwicker. Physical interactions promote turing patterns. 

arXiv preprint arXiv:2302.12521 , 2023. 12 Published as a conference paper at ICLR 2026 GrÃ©goire Mialon, Quentin Garrido, Hannah Lawrence, Danyal Rehman, Yann LeCun, and Bobak T Kiani. Self-supervised learning with lie symmetries for partial differential equations. arXiv preprint arXiv:2307.05432 , 2023. S Chandra Mouli, Danielle C Maddix, Shima Alizadeh, Gaurav Gupta, Andrew Stuart, Michael W Mahoney, and Yuyang Wang. Using uncertainty quantification to characterize and improve out-of-domain learning for pdes. arXiv preprint arXiv:2403.10642 , 2024. AK Nandakumaran and PS Datti. Partial differential equations: classical theory with a modern touch .Cambridge University Press, 2020. Jacob Page, Peter Norgaard, Michael P Brenner, and Rich R Kerswell. Recurrent flow patterns as a basis for two-dimensional turbulence: Predicting statistics from structures. Proceedings of the National Academy of Sciences , 121(23):e2320007121, 2024. Karen M Page, Philip K Maini, and Nicholas AM Monk. Complex pattern formation in reactionâ€“ diffusion systems with spatially varying parameters. Physica D: Nonlinear Phenomena , 202(1-2): 95â€“115, 2005. Demetrios T Papageorgiou and Yiorgos S Smyrlis. The route to chaos for the kuramoto-sivashinsky equation. Theoretical and Computational Fluid Dynamics , 3(1):15â€“42, 1991. Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcast-net: A global data-driven high-resolution weather model using adaptive fourier neural operators. 

arXiv preprint arXiv:2202.11214 , 2022. Saurabh Patil, Zijie Li, and Amir Barati Farimani. Hyena neural operator for partial differential equations, 2023. URL https://arxiv.org/abs/2306.16524 .Anastasia Pentina, Viktoriia Sharmanska, and Christoph H. Lampert. Curriculum learning of multiple tasks, 2014. URL https://arxiv.org/abs/1412.1353 .Md Ashiqur Rahman, Manuel A Florez, Anima Anandkumar, Zachary E Ross, and Kamyar Aziz-zadenesheli. Generative adversarial neural operators. arXiv preprint arXiv:2205.03017 , 2022a. Md Ashiqur Rahman, Zachary E Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural operators. arXiv preprint arXiv:2204.11127 , 2022b. Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics , 378:686â€“707, 2019. Bogdan Raonic, Roberto Molinaro, Tim De Ryck, Tobias Rohner, Francesca Bartolucci, Rima Alaifari, Siddhartha Mishra, and Emmanuel de BÃ©zenac. Convolutional neural operators for robust and accurate learning of pdes. Advances in Neural Information Processing Systems , 36, 2024. Pu Ren, Chengping Rao, Yang Liu, Jian-Xun Wang, and Hao Sun. Phycrnet: Physics-informed convolutional-recurrent network for solving spatiotemporal pdes. Computer Methods in Applied Mechanics and Engineering , 389:114399, 2022. Sebastian Ruder. An overview of multi-task learning in deep neural networks, 2017. URL https: //arxiv.org/abs/1706.05098 .Hermann Schlichting and Klaus Gersten. Boundary-layer theory . springer, 2016. Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael W Mahoney, and Amir Gholami. Towards foundation models for scientific machine learning: Charac-terizing scaling and transfer behavior. arXiv preprint arXiv:2306.00258 , 2023. Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani, Dirk PflÃ¼ger, and Mathias Niepert. Pdebench: An extensive benchmark for scientific machine learning. 

Advances in Neural Information Processing Systems , 35:1596â€“1611, 2022. 13 Published as a conference paper at ICLR 2026 Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems , 35:10078â€“10093, 2022. Simone Venturi and Tiernan Casey. Svd perspectives for augmenting deeponet flexibility and interpretability. Computer Methods in Applied Mechanics and Engineering , 403:115718, 2023. Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets. Science advances , 7(40):eabi8605, 2021. Sifan Wang, Hanwen Wang, and Paris Perdikaris. Improved architectures and training algorithms for deep operator networks. Journal of Scientific Computing , 92(2):35, 2022. Wei Xiong, Xiaomeng Huang, Ziyang Zhang, Ruixuan Deng, Pei Sun, and Yang Tian. Koopman neural operator as a mesh-free solver of non-linear partial differential equations. Journal of Computational Physics , pp. 113194, 2024. Wuzhe Xu, Yulong Lu, and Li Wang. Transfer learning enhanced deeponet for long-time prediction of evolution equations. In Proceedings of the AAAI Conference on Artificial Intelligence , pp. 10629â€“10636, 2023. Yan Yang, Angela F Gao, Jorge C Castellanos, Zachary E Ross, Kamyar Azizzadenesheli, and Robert W Clayton. Seismic wave propagation and inversion with neural operators. The Seismic Record , 1(3):126â€“134, 2021. Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, et al. Artificial intelligence for science in quantum, atomistic, and continuum systems. arXiv preprint arXiv:2307.08423 , 2023. Yinhao Zhu and Nicholas Zabaras. Bayesian deep convolutional encoderâ€“decoder networks for surrogate modeling and uncertainty quantification. Journal of Computational Physics , 366:415â€“ 447, 2018. Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics , 394:56â€“81, 2019. 14 Published as a conference paper at ICLR 2026 

## A SIMULATION SETTINGS 

In this section, we detail the simulation settings for the 2D Diffusion-Reaction, 2D and 3D Incom-pressible Navier-Stokes and 1D Kuramotoâ€“Sivashinsky (see Table 1 for the summary). We will also explain how we prepare simulations for the â€œSpatiotemporal Downsamplingâ€ in Section D.1. To ensure a fair comparison under equivalent simulation costs with the basic form of each PDE, we downsample the original PDE simulations both spatially and temporally. We also introduce the settings for the corresponding reduced spatiotemporal resolution simulations. Note that the simulation cost of these downsampled settings is matched to that of the basic form, which implies that their sample mixture ratios in joint training remain equivalent. To further explore the benefits of our multiphysics joint training approach with this reduced spa-tiotemporal resolution simulation strategies (refer to Ours@Spatiotemporal in Figure 9), we addi-tionally introduce the simulation settings for the spatiotemporally downsampled basic form of the 2D Diffusion-Reaction equation. As we will discuss in Section D.1, our framework is orthogonal to standard downsampling techniques, and combining the two can lead to further reductions in simulation cost. This reduction allows for an increased proportion of basic form samples in the training mixture under a fixed computational budget (see Table 3 for details). A.1 DIFFUSION -R EACTION 

Our simulation setting for Diffusion-Reaction follows (Takamoto et al., 2022). Our solver is Py-Claw (Ketcheson et al., 2012) that uses the finite volume method. We set the initial condition as standard normal random noise u(t = 0 , x, y ) âˆ¼ N (0 , 1.0) . We use the homogeneous Neumann boundary condition. We simulate in a spatial domain of Î© = [ âˆ’1, 1] 2, with resolution 128 Ã— 128 .We simulate 5 seconds and save into 100 temporal steps. 

Reduced Spatiotemporal Resolution. When simulating the original Diffusion-Reaction equation at low spatiotemporal grids (yellow curves in Figure 9), we reduce the spatial resolution from 

128 Ã— 128 to 96 Ã— 96 , and reduce the number of temporal steps from 100 to 50. We then upsample to 128 Ã— 128 Ã— 100 (steps) via bilinear interpolation to match the resolution of simulations of the original PDE. The total simulation interval is maintained at 5 seconds, preserving the underlying physical dynamics. Similarly, we can further simulate our decomposed basic form of Diffusion-Reaction at low spa-tiotemporal resolution (green curve in Figure 9). Table 3 shows the simulation cost of decomposed basic forms with reduced spatiotemporal resolution and the sample mixture ratio. 

Table 3: Summary of 2D Diffusion-Reaction simulation and its decomposed basic forms with reduced spatiotem-poral resolution. â€œSample Mixture Rateâ€: We replace simulations of the original PDE with its decomposed basic form with reduced spatiotemporal resolution and make sure the total simulation cost of the training data can be comparable. GPU: NVIDIA RTX 6000 Ada.                   

> PDE Spatial Resolution Temporal Steps Target Variables Simulation Costs (sec. per step) Sample Mixture Ratio (PDE : Basic Form) Diffusion-Reaction (Eq. 2) 128 Ã—128 100 Activator u, Inhibitor v
> 1.864 Ã—10 âˆ’2
> 1:8 Basic Form (Eq. 3) with 96 Ã—96 50 2.390 Ã—10 âˆ’3
> Reduced Spatiotemporal Resolution

A.2 2D I NCOMPRESSIBLE NAVIER -S TOKES 

Our simulation setting for incompressible Navier-Stokes follows (Takamoto et al., 2022). Our solver is PhiFlow (Holl & Thuerey, 2024). We simulate in a spatial domain of Î© = [0 , 1] 2, with resolution 

256 Ã— 256 . We simulate 5 seconds with a dt = 5 Ã— 10 âˆ’5, and periodically save into 1000 temporal steps. Our initial conditions u0 and forcing term f are drawn from isotropic Gaussian random fields, where the low-frequency components of the spectral density are scaled with scale and high-frequency components are suppressed with power-law decay by smoothness . For u0, scale is 0.15 and smoothness is 3.0. For f , scale is 0.4 and smoothness is 1.0. Boundary conditions are Dirichlet. 15 Published as a conference paper at ICLR 2026 

Reduced Spatiotemporal Resolution. When simulating the original 2D incompressible Navier-Stokes equation at low spatiotemporal grids (yellow curves in Figure 9), we reduce the spatial resolution from 256 Ã— 256 to 100 Ã— 100 . We then spatially upsample to 256 Ã— 256 via bilinear interpolation to match the resolution of simulations of the original PDE. To reduce the temporal resolution while maintaining the same total simulation time and number of recorded frames, we increase the time-step size and proportionally reduce the number of integration steps and output interval. Specifically, we change the time-step from dt = 5 Ã— 10 âˆ’5 to dt = 5 Ã— 10 âˆ’4, and reduce the total number of time steps from nsteps = 100 ,000 to nsteps = 10 ,000 . To preserve the temporal spacing between output frames, we decrease the frame interval from 100 to 10. This ensures the same total simulation duration of 5 seconds and the same number of output frames (1,000). This modification reduces computational cost by roughly 10 times. A.3 3D I NCOMPRESSIBLE NAVIER -S TOKES 

Our solver is PhiFlow (Holl & Thuerey, 2024). We simulate in a spatial domain of Î© = [0 , 1] 3, with resolution 50 Ã— 50 Ã— 89 . We simulate 150 steps with a dt = 2 Ã— 10 âˆ’4. We set the initial u0 as zero and upward buoyancy forcing term fz = 5 Ã— 10 âˆ’4. Unlike the 2D Navier-Stokes, we introduce randomness of the buoyancy forcing term on horizontal directions by uniformly drawing fx fx from 

[âˆ’1, 1] Ã— 10 âˆ’4. We set Dirichlet zero boundary conditions. A.4 1D K URAMOTO â€“S IVASHINSKY 

Our simulation setting for the one-dimensional Kuramotoâ€“Sivashinsky equation follows (Papageor-giou & Smyrlis, 1991; Gudorf & Cvitanovic, 2019). Our solver is a pseudospectral exponential time-differencing fourth-order Rungeâ€“Kutta scheme, implemented with 64 contour points. We use periodic boundary conditions on the spatial domain Î© = [0 , L ] with L = 64 Ï€, discretized with equispaced s = 512 grid points. Spatial derivatives are evaluated spectrally via FFT (fast Fourier transform), and nonlinear terms are treated in physical space through pseudospectral squaring and differentiation. We simulate up to the final time T = 30 , using an internal time step dt = 0 .1, and record nsteps = 200 snapshots uniformly in time. 

## B MODEL STRUCTURE Input 

> (PDE)
> Linear
> Fourier Layer
> FFT
> Linear
> IFFT
> Linear
> GeLU
> 4âœ•
> Linear
> GeLU
> Linear
> Output
> (PDE)
> FFT: Fourier Transform
> IFFT: Inverse Fourier Transform
> FNO
> +
> Conv3D
> Positional Embedding
> Tokens
> Encoder Transformer Block
> (width=768, #heads = 12)
> Linear
> Decoder Transformer Block
> (width=512, #heads = 8)
> 12 âœ•
> 8âœ•
> Output
> (PDE)
> Input
> (PDE)
> VideoMAE
> Output
> (Basic Form)
> Output
> (Basic Form)
> Input
> (Basic Form)
> Input
> (Basic Form)
> Shared
> Linear
> Shared

Figure 8: Our method is agnostic to specific architectures of neural operators: we always share the backbone of the model between learning the original PDE and its basic form, and decouple their predictions in the last layer. 

We mainly consider the Fourier Neu-ral Operator and the transformer in this study. The model structures are shown in Figure 8. The basic form and the original PDE share all layers but the last prediction layer, which is agnostic to specific architectures of neural operators. This task-specific output layer is a very well-known and widely adopted configuration in multi-task learning. It has been high-lighted in survey papers that hard pa-rameter sharing (one input, shared hidden layers, multiple outputs) is the standard setup for multi-task learning due to its efficiency and representa-tional benefits (Ruder, 2017; Craw-shaw, 2020). 

## C MORE IMPLEMENTATION DETAILS 

We summarize our training details in Table 4. We conducted our experiments on NVIDIA RTX 6000 Ada GPUs, each with 48 GB of memory. 16 Published as a conference paper at ICLR 2026 

Table 4: Training details. â€œDRâ€: Diffusion-Reaction.â€œNSâ€: Navier-Stokes.â€œKSâ€: Kuramotoâ€“Sivashinsky.                                                                                                          

> 2D DR (FNO) 2D DR (Transformer) 2D NS (FNO) 2D NS (Transformer) 3D NS (FNO) 3D NS (Transformer) 1D KS (FNO) Input Shape Format HÃ—WÃ—TÃ—C (C = 2) HÃ—WÃ—TÃ—C (C = 3) XÃ—YÃ—ZÃ—TÃ—C (C = 4) TÃ—SNumber of Training Samples (PDE Simulations) 2, 4, 8, 16, 32, 64, 128 2, 4, 8, 16, 32, 48, 64 2, 4, 8, 16, 32, 48, 64 1024 âˆ¼8192 Input Time Steps ( â„“in Section 2.1) 10 10 10 10 10 10 10 Sample Mixture Ratio 1:3 1:3 1:24 1:6 1:3 1:3 1:12 Learning Rate 0.001 0.0003 0.001 0.001 0.001 0.00015 0.001 Batch Size for Primary Data 4816 16 8864 Epochs 100 60 20 30 20 80 50 Auxiliary Task Loss Weight 0.7 0.7 0.7 0.7 0.7 0.7 0.7 Training Hours 0.08 âˆ¼1.83 0.6hr âˆ¼7hr 1âˆ¼29 1.5 âˆ¼45 0.5 âˆ¼6.5 16 âˆ¼120 2âˆ¼18 Gradient Descent Steps Per Epoch 46 âˆ¼2912 23 âˆ¼1456 124 âˆ¼3960 124 âˆ¼3960 70 âˆ¼2240 70 âˆ¼2240 3040 âˆ¼24320 (Baseline and Ours)

## D MORE RESULTS 

Beyond the main comparison between the baseline and our proposed model using FNO showed in Section 4, we further conducted additional experiments to assess our approach. In this section, we will show the results from both FNO and Transformer. D.1 COMPARISON OF DATA EFFICIENCY AND OUT -OF -D ISTRIBUTION GENERALIZATION AGAINST SPATIOTEMPORAL DOWNSAMPLING 

To demonstrate the effectiveness of our method, we conducted an ablation study comparing it with Spatiotemporal Downsampling on both 2D Diffusionâ€“Reaction (DR) and 2D Navierâ€“Stokes (NS). We defined the Spatiotemporal Downsampling method as follows: Neural operators that are trained with a mixture of simulations of the original PDE and simulations at low spatial and temporal resolutions (then linearly interpolated to the original resolution). Similar to our method, we can also save simulation costs with reduced spatiotemporal resolutions. See Appendix A for rates of downsampling and more details. Meanwhile, as our decomposed basic form is orthogonal to spatiotemporal downsampling during simulations, our method can serve as a complementary data augmentation. On 2D Diffusion-Reaction, we can simulate our decomposed basic forms at lower spatiotemporal resolution, leading to further reduced simulation costs and improved data efficiency (green triangle), outperforming the baseline at the lower spatiotemporal resolution (yellow diamond). For 2D Navierâ€“Stokes with the Transformer, our method still outperforms both the baseline and spatiotemporal downsampling at comparable cost. 10 1 10 2

Simulation Costs (Seconds) 

> 0.02
> 0.04
> 0.06
> 0.08
> 0.10
> 0.12
> Normalized RMSE

2D Diffusion-Reaction 

(FNO)  

> Baseline
> Baseline@Spatiotemporal
> Ours
> Ours@Spatiotemporal 10 410 5

Simulation Costs (Seconds) 

> 0.05
> 0.10
> 0.15
> Normalized RMSE

2D Incompressible Navier Stokes 

(FNO)  

> Baseline
> Baseline@Spatiotemporal
> Ours 10 110 2

Simulation Costs (Seconds) 

> 0.2
> 0.4
> 0.6
> 0.8
> Normalized RMSE

2D Diffusion-Reaction 

(Transformer)  

> Baseline
> Baseline@Spatiotemporal
> Ours
> Ours@Spatiotemporal 10 410 5

Simulation Costs (Seconds) 

> 0.04
> 0.06
> 0.08
> 0.10
> Normalized RMSE

2D Incompressible Navier Stokes 

(Transformer) 

> Baseline
> Baseline@Spatiotemporal
> Ours

Figure 9: Joint training neural operators on data of the original PDE and the basic form improves performance and data efficiency. â€œSpatiotemporalâ€: short for â€œSpatiotemporal Downsamplingâ€. Y-axis: normalized RMSE. X-axis: simulation costs (seconds). 

17 Published as a conference paper at ICLR 2026 Table 5 reports the out-of-distribution (OOD) generalization results across both the 2D Diffusion-Reaction and Navier-Stokes equations. Similar to the results in Table 2, here we can see that our approach not only improves in-distribution errors but also consistently enhances generalization to simulations of unseen physical parameters. This robustness holds across both FNO and Transformer architectures, leading to more reliable and consistent neural operators under varying conditions. 

Table 5: Comparisons of OOD generalization for different training methods with the Transformer. Models are evaluated using the best checkpoints from training in Figure 5, under comparable simulation cost settings. â€œSpatiotemporalâ€œ: short for â€œSpatiotemporal Downsamplingâ€.                                                    

> PDE Model Source Target 1 Target 2 Setting nRMSE Setting nRMSE Setting nRMSE Diffusion-Reaction (2D, FNO) Baseline
> Dv
> Du= 5
> 0.0289
> Dv
> Du= 1
> 0.0413
> Dv
> Du= 100
> 0.0770 Baseline@Spatiotemporal 0.0234 0.0303 0.0663 Ours 0.0231 0.0331 0.0538
> Ours@Spatiotemporal 0.0218 0.0298 0.0596 Diffusion-Reaction (2D, Transformer) Baseline
> Dv
> Du= 5
> 0.1056
> Dv
> Du= 1
> 0.1249
> Dv
> Du= 100
> 0.1976 Baseline@Spatiotemporal 0.0542 0.0698 0.0812 Ours 0.0602 0.0782 0.0853 Ours@Spatiotemporal 0.0469 0.0489 0.0671
> Navier-Stokes (2D, FNO) Baseline
> Î½= 0 .01
> 0.0487
> Î½= 0 .05
> 0.0825
> Î½= 0 .0001
> 0.0369 Baseline@Spatiotemporal 0.0442 0.0743 0.0269 Ours 0.0175 0.0222 0.0125
> Navier-Stokes (2D, Transformer) Baseline
> Î½= 0 .01
> 0.0479
> Î½= 0 .05
> 0.0853
> Î½= 0 .0001
> 0.0685 Baseline@Spatiotemporal 0.0496 0.0568 0.0402 Ours 0.0265 0.0397 0.0256

D.2 DATA EFFICIENCY AND OUT -OF -D ISTRIBUTION GENERALIZATION OF TRANSFORMER FOR 3D N AVIER -S TOKES 

Similar to what we have studied in Section 4, we aim to also demonstrate three key benefits: data efficiency, long-term physical consistency, and strong generalization in OOD simulations using Transformer, for 3D Navier-Stokes as well. 10 3 10 4

> Simulation Costs (Seconds)
> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> Normalized RMSE
> 3D Incompressible Navier Stokes
> (Transformer)
> Baseline
> Ours

Figure 10: Joint training neural operators on data of the original 3D Navier-Stokes equation and the basic form improves performance and data efficiency. 

In Figure 10, we can see that joint training (orange square) on both the original and basic forms of the 3D Navier-Stokes equation consis-tently reduces normalized RMSE from baseline (blue circle) across varying simulation budgets. This improvement is observed for Transformer architectures, highlighting enhanced data effi-ciency and generalization, which aligns with the results in Section 4.2. In Table 6, we show that our joint train-ing approach significantly improves out-of-distribution generalization on 3D Navier-Stokes across all test settings, outperforming the base-line for both FNO and Transformer models. To-gether with the results in Table 2 and 5, the consistent gains observed across all OOD setting results underscore the effectiveness and robustness of our method in generalizing to previously unseen physical regimes, particularly under significant shifts in simulation parameters. D.3 MORE LONG -T ERM CONSISTENCY RESULTS OF TRANSFORMER 

In our Figure 11, we show the rollout performance of the transformer on the 2D Diffusion-Reaction and 2D incompressible Navier-Stokes equations. Here, we run the experiments with the best checkpoints from training in Figure 9. Losses will be aggregated for five consecutive time steps. 18 Published as a conference paper at ICLR 2026 

Table 6: Comparisons of OOD generalization on 3D NS for different training methods using Transformer. Models are evaluated using the best checkpoints from training in Figure 10. 

PDE Model Source Target 1 Target 2 Setting nRMSE Setting nRMSE Setting nRMSE 3D Navier-Stokes Baseline 

Î½ = 0 .01 

0.0114 

Î½ = 0 .1

0.0327 

Î½ = 0 .0001 

0.0816 Ours 0.0064 0.0124 0.0322 

We can see that our improvements in Figure 9 further persist across autoregressive steps, leading to improved long-term consistency, aligning with our results in Figure 6. 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 

Rollout 

> 0.06
> 0.07
> 0.08
> 0.09
> 0.10
> 0.11
> 0.12
> 0.13
> Normalized RMSE

2D Diffusion-Reaction 

(Transformer) 

Baseline 

Ours 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 

Rollout 

> 0.04
> 0.06
> 0.08
> 0.10
> 0.12
> 0.14
> Normalized RMSE

2D Incompressible Navier Stokes 

(Transformer) 

Baseline 

Ours 

Figure 11: Joint training neural operators on data of the original PDE and the basic form improves performance with autoregressive inference at different unrolled steps using Transformer. Models are evaluated using the best-performing checkpoints from training shown in Figure 9. 

D.4 MORE RANDOM SEEDS 

To ensure the statistical robustness of our findings, we now run FNO using three different random seeds during initialization and training. For each configuration, we report the average performance across the three runs, and include standard deviation as error bars in all plots in Figure 12. This enables a more rigorous evaluation of model performance, capturing the inherent variance and mitigating the risk of overinterpretation from single-seed outcomes. We can see that the results demonstrate that joint training of neural operators on data from both the original PDE and its decomposed basic form yields consistent improvements in predictive performance and data efficiency, highlighting the effectiveness of this multiphysics learning strategy. 10 1 10 2

> Simulation Costs (Seconds)
> 0.02
> 0.04
> 0.06
> 0.08
> 0.10
> 0.12
> Normalized RMSE

2D Diffusion-Reaction 

(FNO)  

> Baseline
> Baseline@Spatiotemporal
> Ours
> Ours@Spatiotemporal 10 410 5
> Simulation Costs (Seconds)
> 0.05
> 0.10
> 0.15
> Normalized RMSE

2D Incompressible Navier Stokes 

(FNO) 

> Baseline
> Baseline@Spatiotemporal
> Ours

Figure 12: Model performance averaged over three random seeds. Joint training neural operators on data of the original PDE and the basic form consistently improves performance and data efficiency. Dash lines indicate the mean performance, with error bars representing standard deviation. Legends align with the descriptions in Section 4.2. Columns: (left) 2D Diffusion-Reaction, (right) 2D Navier-Stokes. Y-axis: nRMSE. X-axis: Simulation Costs (seconds). 

To further demonstrate the reliability and generalizability of our approach, we take the best-performing checkpoints from Figure 12 and evaluate their rollout accuracy as well as their out-of-distribution behavior. From Figure 13 and Table 7, we observe that the standard deviations remain exceptionally 19 Published as a conference paper at ICLR 2026 small, underscoring the stability of the model across different random seeds. The results show that joint training of neural operators on both the original PDE and its decomposed basic form yields consistent improvements in long-term physical consistency and generalizes better to unseen physical dynamics. Together, these findings statistically prove that the proposed multiphysics learning strategy leads to more stable, more reliable, and more physically consistent neural operators. 1 2 3 4 5    

> Rollout
> 0.03
> 0.04
> 0.05
> 0.06
> 0.07
> Normalized MSE
> 2D Diffusion-Reaction
> (FNO)
> Baseline
> Ours 12345
> Rollout
> 0.02
> 0.04
> 0.06
> 0.08
> 0.10
> Normalized MSE
> 2D Incompressible Navier Stokes
> (FNO)
> Baseline
> Ours

Figure 13: Model performance averaged over three random seeds. Joint training neural operators on data of the original PDE and the basic form consistently improves long-term physical consistency. Dash lines indicate the mean performance, with error bars representing standard deviation. Models are evaluated using the best-performing checkpoints from training shown in Figure 12. Table 7: Comparisons of OOD generalization for different training methods averaged over three random seeds. Models are evaluated using the best checkpoints from training in Figure 12, under comparable simulation cost settings. â€œSpatiotemporalâ€œ: short for â€œSpatiotemporal Downsamplingâ€.                               

> PDE Model Source Target 1 Target 2 Setting nRMSE Mean (SD) Setting nRMSE Mean (SD) Setting nRMSE Mean (SD) Diffusion-Reaction (2D, FNO) Baseline
> Dv
> Du= 5
> 0.0287 (0.0006)
> Dv
> Du= 1
> 0.0411 (0.0008)
> Dv
> Du= 100
> 0.0754 (0.0014) Baseline@Spatiotemporal 0.0235 (0.0001) 0.0305 (0.0005) 0.0664 (0.0041) Ours 0.0232 (0.0003) 0.0329 (0.0003) 0.0532 (0.0014)
> Ours@Spatiotemporal 0.0216 (0.0005) 0.0296 (0.0002) 0.0548 (0.0053) Navier-Stokes (2D, FNO) Baseline
> Î½= 0 .01
> 0.0522 (0.0030)
> Î½= 0 .05
> 0.0855 (0.0028)
> Î½= 0 .0001
> 0.0367 (0.0002) Baseline@Spatiotemporal 0.0462 (0.0017) 0.0729 (0.0013) 0.0272 (0.0004) Ours 0.0174 (0.0002) 0.0222 (0.0001) 0.0131 (0.0005)

D.5 LOSS REWEIGHTING 

In Section 3.2, we define our total loss for joint learning of the original PDE ( Loss Full ) and its fundamental physical knowledge (decomposed basic form Loss Basic ) as: Loss = Loss Full + 0 .7 Ã— Loss Basic .

To address the concern regarding the fixed auxiliary loss weight, we conducted an ablation study exam-ining the effect of auxiliary loss weighting in joint training using FNO for the 2D Diffusionâ€“Reaction system. We evaluated three auxiliary weight settings (0.5, 0.7, and 1.0) across the full range of simulation budgets used in the data-efficiency experiments (Figure 5). As shown in Table 8, the normalized RMSE values are highly consistent across all three weight choices. Importantly, the maximum absolute deviation across weights remains below 0.0063 for all simulation costs, with most gaps around 0.002â€“0.004. Although performance can be further improved through exhaustive hyperparameter fine-tuning, the observed variations across auxiliary weights are small relative to the overall error scale and substantially smaller than the performance gains achieved by increasing the training budget, which is trivially beyond the scope of our work. This result demonstrates that the model is largely insensitive to the specific choice of auxiliary weight and suggests that the improvements achieved through joint training are robust with respect to this hyperparameter. Based on this analysis, we fix the auxiliary weight to 0.7 for all the experiments. 20 Published as a conference paper at ICLR 2026 

Table 8: Ablation study on the auxiliary loss weight in joint training using FNO for the 2D Diffusion-Reaction across three settings: auxiliary weights of 0.5, 0.7, and 1.0. Results indicate that our model is robust to the choice of auxiliary loss weighting. Simulation costs are aligned with the ones used in Figure 5 (top-left).                         

> Simulation Costs (Seconds) Weight = 0.5 Weight = 0.7 Weight = 1 3.73 0.1255 0.1233 0.1214 7.46 0.0674 0.0659 0.0636 14.92 0.0508 0.0478 0.0445 29.84 0.0349 0.0330 0.0314 59.68 0.0274 0.0266 0.0249 119.36 0.0245 0.0238 0.0224 238.72 0.0248 0.0236 0.0221

D.6 CHOICE OF THE FUNDAMENTAL TERM 

To demonstrate the importance of the choice of dropping terms, we conduct an ablation study on 2D Diffusion-Reaction and 2D Navier-Stokes. For Diffusion-Reaction, we simulate reaction term instead of the fundamental diffusion term. The simulation cost for reaction-only term is 2.048 Ã— 10 âˆ’3

seconds per step, corresponding to a 1:9 sample mixture ratio when compared to the simulation cost of the original PDE data. For Navier-Stokes, we simulate diffusion term instead of the fundamental advection term. The simulation cost for diffusion-only term is 0.234 seconds per step, corresponding to 1:12 sample mixture ratio, making sure the results comparable. From Figure 14, we can find that in 2D Diffusion-Reaction, keeping the reaction term and removing the fundamental diffusion term will damage the accuracy with up to 64% increase of nRMSE compared to the baseline, while applying the fundamental diffusion term keeps boosting the model performance with 11% to 24% decrease of nRMSE. Similarly, keeping diffusion term but dropping advection term in 2D Navier-Stokes can damage the accuracy up to 13% increase of nRMSE compared to baseline and up to 179% increase compared to the one using the functional advection term. This ablation study confirms that the correct fundamental basic term can improve the data-efficiency when joint training with the original data, and proved that the source of improvement in Figure 5 is clearly from training with the fundamental term itself. 10 1 10 2 

> Simulation Costs (Seconds)
> 0.025
> 0.050
> 0.075
> 0.100
> 0.125
> 0.150
> 0.175
> 0.200
> Normalized RMSE
> 2D Diffusion-Reaction
> (FNO)
> Baseline
> Ours@Diffusion
> Ours@Reaction 10 410 5
> Simulation Costs (Seconds)
> 0.02
> 0.04
> 0.06
> 0.08
> 0.10
> 0.12
> 0.14
> 0.16
> Normalized RMSE
> 2D Incompressible Navier Stokes
> (FNO)
> Baseline
> Ours@Advection
> Ours@Diffusion

Figure 14: Ablation study of joint training neural operators with two different decomposed terms: the fundamental diffusion term, and reaction term in 2D Diffusion-Reaction, and the fundamental advection term and diffusion term in 2D Navier-Stokes, shows the importance of choice on fundamental terms from PDE equation. Y-axis: nRMSE. X-axis: Simulation Costs (Seconds). 

To further clarify that the choice of fundamental term is not ad hoc, we also provide a principled and quantitative criterion for determining which decomposed components should be retained. For each candidate fundamental operator, we compute the RMSE between the full PDE simulation and the corresponding decomposed simulation over the entire simulation trajectory, using identical initial conditions to ensure a fair comparison. This RMSE here serves as a distance metric that quantifies how much each component contributes to the overall dynamics. In 2D Diffusionâ€“Reaction, the RMSE between the full equation and the diffusion-only simulation is 0.553, whereas the RMSE with the reaction-only simulation is much larger at 5.821, which further confirms the results of the qualitative visualization of the original PDE and its two decomposed term we obtained in Figure 15. 21 Published as a conference paper at ICLR 2026 t = 2.5 t = 5 Initial 

> PDE Basic Form Residual
> (Non -Basic Form)

Figure 15: Qualitative visualization of the original PDE and its decomposed forms for 2D Diffusion-Reaction. This results combined with the quantitative results obtained from the RMSE experiment demonstrate that our choice of fundamental term is principled. 

D.7 DPOT (H AO ET AL ., 2024) ON 2D D IFFUSION -R EACTION 100 200 300 400 

> Simulation Costs (Seconds)
> 0.002
> 0.004
> 0.006
> 0.008
> Normalized RMSE
> 2D Diffusion-Reaction
> (DPOT)
> Baseline
> Ours

Figure 16: Data efficiency on 2D Diffusionâ€“Reaction using the DPOT foundation model, suggesting joint train-ing on the original and basic form yields consistently lower error across simulation-cost budgets 

To further demonstrate the scalability of our method to larger models and datasets, we addi-tionally evaluate our method using DPOT (Hao et al., 2024), which is a state-of-the-art founda-tion model designed specifically for large-scale PDE pretraining. We use the publicly released medium-sized DPOT checkpoint. To adapt our method structure, we applied the model with the task-specific output layers, which contains 124M model parameters. This represents a sub-stantial increase in scale compared to the 0.46M-parameter FNO used in our primary experiments described in Appendix B. We also increase our fine-tuning sample size from 2â€“128 (See Ta-ble 4) to 32â€“256 for DPOT. We then fine-tune DPOT under the same simulation-cost budget for both baseline and our method. Same as the setting in Section 4, we apply our method by reallocating half of the simulation budget to generate auxiliary data from the decomposed basic PDE forms based on the sample mixture ratio defined in Table 1. All experiments use the default DPOT fine-tuning configuration, and apply the same optimization cost (number of gradient descent steps) for both baseline and our method, ensuring a fair and consistent comparison. As shown in Figure 16, our approach continues to improve data efficiency and achieves consistently lower nRMSE compared to fine-tuning DPOT on original PDE data alone across all simulation-cost regimes. Notably, these gains persist despite DPOTâ€™s significantly larger capacity and extensive pretraining, demonstrating that our multiphysics auxiliary tasks provide benefits beyond what is already captured by large-scale foundation pretraining. 22 Published as a conference paper at ICLR 2026 D.8 LIE TRANSFORM ARGUMENT ON 2D I MCOMPRESSIBLE NAVIER -S TOKES 

Lie symmetries offer a way to generate new, physically valid training examples by exploiting the analytic group transformations that map one PDE solution to another. This enables the model to learn representations that are inherently equivariant to fundamental symmetries such as translation, rotation, and scaling. To further prove the strength of our model, we leverage the implementation of Lie point symmetry augmentation from (Brandstetter et al., 2022; Mialon et al., 2023), which is orthogonal to our multiphysics joint training approach, to 2D incompressible Navier Strokes equation. 10 4 10 5

> Simulation Costs (Seconds)
> 0.05
> 0.10
> 0.15
> Normalized RMSE
> 2D Incompressible Navier Stokes
> (FNO, Lie Transform Augmentation)
> Baseline
> Baseline@LieTransformAug
> Ours
> Ours@LieTransformAug

Figure 17: Joint training neural operators on data of the original PDE and the basic form, as a complementary data augmentation orthogonal to Lie-transform augumentation, can further improve performance and data efficiency. Y-axis: normalized RMSE. X-axis: simulation costs (seconds). 

We incorporate the augmentation process into our model. We only apply Lie-transform augmentations exclusively to the velocity ( u) of the original 2D incompress-ible Navier-Stokes, leaving the remaining density and all target variables from the de-composed basic forms unchanged. Follow-ing (Mialon et al., 2023), the Lie transfor-mation is implemented with a second-order Lieâ€“Trotter splitting scheme with two steps, where the five fields (x, y, t, ux, uy ) were transformed in accordance with the sampled generator strengths as follows: a maximum time shift ( g1) strength of 0.1, maximum spa-tial translations ( g2, g3) strength of 0.1 in x

and y respectively, a maximum scaling ( g4)strength of 0.05, a maximum rotation ( g5)strength of 10 â—¦, corresponding to Ï€/ 18 radians, a maximum x-linear boost ( g6) and y-linear boost (g7) strength of 0.2 and a maximum x- and y-quadratic boosts ( g8, g9) strength of 0.05. As our decomposed basic form is orthogonal to the Lie point symmetry augmentation, our method can serve as a complementary data augmentation. In Figure 17, we study prediction errors (nRMSE) of neural operators trained with different numbers of training samples (simulations). As we have already seen (Figure 5), our approach (orange square) significantly outperforms the baseline (blue circle). In contrast, the Lie-transform augmentation alone (yellow diamond) only marginally improves the baseline. As a result, combining our approach with Lie transformations (green triangle) yields strong performance, but is comparable with our approach alone, underscoring the orthogonal and complementary benefits of these two techniques. D.9 VISUALIZATION OF PREDICTIONS 

To show the predicted PDE solution from our jointly training neural operators on original PDE equation and its basic form aligns with the ground truth, we present qualitative visualizations of model predictions across three PDEs, 2D Diffusion-Reaction, 2D and 3D Incompressible Navier-Stokes, and 1D Kuramotoâ€“Sivashinsky, in Figure 18. For the first three cases, the initial state and predicted states at intermediate and final rollout times are shown. For 1D Kuramotoâ€“Sivashinsky, we show the whole predicted trajectory here. The predictions are generated using the FNO model trained with our joint training framework. Across all systems and time points, the predictions closely align with the expected dynamics, accurately capturing both spatial patterns and temporal evolution. These visualizations highlight the modelâ€™s capacity to generalize across scales and exhibit physically coherent behavior. 23 Published as a conference paper at ICLR 2026 Diffusion -Reaction      

> t = 2.5 t = 5
> Prediction (FNO)
> Initial t = 2.5 t = 5
> Prediction (FNO)
> Navier -Stokes (2D)
> Initial t = 1.5 t = 3
> Prediction (FNO)
> Navier -Stokes (3D) Kuramoto -Sivashinsky  (1D)
> Prediction (FNO)
> Ground Truth
> Initial

Figure 18: Qualitative visualization of model predictions for 2D Diffusion-Reaction, 2D Incompressible Navier-Stokes, 3D Incompressible Navier-Stokes systems and 1D Kuramotoâ€“Sivashinsky using FNO trained with our joint framework. For the first three cases, the initial state and predicted states at intermediate and final rollout times are shown. For 1D Kuramotoâ€“Sivashinsky, we show the whole predicted trajectory here. The results demonstrate accurate temporal evolution and spatial coherence. 

24