---
title: "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation"
title_zh: SPARC：面向自动化 C 语言单元测试生成的场景规划与推理
authors: "Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jabbarvand"
date: 2026-02-18
pdf: "https://arxiv.org/pdf/2602.16671v1"
tags: ["query:sr"]
score: 6.0
evidence: 用于代码生成的神经符号框架
tldr: 针对C语言单元测试生成中LLM易出现“跳跃式代码生成”导致编译失败和覆盖率低的问题，本文提出SPARC框架。该框架采用神经符号方法，通过控制流图分析、操作映射、路径定向合成及迭代自修复四个阶段，将LLM推理与程序结构深度对齐。实验表明，SPARC在行覆盖率和变异得分上显著优于基准模型，且生成的测试代码具有极高的可读性，为工业级遗留C代码测试提供了高效且可扩展的解决方案。
motivation: 大型语言模型在直接生成C语言测试代码时，常因忽略指针运算和内存管理等复杂约束而导致代码无法编译或覆盖率低下。
method: 提出一种包含CFG分析、验证工具映射、路径定向合成及基于编译器反馈的迭代自修复四阶段神经符号框架。
result: "在59个实验对象中，SPARC的行覆盖率提升了31.36%，变异得分提升20.78%，性能达到或超过了符号执行工具KLEE。"
conclusion: 通过将LLM推理与程序结构对齐，SPARC有效解决了C语言自动化测试生成的可靠性与可维护性挑战。
---

## 摘要
由于高层程序意图与指针算术及手动内存管理等严格语法约束之间的语义鸿沟，C 语言的自动化单元测试生成仍然是一项艰巨的挑战。尽管大语言模型（LLMs）展现出强大的生成能力，但直接的“意图到代码”合成经常面临“跳跃到代码”（leap-to-code）的失败模式，即模型在没有立足于程序结构、约束和语义的情况下过早地输出代码。这会导致测试无法编译、虚构函数签名、低分支覆盖率以及无法正确捕获漏洞的语义无关断言。我们提出了 SPARC，这是一个基于场景的神经符号框架，通过四个阶段弥合这一鸿沟：(1) 控制流图（CFG）分析，(2) 将 LLM 推理立足于经过验证的实用辅助程序的“操作图”（Operation Map），(3) 针对特定路径的测试合成，以及 (4) 使用编译器和运行时反馈的迭代自校正验证循环。我们在 59 个真实世界和算法主题上对 SPARC 进行了评估，结果显示其在行覆盖率上比原始提示词生成基线高出 31.36%，在分支覆盖率上高出 26.01%，在变异得分上高出 20.78%，在复杂主题上达到或超过了符号执行工具 KLEE。SPARC 通过迭代修复保留了 94.3% 的测试，并生成了开发者评分中可读性和可维护性显著更高的代码。通过将 LLM 推理与程序结构对齐，SPARC 为遗留 C 代码库的工业级测试提供了一条可扩展的路径。

## Abstract
Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.