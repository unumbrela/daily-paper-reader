---
title: Continuous-Time Piecewise-Linear Recurrent Neural Networks
title_zh: 连续时间分段线性循环神经网络
authors: "Alena Brändle, Lukas Eisenmann, Florian Götz, Daniel Durstewitz"
date: 2026-02-17
pdf: "https://arxiv.org/pdf/2602.15649v1"
tags: ["query:sr"]
score: 6.0
evidence: 用于科学发现和动力系统的机械可解释模型
tldr: 动力系统重建（DSR）旨在从观测数据中恢复底层生成模型。虽然离散时间分段线性循环神经网络（PLRNN）在可解释性上表现优异，但难以处理物理世界中的连续时间过程和不规则采样数据。本文提出连续时间分段线性循环神经网络（cPLRNN），利用其分段线性结构开发了无需数值积分的高效训练与仿真算法。该模型不仅能半解析地确定平衡点等拓扑特征，在处理具有不连续性的复杂动力系统时，其性能也优于传统的离散PLRNN和神经常微分方程（Neural ODEs）。
motivation: 现有的离散时间PLRNN无法很好地适配连续时间物理过程及不规则采样数据，而Neural ODEs在动力系统重建性能和数学可分析性上存在局限。
method: 提出一种利用分段线性结构绕过数值积分的cPLRNN训练与仿真算法，并实现了平衡点和极限环等拓扑对象的半解析确定方法。
result: 在动力系统重建基准测试中，cPLRNN在处理包含硬阈值和不连续性的系统时，其表现显著优于离散PLRNN和Neural ODEs。
conclusion: cPLRNN成功结合了连续时间建模的灵活性与分段线性结构的数学可分析性，为科学和医学领域的动力系统重建提供了高效且可解释的工具。
---

## 摘要
在动力系统重建（DSR）中，我们的目标是从观测到的时间序列中恢复其背后的动力系统（DS）。具体而言，我们的目标是学习一个生成式代理模型，该模型能够逼近底层的、生成数据的动力系统，并重现其长期特性（“气候统计”）。特别是在科学和医学领域，这些模型需要具备机制上的可分析性——通过对其进行数学分析，我们希望能够深入了解所恢复系统的运行机制。基于 ReLU 的分段线性（PL）循环神经网络（PLRNNs）在这方面表现出色，它们代表了最先进的 DSR 模型，同时凭借其分段线性设计提供了数学洞察力。然而，目前所有的 PLRNN 变体都是离散时间映射。这与大多数物理和生物过程所假设的连续时间性质不符，且难以处理以不规则时间间隔到达的数据。神经常微分方程（Neural ODEs）是一种解决方案，但它们的 DSR 性能不及 PLRNNs，且往往缺乏后者的可分析性。在本文中，我们开发了连续时间 PLRNNs（cPLRNNs）的理论：我们提出了一种用于训练和模拟此类模型的新算法，通过高效利用其分段线性结构来绕过数值积分。我们进一步展示了如何在训练好的模型中以半解析的方式确定平衡点或极限环等重要的拓扑对象。我们在 DSR 基准测试中将 cPLRNNs 与其离散时间版本以及 Neural ODEs 进行了比较，测试对象包括具有硬阈值导致的不连续性的系统。

## Abstract
In dynamical systems reconstruction (DSR) we aim to recover the dynamical system (DS) underlying observed time series. Specifically, we aim to learn a generative surrogate model which approximates the underlying, data-generating DS, and recreates its long-term properties (`climate statistics'). In scientific and medical areas, in particular, these models need to be mechanistically tractable -- through their mathematical analysis we would like to obtain insight into the recovered system's workings. Piecewise-linear (PL), ReLU-based RNNs (PLRNNs) have a strong track-record in this regard, representing SOTA DSR models while allowing mathematical insight by virtue of their PL design. However, all current PLRNN variants are discrete-time maps. This is in disaccord with the assumed continuous-time nature of most physical and biological processes, and makes it hard to accommodate data arriving at irregular temporal intervals. Neural ODEs are one solution, but they do not reach the DSR performance of PLRNNs and often lack their tractability. Here we develop theory for continuous-time PLRNNs (cPLRNNs): We present a novel algorithm for training and simulating such models, bypassing numerical integration by efficiently exploiting their PL structure. We further demonstrate how important topological objects like equilibria or limit cycles can be determined semi-analytically in trained models. We compare cPLRNNs to both their discrete-time cousins as well as Neural ODEs on DSR benchmarks, including systems with discontinuities which come with hard thresholds.