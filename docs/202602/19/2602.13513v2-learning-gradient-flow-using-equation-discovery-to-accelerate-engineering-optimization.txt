Title: Learning Gradient Flow: Using Equation Discovery to Accelerate Engineering Optimization

URL Source: https://arxiv.org/pdf/2602.13513v2

Published Time: Thu, 19 Feb 2026 01:11:02 GMT

Number of Pages: 44

Markdown Content:
# LEARNING GRADIENT FLOW : USING EQUATION DISCOVERY TO ACCELERATE ENGINEERING OPTIMIZATION 

Grant Norman ∗

Smead Aerospace Engineering Sciences 3775 Discovery Drive Boulder, CO 80309 

grant.norman@colorado.edu 

Conor Rowan ∗

Smead Aerospace Engineering Sciences 3775 Discovery Drive Boulder, CO 80309 

conor.rowan@colorado.edu 

Kurt Maute 

Smead Aerospace Engineering Sciences 3775 Discovery Drive Boulder, CO 80309 

kurt.maute@colorado.edu 

Alireza Doostan 

Smead Aerospace Engineering Sciences 3775 Discovery Drive Boulder, CO 80309 

alireza.doostan@colorado.edu 

# ABSTRACT 

In this work, we investigate the use of data-driven equation discovery for dynamical systems to model and forecast continuous-time dynamics of unconstrained optimization problems. To avoid expensive evaluations of the objective function and its gradient, we leverage trajectory data on the optimization variables to learn the continuous-time dynamics associated with gradient descent, Newton’s method, and ADAM optimization. The discovered gradient flows are then solved as a surrogate for the original optimization problem. To this end, we introduce the Learned Gradient Flow (LGF) optimizer, which is equipped to build surrogate models of variable polynomial order in full- or reduced-dimensional spaces at user-defined intervals in the optimization process. We demonstrate the efficacy of this approach on several standard problems from engineering mechanics and scientific machine learning, including two inverse problems, structural topology optimization, and two forward solves with different discretizations. Our results suggest that the learned gradient flows can significantly expedite convergence by capturing critical features of the optimization trajectory while avoiding expensive evaluations of the objective and its gradient. 

Keywords Equation discovery · Data-driven modeling · Gradient flow · Surrogate models · Optimization 

> ∗

These authors contributed equally. 

> arXiv:2602.13513v2 [math.OC] 17 Feb 2026

# 1 Introduction 

Optimization is a pillar of engineering design and analysis, as well as computational science writ large. As traditionally presented, optimization problems entail finding a set of inputs, or optimization variables, to a scalar objective function which corresponds to a minimum value of that function [ 69 ]. This simple, yet powerful idea finds use in diverse applications across disciplines: partial differential equation (PDE) solutions, eigenvalue problems, optimal control, engineering design, inverse problems, neural network training, and many other tasks in scientific computing; see, e.g., [ 27 , 67 , 13 , 82 , 61 , 81 ]. Because most optimization problems do not have analytical solutions, a common numerical approach is to start with an initial guess and to iteratively evolve the optimization variables in a direction that is expected to decrease the value of the objective. Different optimization algorithms use different techniques to estimate a direction of decreasing objective value. With gradient descent, the gradient of the objective function at the current state of the optimization variables defines the update direction. With a Newton-type optimization scheme, a quadratic approximation of the objective is built around the current iterate, and the update is taken in the direction of the minimum of this quadratic approximation [ 69 ]. In the machine learning community, there are many variants of gradient descent, such as the now-famous “ADAM” algorithm [ 39 ], which leverages first- and second-moment estimates of past gradients to scale the update direction and improve convergence. All of these algorithms have a notion of an optimal search direction and a “step size” or “learning rate,” which controls the size of the update to the optimization variables at each iteration. Note that a systematic comparison of some popular second-order optimizers was recently conducted in [ 42 ]. While many optimization strategies do not require gradient information [ 46 ], we restrict our attention to gradient-based optimization. In the limit as the step size approaches zero, a continuous-time system of ordinary differential equations for the optimization variables is obtained. For example, gradient descent updates resemble a forward Euler strategy for time integrating the optimization variables. Thus, in the continuous time limit of gradient descent, the optimization problem corresponds to “gradient flow” of the objective function [ 48 ]. Stable attractors of this dynamical system are solutions to the optimization problem, and the standard discrete-time gradient descent scheme is returned when the time derivative is approximated with finite differencing. Noting this equivalence between optimization and dynamical systems, many authors have used convergence, stability, and control analyses from the study of ordinary differential equations (ODEs) in order to understand the properties of optimization algorithms. The neural tangent kernel (NTK)—a continuous time formulation of neural network training that linearizes the network parameters around the origin—has been studied extensively to understand the dynamics of training [ 35 ]. In [ 45 ], the authors extend the NTK analysis of training dynamics to parameters that are far from the origin. With the help of continuous time gradient flows, convergence guarantees for training neural networks with rectified linear unit (ReLU) activation functions were given in [ 36 ]. An analysis of the stochastic differential equation corresponding to mini-batched gradient descent is performed in [ 59 , 47 ]to show convergence properties for certain non-convex objective functions. Various stability analyses of neural network training have been conducted using the gradient flow dynamics [ 65 , 32 ]. Drawing from control theory, the authors in 2[22 ] use a proportional-integral-derivative controller on the gradient flow dynamics in order to more aggressively explore the objective landscape. The analysis of continuous time gradient flow is extended to the case of equality-constrained objectives in [ 19 ]. Furthermore, the authors in [ 43 ] show that optimization algorithms with history dependence (e.g., momentum) can also be understood in continuous time. Furthermore, the Nesterov accelerated gradient method can be shown to have a continuous time analogue, which provides insight into its behavior by establishing a clear connection to physical dynamical systems [ 71 ]. For a comprehensive perspective on the connection between accelerated and stochastic optimization and dissipative systems, the reader is referred to [ 31 ]. By way of example, the continuous time Nesterov method has been shown to improve the training of autoencoders [ 30 ]. Similarly, continuous time gradient flows are used in the volume reconstruction literature in order to find minima of objective functionals penalizing the curvature or surface area of the reconstructed volume [11, 12]. The continuous time perspective offers a useful connection between dynamical systems and optimization, but does not offer a remedy for the inherent cost of gradient-based optimization. All such methods require computing at least the objective and its gradient for every iteration, and they may take hundreds or thousands of iterations to converge. In the context of engineering optimization, this often necessitates repeated solves of large-scale PDE systems. Thus, owing to this high computational cost, there is significant interest in devising techniques to accelerate optimization problems. Several strategies have been proposed to this end. Surrogate modeling is a popular technique to replace expensive aspects of the optimization process with cheap-to-evaluate mathematical functions that are fit from data. Some surrogate models replace the entire optimization process, going directly from inputs to the optimization problem to the final solution. For example, in [ 3], a whole class of inverse problems can be solved efficiently by a surrogate that learns the mapping between an output field and the corresponding material property. Similarly, works such as [ 6, 33 ]aim to replace an entire structural topology optimization pipeline with a trained surrogate that maps problem parameters to the final design. Other surrogate models approximate the dependence of the loss on the optimization variables and use this approximation to compute gradients. Though Gaussian process surrogates (sometimes called Kriging models) are usually gradient-free, in [ 83 ], a Kriging surrogate for the objective function is augmented with gradient information around observation points in the context of aerodynamic shape optimization. In [ 5], neural networks are used to approximate the dependence of the loss on the design parameters in order to avoid expensive online forward solves during optimization. We note that even standard second-order optimization methods, such as Newton and quasi-Newton methods, can be conceptualized as constructing local surrogate models of the objective in order to expedite convergence. In a standard Newton approach to finding the stationary point of an objective function, a quadratic surrogate of the objective is made around the current state of the design variables and is used to determine optimal step sizes and directions [ 69 ]. Similarly, with quasi-Newton methods such as BFGS, data from the optimization history is used on an ongoing basis to estimate this quadratic approximation [ 58 ]. Though not typically conceptualized in this way, we suggest that even quasi-Newton methods somewhat resemble data-driven surrogates. When the surrogate model approximates a mapping from one function space to another, “operator learning” frameworks such as the Deep Operator Network (DeepONet, [ 53 ]), Fourier Neural Operators (FNO, [ 50 ]), or the Laplace Neural 3Operator (LNO, [ 17 ]) are often employed. For example, pre-trained surrogate models of this type can be used to make informed initializations for optimization-based PDE solvers, which dramatically decrease the number of iterations to convergence [ 2, 1, 29 ]. An alternative to reducing the number of steps to convergence is to reduce the cost per iteration. On this front, surrogate models have been used to approximate gradients of the loss function based on data seen in an offline training stage. A non-intrusive approach to approximating gradients in the context of inverse problems is proposed in [ 77 ]. Other examples of data-driven surrogates replacing expensive gradient computations are given in [ 60 , 79 ]. Some surrogates for objective gradients have been designed to learn gradient information online, reducing the overhead cost of offline training [78]. While pre-trained surrogates can reduce the number of optimization steps with informed initializations, another strategy to improve performance—both in the sense of the number of steps to converge and the quality of the converged solution—is to devise better optimization algorithms. For example, the popular ADAM algorithm has shown that a carefully constructed update strategy can lead to improved performance even on highly non-convex and stochastic objectives [ 39 ]. Historically, optimization algorithms have been designed and tuned manually. Recognizing this, a number of authors have explored data-driven automation of the design of optimization algorithms. In the “Learning to Optimize” framework, the iterative update to the optimization variables is represented as a neural network with inputs given by the objective function and its derivatives [ 49 ]. Reinforcement learning or gradient-based meta-learning is used to learn the optimal update strategy based on optimization dynamics observed from a given class of problems. This methodology was extended and validated in a series of follow-up works [ 23 , 80 ]. A similar strategy is proposed in [ 4], where the optimization update rule is parameterized and learned for a class of problems. A parallel line of work, ostensibly unrelated to optimization, is the discovery of dynamical systems from data using symbolic regression and equation discovery. Early work on the discovery of laws from data using symbolic regression dates back two decades [ 8], but gained traction in 2016 with the Sparse Identification of Nonlinear Dynamics (SINDy) framework for equation discovery [ 14 ]. SINDy takes the measured trajectory of a system governed by a nonlinear system of ordinary differential equations and returns a sparse approximation of its dynamics. We note that equation discovery uses sparse regression techniques to find an optimal approximation of the system dynamics, whereas symbolic regression uses genetic algorithms to search through a large space of symbolic expressions. An alternative equation discovery framework to the SINDy methodology is “Neural Ordinary Differential Equations” (Neural ODEs), which represents the unknown dynamics as a neural network taking in the system state and its derivatives [ 20 ]. This method has also been studied in a variety of contexts, such as model reduction [ 15 ], fluid mechanics [ 75 ], and reactor systems [ 70 ]. We remark that Gaussian process regression has also been used for the discovery of governing equations by providing uncertainty-aware estimates of the system state and its derivatives, which can then be used to identify terms in a differential equation [ 62 ]. In this work, our focus will be on equation discovery with SINDy, though the methods reviewed above could be used as well. The SINDy methodology has been successfully applied to problems in biology [ 54 ], to systems with noisy measurements [ 56 , 34 , 76 ], and to systems whose dynamics are describable in terms of low-dimensional subspaces [18]. 4Once the connection between optimization and dynamical systems is established, obtaining a surrogate model for the gradients of the objective becomes equivalent to learning a dynamical system. In this work, we aim to bring together the dynamical perspective on optimization, the need for surrogate models to accelerate optimization problems, and ideas from data-driven non-linear equation discovery, here SINDy. Namely, we apply a SINDy-inspired approach while training in order to learn the dynamics of optimization based on past observations of the loss function and its gradient. This SINDy model, built from a user-defined subset of the optimization history, can then be used as a surrogate for the full loss and gradient evaluations to forecast a user-defined number of steps. Our contributions can be broken down as follows: 1. We propose the use of SINDy-based surrogates of the continuous-time dynamics of optimization, 2. We extend this methodology to build surrogates for gradient descent, Newton, and ADAM optimization, 3. We develop a PyTorch optimizer that builds SINDy models from the optimization trajectory data in real time, 4. We showcase the ability of our method to accelerate optimization on five standard problems from engineering optimization and scientific machine learning. The rest of this paper is organized as follows. In Section 2, we introduce the SINDy methodology and discuss how it can be adapted to be used online as a surrogate for the optimization dynamics. In Section 3, we describe our custom optimization framework called Learned Gradient Flow (LGF). In Section 4, we present five numerical examples designed to cover a range of problems from engineering mechanics, including i) linear and nonlinear physics, ii) spectral and finite element discretizations, and iii) forward solves, inverse problems, and topology optimization. In Section 5, we close with concluding remarks and propose directions for future work. 

# 2 Background 

2.1 Modeling ODEs through SINDy 

Here, we introduce notation and basic details for SINDy [ 14 ]. As previously mentioned, SINDy methods construct models for ODE systems based on observations of the state, often involving a relatively cheap solve of a convex program. To be more concrete, consider a state vector a(t) ∈ Rn which evolves according to some (potentially unknown) ordinary differential equation 

da

dt = f (a).

Assume we measure this state k times, yielding a measurement matrix 

A =

h

a(t1) a(t2) . . . a(tk)

iT

∈ Rk×n.

5Through either numerical differentiation or access to f itself, assume we also have the time derivative of the measurement matrix, 

˙A =

h da 

> dt

(t1) da 

> dt

(t2) . . . da 

> dt

(tk)

iT

∈ Rk×n.

Consider a candidate library function ϕ : Rn → Rp. Each element of this function corresponds to one of the p candidate functions in a pre-defined library. For instance, denoting the elements of a as a1, a 2, . . . , a n, a candidate function library including up to quadratic polynomial terms would be 

ϕ(a)T =

h

1 a1 a2 . . . an a21 a1a2 . . . a2

> n

i

.

In some instances, prior knowledge is used to select a specific candidate function library (e.g., trigonometry functions), but in this work, we assume only polynomial libraries, including cross terms, up to some total degree P. The total number of library elements can then be computed as 

p =

n + PP



. (1) Repeatedly applying these (polynomial) candidate functions at different time steps gives the library matrix :

Φ(A) = 



ϕ(a(t1)) T

ϕ(a(t2)) T

...

ϕ(a(tk)) T



∈ Rk×p,

SINDy seeks a coefficient matrix Ξ ∈ Rp×n such that 

˙A ≈ Φ(A)Ξ, (2) and Ξ is sparse. In the SINDy literature, much emphasis is placed on this sparsity, since the goal is often to “interpret” the discovered ODE, giving rise to a multitude of methods such as sequentially thresholded least squares, stepwise sparse regression, sparse relaxed regularized regression, and forward regression orthogonal least squares [ 38 ] 2. In the present work, on the other hand, we are not concerned with interpretability, as we will only use the discovered ODE to generate predictions. Thus, we opt for a least squares solution, but note that sparsity-promoting methods may act as a means to regularize the problem of determining Ξ [16], constituting a possible future research direction. Other SINDy works prioritize a weak form of Eq. (2) , shifting the time derivative from the potentially noisy state variables to analytic test functions by applying integration by parts [ 64 , 57 ]. We refer to this class of Galerkin variants broadly as “WSINDy.” Once Ξ is computed by some combination of these aforementioned methods, new predictions  

> 2[66 ] argues that such sparsity is insufficient for interpretability, and that interpretability should instead emphasize mechanistic understanding.

6can be generated by solving the ODE 

da

dt (t) = ΞT ϕ(a(t)) =: ˆf (a(t)) .

2.2 Other methods for modeling ODEs 

While the methods presented in Sec. 3 will most closely rely upon SINDy, we also overview some related work and possible alternatives for learning ODEs from data, for the sake of completeness. Thus, the task is still to learn an approximation ˆf of f such that 

da

dt = ˆf (a(t)) ,

based on the samples of the state variables, A. Rather than prescribing ˆf as a linear expansion of library functions, we could specify ˆf as a neural network, yielding a neural ODE [ 20 ]. As a result of this nonlinear approximation, the parameters of ˆf (previously Ξ) must now be determined through an iterative minimization process. The objective is given as kX

> i=1

a(ti) −

Z ti

> 0

ˆf (a(τ )) d τ

> 22

.

While avoiding numerical differentiation of potentially noisy data, these methods require numerically integrating the state over time. Further, the nonlinear form of ˆf requires an iterative solver, which is often more expensive than finding the coefficients in a linear representation through a direct solve (if we are not concerned with sparsity). Other autoregressive models or flow maps instead directly learn the mapping from the state at one time to the next, thus omitting an explicit numerical integration scheme [ 24 , 10 ]. Denoting such a model as ˆg(a) and repeated applications with an exponent, the equivalent objective function is 

> k

X

> i=1

a(ti) − ˆgi(a(0)) .

While the contributions presented in Sec. 3.1 could be modified to use either Neural ODEs, autoregressive models, or Gaussian processes, we use a technique similar to SINDy due to its reduced computational cost and clearer connection to ODEs. 

2.3 Gradient flow ODE 

Here, we review an idea mentioned in Sec. 1, namely that an ODE system emerges when the learning rate of gradient descent is taken to zero. For a learning rate η > 0, an objective function z : Rn → R, and current optimization variables 

ak at iteration k, gradient descent computes the optimization variables at iteration k + 1 as 

ak+1 = ak − η · ∂∂a z(ak). (3) 7By noting the similarity to the forward Euler derivative approximation and taking η → 0, we have 

da

dt (tk) = − ∂∂a z(a(tk)) , (4) where a(tk) = ak. Thus, rather than updating ak through discrete applications of Eq. (3) , we can introduce a continuous analogue and evolve a(t) according to the ODE in Eq. (4) . The optimization variables throughout optimization iterations correspond to a solution of an ODE over time, where the iteration number and time value are related through the learning rate η: tk = k · η. Motivated by this, we sometimes refer to the optimization variables as the “state variables,” when considering this dynamical systems interpretation 3. We remark that this continuous-time formulation of gradient descent does not allow for constraints on the objective function. Accordingly, we restrict our attention to unconstrained optimization problems through this work. 

# 3 Method 

In this section, we build on ideas from Sec. 2.1 and 2.3 to give the core contribution of this work. 

3.1 Learned gradient flow 

While we can treat an unconstrained optimization problem as a dynamical system, solving this dynamical system for the state is expensive due to the cost of evaluating ∂z/∂ a. Instead, we can record the optimization variables a at various iterations during some standard training procedure (Eq. (3) ) and use SINDy to build a data-driven model for the gradient flow equation, which we name ˆf . Then, rather than integrating the expensive, true gradient equation, we integrate this cheaper surrogate ODE to evolve a over training iterations. In other words, we evolve the optimization variables according to 

da

dt (t) = ˆf (a(t)) , (5) where we construct ˆf as 

ˆf (a) = ΞT ϕ(a(t)) , (6) based on data generated by an initial training phase where we observe only instances of the optimization variables a.That is, we do not record instances of the gradient, although this is an option we later adopt for ADAM in Sec. 3.2.2. 

Remark 1. In the learned gradient flow framework, one could alternatively record gradient evaluations during the data-generating phase and fit ˆf directly to the right-hand side − ∂∂a z(a), in the same spirit as the gradient-modeling step we later adopt for ADAM in Sec. 3.2.2. For gradient descent with fixed step size η, this “tracked-gradient” approach is equivalent to inferring ˙A from just the trajectory, where we estimate ˙A via forward differences (see Eq. (3) and (4) ). While we generally estimate ˙A using centered differences, in Sec. 4.1 we observed consistent behavior using forward, second-order centered, and fourth-order centered finite differences, possibly due to accurate underlying gradients (low     

> 3While the ODE presented in Eq. (4) is first-order and autonomous, other optimization methods can yield different types of ODEs, such as a second-order, non-autonomous system in the case of Nesterov acceleration [ 72 ]. The methods presented in Sec. 3 can be adapted to other ODE forms, without loss of generalization.

8noise). A systematic comparison of these alternatives (which include directly modeling the right-hand side) is beyond the scope of the present work. A successful construction of ˆf (a) leads to the surrogate ˆf (a) ≈ − ∂∂a z(a).Let K be the number of iterations of the standard training procedure for which the optimization variables were recorded. Then, by specifying a total number of evolution iterations M > K for the learned ODE, we also specify the final integration time t = M · η. The larger the difference between M and K, the greater the computational savings. We summarize these core steps in Algorithm 1. For the time integration, we use the torchdiffeq library, generally with the default Dormand-Prince adaptive method ( dopri5 ) [21]. 

Algorithm 1 Learned Gradient Flow 

Require: a1 ▷ initial state 

Require: K ▷ number of true gradient descent iterations/state samples 

Require: M ≥ K ▷ number of total steps, including surrogate “steps” 

Require: η ▷ gradient descent learning rate, or time between state samples  

> 1:

A ← a1 

> 2:

for k = 1 , . . . , K − 1 do ▷ data generating phase  

> 3:

ak+1 ← ak − η · ∂z (ak)/∂ ak ▷ standard gradient descent update  

> 4:

append ak+1 as a column to A 

> 5:

end for  

> 6:

Φ ← Φ(A) 

> 7:

Compute ˙A ▷ or replace with WSINDy  

> 8:

Solve SINDy system for Ξ 

> 9:

Define ˆf (a) = ΞT ϕ(a) ▷ cheap to evaluate  

> 10:

Integrate da 

> dt

= ˆf (a) from t = K · η to t = M · η ▷ use aK as the “initial condition”  

> 11:

return a(tM)

In Sec. 3.4, we discuss these parameters K and M more and report them for the numerical experiments in Sec. 4. Generally, more complicated optimization dynamics (such as a larger a) require more training data to identify (hence a larger K). We use only polynomial library functions for ϕ, and note that for our examples, a low total order suffices. The size of ϕ(a) scales poorly with a and the total polynomial order P (see Eq. (1) ), thus motivating low polynomial orders and/or applications with few design variables. Alg. 1 is written in a general way to accommodate various SINDy methods. In our examples, we use a simple variant where the derivatives of the state variables ( ˙A) are computed via second-order centered finite differences and the coefficients Ξ are determined through sequentially thresholded least squares applied to the fitting problem associated with Eq. (2) . Our solver resembles that of STLSQ in PySINDy 

where α specifies the ℓ2 regularization weight used in ridge regression, and coefficients less than some threshold are removed from the problem after each iteration of the solve. In almost all of the examples we will present in Sec. 4, we set α to 10 −6 and threshold to 10 −8, normalize columns of Φ(A), complete up to 20 iterations ( max_iter = 20 ), and perform a final unregularized solve to avoid returning a biased Ξ (unbias = True ). For these examples, we observe similar results when using vanilla least squares (i.e., α = 0 and threshold = 0). This suggests that, for our applications, solver choices are secondary to the assumption that past optimization dynamics are representative of future dynamics over the prediction horizon. That said, this insensitivity to α and threshold is not expected in general: ridge 9regularization and thresholding can substantially improve the solved Ξ when ˙A is noisy, the library is ill-conditioned, or candidate terms are strongly correlated. Importantly, the key contribution of LGF is integrating a learned dynamical system within optimization, allowing for developments in data-driven ODE modeling to be repurposed for LGF without loss of generality. For instance, in the case of stochastic or mini-batch gradient descent, the objective gradient is only estimated at each iteration, yielding a noisier optimization trajectory, and we hypothesize that in this case, the more noise-robust WSINDy or regularized (or sparsity-promoting) solution techniques could become increasingly beneficial. More generally, this model-agnostic perspective emphasizes that our contribution is not tied to a particular SINDy instantiation, but rather a general framework in which the model identification step can be replaced by any procedure that infers a surrogate ODE model from the optimization history. 

3.2 Generalization to other optimizers 

While we initially introduced our approach by using gradient descent as an example, other optimization methods such as ADAM and Newton’s method are commonly used in practice [ 9, 40 ]. We overview the generalization to those methods here. 

3.2.1 Newton’s method 

For a damping parameter η, Newton’s method is given as 

ak+1 = ak − η ·

 ∂2

∂a∂a z(ak)

−1 ∂∂a z(ak), (7) where ∂2z/∂ a∂a is the Hessian matrix of the objective z [9]. The Newton update rule in Eq. (7) is similar to gradient descent in that it lends itself to a straightforward dynamical systems interpretation. The continuous-time Newton method is given by 

da

dt = −

 ∂2z∂a∂a

−1 ∂z ∂a , a(0) = a0, (8) where a0 is the initialization of the optimization variables. As a result, we apply the same procedure as in Algorithm 1, simply replacing the gradient descent update in the data-generating phase with Newton updates. Subsequently, we still integrate the optimization variables in time according to Eq. (5) , wherein ˆf seeks to approximate the time derivative of 

a. It is worth highlighting that generating ˆf still relies only on data from the state variables a rather than data directly involving the Hessian. 

3.2.2 ADAM 

ADAM adds both momentum and normalization to gradient descent, relying on two history variables for each parameter [41]. The final update is given as 

ak+1 = ak − η · ˆmk+1 

pˆvk+1 + ϵ , (9) 10 where the square root and division are element-wise, and the unbiased momentum parameters are, respectively, computed for 0 ≤ β1, β 2 < 1 as 

ˆmk+1 = mk+1 

1 − βk+1 1

,

ˆvk+1 = vk+1 

1 − βk+1 2

.

(10) The biased history variables, m and v, begin as zero and are iterated as 

mk+1 = β1mk + (1 − β1) ∂∂a z(ak),

vk+1 = β2vk + (1 − β2)

 ∂∂a z(ak)

2

,

(11) where the square is element-wise. Importantly, ak+1 depends not only on ak as in gradient descent and Newton’s method but also on ak−1, . . . , a0, through the history variables. In modeling the evolution of a(t) with a differential equation, we must therefore either use a delay differential equation or also include m(t) and v(t) in the system. We opt for the latter approach, yielding 

da

dt (t) = −

> m(t)1−βt/η
> 1

r v(t)1−βt/η 

> 2

+ ϵ,

dm

dt (t) = 1

η (1 − β1)

 ∂∂a z(a(t)) − m(t)



,

dv

dt (t) = 1

η (1 − β2)

 ∂∂a z(a(t)) 

2

− v(t)

!

.

(12) In Appendix A, we show how the discretization of Eq. (12) returns the original ADAM update rules in Eq. (9)-(11). While we could attempt to learn each of the right-hand sides in Eq. (12) , this is costly and even unnecessary. This would require building a library with a, m, and v (tripling the size of the state variables) and identifying an explicit dependence on t. In the partially known case, compared to the expensive evaluation of ∂z/∂ a, the remainder of Eq. (12) is quite cheap. Thus, we assume a partially known ODE, where we only seek to model the missing ∂z/∂ a term with SINDy (which is also used within the variance update as (∂z/∂ a)2). Some recent extensions of SINDy focus on such discrepancy modeling [28, 37]. Standard discrepancy modeling would only have access to observations generated by Eq. (12) and would need to isolate the unknown component, requiring the histories of m and v (in addition to a). However, in our case, we have direct access to observations of the unknown component ∂∂a z(a), simplifying this process considerably. We simply record  

> ∂∂a

z(a) during the initial data-generating phase and build a SINDy model just for this expensive term. While collecting 

A over iterations, we also record gradient observations: 

G =

h ∂∂a z(a(t1)) ∂∂a z(a(t2)) . . . ∂∂a z(a(tK)) 

iT

∈ RK× n.

11 Then, we follow the same procedure as for the other methods, but with the modified goal 

G ≈ Φ(A)Ξ,

where we can reuse the same STLSQ solver as before to find Ξ. We replace ∂z/∂ a with the model ˆf (a) = ΞT ϕ(a(t)) ,yielding a new ODE system: 

da

dt (t) = −

> m(t)1−βt/η
> 1

r v(t)1−βt/η 

> 2

+ ϵ,

dm

dt (t) = 1

η (1 − β1)( ˆf (a(t)) − m(t)) ,

dv

dt (t) = 1

η (1 − β2)( ˆf (a(t)) 2 − v(t)) .

(13) We integrate Eq. (13) from t = K · η to t = M · η, beginning from a(tK), m(tK), and v(tK). The evolved optimization variables a(tM) should approximate the result of performing ADAM with the expensive objective gradient after M

total iterations. This method can be used with or without mini-batching of the objective function, and we showcase both in a subsequent numerical example. While the interpretation of optimization as a dynamical system is an important motivation, it is not absolutely necessary for this method to function. As we see in Eq. (13) , the core requirement is the availability of an accurate surrogate for the optimizer’s update direction. In principle, our method can be applied to other optimization methods that do not admit time continuous ODE forms. We reiterate that, nonetheless, in some cases, the continuous perspective can provide some benefit [30, 35, 36] and also acts as a bridge to data-driven dynamics literature. 

3.3 Benefits and limitations of surrogate training dynamics 

After an initial data-generating phase, the SINDy models presented in Sec. 3.1, 3.2.1, and 3.2.2 avoid any evaluation of the objective function z or its derivatives. This has both substantial benefits and possible consequences. Most obviously, evaluating any terms related to z is computationally expensive. In engineering applications, this often corresponds to numerically simulating a system of interest, such as by solving partial differential equations (PDEs). In machine learning, computing the loss and its gradient entails forward and backward passes through large models over substantial datasets, making it the most costly part of training. Thus, the primary aim of our methods is to avoid these costly computations by instead relying on the discovered training dynamics. The utility of using a surrogate model for the evolution of the optimization variables fundamentally depends on its generalization. Because the learned dynamics are based on a finite set of observations, the performance of the surrogate hinges on how well the data-generating phase represents the training as a whole, and the evolution of the SINDy model provides no direct measure of the training quality (such as the loss). To address these concerns, we propose a heuristic variant of the general procedure outlined in Alg. 1, focusing on the case of gradient descent, without loss of generality to Newton, ADAM, or other base optimizers. 12 3.4 Scheduled retraining 

A preliminary approach assumes that these surrogate models decay in accuracy with time. In other words, a model is accurate at times close to the data-generating phase and less accurate when applied at later times. Under this assumption, we present a simple way to improve the generalizability of our method by devising an outer loop around Alg. 1 where we alternate between collecting optimization variable histories and applying a surrogate model. For each iteration of this outer loop, we iterate K steps through the original optimizer to build the history, construct the surrogate model for the dynamics, integrate in time to reach iteration M, and start over from the current values of the optimization variables. This is equivalent to walking through Alg. 1 once, passing the output a(tM) as the initial optimization variables a1

to another application of Alg. 1, and so on. Adjusting the ratio between the history size K and the retrain interval 

M changes how much we rely on some model of the dynamics and thus the savings on the gradient evaluations. For instance, setting M = K results in always evaluating the true gradient and applying the base optimizer, while taking a large M means applying the dynamics model for a long duration. Fig. 1 intuitively illustrates this concept of alternating between training LGF and applying it. 

Figure 1: Scheduled retraining of gradient flow. We alternate between collecting histories for K iterations from the true optimizer with expensive gradient computations (shaded red) and applying the learned gradient flow, ending at iteration 

M (shaded green). Transitioning between these two steps, we must build ˆf as specified in Sec. 3.1. The lines in the plot indicate trajectories of individual optimization variables. 

3.5 High-dimensional optimization problems 

For a state a ∈ Rn with K recorded iterations, the library matrix is Φ ∈ RK× p, where according to Eq. (1) p scales based on n depending on the polynomial order (e.g., p = O(n) for linear library terms and O(n2) for quadratic library terms). In many applications, the optimization variables are very high-dimensional, i.e., n ≫ 1, making p possibly much larger. Storing and using this matrix Φ becomes infeasible in such cases. Thus, rather than building our dynamics model in Rn, we will project to a smaller latent space Rr , model the dynamics here, and then lift back to the original 13 space. For the measurement matrix A ∈ RK× n, we can do this through the rank r singular value decomposition (SVD): 

AT ≈ UΣV T ,

assuming the state variables admit a low-rank representation, i.e., r ≪ n. Here, Σ ∈ Rr×r is a diagonal matrix of 

r largest (in descending order) singular values of AT , U ∈ Rn×r contains the corresponding r left singular vectors (modes), and rows of V ∈ RK× r give the mode coefficients. With this decomposition in hand, we can map a to the latent space via ˜a = UT a, and recover a with a ≈ U˜a. To create a model in the latent space, we must first project the optimization variables into the latent space via 

˜AT = UT AT ≈ ΣV T .

We then continue with the process outlined in Sec. 2.1, but now on this projected space. We define a candidate library function ˜ϕ : Rr → R˜p as all polynomials of up to some total order P (including cross terms), where ˜p is the number of these functions based on r. Then, repeatedly applying this library function to the K rows of the projected optimization history gives the projected library matrix ˜Φ( ˜A) ∈ RK× ˜p. Finally, the coefficients of the dynamics in the latent space 

˜Ξ ∈ R˜p×r are computed such that 

˙˜A ≈ ˜Φ( ˜A) ˜Ξ.

Here, ˙˜A can be computed as usual, but from ˜A (e.g. via finite differencing), or alternatively via ˙˜AT = UT ˙AT . In the case of recorded gradients G as in Sec. 3.2.2, we compute 

˜GT = UT GT ,

to build the latent model 

˜G ≈ ˜Φ( ˜A) ˜Ξ.

After finding ˜Ξ ∈ R˜p×r, we can combine the dimensionality reduction with the dynamics model to prescribe a cheaper dynamics model on the original space: 

da

dt (t) = U ˜ΞT ˜ϕ(UT a(t)) .

This allows us to handle high-dimensional, n ≫ 1, optimization problems. Nonetheless, high-dimensional optimization problems may present difficulties due to storing and operating on the history matrix A ∈ RK× n, yet we remark that this seemingly large memory requirement is present elsewhere in modern optimization. Namely, the limited memory version of BFGS (LBFGS) [ 58 ] has a comparable memory requirement. For the LBFGS history size K′, LBFGS stores roughly 2K′ vectors each of size n. Anecdotally, in Sec. 4.2, we present a high-dimensional problem and use K = 10 , as compared to the default history size for LBFGS K′ = 100 

[ 74 ]. In this example, we use roughly 20 times less memory than the default LBFGS. Of course, this is not a fair comparison as LBFGS may reach an optimum with fewer epochs due to its quasi-Newton nature, but it goes to show that 14 other widely-used algorithms have comparable memory footprints. Further, there are several dimensionality reduction methods that have smaller memory requirements and accomplish the same purpose, including various randomized sketching approaches, which may require only a single pass [25, 26]. 

3.6 Summary of hyperparameters 

Before empirically demonstrating the methods presented in this section, we provide a brief overview of the important hyperparameters presented thus far. We remark that choosing the history size K and retrain interval M is a matter of both trial and error and experience with the optimization problem under study. Our numerical examples illustrate that different optimization problems permit different levels of acceleration with the surrogate model. Hyperparameter Description Learning rate, η The learning rate used by the base optimizer; also determines how frequently observations of the state are recorded or predicted History size, K The number of samples of the state used to build ˆf

Retrain interval, M The number of epochs specifying how often to discard the current ˆf and begin collecting data for building a new surrogate (which are counted towards the count, thus M ≥ K ); this determines the total savings Truncation rank, r If applicable, the dimension of the latent space in which we build the latent dynamics model after doing dimensionality reduction, r ≤ n

Epochs The total number of steps taken between both the optimizer and surrogate dynamics model(s) over the whole training procedure Polynomial order, P The highest (total) degree of the polynomials used as library functions by SINDy 

# 4 Numerical Examples 

We now provide five numerical examples from engineering mechanics to showcase the LGF optimizer and compare its performance to traditional optimization methods. The first is a dynamic inverse problem with two unknown material parameters, which is chosen as a first case because it allows us to visualize movement through the loss landscape. The second problem is density-based topology optimization of a linear elastic structure, which is very high-dimensional and thus necessitates using dimensionality reduction techniques to learn the optimization dynamics. The third problem is a forward solve for nonlinear heat conduction with a spectral basis, and it is used to test the learning of continuous-time dynamics of Newton-based optimization. The fourth problem is that of “full wave inversion,” another dynamic inverse problem which is known to be challenging due to the non-convexity of the loss. As such, we use this problem to test the learned surrogate for ADAM optimization, which is more robust to non-convexity. Finally, in the fifth example, we experiment with LGF on a stochastic optimization problem from physics-informed machine learning. In particular, we solve for the displacement field of a three-dimensional linear elastic structure discretized by a neural network, where the objective function is computed with Monte Carlo integration. This example explores the ability of LGF to perform in the setting of noisy gradients. In each example, we compare LGF to a standard optimizer used to solve the problem at 15 hand. Hyperparameter settings for the LGF optimizer are specified on a case-by-case basis, as the following examples show. 

4.1 Inverse problem with gradient descent 

As an initial illustration of our method, we consider a two-parameter inverse problem to visualize how the LGF optimizer approximates the true gradient flow dynamics. In this problem, we recover the parameterized conductivity of a heated bar from measurements. The space-time evolution of the temperature u(x, t ) in a bar of unit length is governed by 

∂u ∂t = ∂∂x 



κ(x; a1, a 2) ∂u ∂x 



+ b(x, t ), u(0 , t ) = u(1 , t ) = 0 , u(x, 0) = 0 , (14) where b(x, t ) = 2000 sin(2 πx ) sin(2 πt ) is a prescribed heat source and the parameterized conductivity κ(x; a1, a 2)

defines a multi-material bar as 

κ(x; a1, a 2) = 



a1, 0 ≤ x ≤ 0.5,a2, 0.5 < x ≤ 1.

Note that the variable t here is physical time rather than the pseudo-time of the optimization dynamics as in Eq. (4) .With that in mind, the initial temperature distribution is taken to be zero, and the bar has homogeneous Dirichlet boundaries on both ends. The goal of the inverse problem is to estimate the material parameters a1 and a2 from measurement data. To this end, we construct a numerical solution of Eq. (14). The temperature is discretized with 

u(x, t ) ≈

> N

X

> i=1

ui(t) sin( iπx ), (15) which ensures that the boundary conditions are satisfied automatically. Plugging in Eq. (15) , the Bubnov-Galerkin weak form of Eq. (14) is given by 

> N

X

> i=1

∂u i

∂t 

Z 10

sin( iπx ) sin( jπx )dx = −

> N

X

> i=1

ui

Z 10

κ(x; a1, a 2)ijπ 2 cos( iπx ) cos( jπx )dx +

Z 10

b(x, t ) sin( jπx )dx, 

(16) where we have integrated by parts to transfer a spatial derivative onto the test function in the second term on the right-hand side. Defining the material parameters a = [ a1, a 2]T , Eq. (16) can be written more compactly as 

M ∂u

∂t + K(a)u = F(t), u(0) = 0. (17) The mass matrix M, the stiffness matrix K, and the force vector F(t) are defined through the indexed integral quantities of Eq. (16) . Assume that we have full-field measurement data v(x, t ) over the interval t ∈ [0 , T ], which is taken from 16 the system without noise for a given a. The inverse problem is then argmin 

> a,u

12

Z T

> 0

Z 10

 NX

> i=1

ui(t) sin( iπx ) − v(x, t )

2

dxdt 

s.t. M ∂u

∂t + K(a)u − F(t) = 0 , u(0) = 0.

(18) If we treat the temperature parameters u as an explicit function of the material parameters a through the governing equation, the inverse problem can be written without constraints. Using the backward Euler method to time integrate, the temperature parameters are obtained iteratively as a function of the material parameters with 

 M

∆t + K(a)



uk+1 =



Fk+1 + M

∆t uk



. (19) We write temperature parameters which are computed through Eq. (19) as u(t; a). The unconstrained form of the inverse problem is then argmin 

> a

z(a),z(a) = 12

Z T

> 0

Z 10

 NX

> i=1

ui(t; a) sin( iπx ) − v(x, t )

2

dx dt. 

(20) The measurement data v(x, t ) is generated by time integrating Eq. (17) with the backward Euler method and N = 30 

basis functions in the spatial discretization. The true material parameters are atrue = [2 , 1] T . In solving Eq. (20) , we compare standard gradient descent (GD) against the LGF optimizer with a linear approximation of the continuous-time optimization dynamics. The history size id K = 10 , and the retrain interval is M = 30 . The gradient ∂z/∂ a is computed using automatic differentiation with PyTorch by backpropagating through the time integration scheme of Eq. (19) . With a fixed number of epochs, we note that our chosen parameter settings for the surrogate model indicate 

3× savings in terms of gradient evaluations. The learning rate for both optimizers is set at lr = 1 × 10 −2, and the optimization is run for 700 epochs, which we observe to be sufficient for convergence. See Table 1 for a summary of the problem setup and Figure 2 for the results of the comparison. We define the acceleration of the method as 

100 × (M/K − 1) . The recovered parameters from the two methods are 

aGD =

1.97 1.06 

 , aLGF =

1.97 1.04 

 .

Both achieve similar accuracies in recovering the parameters in the dynamic inverse problem and have similar final losses ( z(aGD ) = 2 × 10 −4 and z(aLGF ) = 1 × 10 −4), though the LGF model saves 20 out of every 30 evaluations of the objective gradient, which involves back-propagating through the time integration of the temperature field. 17 Problem parameter Setting 

# optimization variables 2

lr 1 × 10 −2

Retrain interval, M 30 

History size, K 10 

Polynomial order, P 1

epochs 700 

Truncation rank, r N/A Table 1: Problem parameters for the 1D dynamic inverse problem. 

Figure 2: Convergence of the two optimizers to the minimum of the data loss defining the inverse problem. The optimizer based on the SINDy surrogate for the dynamics tracks closely with standard gradient descent and accelerates the optimization by 200% .

4.2 Topology optimization with gradient descent and dimensionality reduction 

We now test our method on a standard high-dimensional optimization problem from engineering mechanics: density-based topology optimization with a strain energy objective. In continuous form, this optimization problem reads argmin 

> ρ(x),u(x)

Z

> Ω

12 ϵij ρ(x)pCijkℓ ϵkℓ dΩ 

s.t. ∂σ ij 

∂x j

+ bi = 0 ,

Z

ρ(x) dΩ − V0 = 0 , 0 ≤ ρ ≤ 1,

(21) where Cijkℓ is the linear elastic constitutive tensor for an isotropic, homogeneous material, p is a “SIMP” penalty parameter [ 7 ], V0 is the volume constraint, ρ(x) is the density field, and the strain and stress are defined in terms of the displacement u(x) as 

ϵij = 12

 ∂u i

∂x j

+ ∂u j

∂x i



, σij = ρpCijkℓ ϵkℓ .

18 Working with a finite element discretization, the displacement components are stored on the nodes of the finite element mesh, and the density is discretized as constant over elements. The governing equation for static equilibrium in Eq. (21) is converted to the Bubnov-Galerkin weak form. Furthermore, we can enforce the box constraint on the density automatically by defining the elemental densities through the optimization variables a. The densities are then computed as 

ρ = 12



tanh( a) + 1 



, (22) The discretized optimization problem can be written as argmin 

> a,u

12 uT K(a)u

s.t. K(a)u − F = 0,

> nel

X

> e=1

ρ(ae)Ω e − V0 = 0 ,

(23) where nel is the number of elements in the finite element mesh, Ωe are the volumes of each element, u are the nodal displacements, K is the stiffness matrix, and F is the force vector. We convert this problem to an unconstrained form by treating the displacement as an explicit function of the optimization variables through the governing equation for equilibrium and by transforming the volume constraint into a penalty. At each setting of the optimization variables a,the displacement is computed through the governing equation of equilibrium with 

K(a)u(a) = F. (24) As such, the unconstrained form of the topology optimization problem of Eq. (23) is argmin 

> a

z(a),z(a) = 12 u(a)T K(a)u(a) + β

2

 nel X

> e=1

ρ(ae)Ω e − V0

2

,

(25) where β is a penalty hyperparameter. We can enforce that the displacement satisfies the governing equation at each design iteration in the computation of the gradient of this loss. Differentiating the objective in Eq. (25) , we note that both the stiffness matrix and the displacement depend on the optimization variables. The gradient is thus 

∂z ∂a j

= 12 uT ∂K

∂a j

u + uT K ∂u

∂a j

+ β

 nel X

> e=1

ρ(ae)Ω e − V0

 Ωj

2 (1 − tanh 2(aj )) . (26) We can obtain the “sensitivity derivative” ∂u/∂ a by differentiating the governing equation Eq. (24): 

K(a) ∂u

∂a = − ∂K

∂a u(a). (27) 19 Plugging Eq. (27) into Eq. (26), the gradient of the objective takes on a particularly simple form 

∂z ∂a = − 12 uT ∂K

∂a u + β

 nel X

> e=1

ρ(ae)Ω e − V0

 Ω

2 (1 − tanh 2(a)) , (28) where Ω is the vector of element volumes. The strain energy gradient in Eq. (28) can be assembled from elemental contributions. Using Eq. (22) and the SIMP penalization of the elemental stiffness matrix, the elemental contribution to the strain energy gradient is given as 



− 12 uT ∂K

∂a u



> j

= − 14 pρ (aj )p−1(1 − tanh 2(aj ))( uej )T Ke(uej ), (29) where Ke is the elemental stiffness matrix and uej is the elemental displacement vector corresponding to the j-th element. With Eq. (29) , the gradient of the strain energy can be assembled by looping over elements in the finite element mesh. With an unconstrained objective and its gradient defined, we can set up a problem and carry out the topology optimization. See Figure 3 for the problem setup. We take the design domain to be the unit cube ( L = 1 ), but use symmetry to define the computational domain as Ω = [0 , 1/2] × [0 , 1/2] × [0 , 1] . The displacement field is clamped in a cube with side length 0.05 around the origin. We use 8-node brick finite elements with linear interpolations of the displacement components and 24 elements per side of the computational domain. This amounts to 13824 equal-sized elements and 

46875 degrees of freedom before applying the Dirichlet and symmetry boundary conditions. We compare the LGF optimizer to standard gradient descent. We take the learning rate to be lr = 1 , and aim for 100% cost savings with history size K = 20 and retrain interval M = 40 . We use a linear surrogate for dynamics with polynomial order P = 1 

and build the dynamics in a low-dimensional subspace with truncation rank r = 2 . Building the reduced-order LGF surrogate requires computing a rank-r basis, which has cost O(n r K) for truncated SVD. This procedure is performed once per retraining interval M, so its cost is amortized over the subsequent surrogate steps. By contrast, each “true” objective/gradient evaluation in topology optimization requires solving the sparse equilibrium system K(a)u = F and assembling sensitivities over elements. Thus, while the computational cost of computing the truncated SVD (once per retrain interval) is not insignificant, it also does not dominate the cost of the problem by comparison to the regular training objective. This amortized basis-construction cost mirrors the standard offline/online trade-off in reduced-order modeling, where an upfront low-rank basis is built at nontrivial cost to enable many subsequent inexpensive evaluations. Given the small dimension of the surrogate model, we do not enforce sparsity of the learned coefficients, setting both the ridge regression weight α and threshold to 0. See Figure 4 for the results. After 500 iterations, the difference between the density fields is R 

> Ω

|ρGD (x) − ρLGF (x)|2dΩ

R 

> Ω

|ρGD (x)|2dΩ = 2 .0 × 10 −4.

The accuracy of the surrogate is striking not just because of the savings in objective/gradient evaluations, but especially because the dynamics are built in a two-dimensional subspace. This is an incredible contrast with the apparent dimensionality of the optimization problem of 13824 . Here, the dynamics of density-based topology optimization has 20 Problem parameter Setting 

# optimization variables 13824 

Volume constraint ( V0) 0.3

SIMP penalty ( p) 4

lr 1

Retrain interval, M 40 

History size, K 20 

Polynomial order, P 1

epochs 500 

Truncation rank, r 2

Table 2: Problem parameters for the 3D density-based topology optimization problem. an intrinsically low-dimensional and predictable dynamical structure, in spite of the complexity of the physics of the problem. We note that our choice to avoid box constraints by using hyperbolic tangent functions dramatically delays convergence of the density field to binary values, as Eq. (22) only converges to 0 or 1 in a limiting sense. 

Figure 3: Design domain, boundary conditions, and loading for the compliance minimization design problem. We take 

L = 1 and use the symmetry of the problem to only simulate a quarter of the domain. Keeping all other problem parameters constant, we run the problem again with a history size K = 20 and retrain interval 

M = 70 , corresponding to 250% savings in evaluations of the objective and its gradient. The optimization is run for 

900 epochs. See Figure 5 for the results. Even with the increased acceleration, the optimized design from the surrogate agrees with the design obtained with gradient descent. These examples suggest that, despite the complexity of the physics of the topology optimization problem, the optimization dynamics are simple. It is an interesting direction for future work to focus specifically on learned surrogates for topology optimization, and systematically study to what extent the optimization process can be accelerated before failure occurs. 

4.3 Nonlinear heat conduction with Newton’s method 

In continuous time, Newton optimization is given by Eq. (8) . The LGF optimizer must learn the dynamics from historical data on the optimization trajectories, so, for LGF to be effective, an appropriate test problem must require 21 Figure 4: Results of the density-based topology optimization with 100% acceleration using the surrogate model. We build the linear surrogate in a 2-dimensional subspace. This dramatic reduction in the dimensionality (from n = 13824 

to r = 2 ) keeps surrogate evaluations lightweight once constructed, as each update is carried out in a rank-r subspace. a large number of Newton steps to converge. One such problem is the scalar P -Laplace equation—which is used as a nonlinear generalization of heat conduction [ 51 ]—with the addition of radiative heat loss. Given its variational formulation, this problem allows us to conveniently represent a PDE forward solve as an optimization problem. In the continuous setting, the variational energy is given by 

z(u(x)) = 

Z

> Ω

κ(x)|∇ u · ∇ u|P + σ

5 u5 − b(x)u dΩ ,

22 Figure 5: Results of the density-based topology optimization with 250% acceleration using the surrogate model. The learned model for the linear optimization dynamics is built in a 2-dimensional subspace. The surrogate model again closely tracks the reference solution obtained with gradient descent. where u(x) is the temperature field, κ(x) is the conductivity, P determines the order of the nonlinearity, σ is the emissivity of the material, and b(x) is a heat source. We note that choosing to raise ∇u · ∇ u to the P -th power (as opposed to computing the P -th power of |∇ u|) is non-standard but streamlines calculations and is inconsequential for this example. For our problem, we take the domain to be Ω = [0 , 1] 2 and the boundaries to be homogeneous Dirichlet 23 u(x) ∂Ω

= 0 . We discretize the solution with a spectral basis per 

u(x) ≈

> M

X

> i=1
> M

X

> j=1

˜aij sin( iπx 1) sin( jπx 2) :=  

> M2

X

> i=1

aifi(x),

where the relation between the coefficients ˜aij and ai (as well as their corresponding basis functions) is a reshape operation. Plugging the discretization into the variational energy, the governing equation for the temperature field is argmin 

> a

z(a)

z(a) = 

Z

> Ω

κ(x) 

> M2

X 

> i=1
> M2

X

> j=1

aiaj ∇fi · ∇ fj

> P

+ σ

5

 

> M2

X

> i=1

aifi(x)



> 5

− b(x) 

> M2

X

> i=1

aifi(x)

dΩ.

(30) Newton-type optimization requires a quadratic approximation of the objective, meaning that we need both the gradient of the objective and the Hessian matrix. These can be computed as 

∂z ∂a k

=

Z

> Ω

"

2P κ (x) 

> M2

X 

> i=1
> M2

X

> j=1

aiaj ∇fi · ∇ fj   

> P−1M2

X

> j=1

aj ∇fj · ∇ fk + σ

 

> M2

X

> i=1

aifi(x)



> 4

fk(x) − b(x)fk(x)

#

dΩ

∂2z∂a k∂a ℓ

=

Z

> Ω

"

4κ(x)P (P − 1)  

> M2

X 

> i=1
> M2

X

> j=1

aiaj ∇fi · ∇ fj 

> P−2

 

> M2

X

> j=1

aj ∇fj · ∇ fk

 

> M2

X

> i=1

ai∇fi · ∇ fℓ



+2 P κ (x) 

> M2

X 

> i=1
> M2

X

> j=1

aiaj ∇fi · ∇ fj 

> P−1

∇fk · ∇ fℓ + 4 σ

 

> M2

X

> i=1

aifi(x)



> 3

fk(x)fℓ(x)

#

dΩ.

We will solve the Newton-type optimization problem for the temperature field given by Eq. (30) for a multi-material object. The problem parameters for the governing equation and discretization are as follows: 

b(x) = 10 7x1 sin(4 πx 1) sin(3 πx 2),κ(x) = 



20 , if max (|x − [1 /2, 1/2] T |) ≤ 0.25 ,

1, else ,M = 15 ,

a0 ∼ U (−3, 3) ,

where a0 is the initial guess for the coefficients. In the LGF optimizer, we relax the Newton solve with ∆t = lr = 0 .15 

in order to ensure stability, take the polynomial order of the surrogate as P = 1 , and use the history size K = 15 and retrain interval M = 20 . Note that we use the same step relaxation of 0.15 for the true Newton solve as well. We form the objective and its gradients with 5625 equally spaced integration points throughout the unit square domain, and the optimization is run for 300 epochs. See Table 3 for a summary of the problem set up and Figure 6 for the comparison of a standard Newton updating vs. the surrogate for the Newton dynamics. For both optimizers, the initial norm of the 24 Problem parameter Setting 

# optimization variables 225 

Emissivity ( σ) 4

Order of nonlinearity ( P ) 2

lr 1.5 × 10 −1

Retrain interval, M 20 

History size, K 15 

Polynomial order, P 1

epochs 300 

Truncation rank, r N/A Table 3: Problem parameters for the nonlinear heat conduction energy minimization problem. gradient is |∂z (a0)/∂ a| := R0 = 2 .38 × 10 11 , and after 300 steps, the norm of the gradient is reduced to 

RNEWTON = 0 .7, RLGF = 1 .2.

Both residuals have been reduced by more than ten orders of magnitude. The normalized difference between the two temperature fields is R 

> Ω

|uNEWTON (x) − uLGF (x)|2dΩ

R 

> Ω

|uNEWTON (x)|2dΩ = 1 .7 × 10 −13 ,

which represents remarkable agreement between the Newton optimization and the surrogate for its dynamics, which saves 5 out of every 20 evaluations of the right-hand side of the continuous-time Newton dynamics. In the case of Newton optimization, evolving the dynamical system for optimization involves computing the gradient of the objective, its Hessian, the inverse of the Hessian, and the matrix-vector product of the inverse Hessian with the gradient. By comparison, the surrogate is extremely cheap to evaluate and leads to a solution that, in this case, is indistinguishably close to the true Newton method. Thus far, no guidelines have been provided on how to choose the acceleration of the learned gradient flow optimizer. As this is an initial work showcasing the possibility of accelerating optimization with equation discovery, future work is required to explore automating the acceleration. However, to provide some insight into the pitfalls of choosing an overly aggressive acceleration ratio, we re-run the problem with M = 30 while keeping all other problem parameters constant. See Figure 7 for the results. The two optimizers closely track each other in the initial stages of the optimization, yet the surrogate for the Newton dynamics consistently drives the norm of the residual up after epoch 175 . Though the magnitude of the error between the two solutions is still small, the surrogate model is taking steps that later need to be corrected by the true optimization dynamics. Here, because the aggressive acceleration consistently increases the loss, the surrogate provides little benefit. We note from experience that, as expected, even larger acceleration values correspond to increased discrepancy between the true and surrogate Newton dynamics. 

4.4 Full wave inversion with ADAM optimization 

A well-known inverse problem from geophysics is that of full wave inversion (FWI), whereby the material properties of a medium are inferred from data on the propagation of waves in that medium [ 55 ]. This problem is challenging due to 25 Figure 6: Comparing the surrogate for the full Newton dynamics to the true dynamics. Using the LGF optimizer represents a 25% acceleration. The surrogate briefly departs from the true Newton dynamics as the parameters begin to converge, but quickly recovers, leading to a final solution with comparable accuracy to the true Newton method. ill-posedness and nonconvexity. In particular, the presence of many local minima may render standard gradient descent strategies ineffective [ 44 ]. Thus, we use the FWI problem as a test case for the LGF implementation of the ADAM algorithm, where the gradient is approximated with the surrogate. The propagation of pressure waves in a stationary medium is governed by 

m2(x) ∂2u∂t 2 − ∇ 2u = f (x, t ), u(x, 0) = 0 , (31) 26 Figure 7: Comparing the surrogate model against the true Newton dynamics with a 100% acceleration. Because the acceleration is too aggressive, the surrogate model for the Newton dynamics ceases to be accurate and drives the norm of the residual up when active. where u(x, t ) is the pressure field, f (x, t ) is a source term, and m(x) is the “slowness” of the medium, or the inverse of the wave speed. Note that the variable t here is physical time rather than the pseudo-time of the optimization dynamics as in Eq. (4). We take the computational domain to be Ω = [0 , 1] 2 with the following boundary conditions 

u(x) = 0 , x ∈ ∂ΩD ,

∇u · ˆn = 0 , x ∈ ∂ΩN ,

(32) 27 Figure 8: Problem setup for the full wave inversion problem. Three of the four surfaces have homogeneous Dirichlet boundaries, while the upper horizontal surface is free. A sinusoidal point force is applied at the center of this surface. There are nel = 346 elements in the mesh, and the true slowness field is a linearly graded material. where ∂ΩD is the Dirichlet portion of the boundary, ∂ΩN is the Neumann portion, and ˆn is the outward-facing normal vector. The source term is f (x, t ) = δ(x1 − 1/2, x 2 − 1)10 sin(4 πt ). The true slowness field is that of a functionally graded material and is given by 

m2

> true

(x) = 1 + 2 x2.

Three of the surfaces of the geometry have zero-pressure boundary conditions, whereas the upper surface is free. See Figure 8 for the problem geometry, boundary conditions, and finite element mesh. The goal of the inverse problem is to recover the squared slowness field m2(x) from data. Analogous to the density in the topology optimization problem, we discretize the squared slowness field as constant over elements in a finite element mesh. Thus, the optimization variables a represent the elemental values of the squared slowness fields. Similarly, we discretize the pressure field in space and time with 

u(x, t ) = 

> nnode

X

> n=1

ui(t)Ni(x),

where the Ni(x) are linear finite element functions on a triangular mesh and nnode is the number of nodes in the mesh. We compute the Bubnov-Galerkin weak form of Eq. (31) in space to obtain the discretized governing equation as 

M(a) ∂2u

∂t 2 + Ku (t) = F(t), u(0) = 0. (33) By assumption, the boundary conditions of Eq. (32) are built into the finite element discretization of the pressure field. To set up the inverse problem, we assume that the nodal displacements are observed continuously in time without noise. The measurement data is given by v(t) where t ∈ [0 , T ]. The inverse problem for the material property field is then argmin 

> u,a

12

Z T

> 0

∥u(t) − v(t)∥2dt 

s.t. M(a) ∂2u

∂t 2 + Ku (t) − F(t) = 0, u(0) = 0.

(34) 28 As in the previous problems, we can treat the pressure as an explicit function of the material parameters a through the governing equation. To compute the loss with the measurement data in Eq. (34) at a given a, we time integrate the governing equation with central differencing, which gives rise to the update rule 

uk+1 = ∆ t2M(a)−1

Fk − Ku k



+ 2 uk − uk−1. (35) We will denote a solution for pressure through Eq. (35) as u(t; a). The constrained optimization problem of Eq. (34) can be reformulated as argmin 

> a

z(a)

z(a) = 12

Z T

> 0

∥u(a, t ) − v(t)∥2dt. 

(36) In order to solve this unconstrained optimization problem, we require the gradient ∂z/∂ a, which in turn requires ∂u/∂ a.Though adjoint state methods are a standard tool in FWI problems to obtain this gradient, we opt to use automatic differentiation in PyTorch to compute the necessary gradients of Eq. (36) . The data is generated by time integrating Eq. (33) with the true slowness field and given source term. To avoid a complex solution field from interacting reflections, we choose the simulation time as T = 1 . We use 300 time integration points in the central difference method of Eq. (35) .We compare a standard ADAM optimizer to the continuous-time ADAM with the surrogate for the gradient. We take the learning rate to be lr = 2 × 10 −2, assume the standard Adam hyperparameters as β1 = 0 .9, β 2 = 0 .999 , ϵ = 10 −8,use a quadratic approximation of the gradient dynamics with a polynomial order of P = 2 , and set M = 30 and 

K = 20 . Unlike the other examples, within the STLSQ solver, we set α = 0 and max_iter = 5 due to the high cost of this problem. We choose to run the optimization for a fixed number of epochs and verify convergence by observing the objective value, which reports the error between the measured solution and the solution predicted from the current material properties. In this example, we run the optimization for 2200 epochs. The pointwise error in the recovered slowness field is taken as |a(x) − m2

> true

(x)| where a(x) is the slowness field obtained from the chosen optimizer. See Table 4 for a summary of the problem setup and Figure 9 for the results. The final value of the error between the measurement data and predicted solution in Eq. (36) from the two optimizers is 

z(aADAM ) = 4 .8 × 10 −4, z(aLGF ) = 5 .5 × 10 −4.

We remark that, near convergence, the elemental slownesses have a very small influence on the predicted spacetime solution field. This means that the corresponding objective gradient terms become small and the slowness field converges slowly. Accordingly, we do not necessarily expect perfect recovery of the material parameters. However, we do demand that the ADAM variant of the LGF optimizer has comparable performance to the standard implementation of ADAM. The relative mean-squared errors with the exact solution from the two optimizers are given by 

R 

> Ω

|aADAM (x) − m2

> true

(x)|2dΩ

R 

> Ω

|m2

> true

(x)|2 = 7 × 10 −4,

R 

> Ω

|aLGF (x) − m2

> true

(x)|2dΩ

R 

> Ω

|m2

> true

(x)|2 = 9 × 10 −4.

29 Problem parameter Setting 

# optimization variables 346 

lr 2 × 10 −2

Retrain interval, M 30 

History size, K 20 

Polynomial order, P 2

epochs 2200 

Truncation rank, r N/A Table 4: Problem parameters for full wave inversion problem. In the context of a dynamic inverse problem, each time the surrogate is used in place of evaluating the gradient, we avoid backpropagating through the time integration, which finds the solution field at the current material properties. Both the surrogate and ADAM achieve comparable performance on the FWI problem, driving the objective down 5

orders of magnitude and leading to accurate recovery of the true slowness field. As in the previous example, we experiment with more aggressive acceleration of the FWI problem. With all other problem parameters equivalent, we set the retrain interval at M = 40 , corresponding to 100% acceleration. The relative mean-squared error obtained by the LGF is 2.3 × 10 −3. See Figure 10 for the results. In the first 1750 epochs of optimization, the surrogate drives down the loss monotonically, but then fails to capture the optimization dynamics closer to convergence. In these cases, the errors incurred by the surrogate need to be corrected by ADAM, which indicates that the surrogate is not providing any benefit. Again, this example illustrates the importance of appropriately tuning the history size and retrain interval hyperparameters. 

4.5 Deep Ritz method with ADAM optimization 

As a final test of our method, we show an example involving neural network training with ADAM optimization. In particular, we train a neural network to represent the displacement field of a three-dimensional linearly elastic structure. In Eq. (21) , we work with the strong form of the governing equation for linear elasticity. An alternative formulation of static equilibrium is that of the principle of minimum total potential energy, which says that the total potential energy functional is minimized. In the continuous setting, this reads 

Π = 

Z

> Ω

12 σij (x)ϵij (x) − bi(x)ui(x)dΩ −

Z

> ∂Ω

ti(x)ui(x)dS, δΠ = 0 , (37) where ti is the surface traction and Π is the total potential energy. The Deep Ritz Method [ 27 ] approximates a solution to this problem by discretizing the displacement field with a neural network and minimizing the energy functional in terms of the neural network parameters. Because Eq. (37) assumes that the Dirichlet boundary conditions are satisfied by the displacement automatically, we choose to build in boundary conditions to the neural network discretization with a distance function method [73]. The displacement field is thus discretized as 

u(x; a) = D(x)N (x; a), (38) where N : R3 → R3 is a multi-layer perceptron neural network, D(x) is a distance function which is zero along the 30 Figure 9: Comparing standard ADAM optimization to the continuous time ADAM with gradient surrogate. Note that the elemental slowness parameters have very little influence on the predicted solution (and thus, the loss) as the optimizer approaches convergence. The surrogate model tracks closely with the parameter evolution from true ADAM optimization. The error reported here is the absolute value of the difference between the true and recovered slowness fields. This FWI problem is accelerated by 50% .Dirichlet portion of the boundary, and a are the optimization variables, which are the parameters of the neural network. Note that the form of Eq. (38) assumes homogeneous Dirichlet boundary conditions. In our example, we take the computational domain to be the unit cube Ω = [0 , 1] 3, the x3 = 0 surface to be clamped, and the remaining portion of the boundary to be traction-free. The distance function is taken as D(x) = x3, and we use a two-hidden-layer neural 31 Figure 10: Comparing standard ADAM optimization to the continuous-time ADAM with gradient surrogate and 100% 

acceleration. Towards the end of the optimization, the surrogate consistently increases the objective function, indicating that the optimization dynamics are inaccurately approximated. The differences in the recovered slowness fields between the two methods are now pronounced. network with 25 hidden units per layer and hyperbolic tangent activation functions. The integration grid used to form the total potential energy is 50 × 50 × 50 evenly spaced points. The volumetric force is taken to be that of a torsional displacement field, i.e., 

b(x) = b0

h

−(x2 − 12 ) (x1 − 12 ) 0

iT

.

32 Problem parameter Setting 

# optimization variables 825 # integration points 125000 

lr 2.5 × 10 −3

Retrain interval, M 50 

History size, K 35 

Polynomial order, P 1

epochs 2000 

Truncation rank, r N/A Table 5: Problem parameters for Deep Ritz solution to linearly elastic deformation. where b0 = 10 controls the magnitude of the forcing. We compare ADAM optimization against the LGF optimizer using the ADAM setting with a linear approximation of the gradient, history size K = 35 , and retrain interval M = 50 .A summary of the parameter settings is given in Table 5. See Figure 11 for the results. The final energy values after 

2000 epochs of true ADAM optimization and ADAM optimization with a linear surrogate for the gradient are given by 

ΠADAM = −7.79 , ΠLGF = −7.60 .

This corresponds to a normalized difference in predicted displacement fields given by 

R

> Ω

∥uADAM (x) − uLGF (x)∥2dΩ

R

> Ω

∥uADAM (x)∥2dΩ = 1 .3 × 10 −3.

Note that the optimization variable trajectories from the two optimizers do not coincide as neatly as the cases of the previous examples. However, this does not indicate decreased performance of the surrogate model, as seen in close agreement of the final values of the loss and the predicted displacement fields. Non-uniqueness in the dependence of the neural network predictions on the optimization variables suggests that, in this case, the loss trajectory is a better indicator of performance than the agreement of the parameter trajectories from the two optimizers. The true ADAM dynamics lead to a lower converged energy value, but this leads only to slight differences in the displacement fields. With this Deep Ritz example, we again gain insight into the pitfalls of choosing too large an acceleration value. Keeping all problem parameters constant, we increase the acceleration to 100% by setting M = 70 . See Figure 12 for the results. The surrogate model consistently drives up the loss, and these errors need to be reversed by the true ADAM dynamics. We see that the surrogate model provides no benefit when the acceleration is too large. We also test our method on a stochastic version of the strain energy objective given in Eq. (37) . Such a test demonstrates that the learned gradient flow can perform on noisy gradients from machine learning problems where the training data is mini-batched, as is often the case in practice. The analogue of mini-batching in the physics-informed setting is to use Monte Carlo integration to form the total potential energy at each optimization epoch. Namely, we approximate the energy objective as 

ˆΠ ≈ 1

B

> B

X

> i=1

12 σij (xi)ϵij (xi) − bi(xi)ˆ ui(xi), xi

> i.i.d.

∼ U (0 , 1) , (39) 33 Figure 11: Comparing standard ADAM optimization with the LGF version of ADAM. We save 15 out of every 50 

evaluations of the gradient of the loss function using the surrogate model, corresponding to an acceleration of 43% .where B is the size of the mini-batched integration. We take the integration batch to be B = 5000 , which is 4% of the size of the full-batch integration grid, and run the optimization for 3000 epochs with all other problem and optimization variables set at the same values. We remark that the stochastic integration routine requires 1.5 × 10 7 evaluations of the neural network for the displacement, whereas the full-batch integration requires 2.5 × 10 8 evaluations. Mini-batching integration can be an effective strategy to accelerate neural network-based physics problems, as noted in [ 68 ]. See Figure 13 for the results of training with the stochastic objective. The final values of the energy for the two optimizers are 

ˆΠADAM = −7.62 , ˆΠLGF = −7.39 .

34 Figure 12: When the acceleration is increased to 100% , the surrogate model does not accurately reflect the loss landscape. When switched on, it consistently drives the total potential energy up, providing no benefit to the optimization process. The normalized difference in the displacement fields obtained by minimizing the stochastic objective with the two optimizers is R

> Ω

∥ˆuADAM (x) − ˆuLGF (x)∥2dΩ

R

> Ω

∥ˆuADAM (x)∥2dΩ = 3 .1 × 10 −3.

Again, we note that the trajectories of the two sets of neural network parameters do not overlap as closely in the Deep Ritz problem as in previous examples. This is a consequence of the non-uniqueness of neural network approximations. 

Figure 13: Comparing the two optimizers with a Monte Carlo approximation of the total potential energy. The loss convergence indicates noisy gradients, yet the surrogate model still captures relevant features of the optimization dynamics. Once again, we save 15 out of every 50 evaluations of the gradient of the loss function using the surrogate model, corresponding to an acceleration of 43% .35 Optimization problem Method Acceleration 

Dynamic inverse problem Gradient descent 200% 

Topology optimization Gradient descent 250% 

Nonlinear heat conduction Newton’s method 33% 

Full wave inversion ADAM 50% 

Deep Ritz training ADAM (full-batch) 43% 

Deep Ritz training ADAM (mini-batch) 43% 

Table 6: Summary of the optimization problems studied, the optimization method, and the acceleration observed from using the data-driven surrogate. The acceleration is computed as 100 × (M/K − 1) .

# 5 Conclusion 

In this work, we connected two key ideas: gradient flows and data-driven equation discovery. During training, optimization variables evolve according to an underlying gradient flow, which can be viewed as a dynamical system. Separately, equation discovery methods seek to approximate the behavior of dynamical systems from their observations as a means to generate future predictions. We combined these perspectives by learning a dynamical system that approximates the gradient flow of the optimizer. This learned gradient flow then predicts the trajectory of the optimization variables, resulting in computational savings by avoiding evaluations of the true gradient dynamics, which often involve expensive PDE solves. We initially derived this method for gradient descent, but generalized it to Newton’s method and ADAM. We experimentally demonstrated the utility of this idea across a range of mechanics problems, including topology optimization, a nonlinear heat conduction inverse problem, full wave inversion, and the Deep Ritz method. Across these examples, our learned gradient flow reduced the number of gradient evaluations by factors ranging from 33% up to 250% , as summarized in Table 6. While these results demonstrate clear benefits, several challenges and open questions remain. A particularly important direction is developing principled guidance for selecting the algorithmic parameters that govern when and how the surrogate is learned and deployed. While we provided cursory examples of changing the retrain interval M at the end of Sec. 4.3 and 4.4, the selection of this algorithmic parameter warrants further study well beyond what we have provided here. Rather than alternating between the surrogate and true training dynamics on a fixed schedule, a possible alternative is to design explicit criteria for determining when retraining is necessary, thus automating the choice of acceleration. Similarly, some methods may automate selection of the model form ( P and r in our case) based on a train-validation split. Many advances in dynamical systems discovery may also be applied to this scenario, opening a multitude of possible improvements. In particular, one could modify this approach with an online version of SINDy that operates on streaming data, or perhaps decouple the model-building step from the training so it can run in parallel or on a separate device. Similarly, the (optional) dimensionality reduction step could be replaced with an online equivalent. Another direction could investigate incorporating components of other optimizers into this paradigm, such as the Hessian approximation afforded by LBFGS or various improvements to ADAM [ 63 , 52 ]. The LGF paradigm is also broadly applicable to iterative optimization methods in general, including gradient-free methods, constituting another possible research direction. On the practical side, several considerations merit further attention: developing 36 implementations for distributed optimization problems, automating hyperparameter selections, and reducing memory overhead. Finally, it is important to recognize that this approach is, unsurprisingly, not universally applicable. While some of the aforementioned techniques may help to generalize to high-dimensional optimization problems, there will always be problems whose training dynamics cannot be effectively captured by a surrogate learned from limited, original optimization iterations. In particular, LGF may struggle in constrained optimization settings where the algorithm intermittently switches between different constraint regimes (e.g., via projection steps, feasibility restoration, active-set updates, or complementarity conditions). Such switching can introduce non-smooth behavior in the update direction, producing abrupt changes in the observed trajectories that an autonomous, smooth surrogate learned from a short window of iterations is unlikely to extrapolate reliably. 

# Acknowledgment 

This work was supported by the Department of Energy’s National Nuclear Security Administration under Award Numbers DE-NA0003962 and DE-NA0003968. C. Rowan’s work was funded by the National Defense Science and Engineering Graduate Fellowship (NDSEG) through the Department of Defense (DOD) and the Army Research Office (ARO). 

# References 

[1] Anas Abdelrehim, Dhairya Gandhi, Sharan Yalburgi, Ashutosh Bharambe, Ranjan Anantharaman, and Chris Rackauckas. Active Learning Enhanced Surrogate Modeling of Jet Engines in JuliaSim, January 2025. arXiv:2501.07701 [cs]. [2] Joubine Aghili, Emmanuel Franck, Romain Hild, Victor Michel-Dansac, and Vincent Vigon. Accelerating the convergence of Newton’s method for nonlinear elliptic PDEs using Fourier neural operators. Communications in Nonlinear Science and Numerical Simulation , 140:108434, January 2025. arXiv:2403.03021 [math]. [3] Govinda Anantha Padmanabha and Nicholas Zabaras. Solving inverse problems using conditional invertible neural networks. Journal of Computational Physics , 433:110194, May 2021. [4] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by gradient descent, November 2016. arXiv:1606.04474 [cs]. [5] Jan Backhaus, Marcel Aulich, Christian Frey, Timea Lengyel, and Christian Vob. Gradient Enhanced Surrogate Models Based on Adjoint CFD Methods for the Design of a Counter Rotating Turbofan. In Turbine Technical Conference and Exposition , 2013. 37 [6] Sami Barmada, Nunzia Fontana, Alessandro Formisano, Dimitri Thomopulos, and Mauro Tucci. A Deep Learning Surrogate Model for Topology Optimization. IEEE Transactions on Magnetics , 57(6):1–4, June 2021. [7] Martin P. Bendsøe and Ole Sigmund. Topology optimization by distribution of isotropic material. In Martin P. Bendsøe and Ole Sigmund, editors, Topology Optimization: Theory, Methods, and Applications , pages 1–69. Springer, Berlin, Heidelberg, 2004. [8] Josh Bongard and Hod Lipson. Automated reverse engineering of nonlinear dynamical systems. Proceedings of the National Academy of Sciences of the United States of America , 104(24):9943–9948, June 2007. [9] Stephen Boyd and Lieven Vandenberghe. Convex optimization. 2004. [10] Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural PDE solvers, March 2023. [11] Elie Bretin, Roland Denis, Simon Masnou, and Garry Terii. Learning phase field mean curvature flows with neural networks. Journal of Computational Physics , 470:111579, December 2022. arXiv:2112.07343 [math]. [12] Elie Bretin, Chih-Kang Huang, and Simon Masnou. A penalized Allen-Cahn equation for the mean curvature flow of thin structures, May 2024. arXiv:2310.10272 [math]. [13] William L. Brogan. Modern Control Theory . Prentice Hall, 1991. Google-Books-ID: OPFQAAAAMAAJ. [14] Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences , 113(15):3932– 3937, April 2016. Publisher: Proceedings of the National Academy of Sciences. [15] Matteo Caldana and Jan S. Hesthaven. Neural ordinary differential equations for model order reduction of stiff systems, August 2024. [16] Emmanuel Candes, Justin Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. Technical report, 2005. [17] Qianying Cao, Somdatta Goswami, and George Em Karniadakis. LNO: Laplace Neural Operator for Solving Differential Equations, May 2023. arXiv:2303.10528 [cs]. [18] Kathleen Champion, Bethany Lusch, J. Nathan Kutz, and Steven L. Brunton. Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences , 116(45):22445–22451, November 2019. Publisher: Proceedings of the National Academy of Sciences. [19] Fei Chen and Wei Ren. Sign projected gradient flow: A continuous-time approach to convex optimization with linear equality constraints. Automatica , 120:109156, October 2020. [20] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural Ordinary Differential Equations, December 2019. arXiv:1806.07366 [cs]. [21] Ricky T. Q. Chen. torchdiffeq, 2018. Available at https://github.com/rtqichen/torchdiffeq .38 [22] Song Chen, Jiaxu Liu, Pengkai Wang, Chao Xu, Shengze Cai, and Jian Chu. Accelerated optimization in deep learning with a proportional-integral-derivative controller. Nature Communications , 15(1):10263, November 2024. Publisher: Nature Publishing Group. [23] Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang Wang, and Wotao Yin. Learning to Optimize: A Primer and A Benchmark, July 2021. arXiv:2103.12828 [math]. [24] Victor Churchill and Dongbin Xiu. Flow map learning for unknown dynamical systems: Overview, implementation, and benchmarks, 2023. [25] Alec M. Dunton, Lluís Jofre, Gianluca Iaccarino, and Alireza Doostan. Pass-efficient methods for compression of high-dimensional turbulent flow data. Journal of Computational Physics , 423:109704, December 2020. [26] Alec Michael Dunton and Alireza Doostan. Deterministic matrix sketches for low-rank compression of high-dimensional simulation data, 2021. arXiv:2105.01271. [27] Weinan E and Bing Yu. The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems, September 2017. arXiv:1710.00211 [cs]. [28] Megan R. Ebers, Katherine M. Steele, and J. Nathan Kutz. Discrepancy Modeling Framework: Learning missing physics, modeling systematic residuals, and disambiguating between deterministic and random effects, November 2023. arXiv:2203.05164 [stat]. [29] Lina Fesefeldt, Sabine Le Borne, Alexander Düster, and Lars Radtke. Using surrogate models to accelerate load step methods for nonlinear finite element problems in hyperelasticity. PAMM , 24(3):e202400081, 2024. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pamm.202400081. [30] Kyriakos Flouris, Anna Volokitin, Gustav Bredell, and Ender Konukoglu. Explicit and data-Efficient Encoding via Gradient Flow, January 2025. arXiv:2412.00864 [stat]. [31] Guilherme França, Daniel P. Robinson, and René Vidal. Gradient flows and proximal splitting methods: A unified view on accelerated and stochastic optimization. Physical Review E , 103(5):053304, May 2021. arXiv:1908.00865 [math]. [32] Kunal Garg and Dimitra Panagou. Fixed-Time Stable Gradient Flows: Applications to Continuous-Time Opti-mization. IEEE Transactions on Automatic Control , 66(5):2002–2015, May 2021. [33] Matteo Giacomini and Antonio Huerta. A surrogate model for topology optimisation of elastic structures via parametric autoencoders, July 2025. arXiv:2507.22539 [math]. [34] Jeffrey M Hokanson, Gianluca Iaccarino, and Alireza Doostan. Simultaneous identification and denoising of dynamical systems. SIAM Journal on Scientific Computing , 2022. [35] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and Generalization in Neural Networks. In Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. 39 [36] Arnulf Jentzen and Adrian Riekert. Convergence analysis for gradient flows in the training of artificial neural networks with ReLU activation. Journal of Mathematical Analysis and Applications , 517(2):126601, January 2023. [37] Kadierdan Kaheman, Eurika Kaiser, Benjamin Strom, J. Nathan Kutz, and Steven L. Brunton. Learning discrepancy models from experimental data, September 2019. [38] Alan A. Kaptanoglu, Brian M. de Silva, Urban Fasel, Kadierdan Kaheman, Andy J. Goldschmidt, Jared Callaham, Charles B. Delahunt, Zachary G. Nicolaou, Kathleen Champion, Jean-Christophe Loiseau, J. Nathan Kutz, and Steven L. Brunton. PySINDy: A comprehensive Python package for robust sparse system identification. Journal of Open Source Software , 7(69):3994, January 2022. [39] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. CoRR , December 2014. [40] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization, January 2017. arXiv:1412.6980 [cs]. [41] Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In ICLR , 2015. [42] Elham Kiyani, Khemraj Shukla, Jorge F. Urbán, Jérôme Darbon, and George Em Karniadakis. Optimizing the Opti-mizer for Physics-Informed Neural Networks and Kolmogorov-Arnold Networks, August 2025. arXiv:2501.16371 [cs]. [43] Nikola B. Kovachki and Andrew M. Stuart. Continuous Time Analysis of Momentum Methods, May 2021. arXiv:1906.04285 [cs]. [44] Kuldeep and Bharath Shekar. Full waveform inversion with random shot selection using adaptive gradient descent. 

Journal of Earth System Science , 130(4):183, September 2021. [45] Akshay Kumar and Jarvis Haupt. Towards Understanding Gradient Flow Dynamics of Homogeneous Neural Networks Beyond the Origin, May 2025. arXiv:2502.15952 [cs]. [46] Jeffrey Larson, Matt Menickelly, and Stefan M. Wild. Derivative-free optimization methods. Acta Numerica ,28:287–404, May 2019. arXiv:1904.11585 [math]. [47] Jonas Latz. Analysis of stochastic gradient descent in continuous time. Statistics and Computing , 31(4):39, May 2021. [48] Claude Lemaréchal. Cauchy and the gradient method | EMS Press, 2012. [49] Ke Li and Jitendra Malik. Learning to Optimize, June 2016. arXiv:1606.01885 [cs]. [50] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier Neural Operator for Parametric Partial Differential Equations, May 2021. arXiv:2010.08895 [cs]. [51] Peter Lindqvist. A NONLINEAR EIGENVALUE PROBLEM. [52] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization, January 2019. 40 [53] Lu Lu, Pengzhan Jin, and George Em Karniadakis. DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. Nature Machine Intelligence ,3(3):218–229, March 2021. arXiv:1910.03193 [cs]. [54] Gemma Massonis, Alejandro F. Villaverde, and Julio R. Banga. Distilling identifiable and interpretable dynamic models from biological data. PLOS Computational Biology , 19(10):e1011014, October 2023. Publisher: Public Library of Science. [55] Arnaud Mercier, Christian Boehm, and Hansruedi Maurer. Designing full waveform inverse problems: a combined data and model approach. Geophysical Journal International , 241(3):1479–1494, June 2025. [56] Daniel A. Messenger and David M. Bortz. Weak SINDy For Partial Differential Equations. Journal of Computa-tional Physics , 443:110525, October 2021. arXiv:2007.02848 [math]. [57] Daniel A. Messenger and David M. Bortz. Weak SINDy: Galerkin-Based Data-Driven Model Selection. Multiscale Modeling & Simulation , 19(3):1474–1497, January 2021. [58] Jorge Nocedal and Stephen J. Wright. Numerical Optimization . Springer, 2nd edition, 2006. [59] Antonio Orvieto and Aurelien Lucchi. Continuous-time Models for Stochastic Optimization Algorithms, March 2020. arXiv:1810.02565 [math]. [60] Defne E. Ozan and Luca Magri. Data-driven computation of adjoint sensitivities without adjoint solvers: An application to thermoacoustics. Physical Review Fluids , 9(10):103902, October 2024. Publisher: American Physical Society. [61] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics , 378:686–707, February 2019. [62] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Machine learning of linear differential equations using Gaussian processes. Journal of Computational Physics , 348:683–693, November 2017. [63] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the Convergence of Adam and Beyond, April 2019. [64] Patrick A. K. Reinbold, Daniel R. Gurevich, and Roman O. Grigoriev. Using noisy or incomplete data to discover models of spatiotemporal dynamics. Physical Review E , 101(1):010203, January 2020. [65] Mihaela Rosca, Yan Wu, Chongli Qin, and Benoit Dherin. On a continuous time model of gradient descent dynamics and instability in deep learning, September 2023. arXiv:2302.01952 [stat]. [66] Conor Rowan and Alireza Doostan. On the definition and importance of interpretability in scientific machine learning, May 2025. arXiv:2505.13510 [cs]. [67] Conor Rowan, John Evans, Kurt Maute, and Alireza Doostan. Solving engineering eigenvalue problems with neural networks using the Rayleigh quotient, June 2025. arXiv:2506.04375 [math]. 41 [68] Conor Rowan, Sumedh Soman, and John A. Evans. Variational volume reconstruction with the Deep Ritz Method, August 2025. arXiv:2508.08309 [eess]. [69] Andrzej Ruszczynski. Nonlinear Optimization . Princeton University Press, 2006. [70] Farshud Sorourifar, You Peng, Ivan Castillo, Linh Bui, Juan Venegas, and Joel A. Paulson. Physics-Enhanced Neural Ordinary Differential Equations: Application to Industrial Chemical Reaction Systems. Industrial & Engineering Chemistry Research , 62(38):15563–15577, September 2023. Publisher: American Chemical Society. [71] Weijie Su, Stephen Boyd, and Emmanuel J. Candes. A Differential Equation for Modeling Nesterov’s Accelerated Gradient Method: Theory and Insights, October 2015. arXiv:1503.01243 [stat]. [72] Weijie Su, Stephen Boyd, and Emmanuel J. Candès. A Differential Equation for Modeling Nesterov’s Accelerated Gradient Method: Theory and Insights. Journal of Machine Learning Research , 17(153):1–43, 2016. [73] N. Sukumar and Ankit Srivastava. Exact imposition of boundary conditions with distance functions in physics-informed deep neural networks. Computer Methods in Applied Mechanics and Engineering , 389:114333, February 2022. [74] Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J van der Walt, Matthew Brett, Joshua Wilson, K Jarrod Millman, Nikolay Mayorov, Andrew R J Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \.Ilhan Polat, Yu Feng, Eric W Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E A Quintero, Charles R Harris, Anne M Archibald, Antônio H Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental algorithms for scientific computing in python. 

Nature Methods , 17:261–272, 2020. [75] Cong Wang, Aoming Liang, Fei Han, Xinyu Zeng, Zhibin Li, Dixia Fan, and Jens Kober. Learning Adaptive Hydrodynamic Models Using Neural ODEs in Complex Conditions, October 2024. arXiv:2410.00490 [cs]. [76] Jacqueline Wentz and Alireza Doostan. Derivative-based SINDy (DSINDy): Addressing the challenge of discovering governing equations from noisy data. Computer Methods in Applied Mechanics and Engineering ,413:116096, August 2023. [77] Cong Xiao, Ya Deng, and Guangdong Wang. Deep-Learning-Based Adjoint State Method: Methodology and Preliminary Application to Inverse Modeling. Water Resources Research , 57(2):e2020WR027400, 2021. _eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020WR027400. [78] Yi Xing, Takayuki Yamada, and Liyong Tong. Accelerating level-set based topology optimization using gradient and stochastic gradient online learning and prediction methods. Structural and Multidisciplinary Optimization ,68(6):124, June 2025. [79] Mengfei Xu, Shufang Song, Xuxiang Sun, Wengang Chen, and Weiwei Zhang. Machine learning for adjoint vector in aerodynamic shape optimization, December 2020. arXiv:2012.15730 [physics]. 42 [80] Wotao Yin, Daniel McKenzie, and Samy Wu Fung. Learning to Optimize: Where Deep Learning Meets Optimization and Inverse Problems | SIAM, December 2022. [81] Sean I. Young, Yaël Balbastre, Bruce Fischl, Polina Golland, and Juan Eugenio Iglesias. Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 11535–11545, Seattle, WA, USA, June 2024. IEEE. [82] Zeyu Zhang, Yu Li, Weien Zhou, Wen Yao, and Xiaoqian Chen. Non-Linear Topology Optimization Via Neural Representations and Material Point Method Part I: Quasi-Static Problem, May 2024. [83] Emre Özkaya, Jan Rottmayer, and Nicolas R. Gauger. Gradient Enhanced Surrogate Modeling Framework for Aerodynamic Design Optimization. In AIAA SCITECH 2024 Forum . American Institute of Aeronautics and Astronautics, 2024. _eprint: https://arc.aiaa.org/doi/pdf/10.2514/6.2024-2670. 43 Appendix A ADAM ODE Verification 

Given the ODE system for ADAM in Eq. (12) , we show how we can recover the standard ADAM update rules in Eq. (9) – (11) through time discretization. We repeat the continuous system below for convenience: 

da

dt (t) = −

> m(t)1−βt/η
> 1

r v(t)1−βt/η 

> 2

+ ϵ,

dm

dt (t) = 1

η (1 − β1)

 ∂∂a z(ak) − m(t)



,

dv

dt (t) = 1

η (1 − β2)

 ∂∂a z(ak)

2

− v(t)

!

.

(12) We will use a semi-implicit method to discretize this system, applying forward Euler for m and v and backward Euler for a. We apply the forward Euler update at tk = η · k with a step size of η, denoting mk = m(tk), vk = v(tk), and 

ak = a(tk), this gives 

mk+1 − mk

η = 1

η (1 − β1)

 ∂∂a z(ak) − mk



,

vk+1 − vk

η = 1

η (1 − β2)

 ∂∂a z(ak)

2

− vk

!

.

Expanding the (1 − β1) and (1 − β2) terms with mk and vk and simplifying gives the update rule for the biased moments from Sec. 3.2.2: 

mk+1 = β1mk + (1 − β1) ∂∂a z(ak),

vk+1 = β2vk + (1 − β2)

 ∂∂a z(ak)

2

.

(11) Next, returning to the ODE system in Eq. (12) , applying backward Euler for the update to a, and noting tk+1 = ( k+1) ·η,

ak+1 − ak

η = −

> mk+1
> 1−βk+1 1

q vk+1 

> 1−βk+1 2

+ ϵ . (40) For the next step, recall the unbiased moment formulas: 

ˆmk+1 = mk+1 

1 − βk+1 1

, ˆvk+1 = vk+1 

1 − βk+1 2

. (10) The nested fractions in Eq. (40) correspond to the unbiased estimators of Eq. (10) , and through that substitution, we have recovered the original update rule in Eq. (9): 

ak+1 = ak − η · ˆmk+1 

pˆvk+1 + ϵ .

Thus, through a semi-implicit discretization of the ODE system in Eq. (12) , we recover the discrete ADAM update rule. 44