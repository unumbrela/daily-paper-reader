---
title: "Learning Gradient Flow: Using Equation Discovery to Accelerate Engineering Optimization"
authors: "Grant Norman, Conor Rowan, Kurt Maute, Alireza Doostan"
date: 2026-02-13
pdf: "https://arxiv.org/pdf/2602.13513v2"
tags: ["query:sr"]
score: 9.0
evidence: 动力系统和优化的数据驱动方程发现
tldr: 本研究提出了一种名为“学习梯度流（LGF）”的优化框架，旨在通过数据驱动的方程发现技术加速工程优化过程。针对目标函数及其梯度计算成本高昂的挑战，该方法利用优化轨迹数据学习梯度下降、牛顿法等算法的连续时间动力学，并构建代理模型进行求解。在结构拓扑优化和逆问题等多个工程案例中，LGF显著提升了收敛速度并降低了计算开销。
motivation: 旨在解决传统工程优化中频繁评估高成本目标函数及其梯度所导致的计算效率低下问题。
method: 通过方程发现技术从优化变量轨迹中学习连续时间动力学，并构建全维或降维空间的代理模型来替代原始优化过程。
result: 在逆问题、拓扑优化及前向求解等多个实验中，该方法能够准确捕捉优化轨迹特征并大幅加快收敛速度。
conclusion: 学习梯度流为加速复杂工程优化提供了一种高效的新途径，通过数据驱动的动力学建模有效规避了昂贵的函数评估。
---

## Abstract
In this work, we investigate the use of data-driven equation discovery for dynamical systems to model and forecast continuous-time dynamics of unconstrained optimization problems. To avoid expensive evaluations of the objective function and its gradient, we leverage trajectory data on the optimization variables to learn the continuous-time dynamics associated with gradient descent, Newton's method, and ADAM optimization. The discovered gradient flows are then solved as a surrogate for the original optimization problem. To this end, we introduce the Learned Gradient Flow (LGF) optimizer, which is equipped to build surrogate models of variable polynomial order in full- or reduced-dimensional spaces at user-defined intervals in the optimization process. We demonstrate the efficacy of this approach on several standard problems from engineering mechanics and scientific machine learning, including two inverse problems, structural topology optimization, and two forward solves with different discretizations. Our results suggest that the learned gradient flows can significantly expedite convergence by capturing critical features of the optimization trajectory while avoiding expensive evaluations of the objective and its gradient.