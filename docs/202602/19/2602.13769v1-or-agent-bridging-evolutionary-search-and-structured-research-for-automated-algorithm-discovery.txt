Title: OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery

URL Source: https://arxiv.org/pdf/2602.13769v1

Published Time: Tue, 17 Feb 2026 01:45:26 GMT

Number of Pages: 52

Markdown Content:
# OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery 

# Qi Liu 1 and Wanjing Ma 21,2 Key Laboratory of Road and Traffic Engineering of the Ministry of Education, College of Transportation, Tongji University, Shanghai, P.R.China, 201804 

# {liu qi, mawanjing }@tongji.edu.cn 

# February 17, 2026 

Abstract 

Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment in-teraction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent or-ganizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation–crossover loops. At its core, we introduce an evolutionary–systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehen-sive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks—including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems—as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent .

# 1 Introduction 

Scientific research is traditionally an iterative and human-driven process. Researchers survey existing literature, formulate hypotheses, design algorithms or models, conduct experiments, analyze outcomes, and refine ideas through repeated feedback cycles. This workflow is rarely linear: it involves branching exploration, backtracking from unproductive directions, local refinement around promising ideas, and continual integration of new insights. Although effective, such processes are inherently constrained by human cognitive capacity, limited time, and the difficulty of systematically exploring vast solution spaces [TXLH25]. Recent advances in large language models (LLMs) and autonomous agents [GWD +25] have raised an increasingly compelling question: Can AI systems act as research collaborators, supporting or par-tially automating the scientific discovery process? Emerging work has demonstrated that LLMs can generate code, propose hypotheses, perform tool-based reasoning, and even participate in iterative re-finement [RPBN +24, LLL +24]. However, most existing approaches focus either on single-shot program generation or on evolutionary-style search with limited structured memory and workflow management [RPBN +24, LTYZ23, LTY +24, YWC +24, NVE +25]. A comprehensive framework that integrates idea generation, implementation, experimentation, reflection, and structured exploration remains underde-veloped. 1

> arXiv:2602.13769v1 [cs.AI] 14 Feb 2026

In this paper, we introduce Open Research Agent (OR-Agent), a multi-agent system designed to emulate and systematize the workflow of scientific research, with a particular focus on operations research (OR) problems. Operations research (OR) problems offer an ideal testbed for advancing AI-driven scientific discovery. These problems provide well-defined abstract environments—ranging from simulation platforms such as traffic simulators to constrained optimization scenarios like the logistics problems—while maintaining strong connections to practical real-world applications. Many such problems can be naturally framed as program synthesis tasks, enabling LLMs to operate directly in code space while receiving quantitative feedback from structured evaluators. OR-Agent treats research ideas and programs as research artifacts to be iteratively evolved, evalu-ated, and refined [LTY +24]. Rather than relying solely on mutation or crossover style search iteration [RPBN +24, LTYZ23, LTY +24, NVE +25], OR-Agent organizes the research process as a structured tree of investigations. Each branch represents a research hypothesis or modification direction, and ex-ploration is guided by both quantitative evaluation, environment exploration feedback, and reflective analysis. The system integrates evolutionary initialization, deep local investigation, and memory-based reflection to balance exploration and exploitation under finite computational budgets. The core contributions of this work are as follows: 

• We design a multi-agent research framework with exploratory capabilities in complex environ-ment, enabling automated scientific discovery in domains with rich experimental settings. 

• We propose a structured tree-based research workflow that captures branching hypothesis explo-ration and systematic backtracking, enabling explicit management of research trajectories rather than relying solely on mutation and crossover iterations. 

• We introduce an evolutionary–systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and structured exploration within a tree-based workflow. Each round begins from evolutionarily selected solutions, from which agents generate a comprehensive research plan and explore refinement directions in a coordinated and structured manner. 

• We introduce a hierarchical optimization-inspired reflection system. Experimental reflection and summary function as a form of verbal gradient, providing immediate corrective signals based on recent experimental feedback. Long-term reflection accumulates insights as verbal momentum. Memory compression acts as a regularization mechanism analogous to weight decay, preserving essential signals while discarding redundancy. Together, these components form a principled hierarchical reflection architecture that governs research dynamics in OR-Agent. 

• We conduct extensive experiments across classical combinatorial optimization problems (e.g., traveling salesman problem, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack) and simulation-based cooperative driving scenarios, demonstrating the effectiveness and generality of the proposed framework. Results demonstrate OR-Agent’s capability to outperform baseline algorithms across diverse prob-lem domains. Beyond raw performance gains, OR-Agent constitutes a general and configurable research framework that can be customized to accommodate heterogeneous problem structures, evaluation pro-tocols, and exploration strategies. Its modular design enables flexible adaptation and extension, pro-viding a reproducible and inspectable infrastructure for AI-assisted scientific discovery. In this way, OR-Agent bridges evolutionary program search with open-ended, environment-interactive research workflows while remaining extensible for future methodological development The remainder of the paper is organized as follows. Section 2 presents the architecture and core algorithms of OR-Agent. Section 3 presents case studies. Section 4 describes the benchmark prob-lems, experiments, ablation studies and analytical findings. Section 5 reviews related work in AI for operations research, evolutionary algorithm discovery, and automated scientific research. Section 6 concludes and outlines future directions. 22 Open Research Agent 

# 2.1 OR-Agent Framework Overview 

The Open Research Agent (OR-Agent) is designed as a co-scientist agent for solving complex operations research (OR) and related scientific problems that admit automated evaluation. Such problems are characterized by large combinatorial search spaces, delayed feedback, and a tight coupling between conceptual ideas, algorithmic implementations, and empirical validation. OR-Agent aims to emulate and systematize the workflow of human scientific research by integrating evolutionary exploration, deep iterative refinement, and structured experimentation within a unified multi-agent framework. 

Problem Specification and Open Research Canvas. Each research process begins with a human-in-the-loop problem specification stage. OR-Agent provides an Open Research Canvas (OR-Canvas) as a shared interface for human researchers and AI agents to collaboratively define the research objective, evaluation protocol, and seed solution. The OR-Canvas serves as a persistent specification artifact that grounds subsequent automated exploration, ensuring alignment between human intent and agent-driven research while minimizing ad-hoc prompt engineering. 

Evolutionary Initialization and Deep Investigation. At the start of each research round, a 

Lead Agent samples one or more parent solutions from a structured solution database, analogous to population initialization in evolutionary algorithms. Mutation- and crossover-like operations are realized through large language model (LLM)–driven idea recombination and variation, similar in spirit to prior LLM-based genetic approaches such as AEL, EoH, and ReEvo. However, unlike these methods, OR-Agent does not rely on frequent evolutionary operators alone. Instead, it emphasizes extensive and systematic investigation around each evolutionary starting point before progressing to new ones. This design choice reflects a key departure from prior work: in scientific research, generating ideas via mutation or crossover is only the beginning. High-quality discoveries typically emerge from iterative refinement, targeted experimentation, and repeated diagnosis of failure modes. OR-Agent explicitly allocates computational effort toward deep local exploration once a promising direction is identified, closely mirroring human research practice. 

Multi-Agent Architecture. OR-Agent is implemented as a coordinated multi-agent system with clearly separated responsibilities: 

• OR Agent (ORAgent ): The system entry point that manages global configuration, coordinates multiple Lead Agents, and maintains the shared solution database. 

• Solution Database (SolutionDatabase ): A persistent repository that stores all generated solutions and associated metadata, serving as a shared evolutionary memory across agents and research rounds. 

• Lead Agent (LeadAgent ): Acts as a principal investigator for a research round, selecting parent solutions, managing the research workflow, and deciding when to expand, backtrack, or terminate exploration. 

• Idea Agent (IdeaAgent ): Generates and refines high-level solution ideas based on prior exper-iments, accumulated reflections and current research tree progress. 

• Code Agent (CodeAgent ): Translates ideas into executable implementations and performs iterative debugging and refinement. 

• Experiment Agent (ExperimentAgent ): Executes experiments, explores complex environ-ments, diagnoses failures, and summarizes experimental findings. Multiple Lead Agents may operate concurrently, enabling parallel exploration of distinct research directions while sharing knowledge through the common solution database. 3Figure 1: Overall framework of OR-Agent, illustrating the interaction between evolutionary initializa-tion, multi-agent research workflows, experimentation, reflection, and the shared solution database. 

Tree-Structured Research Workflow and Refinement Loops. Each Lead Agent organizes its research process as a tree-structured workflow, implemented via a FlowGraph . Each node corresponds to an intermediate research state, consisting of an idea, an implementation, and experimental evi-dence. Child nodes represent refined variants produced through idea modification, code changes, and experimental insights. OR-Agent adopts a Refinement Loop Paradigm, in which idea generation, code implementation, and experimentation are all embedded within iterative refinement loops. Idea refinement loops continuously adjust hypotheses, code refinement loops address implementation errors and performance bottlenecks, and experiment refinement loops repeatedly probe the environment to extract informative feedback. This explicit structuring of refinement distinguishes OR-Agent from linear or single-pass research agents. 

Motivation: Breadth, Depth, and Context Management. Single-agent research systems can achieve deep reasoning but often suffer from limited exploratory breadth and early commitment to suboptimal hypotheses. Conversely, naive multi-agent systems may explore broadly but struggle to maintain coherent long-term reasoning due to context fragmentation and unstructured coordination. OR-Agent addresses this trade-off by combining multi-agent parallelism with explicit workflow control, structured memory, and hierarchical context management. By distributing exploration across agents while centralizing knowledge through the solution database and reflections, OR-Agent achieves both exploratory breadth and reasoning depth. 

Summary of Framework Design. In summary, OR-Agent integrates evolutionary initialization, tree-structured research control, multi-agent specialization, iterative refinement, and environment-aware experimentation into a single coherent framework. This design allows the system to explore previously unvisited regions of the solution space while efficiently converging toward high-quality local optima, providing a flexible and extensible foundation for automated scientific research. 

# 2.2 Task Specification 

OR-Agent is designed to operate over a broad class of research tasks that can be formulated as program synthesis and automated evaluation problems. To support both expert-defined and interactive problem formulation, the framework introduces a unified task specification interface that bridges human intent, large language models, and executable evaluation environments. To enable automated research, users must decompose their target problem into a structured specifi-cation comprising several key components. This decomposition makes explicit the problem semantics, 4evaluation methodology, and interface constraints that guide the evolutionary search process. The required specification elements are: 

• Problem Description : A natural language description of the optimization or decision problem, including its objectives, constraints, and domain context. This description provides high-level semantic guidance to the LLM agents during ideation and implementation. 

• Function Description : A formal specification of the target function(s) to be evolved, including function signatures, input-output contracts, expected behavior, and interface requirements. This component defines the API through which evolved solutions interact with the evaluation infras-tructure. Importantly, OR-Agent supports evolution at the module level, enabling synthesis of multiple interrelated functions rather than restricting to single-function optimization. 

• Evaluation Script : An automated evaluation procedure that executes candidate solutions and computes performance metrics. The evaluation script must implement a standardized interface that accepts solution code, executes it within the problem environment, and returns structured performance data including scalar fitness scores, feature vectors for diversity maintenance, and detailed metrics for diagnostic analysis. 

• Evaluation Description : A textual description of the evaluation methodology, performance metrics, and fitness computation. For simple evaluation procedures, this may consist of the evaluation script itself. For complex or proprietary evaluation methodologies, users may provide a condensed description that abstracts implementation details while conveying essential evaluation semantics to the LLM agents. 

• Callbacks Description : An optional but recommended specification for problems involving complex simulation environments. This component describes mechanisms through which agents can actively interact with the environment, control simulation outputs, request specific diagnos-tic information, and explore environmental states. The callbacks interface enables systematic environment exploration, which is critical for problems where relevant information cannot be fully captured in standard performance metrics. Beyond these core components, the task specification may include seed solutions (initial implemen-tations to bootstrap the evolutionary process), default callback configurations (standard environment interaction patterns), dataset generation scripts (for problems requiring test instances), and external knowledge resources (domain-specific reference materials). 

Open Research Canvas (OR-Canvas). To facilitate flexible and user-centric task formulation, OR-Agent introduces the Open Research Canvas (OR-Canvas) (See Figure 2), an interactive interface that allows users to collaboratively specify research problems with the assistance of large language mod-els. OR-Canvas supports the incremental construction of problem descriptions, function signatures, evaluation criteria, and optional environment interaction protocols. By externalizing task specification into a structured canvas, the framework lowers the barrier to defining new research tasks and enables rapid prototyping without requiring users to directly author full evaluation pipelines from scratch. 

Solution Interface. Candidate solutions consist of textual ideas that articulate the high-level con-ceptual strategy, Python modules that implement the proposed method, associated metrics and ex-periment summaries that document empirical performance and analytical insights generated after experiment agent running. 

Evaluation Protocol. The evaluation script executes a candidate solution within the task envi-ronment and produces three standardized outputs ([RPBN +24]: metrics: a dictionary of task-specific performance indicators used for analysis and reflection; features: a tuple of discrete descriptors derived from metrics, used for clustering and diversity preservation in the solution database; score: a scalar fitness value that determines solution quality and guides search. 5Figure 2: User Interface of OR-Canvas and OR-Agent. 

Environment Interaction and Callbacks. For tasks involving complex or dynamic environments, the task specification may expose callback interfaces that enable intermediate interaction during eval-uation. These callbacks allow the Experiment Agent to probe environment states, collect diagnostic feedback, and perform targeted exploration beyond final performance scores. Such mechanisms are particularly important for simulation-based tasks, where delayed or sparse rewards alone are insufficient for effective refinement. 

Generality and Reusability. The proposed task specification unifies a diverse range of problem types under a single abstraction, including classical operations research benchmarks, neural or hybrid solvers with fixed model parameters, and direct policy construction. By enforcing a consistent solution interface and evaluation protocol, OR-Agent and OR-Canvas together enable seamless task reuse, fair benchmarking, and systematic extension across domains without modifying the core agent workflow. This design ensures that OR-Agent remains a reusable research engine, while OR-Canvas provides a human-in-the-loop entry point for defining, refining, and extending research tasks in a structured and reproducible manner. 

# 2.3 Structured Evolutionary Solution Database 

OR-Agent maintains a shared structured evolutionary solution database that stores previously eval-uated programs together with rich evaluation feedback and experiment summaries. All Lead Agents access the same database: at the beginning of each research round, parent solutions are retrieved from the database (e.g., by stochastic sampling for exploration, or by selecting current elite for ex-ploitation), and are used as evolutionary starting points for initializing the subsequent tree-structured research workflow. 

Motivation and Relation to Program Databases. The design is inspired by the program database paradigm used in LLM-driven evolutionary systems such as FunSearch and AlphaEvolve. In particular, AlphaEvolve [NVE +25] reports that an evolutionary database combining MAP-Elites with island-based population models can effectively resurface useful ideas while maintaining diversity. 6Figure 3: Illustration of the population ruin phenomenon. A single dominant solution rapidly takes over the population, and subsequent mutations around this dominant mode produce invalid solutions (e.g., due to timeouts), leading to collapse of population quality. OR-Agent adopts the same high-level principle by integrating (i) an island-based population structure for parallel, semi-independent exploration and (ii) a MAP-Elites-style discretization over behavioral features to prevent premature collapse into a single mode. 

Why a Persistent Database is Necessary: Population Ruin. A key motivation for maintain-ing a persistent evolutionary database is an empirically observed failure mode termed “population ruin”. Without an explicit mechanism to preserve diversity and prevent over-concentration, a single seemingly “good” solution can rapidly dominate the population, even if it is brittle or invalid under refinement. In practice, such dominance is dangerous because the dominating solution may contain degenerate behaviors (e.g., excessive computation leading to timeouts or invalid outputs), and subse-quent mutations around this solution can wipe out the remaining viable population, yielding a large fraction of invalid candidates. This phenomenon is especially pronounced in fixed-size evolutionary loops where elites generate disproportionately many descendants. Consider an extreme setting in which, at every iteration, the current elite produces a fraction m ∈ (0 , 1) of the next generation (e.g., via mutation) from elite solution, and the population is then downsampled to a constant size. Even under random down-sampling, the expected fraction pn of individuals that are direct elite descendants can be modeled by the recurrence: 

pn+1 = pn + m

1 + m . (1) Solving yields: 

pn = 1 − (1 − p0)

 11 + m

n

, (2) which converges exponentially fast to 1 for any m > 0. For the illustrative case m = 12 and p0 = 0, one obtains pn = 1 −   23

n, implying that the population can becomes almost entirely composed of elite descendants within only a few iterations. In contrast, the structured database mitigates this collapse by separating populations into islands, clustering solutions into feature bins, and controlling resurfacing via temperature-based sampling. Figure 3 provides a representative example of this failure mode observed during our experiments. 

Island-Based and MAP-Elites Organization. The database is structured in the sense that each stored solution is not only associated with a scalar fitness score, but also with additional evaluation signals. Even when the optimization objective is ultimately single-scalar, providing multi-faceted feed-back is valuable: it improves interpretability, behavior-based diversity, and supplies richer signals for LLM-based diagnosis and refinement, consistent with observations in AlphaEvolve [NVE +25]. Con-cretely, OR-Agent requires the evaluation script to output all three fields, with features optionally set to None to disable MAP-Elites when no meaningful discretization is available. The specification 7of evaluation metrics, feature representations, and scalar fitness functions plays a important role in determining the quality and trajectory of the resulting solutions. Although OR-Canvas can assist in structuring and partially automating this specification process, these design choices are sufficiently im-portant that they should not be delegated entirely to large language models. Careful human oversight is often necessary to ensure that the evaluation criteria faithfully capture domain-specific objectives and constraints. In Appendix A, we provide a concrete instantiation for the cooperative driving prob-lem used in our experiments. Different Lead Agents may develop different research memories, while the shared database functions as a communication channel through which high-quality and diverse so-lutions can be surfaced and reused across research rounds, analogous to how scientists share discoveries through repositories like arXiv. 

# 2.4 Tree-Structured Scientific Discovery Workflow 

Deep scientific research is inherently challenging, requiring both broad exploration over diverse high-level approaches and deep, sustained refinement within promising directions. Recent studies suggest that, for difficult problems, independently resampling solutions or deploying tree-search strategies over structured reasoning processes is often a more effective use of test-time computation than linear or flat exploration schemes [SLXK24, HMF +25]. Motivated by these observations, OR-Agent adopts a tree-structured research workflow to explicitly organize, control, and reason about the research process. 

Tree-Based Organization of Research Progress. OR-Agent represents the progression of a research round as a dynamically evolving tree, where each node corresponds to a concrete research hypothesis together with its implementation code and experimental outcomes. A research round begins from one or more evolutionary starting points sampled from the solution database, which serve as roots of the research tree. These starting points typically correspond to previously discovered elite or diverse solutions and define the initial research context. Compared to flat evolutionary frameworks, the tree-based workflow enables OR-Agent to conduct systematic and targeted exploration in the neighborhood of each evolutionary starting point. Rather than generating isolated offspring and immediately discarding historical context, OR-Agent explicitly maintains the structural relationships among hypotheses, allowing deeper investigation along promising branches while preserving alternative directions for later exploration. Crucially, during idea generation, OR-Agent incorporates the entire research tree—including node ideas, evaluation scores, and expansion status—into the generation context. This global awareness provides the agent with a holistic view of research progress, analogous to how a human scientist tracks explored directions, failed attempts, and promising leads over time. This design contrasts with conventional LLM-based evolutionary methods that generate new candidates largely independently of the broader research trajectory. 

Tree Shape as a Control Mechanism for Research Modes. The depth and breadth of the research tree naturally correspond to the depth and breadth of scientific inquiry. Deeper trees empha-size intensive refinement of specific ideas, while wider trees favor exploration of diverse hypotheses. OR-Agent exposes multiple parameters to explicitly control tree shape, including the maximum num-ber of children per node, maximum tree depth, and dynamic child allocation strategies. Additional mechanisms further modulate exploration behavior. For example, elite solutions may be assigned more children when used as parents, reflecting higher confidence in their research potential. Crossover op-erations may optionally employ fast exploration settings with shallower depth and fewer experiments to rapidly probe new directions. Through these controls, OR-Agent supports various research modes, allowing users to trade off exploration and exploitation in a principled manner. Representative modes such as “deep research” and “extensive search” are illustrated in Figure 4. While related systems such as AI Scientist V2[YLL +25] and FlowSearch [HMF +25] also employ tree- or graph-based workflows, their structures are designed to support more general task decompo-sition and knowledge propagation. In contrast, the Tree-Structured Research Workflow in OR-Agent is specifically tailored to scientific idea development, where each node represents a complete research hypothesis grounded in executable code and empirical validation, and edges encode natural derivation relationships between ideas. 8Figure 4: Use tree shape as a control mechanism for research modes illustration. 

Coordinated Idea Generation. Prior studies have observed that naively scaling LLM-based idea generation often leads to severe redundancy and limited diversity [SYH24]. Empirically, we observe the same phenomenon in OR-Agent: when child ideas are generated independently for a fixed parent solution, even with increased sampling temperature, the resulting ideas exhibit high semantic overlap and limited coverage of the design space (See B for example). OR-Agent addresses this limitation through coordinated idea generation . For each node expansion, the Idea Agent generates a set of child ideas jointly, conditioning on the shared parent context or even entire research tree and the requirement that the ideas represent distinct directions. This process treats the set of children as a coherent research plan rather than independent samples. As a result, idea redundancy is substantially reduced and coverage over alternative hypotheses is improved (See B for example). Conceptually, coordinated generation mirrors human scientific practice: a researcher typically enumerates multiple plausible improvement directions simultaneously before selecting which ones to pursue in depth. Different research trees, initialized from different evolutionary starting points, can be viewed as analogous to parallel research efforts by different scientists. Within a tree, sibling nodes correspond to competing hypotheses, while deeper descendants represent iterative refinement of a specific direction. We note that coordinated generation also introduces practical trade-offs. Generating multiple ideas in a single pass tends to produce much shorter individual descriptions, and the total number of ideas is constrained by context window limits. While length constraints can be explicitly enforced, determining an optimal granularity a priori remains an open challenge. 

Research Tree Traversal. The Lead Agent is responsible for traversing and expanding the research tree. At each step, Lead Agent selects a leaf node to expand according to a simple greedy policy: among all unfinished leaf nodes, the one with the best evaluation score is selected as the current most promis-ing research direction. This choice reflects a strong exploitation bias, though alternative traversal strategies—such as PUCB-style exploration [Ros11, SHM +16]—are compatible with the framework and left for future investigation. The tree traversal and expansion procedure for a single research round is summarized in Listing 1: Listing 1: OR-Agent research tree traversal procedure.             

> Initialization : -Sample parent solutions from the solution database to form the root node .

9Figure 5: Research tree management process illustration.                                                              

> Research Start : -Expand the root node to generate up to max_children children nodes . Research Loop : 1. Select the best unfinished leaf node N_i . 2. Extend N_i by generating child ideas , code implementations , and experimental results . 3. Truncation and local optimality test : -Retain only children that improve upon N_i . -If no child improves upon N_i , mark N_i as terminal . 4. Terminate when all leaf nodes are terminal ; otherwise repeat .

Unlike conventional evolutionary algorithms, OR-Agent allows temporary performance degradation when exploring new research directions. Child nodes are not required to outperform the root or previous elite immediately, reflecting the reality that early experiments along a novel direction may initially underperform. Termination decisions are instead governed by sustained lack of improvement, controlled via configurable parameters. The research tree management process is illustrated in 5. 

Context Engineering for Tree Awareness. When expanding a node in the research tree and generating child ideas, different strategies can be adopted for constructing the input context provided to the Lead Agent. In particular, OR-Agent supports multiple levels of contextual scope: i) including only the immediate parent node; ii) including the full ancestry path from the root to the current node; iii) including the entire research tree to provide a global view of the ongoing research process. OR-Agent exposes these options as configurable choices and, by default, adopts the third strategy, enabling the Lead Agent to maintain global situational awareness over the research landscape. This design allows the agent to reason not only about local refinements but also about the overall exploration–exploitation balance across the tree. To facilitate effective comprehension by large language models, the research tree is serialized into a structured textual representation with explicit formatting conventions. In particular, the notation 10 format and legend are presented before the tree content itself. This ordering reduces ambiguity during autoregressive token processing, ensures that symbolic meanings are well established before they are encountered in context, reducing misinterpretation and improving downstream reasoning. A recom-mended serialization format is illustrated in Figure 6.             

> Format: Node <ID> (<score>): ’<idea>’ Legend:
> ⊕= solution expanded with improved children solution(s)
> ◦= solution pending expansion
> ✓= terminal solution (approximate local optimum, no improvement found) ======================================== Node 0 (0.80): Implement a look-ahead nearest neighbor... +--⊕Node 1 (0.90): Design lightweight structural proxies... |+--✓Node 3 (0.85): Construct normalized distance-to-demand ratio matrix... +--⊕Node 2 (0.70): Leverage graph Laplacian structure... +--◦Node 4 (0.75): Use min-max demand normalization... ======================================== Total expanded solutions: 2 | Total pending leaves: 1 | Total terminal leaves: 1

Figure 6: Research tree representation used by OR-Agent. Figure 7 provides an illustrative example of multiple research rounds, showing how the size and shape of the research tree evolve dynamically over time. Promising research directions are explored more deeply, while unproductive branches are pruned. Although many high-quality solutions originate from elite parent nodes, the figure also highlights that valuable discoveries can emerge from non-elite starting points, underscoring the importance of maintaining sufficient exploratory breadth. The Tree-Structured Research Workflow constitutes a core distinction between OR-Agent and prior LLM-based evolutionary or agentic frameworks. By explicitly modeling scientific research as a structured, evolving tree of hypotheses—with coordinated idea generation, controlled traversal, and global context awareness—OR-Agent enables deeper, more systematic exploration of complex problem spaces while retaining flexibility and empirical grounding. 

# 2.5 Iterative Experimentation and Environment Probing 

Modern agentic frameworks emphasize that effective problem solving is fundamentally feedback-driven: agents must iteratively act, observe environmental signals, and update their hypotheses and imple-mentations accordingly (e.g., ReAct[YZY +22]-style interaction loops). This principle is particularly critical for scientific and engineering tasks where candidate programs are executed in complex environ-ments (simulators, or black-box pipelines) that expose rich failure modes. In such settings, a single-pass generation-and-evaluate procedure is often inefficient because candidate code frequently contains minor defects (e.g., missing imports, interface mismatches, off-by-one errors) that can be corrected through lightweight debugging, and because early solutions typically require parameter calibration and tar-geted refinements to become competitive. Purely evolutionary LLM approaches that rely primarily on mutation/crossover without an explicit experimentation loop (e.g., many program-evolution baselines) may therefore waste substantial compute on avoidable execution failures and shallow evaluations. To address this, OR-Agent introduces an Experiment Agent that performs iterative experimentation and environment-aware refinement for each candidate solution node. The Experiment Agent forms a closed-loop process that (i) diagnoses execution and performance issues, (ii) applies bounded code and parameter updates, (iii) probes the environment via callbacks when necessary, and (iv) produces an experiment summary that distills actionable insights for subsequent idea development. 

Feedback-Driven Refinement Loop. Given a candidate solution implementation, the Experiment Agent executes the evaluation script and receives environment outputs (metrics, logs, errors, traces). It then iteratively proposes modifications in three categories: 

• Debugging and executability fixes : resolving syntax/runtime errors, missing imports, and minor interface issues. In practice, a small set of standard library imports and common utilities can be pre-inserted into the solution template to reduce frequent failure cases. 11 Figure 7: Illustration of dynamic research tree evolution across multiple research rounds. 

• Parameter tuning : adjusting algorithmic hyperparameters to improve objective score while preserving validity constraints (e.g., safety or feasibility). 

• Micro-refinements of algorithm logic : localized changes that do not constitute a conceptual redesign (major redesigns are deferred to new research ideas and new tree nodes). To make code updates precise and automatable, OR-Agent constrains modification proposals to a deterministic patch format using conflict-marker-style diff blocks[NVE +25]. 

Environment Exploration via Callbacks. In complex environments, aggregate metrics alone are frequently insufficient to diagnose failure causes (e.g., timeouts, deadlocks, safety violations, or brittle corner cases). OR-Agent therefore augments experimentation with environment exploration through callbacks. The Experiment Agent can request changes to callback definitions that control what the environment exposes (e.g., intermediate state summaries, per-step diagnostics, constraint violation traces). By iteratively refining callbacks, the agent can progressively narrow down hypotheses about failure mechanisms, closely mirroring the scientific practice of instrumenting an experiment to observe previously hidden variables. Because callback classes are typically lightweight, OR-Agent permits complete callback rewrites when necessary. Moreover, the Experiment Agent is instructed to generate detailed and structured analyses of experimental outcomes wrapped by “thinking” tags (inspired by the benefits of explicit reasoning chains [WWS +22]), including the identification of recurring patterns across trials and the proposal of targeted interventions for subsequent experiments. At present, OR-Agent supports environment exploration primarily through textual outputs. While this design choice simplifies integration with a wide range of simulators and evaluation pipelines, it also limits the expressiveness of environmental feedback. Extending the framework to incorporate richer modalities—such as visual traces, trajectory plots, or interactive state visualizations—represents a promising direction for future work and may further enhance the agent’s ability to diagnose complex failure modes. 

Dynamic Allocation of Experiment Rounds. A fixed budget for experimentation is subopti-mal: promising directions often deserve more iterations, while clearly unpromising candidates should 12 be terminated early to conserve compute. OR-Agent implements dynamic experiment round allocation ,where the maximum number of refinement attempts for a candidate is adapted based on its observed potential (e.g., relative improvement over parent, elitist status, and objective direction). Intuitively, candidates that show meaningful improvement receive expanded experimentation budgets, while can-didates that regress are allocated fewer attempts. In addition, the Experiment Agent may terminate early when (i) the primary bottleneck has been identified but cannot be fixed by minor edits, or (ii) repeated attempts fail to yield progress, in which case the agent produces an experiment summary emphasizing persistent failure causes and actionable redesign directions. 

Context Compression for Long Experiment Histories. Iterative experimentation introduces a well-known systems challenge: the context required to condition subsequent decisions grows with the number of experiments, potentially overwhelming model attention even when the nominal context win-dow is large. OR-Agent incorporates a context compression mechanism that periodically summarizes experiment history into a progressive report. This enables long experiment chains (e.g., > 50 rounds) while preserving the most salient information: best score so far, performance evolution, key insights, and persistent issues (See Appendix C for example). In parallel, verbose raw environment output for each evaluation is truncated using a simple but robust strategy (head+tail retention) to preserve early configuration signals and late-stage error traces (e.g., exception backtraces), while still informing the agent of the total length of omitted content. Alternative summarization strategies (e.g., learned extractors or LLM-based compression of raw logs) are compatible and left as extensible components. 

Experiment Summarization and Knowledge Extraction. At the end of experimentation, OR-Agent requires the Experiment Agent to produce a holistic report that aggregates findings across trials. The report emphasizes (i) overarching performance trends, (ii) parameter impacts, (iii) documented failures and negative results, (iv) success factors that consistently improved metrics, and (v) unresolved issues requiring conceptual redesign. This summary is not merely a log; it functions as distilled scientific evidence that is attached to the solution node and used by the Lead Agent and Idea Agent to generate higher-quality subsequent hypotheses (See example in Appendix C. A complete solution artifact in OR-Agent therefore comprises three components: (i) a concise natural-language research idea that articulates the conceptual direction, (ii) an executable implementation accompanied by docstrings and explanatory comments detailing the design rationale, and (iii) an experiment summary that records empirical findings, diagnostic insights, and refinement trajectories across iterations. 

Learning from Failures and Invalid Outcomes. Complex evaluators frequently produce par-tial successes (e.g., improvements on small instances but timeouts on larger ones). OR-Agent treats such outcomes as valuable learning signals rather than discarding them as failures. The Experiment Agent is instructed to extract actionable insights from invalid outcomes (e.g., diagnosing computa-tional complexity as the limiting factor, identifying which subroutine dominates runtime, or proposing approximations). These insights directly inform subsequent research rounds and can guide idea gen-eration toward scalable algorithmic structures (See example in Appendix D. 

Solution Reversion Mechanisms. Iterative refinement can regress performance. OR-Agent sup-ports two complementary reversion strategies. “Soft reversion” encourages the Experiment Agent—through explicit prompting near the final attempt—to revert to a previously better configuration if recent changes degrade performance. Because soft reversion relies on the model’s compliance, OR-Agent additionally implements “hard reversion”: the experimentation controller tracks the best-performing code snapshot during the experiment sequence and restores it upon termination when the final version is worse. The final experiment summary explicitly records the reversion decision and the rationale, ensuring that the downstream workflow reasons over the best available implementation while retaining diagnostic knowledge from the unsuccessful variants (See example in Appendix E). 

Experiment Agent Workflow Summary. Overall, the Experiment Agent executes a closed-loop workflow as shown by Listing 2. The experimentation module constitutes a key capability for deploying OR-Agent in realistic environments, where effective research progress depends on iterative diagnosis, controlled refinement, and systematic extraction of empirical knowledge. 13 Listing 2: Experiment Agent workflow.                                                              

> Step 0) Prepare the experiment ( initialize code +callbacks ) Step 1) Experiment loop : 1.1) Execute evaluation and collect outputs 1.2) Analyze results and decide next action 1.3) If update_code : apply diff patch ( minor edits only ) If update_callbacks : rewrite callbacks for deeper probing If terminate : stop experimentation 1.4) Periodically compress history into aprogressive summary 1.5) Stop if ‘ max_experiment_repeats ‘ reached Step 2) Revert to best snapshot if needed and generate final experiment summary

# 2.6 Reflection Mechanism as Semantic Optimizer 

Reflection has been identified as a critical capability for autonomous agents to improve performance through iterative self-evaluation and revision [SCG +23]. In OR-Agent, reflection is not treated as an auxiliary narrative component, but as a core control primitive that organizes empirical evidence, guides subsequent edits, and stabilizes long-horizon search. The central view adopted in this work is that 

designing a reflection mechanism is analogous to designing an optimizer in conventional numerical optimization . Whereas classical optimizers transform gradients into parameter updates, reflection transforms environment feedback into structured semantic signals that determine what to change, how aggressively to change it, and when to stop refining a direction. 

Hierarchical Reflections. OR-Agent implements a hierarchical reflection stack that aggregates feedback at increasing temporal scales, similar to how optimization algorithms accumulate information over minibatches, epochs, and training trajectories. Three levels of reflections are maintained: 

• Experiment reflections are produced after each experiment step. They capture local diag-nostics (e.g., failure causes, metric changes, suspected bottlenecks) and are appended to the immediate experimentation context in a ReAct-style feedback loop. 

• Experiment summaries are produced at the end of a solution’s experimentation process. They compress the entire refinement trajectory for a single solution node, highlighting performance evolution, causal attributions for parameter changes, persistent issues, and actionable recommen-dations. These summaries are stored as part of the solution artifact and serve as the primary interface between experimentation and higher-level idea development. 

• Long-term reflections are maintained across an entire research round by the Lead Agent. They are updated when several new solution summaries become available, distilling cross-solution reg-ularities (e.g., repeatedly successful heuristics, recurring failure modes, robust design principles) into compact guidance that biases subsequent expansions of the research tree. This hierarchy forms a directed aggregation pipeline: 

Experiment reflections → Experiment summaries → Long-term reflections .

Reflection as a Semantic Analogue of Optimization Dynamics. Prior work has characterized short-term reflection as a form of verbal gradient [YWC +24], where natural-language feedback plays a role analogous to gradient information in numerical optimization. OR-Agent extends this perspective by mapping different reflection levels to distinct optimizer components: 

• Experiment reflections as verbal gradients. Each step-level reflection encodes a local direction of improvement derived from the latest environment signal (e.g., “reduce teleport events by adding deadlock resolution” or “tighten feasibility by penalizing bottleneck weights”). These reflections guide the next micro-update to code or parameters. 

• Experiment summaries as batch-averaged verbal gradients. By integrating evidence across multiple trials, experiment summaries reduce variance and emphasize consistent causal effects, resembling minibatch or epoch-averaged gradient estimates. They also explicitly distin-guish transient improvements from stable gains, improving downstream decision-making. 14 • Long-term reflections as semantic momentum. Long-term reflections accumulate repeated lessons across multiple solution trajectories, thereby stabilizing search and preventing oscillation between incompatible heuristics. This is analogous to momentum terms that integrate historical gradients to accelerate consistent directions and dampen noise. Moreover, reflection compression constraints provide an additional control handle. Imposing a fixed budget on reflection length forces iterative rewriting to preferentially retain the most salient informa-tion while discarding less useful details. This behavior can be interpreted as a semantic analogue of exponential decay in optimizer state: older or weaker signals gradually vanish unless repeatedly rein-forced. Together, these components form a principled hierarchical reflection architecture that governs research dynamics in OR-Agent. 

Adaptive Control via Reflection-Guided Resource Allocation. Beyond content guidance, re-flections also support agentic control over computational resource allocation. In classical adaptive optimizers, effective step sizes are adjusted based on gradient statistics: small gradients permit larger steps, while large or unstable gradients encourage conservative updates. OR-Agent supports analogous adaptation in the semantic domain. When experimentation indicates a rapidly improving direction (e.g., consistent score increases or clear diagnostic clarity), the system can allocate additional ex-perimentation budget, increase sampling breadth, or employ stronger inference resources (e.g., larger LLMs). Conversely, when reflections indicate instability, stagnation, or irreducible bottlenecks under minor edits, the system reduces budget and terminates early, returning a high-quality summary for higher-level redesign. This view connects reflection design to optimizer design: reflections become the state variables that modulate “step size” (edit magnitude), “batching” (summary cadence), and “computation” (experiment rounds and model capacity). 

Customizability and Research-Time Tuning. OR-Agent exposes multiple configuration points for reflection, enabling users to instantiate different “semantic optimizers” depending on task charac-teristics and compute budgets: 

• Update cadence: how frequently long-term reflections are refreshed (e.g., after accumulating a fixed number of solution summaries). 

• Persistence: whether long-term reflections persist across research rounds or are reset per round. 

• Compression budget: target length constraints for experiment history compression and long-term reflections, trading completeness for stability and scalability. These controls are practically important: overly frequent long-term updates may overfit to early noisy signals, whereas overly sparse updates may fail to propagate valuable discoveries in time. Similarly, persistent long-term reflections may accelerate progress across rounds but risk entrenching suboptimal biases if not periodically refreshed. An example long-term reflection accumulated during heuristic search for a multi-dimensional knap-sack setting (MKP ACO) is shown in Appendix F. It highlights the role of long-term reflections in capturing robust principles (e.g., bottleneck-aware ratios and simplicity) and in identifying persistent gaps (e.g., the inability of static heuristics to adapt online), thereby guiding future idea generation. In summary, OR-Agent operationalizes reflection as a semantic optimization mechanism : environment feedback is progressively transformed into structured guidance that controls refinement, stabilizes long-horizon search, and allocates computation adaptively. This optimizer-inspired view unifies short-term debugging signals, solution-level experimental evidence, and round-level scientific principles into a coherent control hierarchy that supports efficient and systematic deep research. 

# 2.7 OR-Agent as a Framework for Balancing Exploration and Exploitation 

This section has presented the design of the OR-Agent, which is explicitly targeted at complex opera-tions research problems characterized by large, structured search spaces, delayed feedback, and strong interdependencies between ideas, implementations, and experimental outcomes. Such problems require both the generation of diverse, independent research hypotheses and the ability to deeply investigate 15 and refine promising directions. OR-Agent addresses this challenge by combining evolutionary explo-ration mechanisms with structured, feedback-driven exploitation, and by exposing fine-grained control over the trade-offs between them. At a high level, OR-Agent employs evolutionary crossover and mutation to generate independent re-search ideas, supporting broad exploration of the hypothesis space. These ideas are then organized and refined through a tree-structured research workflow, which enables focused, iterative investigation of high-potential directions. This design reflects a deliberate separation between exploration —discovering qualitatively different approaches—and exploitation —systematically improving and validating promis-ing candidates through experimentation and reflection. Striking an effective balance between exploration and exploitation is a classical and well-studied challenge in optimization and search. Traditional genetic algorithms enforce this balance implicitly through population-level operators and selection pressure, but such mechanisms are often rigid and insufficiently expressive for complex research settings. OR-Agent adopts a more flexible and explicit approach, allowing the exploration-exploitation trade-off to be controlled at multiple levels of the research process. Concretely, this trade-off manifests in several orthogonal design dimensions within OR-Agent: 

• Research tree shape control : A shallow and wide research tree emphasizes parallel exploration across diverse ideas, while a deep and narrow tree prioritizes intensive refinement of a small number of directions. 

• Dynamic allocation of experimentation resources : Experimentation budgets are adapted based on observed improvement signals, enabling promising solutions to receive more refinement while curtailing unproductive directions early. 

• Parent selection from the solution database : Sampling strategies range from near-uniform selection (favoring exploration) to elite-biased selection (favoring exploitation), with temperature-controlled interpolation between the two. The optimal configuration of these controls is problem-dependent. Empirically, simpler tasks tend to benefit from predominantly evolutionary crossover and mutations, whereas more difficult problems often require a careful balance between parallel exploration and deep exploitation. While it is generally infeasible to prescribe a universally optimal ratio, OR-Agent is designed to be sufficiently flexible: it exposes a rich set of parameters that allow users to adapt the system to task complexity, computational budget, and desired research style. OR-Agent integrates and generalizes ideas from prior LLM-based evolutionary and agentic systems. Through appropriate configuration (e.g., tree depth, branching factor, database sampling strategy), existing methods such as FunSearch [RPBN +24], AEL [LTYZ23], and ReEvo [YWC +24] can be closely approximated within the OR-Agent framework. At the same time, native implementations of baseline algorithms are retained for direct comparison and ablation studies. Figure 8 provides an overview of the end-to-end solution generation process of OR-Agent, illustrating how evolutionary initialization, tree-structured refinement, experimentation, reflection, and database interaction are orchestrated into a unified research workflow. 

# 3 Case Study 

We present two representative case studies: the Traveling Salesman Problem (TSP) and the Coop-erative Driving Problem. The TSP serves as a combinatorial optimization benchmark with clearly defined objective and fixed environment feedback. In contrast, the cooperative driving task involves a complex traffic simulation environment with multiple objectives, long-horizon dependencies and rich environment feedback. Together, these two tasks evaluate OR-Agent across both classical optimization settings and complex dynamic systems. 

# 3.1 Traveling Salesman Problem 

The Traveling Salesman Problem (TSP) is a classical combinatorial optimization problem that seeks the shortest possible tour visiting each city exactly once and returning to the starting city. Due to its NP-hard nature, TSP serves as a standard benchmark for evaluating heuristic design, reinforcement 16 Figure 8: Overview of the solution generation process in OR-Agent. 17 learning policies, and combinatorial search methods. In this case study, TSP is solved using the POMO (Policy Optimization with Multiple Optima) [KCK +20] reinforcement learning framework. Rather than training the policy end-to-end, the objective of OR-Agent is to design a task-specific heuristics 

function that generates an attention bias matrix for the POMO neural network. The quality of a candidate solution is evaluated by the average tour distance across test instances of different sizes (200, 500, and 1000 cities). For this relatively structured combinatorial problem, no environment callbacks are required. 

Round-Level Iterative Refinement. Experiments on TSP POMO during one research round is shown in Appendix G, illustrating how OR-Agent conducts structural improvement and parameter refinement. 

Experiment 1: Timeout Diagnosis and Structural Simplification. The initial candidate solution introduced a geometric graph ensemble combining Gabriel graph, Relative Neighborhood Graph (RNG), and k-nearest neighbor graph (k-NNG). However, evaluation resulted in a 300-second timeout for large-scale instances. The Experiment Agent correctly identified that nested triple loops in the Gabriel and RNG constructions resulted in O(n3) complexity, rendering the approach compu-tationally infeasible for large TSP instances. The agent compared this solution to its parent (Node 54), which achieved stable performance. Consequently, the entire heuristics function was rewrit-ten (via a full merge-conflict diff update) to remove cubic geometric constructions while preserving effective structural components. For relatively simple OR problems such as TSP, it was observed that the Experiment Agent frequently performs complete functional rewrites rather than minor parameter adjustments—highlighting a deviation from the intended “small-step refinement” assumption. Re-evaluation yielded: {200 : 11 .112 , 500 : 20 .929 , 1000 : 31 .413 }, score = 21 .151. The timeout issue was resolved while preserving performance. 

Experiment 2: Structural Refinement of k-NNG. The Experiment Agent next refined the k-NNG computation, making k adaptive to problem size and adjusting bonus weights. This revi-sion targeted large-scale connectivity, hypothesizing that larger instances require denser neighborhood structures. The updated evaluation results were: {200 : 11 .114 , 500 : 20 .870 , 1000 : 31 .288 }, score = 21 .091. Although the agent described the 200-city result (11.114 vs. 11.112) as a “slight improvement,” this was in fact a minor degradation. This illustrates a known limitation of LLM reasoning over floating-point comparisons. Nevertheless, the overall score improved due to gains in larger instances, demonstrating effective large-scale bias tuning. 

Experiment 3: Parameter-Level Tuning. Subsequent revisions focused on fine-tuning MST bonus coefficients and size-dependent scaling factors. These adjustments were minor relative to the structural changes in earlier steps, reflecting a shift from architectural revision toward local optimiza-tion. 

Research Tree Dynamics. The complete research tree for this round is shown in Figure 9. The round was initialized from an elite solution (Node 51) as the root. Five child nodes (Nodes 52–56) were generated, representing distinct research directions forming a comprehensive exploration plan. OR-Agent does not require immediate improvement at depth 1; deeper expansions are permitted. The maximum tree depth was set to 3. A child node represents a refinement or extension of its parent’s idea. For example, Node 57 inherits the core concept introduced in Node 53 and achieves a modest performance improvement. Except for the root, child nodes are retained only if they achieve perfor-mance improvement relative to their parent. After maximum number of failed expanding attempts, a non-improving leaf node is marked as “local approximate optimum” and no longer expanded. All ter-minal nodes are returned to the solution database; alternative filtering mechanisms may be considered in future implementations. The final research tree contained 14 nodes, of which 7 were expanded and 7 were terminal. The best solution of the round improved the database record from 21.12 to 20.98. This example demonstrates several important behaviors of OR-Agent: 1. Progressive revision through staged refinement. Individual experimental attempts could yield temporary degradations in performance. Performance improvements typically emerged through multiple experimental iterations rather than single-step updates. 2. Systematic branching. Multiple independent directions were explored concurrently, increasing the probability of discovering non-obvious improvements. 18 3. Depth-based discovery. The best solution did not appear among immediate children but required deeper refinement. Overall, this case study illustrates how OR-Agent integrates evolutionary initialization, tree-structured exploration, and iterative refinement to progressively improve heuristic quality. 

>[LeadAgent] Final research tree for research round #12: Format: Node <ID> (<score>): "<idea>" Legend: 

⊕ = solution expanded with improved children solution(s) 

✓ = terminal solution (local optimum, no improvement found) PS: Only root node may contain more than one solutions. ======================================== Node [51 (21.12)]: ["Integrate Delaunay triangulation as a complementary global structure to MST..."] +- ✓ Node 52 (21.07): "Develop a scale-invariant normalization scheme for distance-based heuristics..." +-- ⊕ Node 53 (21.05): "Replace the KNN-based Delaunay approximation with a mutual Gabriel graph construction..." | +-- ✓ Node 57 (21.03): "Develop a hybrid geometric-graph heuristic that combines the mutual Gabriel graph with a k-Nearest Neighbor Graph (k-NNG) using adaptive edge fusion..." +-- ⊕ Node 54 (21.15): "Introduce dynamic, instance-aware bonus allocation by computing structural complexity metrics (e.g., coefficient of variation in local density, MST total weight relative to random baseline) and using them to modulate MST and Delaunay-like bonuses through learned or rule-based functions..." | +-- ✓ Node 71 (21.09): "Develop a coordinate-free geometric graph ensemble that combines multiple sparse proximity graphs..." | +-- ⊕ Node 72 (21.10): "Replace hand-crafted bonus scaling with a lightweight, offline-trained meta-heuristic predictor that maps structural features of the distance matrix to optimal attention bias coefficients.." | +-- ✓ Node 75 (21.09): "Develop a dynamic k-selection mechanism for the Delaunay-like k-NN graph that adapts k per node based on local density metrics..." +-- ✓ Node 55 (21.10): "Implement a lightweight coordinate reconstruction module using differentiable classical MDS (Multidimensional Scaling) as a pre-processing step..." +-- ⊕ Node 56 (21.12): "Reformulate the local neighborhood heuristic to use entropy-weighted ranking instead of uniform rank decay..." +-- ⊕ Node 65 (21.02): "Develop a hybrid attention bias scheme that combines coordinate-free geometric graphs (MST, mutual Gabriel graph, and k-NNG) with a lightweight differentiable MDS module to reconstruct 2D coordinates..." | +-- ✓ Node 67 (20.98): "Develop a learned, lightweight fusion mechanism that dynamically weights the contributions of local neighborhood heuristics..." +-- ⊕ Node 66 (21.12): "Design a dynamic, instance-adaptive normalization and bonus allocation mechanism that replaces fixed scaling factors (e.g., MST bonus = 0.5) with data-driven modulation based on intrinsic problem characteristics..." +-- ✓ Node 70 (21.09): "Introduce a tour-construction-aware dynamic biasing mechanism that simulates partial greedy path extensions during heuristic computation to identify ’critical edges’|those that frequently appear in early-stage greedy tours or resolve bottlenecks in sparse regions..." ======================================== Total expanded solutions: 7 | Total pending leaves: 0 | Total terminal leaves: 7 >[LeadAgent] number of solutions to return: 7 (IDs: 52, 55, 57, 67, 70, 71, 75) >[Database] Best score updated to 20.98 

Figure 9: Research tree example for one research round on TPS POMO problem. 

# 3.2 Cooperative Driving Problem 

This case study considers a cooperative driving control problem implemented in the SUMO (Simulation of Urban MObility) traffic simulator [Ecl26]. The objective is to design a Python function named 

driving actions , which takes road segment information and real-time vehicle states as inputs and outputs driving actions (including acceleration and lane-change decisions) for all vehicles on a given 19 road segment. The resulting algorithm is evaluated using multi-objective criteria including safety, speed efficiency, and traffic smoothness, and the overall goal is to maximize a scalar performance score derived from these metrics. Unlike the Traveling Salesman Problem, this task involves a complex simulation environment. It exhibits several characteristics that significantly increase difficulty. 

Diverse and Complementary Idea Generation. At the beginning of a research round, OR-Agent generated multiple complementary high-level strategies, forming a coherent research plan rather than isolated mutations. Representative examples are shown in Listing 3. These ideas demonstrate signif-icant structural diversity while addressing different dimensions of the coordination problem (conflict anticipation, dynamic routing, cooperative grouping, and adaptive safety modeling). This joint gen-eration phase reflects OR-Agent’s ability to explore heterogeneous research directions. Listing 3: Example of coordinated idea generation for cooperative driving problem.                                                                                                                                

> 1. Develop apredictive conflict - resolution framework that uses short - term trajectory forecasting to proactively resolve lane - change conflicts before they arise , replacing reactive swap detection with anticipatory coordination based on vehicle intentions and kinematic constraints . 2. Implement an adaptive urgency - aware lane assignment system that dynamically re - evaluates target lanes using real - time congestion metrics from ’ potential_target_lanes ’ , prioritizing lanes that balance route continuation with current traffic flow efficiency rather than static best - lane offsets . 3. Design ahierarchical cooperative driving protocol where vehicles form temporary coordination groups based on shared route segments and spatial proximity , enabling synchronized acceleration / deceleration patterns that reduce speed variance and improve platoon stability beyond simple pairwise interactions . 4. Introduce amachine learning - based gap acceptance model trained on historical safe lane - change scenarios to replace fixed gap thresholds , allowing the algorithm to adapt gap requirements based on contextual factors like vehicle types , speeds , and traffic density while maintaining safety guarantees .

Structured Output Engineering. The Experiment Agent must output multiple structured com-ponents, including: (1) analytical reasoning, (2) code updates, (3) callback updates, and (4) a termi-nation decision in JSON format. Some LLMs prematurely terminated generation immediately after emitting a JSON block, treating it as an end-of-response signal. To mitigate this issue, the output format was restructured to require reasoning wrapped within <thinking> tags at first, then code re-visions expressed in conflict-marker diff blocks, callback updates in fenced Python blocks, and the termination decision strictly placed at the end in JSON format. 

Environment Exploration via Callbacks. The Experiment Agent introduced monitoring call-backs to expose intermediate state information (e.g., number of lane change attempts, low-speed vehicle counts, congestion warnings). The first callback-enhanced evaluation produced more than 120 lines of diagnostic output for each test case, which exceeded context limits and required truncation. After refining callbacks, the output was reduced to around 12 focused diagnostic lines, enabling clearer hypothesis formation. Check Appendix H and public repository [LM26] for more details. 

Soft Reversion. During experimentation, performance occasionally regressed due to overly aggres-sive parameter modifications. In one instance, increasing deceleration severity reduced TTC violations but reintroduced collisions. The Experiment Agent autonomously reverted to a previous configuration, recognizing that safety improvements must not compromise stability (Listing 4). This soft reversion mechanism mirrors human debugging practice, where overfitting corrections are rolled back to restore balance. Listing 4: Example of soft reversion for cooperative driving problem.                  

> >[ ExperimentAgent ] Thinking : The latest experiment ( Experiment #4) shows asignificant regression from Experiment #3. The score dropped from 83.669

20 to 62.611 , a decrease of over 21 points ..( omitted for brevity ) ... The issue seems to be that increasing the conflict severity factor made vehicles brake too hard to avoid predicted conflicts , creating new safety problems . We need to revert to a more conservative approach while still maintaining the improvements in critical TTC count . 

Termination of Unpromising Directions. In several cases, repeated experiments revealed persis-tent structural deficiencies, such as near-zero vehicle mobility, massive collision counts, and deadlock formation. When incremental parameter tuning failed to address architectural flaws, the Experiment Agent terminated the direction and documented the need for a fundamental redesign (Listing 5). This behavior prevents wasted computational effort on irreparable solution families. Listing 5: Example of termination of unpromising directions. 

>[ Evaluator ] Evaluating solution : lead1_round0_count3_id3 metrics : { ’ emergencyStops ’: 0.0 , ’ emergencyBraking ’: 0.0 , ’ teleports ’: 8.0 , ’ avg_speed ’: 0.32 , ’ critical_ttc_count ’: 1114.0 , ’ collisions ’: 5117.0 , ’ speed_variance ’: 2.4} features : (0 , 0 , 3) scores : -128252.668 >[ ExperimentAgent ] Thinking : The latest experiment failed due to a syntax error , preventing proper evaluation . However , analyzing the pattern across all previous experiments reveals fundamental and persistent issues that cannot be resolved through simple parameter tuning or minor code adjustments ...( omitted for brevity ) ... The problem requires a fundamental shift from individual optimization to coordinated multi - agent interaction with explicit protocols for conflict resolution , priority management , and deadlock prevention >[ ExperimentAgent ] Action after experiment 4: terminate 

Parameter Tuning in High-Dimensional Space. The cooperative driving case also demonstrates fine-grained parameter tuning behavior. Adjustments to safety margins (e.g., SAFE TTC , MIN GAP ) often revealed trade-offs between safety and efficiency. Increasing safety buffers reduced TTC violations but degraded traffic throughput, illustrating the inherent multi-objective tension (Listing 6). Listing 6: Example of parameter tuning. 

>[ ExperimentAgent ] Thinking : The latest experiment shows continued safety issues ...( omitted for brevity ) ... >[ ExperimentAgent ] Action after experiment 2: update code <<<<<<< SEARCH # Safety parameters SAFE_TTC = 2.5 # Safe time - to - collision threshold ( seconds ) MIN_GAP = 2.0 # Minimum gap for car following ( meters ) ; Note : gap does not include vehicle length LANE_CHANGE_GAP_BASE = 15.0 # Base gap required for lane changes ( meters ) ; Note : gap does not include vehicle length OFFSET_BUFFER = 2.0 # Safety buffer added to vehicle length for lane - changing ( meters ) ; Note : buffer does not include vehicle length ======= # Safety parameters SAFE_TTC = 3.0 # Safe time - to - collision threshold ( seconds ) MIN_GAP = 1.6 # Minimum gap for car following ( meters ) ; Note : gap does not include vehicle length LANE_CHANGE_GAP_BASE = 10.0 # Base gap required for lane changes ( meters ) ; Note : gap does not include vehicle length OFFSET_BUFFER = 1.6 # Safety buffer added to vehicle length for lane - changing ( meters ) ; Note : buffer does not include vehicle length 

21 >>>>>>> REPLACE 

Emergence of Fragile “Spaghetti Code”. An interesting phenomenon was observed in exper-iments: certain highly complex rule-based implementations achieved strong local performance but exhibited extreme sensitivity to minor changes. Small parameter perturbations caused catastrophic score collapse. This resembles the “spaghetti code” phenomenon observed in human software engineer-ing—systems that function under narrow conditions but lack structural robustness. This observation suggests that LLM-generated rule systems can overfit to simulation idiosyncrasies. A representative example is provided in the public repository [LM26]. 

Baseline-Surpassing Solution. After approximately four days of single-process execution, OR-Agent discovered a cooperative driving algorithm that surpassed the SUMO baseline controller. The resulting solution employed simple cooperative swap and platoon coordination primitives combined flexibly rather than relying on heavy global optimization. Compared to traditional operations research approaches that attempt to jointly optimize all vehicles through centralized mathematical program-ming—which is difficult to model and scale—the discovered strategy achieved strong empirical perfor-mance through decentralized yet coordinated rules. Implementation details and video comparisons are available in the public repository [LM26]. 

# 4 Experiments 

# 4.1 Experimental Setup 

Benchmarks. The benchmark suite is adapted from ReEvo and extended with additional problems, including a cooperative driving task implemented in SUMO [Ecl26]. The complete benchmark specifi-cation is publicly available in the project repository. The suite includes 12 classical combinatorial op-timization problems across multiple paradigms (ACO, GA, POMO, LEHD, direct constructive heuris-tics), as well as a complex cooperative driving problem. These problems span routing (TSP, CVRP), packing (BPP, MKP), electronic design (DPP), orienteering (OP), and simulation-based multi-vehicle coordination. 

Baselines. We compare OR-Agent against representative LLM-based algorithm discovery frame-works: FunSearch [RPBN +24], AEL [LTYZ23], EoH [LTY +24], and ReEvo [YWC +24]. For the coop-erative driving problem, we additionally compare against the SUMO default driving model. 

LLM Configuration. Two LLMs were evaluated: Qwen3 [YLY +25] for the 12 classical OR prob-lems, and DeepSeek V3.2 [LML +25] for the cooperative driving problem. 

Performance Metric. To enable fair comparison across heterogeneous problems, we report a nor-malized score . For problem i and algorithm j, the normalized score is defined as: NormalizedScore ij = score ij − best i

best i − worst i .

This metric measures the relative performance gap to the best observed solution on that problem. 

Computation Budget. Performance is evaluated under two complementary computational budgets: Number of LLM calls, and Number of function evaluations. Depending on the application scenario, either LLM inference cost or evaluation cost may become the primary bottleneck. Therefore, we report performance curves with both quantities as the horizontal axis. All test scripts, hyperparameters, and full logs are available in the repository for reproducibility. 22 Table 1: Normalized scores across 12 classical OR benchmark problems. Problem FunSearch AEL EoH ReEvo OR-Agent TSP-Constructive 0.000 0.878 0.788 1.000 0.959 TSP-ACO 0.252 0.000 0.386 0.258 1.000 

TSP-POMO 0.000 0.975 1.000 0.771 0.884 TSP-LEHD 0.648 0.000 0.175 0.259 1.000 

CVRP-ACO 0.000 0.403 1.000 0.471 0.680 CVRP-POMO 0.000 1.000 0.418 0.681 0.986 CVRP-LEHD 0.000 0.285 0.855 0.033 1.000 

BPP-Online 0.913 1.000 0.728 0.000 0.948 BPP-Offline-ACO 0.290 0.270 0.000 0.290 1.000 

DPP-GA 0.294 0.000 1.000 0.190 0.787 MKP-ACO 0.833 0.000 0.633 0.951 1.000 

OP-ACO 0.653 0.916 0.000 1.000 0.839 

Average 0.323 0.477 0.582 0.492 0.924 

Algorithm LLM Calls : Evaluations Ratio FunSearch / AEL / EoH ≈ 1 : 1 ReEvo ≈ 7 : 5 OR-Agent ≈ 8 : 5 Table 2: Ratio between number LLM calls and number of evaluations. 

# 4.2 Results on Classical OR Problems 

Figures 10 and 11 present objective value curves across 12 benchmark problems. Table 1 shows the normalized scores across 12 benchmark problems. OR-Agent achieves the highest average normalized score (0.924), substantially outperforming all baselines. 

Behavioral Observations. Algorithms without deep refinement mechanisms (e.g., FunSearch, AEL) tend to obtain most performance gains within the first 20–50 generations. Initial heuristics are rel-atively weak and easy to improve upon, but subsequent gains become marginal. FunSearch, relying primarily on mutation, exhibits the largest performance variance across problems. In our settings, OR-Agent periodically performs in-depth refinements. This results in step-like improvements in per-formance curves. 

Computation Characteristics. Compared to other methods, OR-Agent makes more LLM calls per evaluation (Table 2) because it performs multiple revisions per solution (approximately three on average under our settings). The additional LLM usage reflects iterative refinement rather than naive generation. 

Tree Size and Computational Scaling. When tree depth is unconstrained, completing a full research tree may require extended running time. Because tree expansion continues as long as performance-improving directions can be identified, deep research rounds can become computationally intensive. The size of the research tree grows exponentially with the branching factor (number of children per node). Table 3 and the tree visualizations in Appendix I illustrate this scaling behavior. “Total solutions generated” refers to all solutions ever produced during the search process, including those leading to worse performance and subsequently pruned from the tree. “Total tree nodes” counts all nodes retained in the tree structure, including those later superseded by better solutions. As shown in Table 3, increasing num children from 1 to 4 results in exponential growth in both tree size and computational time. All measurements were conducted on a system with 4 Intel(R) Xeon(R) CPU Max 9468 processors and 16 GB RAM. Based on these observations, we recommend using num children =2 or 3 for simpler problems and num children =4 or 5 for more complex problems, while scaling hardware resources accordingly to 23 Figure 10: Performance comparisons on six problems ( TSP CONSTRUCTIVE , TSP POMO , CVRP ACO ,

CVRP LEHD , BPP OFFLINE ACO , MKP ACO ). 24 Figure 11: Performance comparisons on the remaining six problems ( TSP ACO , TSP LEHD , CVRP POMO ,

BPP ONLINE , DPP GA , OP ACO ). 25 num children Total solutions Total tree Returned Time (h) generated nodes solutions 

1 5–10 2–4 1 0.5–1 2 10–30 4–8 2–4 1–4 4 30–80 8–16 4–8 4–16 Table 3: Effect of branching factor (number of children) on computational requirements. 

Figure 12: Growth of long-term reflection length (in characters) during a single experiment without explicit compression limits. maintain computational efficiency. In practice, running multiple lead agents in parallel is recommended to balance deep exploitation and broad exploration, with some agents focusing on in-depth refinement while others performing faster exploratory searches. Controlling the size of the research tree actually provides a mechanism for managing the explo-ration–exploitation trade-off. One potential strategy is to impose stricter child-admission criteria, allowing only directions that demonstrate sufficiently significant improvement to be retained. How-ever, such an approach requires defining what constitutes “significant” improvement, which may vary across tasks and introduce additional hyperparameters. To avoid problem-specific tuning, we instead adopt a simpler and more uniform strategy in this study: directly imposing a maximum tree depth. The same configuration is applied consistently across all benchmark problems. 

Reflection Growth Analysis Figure 12 shows the growth of long-term reflection length during one research round without explicit limit. Reflection length increases progressively and then plateaus at a model-dependent steady length which is smaller than the context window. For both Qwen3 and DeepSeek V3.2, reflection memory saturates at approximately 1,000 tokens. This suggests that long-term reflection behaves like bounded momentum memory rather than unbounded accumulation. 

# 4.3 Results on Cooperative Driving 

Figure 13 presents driving performance comparisons under equal computational budgets. Under con-strained runtime, OR-Agent achieves a score of 48.00, significantly surpassing the highest baseline score (16.10), as shown by Table 4. Baseline methods frequently converge to overly conservative strategies: vehicles avoid collisions but move extremely slowly, forming congested local optima. OR-Agent es-capes such traps through structured research trees, environment exploration, and repeated refinement. The gap becomes more pronounced as runtime increases. Notably, OR-Agent is the only algorithm that discovers a solution exceeding the SUMO default driving model after extended running. The OR-Agent solution improves both efficiency and smoothness while maintaining zero collisions. Under extended runtime, OR-Agent surpasses the SUMO default behavior, achieving an average score of 90.24 compared to 85.25. 26 Figure 13: Performance comparisons on driving problem. Table 4: Detailed performance comparison on the Cooperative Driving Problem. Safety metrics include collisions and critical TTC events. Efficiency is measured by average speed (m/s), and smoothness by speed variance.                                  

> Model Case Collisions Critital TTC Avg Speed Speed Var. Case Avg (m/s) Score Score SUMO Default Light Traffic 010 11.63 6.48 95.18 85.25 Heavy Traffic 060 10.36 11.00 75.32 OR-Agent (Best) Light Traffic 012 11.94 5.31 96.08 90.24
> Heavy Traffic 046 11.12 7.74 84.40

# 4.4 Ablations 

We conduct ablation studies along four dimensions: thinking depth, elite-root scheduling, memory compression, and long-term reflection usage during crossover. All ablations are performed under fixed computational budgets to ensure fair comparison. 

Ablation on Thinking Depth. We investigate the classical exploration–exploitation trade-off by varying the depth and per-node experimentation budget of the research tree. Under a fixed total computational budget, deeper investigation of specific regions necessarily reduces overall coverage of the search space. We compare three configurations: “deep exploration” mode with increased maximum tree depth and per-node experiment limit; “fast exploration” mode with reduced depth and limited refinement iterations; and “standard” mode with intermediate parameters (default setting). As shown in Figure 14, both “deep exploration” and “fast exploration” underperform the standard configuration. Excessively deep search limits diversity and increases the risk of over-investing in suboptimal regions, while overly shallow search sacrifices refinement quality. The standard configuration achieves a better balance between global coverage and local optimization, highlighting the importance of calibrated thinking depth. 

Ablation on Elite-Root Periodicity. Another critical factor in exploration–exploitation balance is how frequently the algorithm restarts research from elite solutions. Using elite solutions as roots intensifies exploitation, while less frequent elite selection promotes diversity. We evaluate different elite-root periods (2, 4, 8, 16, 32). As shown in Figure 15, a period of 8 yields the best overall performance among tested values. Short periods (e.g., 2 or 4) overly bias the search toward elite refinement, reducing diversity. Long periods (e.g., 16 or 32) weaken exploitation and slow convergence. These results suggest that moderate elite reuse provides an effective balance. 

Ablation on Memory Compression. Long-term reflection serves as accumulated “semantic mo-mentum”. Compressing reflection memory functions analogously to gradient decay in optimization, potentially attenuating accumulated directional signals. We evaluate four compression levels: 100 words, 200 words, 400 words, and no explicit compression (approximately 4,000 words under the tested 27 Figure 14: Ablation on thinking depth. Balanced tree depth yields superior performance compared to both excessively deep and overly shallow configurations. models). Across six representative problems, no compression achieves the best performance in four cases and yields the highest overall average score. Interestingly, aggressive compression to 100 words performs second-best, while moderate compression (400 words) performs worst. This non-monotonic pattern suggests that partial compression may remove useful high-level signals without sufficiently improving signal-to-noise ratio. The exact mechanisms underlying this phenomenon warrant further investigation. 

Ablation on Long-Term Reflection Usage During Crossover. When parent solutions are sampled for crossover, an important design choice is whether to include long-term reflection in the prompt context. Two competing hypotheses are evaluated: (i) leveraging accumulated knowledge through reflection can accelerate search by building on past insights; (ii) removing reflection encourages innovative thinking—akin to scientists thinking “outside the box”. We tested two configurations: enabling and disabling long-term reflection usage during crossover. Across six benchmark problems, each configuration outperforms the other in three cases (Figure 17). No universally superior strategy emerges. These results suggest that the optimal use of reflection during crossover may be problem-dependent, and adaptive mechanisms could be explored in future work. 

# 5 Related Work 

# 5.1 AI for Operations Research 

The application of AI techniques to operations research (OR) has evolved through several method-ological paradigms. Early successes were driven by reinforcement learning (RL) and neural combi-natorial optimization (NCO), where neural policies are trained end-to-end to construct or improve solutions for combinatorial optimization problems. Representative approaches include policy-gradient or actor-critic methods for routing and scheduling [ZD95, SSK20], as well as NCO frameworks such 28 Figure 15: Ablation on elite-root usage period. A moderate period (8) achieves the best trade-off between exploitation and diversity. as POMO[KCK +20] and LEHD[LLL +23], which define a structured solution pipeline and search for effective policies within that space. These methods demonstrate strong performance when large-scale training data and stable problem distributions are available, but they often require substantial retrain-ing when problem settings change and provide limited interpretability of the resulting heuristics. More recently, large language models have been explored as optimizers or reasoning engines for OR problems. In this line of work, LLMs are used to directly generate candidate solutions, improve solution quality through iterative prompting, or translate natural language problem descriptions into mathematical formulations and executable code. While such approaches benefit from the general reasoning and coding capabilities of LLMs, they typically operate in a shallow optimization loop and rely heavily on in-context learning. As a result, they face scalability challenges when the search space is large or when critical information about the problem cannot be fully encoded within the model context. A parallel line of work has focused on using LLMs to formulate optimization problems rather than solve them directly. Systems such as ORLM [HTH +25], LLM-OPT [JSQ +24], and OPTIMUS [AGU23] translate natural language problem descriptions into mathematical formulations and executable code. Hyper-heuristic (HH) methods provide a unifying perspective on these developments. Classical HHs search over a space of heuristics or heuristic components to identify effective strategies for a given problem class. Recent LLM-based approaches further expand this space by allowing heuristics to be represented and manipulated as code or natural language descriptions. However, most existing HH and LLM-based OR systems remain limited to relatively simple evaluation settings and lack mecha-nisms for deep, environment-driven exploration. OR-Agent builds on this line of work by explicitly targeting complex environments, where effective optimization requires iterative experimentation and active information gathering beyond static prompts. 

# 5.2 Evolutionary Algorithm Discovery with LLMs 

Evolutionary computation and genetic programming have a long history in automatic algorithm and heuristic design. Traditional genetic programming evolves programs through predefined mutation and crossover operators applied to a population of candidate solutions [Koz94, SB01]. While powerful in principle, these methods depend critically on the choice of primitives and operators, which are often difficult to design for complex, real-world problems. Large language models have recently been introduced as flexible evolutionary operators, signifi-cantly broadening the scope of evolutionary program search [LGJ +23]. FunSearch [RPBN +24, LTYZ23] demonstrated that LLM-guided mutation and selection can lead to genuine scientific discoveries in mathematics, establishing program evolution as a viable paradigm for AI-driven discovery. LMX [MNB +24] showed that feeding multiple parent programs into an LLM enables semantically meaningful crossover across domains including code, mathematics, and natural language. Subsequent works such as EoH [LTY +24] further explored LLM-based evolution of heuristics, including joint evolution of high-level ideas and low-level code. ReEvo [YWC +24] framed LLMs as language hyper-heuristics, integrating reflective mechanisms to guide evolutionary search, while AlphaEvolve [NVE +25] scaled this paradigm to large, real-world algorithmic discovery tasks, emphasizing code-level evolution for interpretability and deployability. 29 Figure 16: Ablation on long-term reflection compression. No compression achieves the strongest overall performance, while moderate compression performs worst. Despite these advances, comparative studies have shown that LLM-based evolutionary program search methods exhibit highly variable performance across problems and models, and that generative capability alone is insufficient for robust heuristic design. A common limitation is that evolution is often driven by shallow fitness feedback, with limited capacity to probe the underlying problem structure or to systematically refine promising solutions through targeted experimentation. Moreover, most approaches emphasize population-level evolution through mutation and crossover without systematic local exploration and refinement around promising candidates—a critical component of human research methodology. OR-Agent extends this body of work in several directions. First, it retains the strengths of LLM-based evolutionary operators for generating diverse candidate programs, but embeds them within a tree-search-based workflow that supports deep, iterative refinement. Second, it augments evolutionary search with explicit environment exploration, enabling agents to discover critical problem features that are not accessible through static evaluation alone. Finally, the proposed reflection mechanisms reinterpret optimization concepts such as gradients and momentum at the level of natural language and code, providing a principled way to stabilize and accelerate evolutionary search. 

# 5.3 Automated Scientific Research Systems 

Beyond algorithm and heuristic discovery, a growing literature aims to automate broader aspects of the scientific research process. Early systems sought to partially automate hypothesis generation or experiment execution, while more recent agent-based frameworks attempt to orchestrate the en-tire research lifecycle. The AI Scientist [LLL +24, YLL +25] introduced a pipeline that autonomously performs idea generation, experimentation, analysis, and writing, highlighting both the promise and the limitations of linear research workflows. Follow-up systems like Dolphin [YYS +25] and InternA-gent [ZFY +25] explored more sophisticated orchestration strategies [TXLH25] like closed-loop feed-back systems between hypothesis and verification. Tree-structured approaches organize research as a branching process, enabling systematic exploration of alternative hypotheses before committing to a 30 Figure 17: Ablation on long-term reflection usage during crossover. Neither configuration dominates across all problems. specific direction [HMF +25, YLL +25]. Multi-agent systems further decompose the research process into specialized roles, such as ideation, coding, and evaluation, improving modularity and scalability [GWD +25, HZF +25]. While these systems demonstrate impressive autonomy, most are designed for relatively abstract or self-contained research tasks, such as benchmark-driven machine learning experiments. Their ap-plicability to domains with complex, partially observable environments remains limited. In particular, they often assume that the problem description and evaluation criteria can be fully specified upfront, reducing the need for active environment exploration. OR-Agent positions itself at the intersection of automated scientific research and operations research. It adopts a multi-agent architecture and tree-search-based workflow inspired by recent research agents, but grounds them in the concrete set-ting of OR problems with executable environments. By integrating evolutionary ideation, structured workflow management, and environment-aware experimentation, OR-Agent bridges the gap between program evolution methods and general research automation frameworks. In doing so, it provides a practical step toward AI systems that can collaborate with human researchers on complex scientific problems, rather than merely executing predefined optimization routines. Prior work has established strong foundations in AI for OR, LLM-guided evolutionary program search, and automated scientific research. However, existing approaches often struggle with complex environments, rely on shallow feedback signals, or lack structured mechanisms for balancing exploration and refinement. OR-Agent addresses these gaps by unifying evolutionary program discovery with tree-search-based research workflows and explicit environment exploration, offering a coherent framework for AI-assisted scientific discovery in operations research. 

# 6 Conclusion 

In this work, we presented OR-Agent , an Open Research Agent designed for complex operations re-search and scientific discovery tasks. OR-Agent integrates evolutionary population management, tree-31 structured research workflows, hierarchical reflection mechanisms, and structured solution databases into a unified multi-agent framework. Unlike prior LLM-based evolutionary approaches that rely primarily on mutation and crossover, OR-Agent explicitly models scientific research as a structured process combining exploration, system-atic refinement, experimentation, and knowledge accumulation. By organizing research trajectories as trees rather than flat populations, OR-Agent enables deeper local optimization while preserving global diversity. The structured evolutionary database mitigates premature convergence and popula-tion ruin, while hierarchical reflections function as semantic optimization signals that guide iterative improvement across research rounds. Extensive experiments on twelve classical operations research benchmark problems demonstrate that OR-Agent achieves the strongest overall performance among state-of-the-art LLM-based algorithm discovery methods. On the cooperative driving problem—a significantly more complex environment with multi-objective and long-horizon characteristics—OR-Agent not only substantially outperforms baseline LLM-based approaches under constrained budgets, but also discovers solutions that surpass the default SUMO driving model after extended exploration. These results highlight the importance of structured research workflows, controlled exploration–exploitation trade-offs, reflection-based knowl-edge accumulation, and environment exploration ability. Overall, OR-Agent provides a flexible and extensible blueprint for building autonomous research agents capable of structured reasoning, iterative experimentation, and long-term knowledge integra-tion. 

Future Directions. This project represents an initial step toward general-purpose autonomous research agents. Several promising directions remain open. First, multi-modal feedback integration is not yet supported. Currently, OR-Agent operates exclusively on textual feedback from evaluation environments. Many scientific and engineering tasks naturally produce visual or temporal outputs (e.g., simulation videos, trajectory plots, diagrams). Incorporating visual and video-based feedback could significantly enhance diagnostic capability and reasoning depth. Second, online knowledge retrieval and literature integration would substantially broaden OR-Agent’s research capacity. Enabling the agent to retrieve and synthesize relevant academic papers, extract common methodological patterns, and incorporate domain knowledge could accelerate idea generation and improve solution quality. Such functionality would move OR-Agent closer to a true AI co-scientist capable of situating discoveries within existing research landscapes. Third, greater automation of the full research lifecycle is desirable. Beyond generating algorithms, future versions could automatically draft research reports, document experimental trajec-tories, and maintain structured research logs. Recording the entire research process—including failed attempts—would enhance reproducibility and meta-level learning. Fourth, the current memory mechanism relies primarily on compressed textual summaries. More structured memory representations—such as graph-based knowledge stores, structured experiment logs, or hierarchical semantic embeddings—may allow more efficient retrieval and reasoning over ac-cumulated insights. Finally, multi-agent collaboration mechanisms can be further expanded. In the present design, different lead agents exchange knowledge indirectly through the shared solution database. More direct collaboration—such as cross-agent critique, joint hypothesis refinement, or coordinated division of research space—may further improve exploration efficiency and reduce redundant search. We believe that advancing these directions will contribute toward more capable, autonomous, and collaborative AI research systems that complement human scientific discovery. 

# 7 Acknowledgment 

This research was funded by the National Natural Science Foundation of China (grant number 52325210, 52131204, 52402407) and Shanghai Baiyulan Talent Project Pujiang Program (grant number 24PJD115). 32 References 

[AGU23] Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell. Optimus: Optimization modeling using mip solvers and large language models. arXiv preprint arXiv:2310.06116 , 2023. [Ecl26] Eclipse Foundation. Eclipse SUMO – Simulation of Urban MObility. https://eclipse. dev/sumo/ , 2026. Accessed: 2026-02-14. [GWD +25] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864 , 2025. [HMF +25] Yusong Hu, Runmin Ma, Yue Fan, Jinxin Shi, Zongsheng Cao, Yuhao Zhou, Jiakang Yuan, Xiangchao Yan, Wenlong Zhang, Lei Bai, et al. Flowsearch: Advancing deep research with dynamic structured knowledge flow. arXiv preprint arXiv:2510.08521 , 2025. [HTH +25] Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, and Zizhuo Wang. Orlm: A customizable framework in training large models for automated optimization modeling. Operations Research , 2025. [HZF +25] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, et al. Owl: Optimized workforce learn-ing for general multi-agent assistance in real-world task automation. arXiv preprint arXiv:2505.23885 , 2025. [JSQ +24] Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun Zhou, Aimin Zhou, and Yang Yu Llmopt. Learning to define and solve general optimization problems from scratch. arXiv preprint arXiv:2410.13213 , 2024. [KCK +20] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Se-ungjai Min. Pomo: Policy optimization with multiple optima for reinforcement learning. 

Advances in Neural Information Processing Systems , 33:21188–21198, 2020. [Koz94] John R Koza. Genetic programming as a means for programming computers by natural selection. Statistics and computing , 4(2):87–112, 1994. [LGJ +23] Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O Stanley. Evolution through large models. In Handbook of evolutionary machine learning ,pages 331–366. Springer, 2023. [LLL +23] Fu Luo, Xi Lin, Fei Liu, Qingfu Zhang, and Zhenkun Wang. Neural combinatorial op-timization with heavy decoder: Toward large scale generalization. Advances in Neural Information Processing Systems , 36:8845–8864, 2023. [LLL +24] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292 , 2024. [LM26] Qi Liu and Wanjing Ma. OR-Agent. https://github.com/qiliuchn/OR-Agent , 2026. Accessed: 2026-02-14. [LML +25] Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556 , 2025. [LTY +24] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language model. arXiv preprint arXiv:2401.02051 , 2024. [LTYZ23] Fei Liu, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Algorithm evolution using large language model. arXiv preprint arXiv:2311.15249 , 2023. 33 [MNB +24] Elliot Meyerson, Mark J Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompt-ing. ACM Transactions on Evolutionary Learning , 4(4):1–40, 2024. [NVE +25] Alexander Novikov, Ngˆ an V˜ u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: A coding agent for scientific and algorithmic discovery. 

arXiv preprint arXiv:2506.13131 , 2025. [Ros11] Christopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artificial Intelligence , 61(3):203–230, 2011. [RPBN +24] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Ba-log, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large lan-guage models. Nature , 625(7995):468–475, 2024. [SB01] Stefan Sette and Luc Boullart. Genetic programming: principles and applications. Engi-neering applications of artificial intelligence , 14(6):727–736, 2001. [SCG +23] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural In-formation Processing Systems , 36:8634–8652, 2023. [SHM +16] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. 

nature , 529(7587):484–489, 2016. [SLXK24] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time com-pute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314 , 2024. [SSK20] Chathurangi Shyalika, Thushari Silva, and Asoka Karunananda. Reinforcement learning in dynamic task scheduling: A review. SN Computer Science , 1(6):306, 2020. [SYH24] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109 ,2024. [TXLH25] Jiabin Tang, Lianghao Xia, Zhonghang Li, and Chao Huang. Ai-researcher: Autonomous scientific innovation. arXiv preprint arXiv:2505.18705 , 2025. [WWS +22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824–24837, 2022. [YLL +25] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foer-ster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066 , 2025. [YLY +25] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025. [YWC +24] Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo Park, and Guojie Song. Reevo: Large language models as hyper-heuristics with reflective evolution. Advances in neural information processing systems , 37:43571–43608, 2024. [YYS +25] Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, and Bowen Zhou. Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. arXiv e-prints , pages arXiv–2501, 2025. 34 [YZY +22] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations , 2022. [ZD95] Wei Zhang and Thomas G Dietterich. A reinforcement learning approach to job-shop scheduling. In Ijcai , volume 95, pages 1114–1120, 1995. [ZFY +25] Bo Zhang, Shiyang Feng, Xiangchao Yan, Jiakang Yuan, Runmin Ma, Yusong Hu, Zhiyin Yu, Xiaohan He, Songtao Huang, et al. Internagent: When agent becomes the scientist– building closed-loop system from hypothesis to verification. arXiv e-prints , pages arXiv– 2505, 2025. 35 A Metrics, Feature, and Score Definitions for the Driving Problem 

This section describes the evaluation protocol used for the Cooperative Driving Problem. The evalua-tion script returns three components: raw metrics extracted from the simulator, feature representations for diversity preservation, and a scalar fitness score for optimization. It illustrates how metrics, fea-tures, and scores are formally defined to enable automated problem evaluation. 1. Raw metrics extracted from the simulation environment, 2. Feature vectors derived from metrics for diversity preservation, 3. A scalar fitness score used for optimization. Together, these components define the optimization landscape explored by OR-Agent. 

# A.1 Raw Evaluation Metrics 

Each candidate driving policy is evaluated in the SUMO simulator under multiple traffic scenarios. The simulator returns aggregated metrics per test case. An example output is shown below: Listing 7: Example evaluation metrics returned by the simulator.         

> {’ critical_ttc_count ’: 28 , ’ collisions ’: 0 , ’ emergencyStops ’: 0 , ’ emergencyBraking ’: 4 , ’ teleports ’: 0 , ’ avg_fuel_consumption ’: 8.32 , ’ avg_speed ’: 12.51 , ’ speed_variance ’: 16.22 }

The metrics are grouped into three categories: 

• Safety indicators : collisions, emergency stops, emergency braking events, teleports (simulation failures), and critical time-to-collision (TTC) counts. 

• Efficiency indicators : average speed and fuel consumption. 

• Smoothness indicators : speed variance. Metrics are aggregated across multiple test scenarios before further processing. 

# A.2 Feature Vector Construction 

Feature vectors are used for diversity preservation in the solution database (e.g., in MAP-Elites style clustering). Unlike scalar fitness, features characterize behavioral signatures rather than overall quality. 

Behavioral Signature (Recommended). The recommended feature representation discretizes ag-gregated metrics into interpretable categorical levels representing safety, efficiency, and smoothness. Each dimension is mapped to a discrete scale: Signature = (safety level , speed efficiency level , smoothness level) ,

where each level takes values in {0, 1, 2, 3} (0 for “poor”; 1 for “weak”; 2 for “good”; 3 for “excellent”). 

Safety level is primarily determined by collisions and teleports, followed by emergency events and critical TTC counts. Speed efficiency level measures deviation from a predefined urban target speed. Smoothness level is determined by speed variance, with lower variance corresponding to higher smoothness. This discretization yields interpretable behavioral clusters such as: Safe–Conservative–Smooth or 

Risky–Fast–Aggressive . The granularity of discretization determines the potential diversity resolution of the archive. 36 Alternative Representations. Other feature construction strategies are also supported: 

• Continuous normalized features : for clustering methods such as k-means, 

• Risk-profile signatures : emphasizing safety–performance trade-offs. OR-Agent does not enforce a fixed representation; feature design is treated as part of task specification. 

# A.3 Scalar Fitness Function 

While feature vectors maintain diversity, optimization is guided by a scalar fitness score. The scoring function reflects three prioritized objectives: Safety , Speed efficiency , and Driving smoothness .For each test case, three sub-scores are computed. 

Safety Score. Safety begins with a baseline of 100 and receives additive penalties: 

Ssafety = 100 −50 ·collisions −30 ·teleports −5·emergencyStops −2·emergencyBraking −0.5·criticalTTC .

Collisions and teleports incur the most severe penalties, reflecting their critical importance. 

Speed Efficiency Score. Speed efficiency is measured by deviation from a predefined urban target speed v∗. Scores decay piecewise as deviation increases: 

Sspeed = f  |v − v∗|,

where f (·) is a monotonic decreasing function. Maximum score is achieved when average speed is close to the target range. 

Smoothness Score. Smoothness is inversely related to speed variance: 

Ssmoothness = g(speed variance) ,

where g(·) is a piecewise linear decreasing function. Lower variance yields higher smoothness scores. 

Final Combined Score. The final score for each test case is a weighted combination: Score = 0 .5 · Ssafety + 0 .3 · Sspeed + 0 .2 · Ssmoothness .

Safety receives the highest weight, reflecting its dominance in real-world driving systems. Speed efficiency and smoothness serve as secondary objectives once safety constraints are satisfied. 

# A.4 Design Principles 

The evaluation design follows three guiding principles: 1. Safety dominance. Severe penalties ensure unsafe policies are strongly disfavored. 2. Targeted efficiency. Policies are rewarded for maintaining realistic urban speeds. 3. Stability preference. Lower speed variance promotes smooth traffic flow. By separating behavioral features from scalar fitness, OR-Agent simultaneously encourages diversity exploration and objective-driven optimization. 

Remark. The definitions of metrics, features, and scoring functions fundamentally shape the op-timization landscape. OR-Agent deliberately exposes these definitions as configurable components rather than hard-coded constraints, enabling domain experts to tailor objective trade-offs according to practical requirements. 37 B Coordinated Idea Generation vs. Independent Idea Gener-ation Example 

This section compares two idea generation strategies used during research tree expansion: independent generation and coordinated generation . Empirical observations suggest that the choice of generation strategy significantly affects diversity, coverage, and systematic exploration. 

# B.1 Independent Idea Generation 

In the independent generation setting, child ideas are produced by invoking the LLM multiple times separately, each time conditioned only on the parent solution. Even when increasing temperature (e.g., up to 1.3), the diversity of generated ideas remains surprisingly limited. As an illustrative example, consider the Online Bin Packing Problem (BPP-Online). Twenty ideas were independently generated from the same parent solution. A representative sample is shown in Listing 8. Listing 8: Twenty independently generated child ideas for BPP-Online from the same parent solution. 

1. Combine value - to - weight ratio with a sparsification mechanism that zeros out items whose total weight exceeds a dynamic threshold based on average knapsack capacity usage . Specifically , compute the initial heuristic as prize divided by the L1 norm of weight ( as in the elite solution ) , then set to zero any item whose max weight across constraints exceeds 1/ sqrt ( m ) ( reflecting tighter per - dimension feasibility ) , or alternatively , zero out the bottom 30% of items by this ratio to focus search on most promising candidates . 2. Combine value - to - weight ratio with a sparsification mechanism that zeros out items whose total weight exceeds the average capacity usage per item , to focus selection on both efficient and feasible items . Specifically , compute the initial heuristic as prize divided by the L1 norm of weight ( as in the elite solution ) , then set to zero any item whose total weight ( sum across constraints ) is greater than the median total weight of all items , thereby promoting inclusion of lighter , high - value items while pruning potentially infeasible heavy ones early . 3. Combine value - to - weight ratio with a sparsification mechanism that zeros out items whose total weight exceeds a dynamic threshold based on average knapsack capacity usage . Specifically , compute the initial heuristic as prize divided by the sum of weights ( as in the elite solution ) , then set to zero any item whose maximum weight across all constraints exceeds 1 /m ( the average per - constraint capacity ) , making it less likely to be selected due to high risk of violating constraints . 4. Combine value - to - weight ratio with a sparsification mechanism that zeros out items whose total weight exceeds the average capacity usage per item . Specifically , compute the standard value - to - total - weight ratio ( prize /sum ( weight , axis =1) ) , then identify items with total weight greater than the median total weight across all items ; set their heuristic value to zero to discourage selection of overly heavy items unless they are exceptionally valuable . This balances desirability with feasibility under multiple constraints . 5. Combine value - to - weight ratio with a sparsification mechanism that zeros out items whose total weight exceeds the average capacity pressure across constraints . Specifically , compute the initial heuristic as prize divided by the L1 norm of weight ( as in the elite solution ) , then set to zero any item where the maximum weight across constraints exceeds a threshold ( e . g . , 1/ m or the median of max weights ) . This focuses ant attention on items that are both valuable per unit resource and feasible under tightest constraints . ...( Check public repository for the complete list ) ... 

Despite nominal stochastic variation, most ideas follow nearly identical structural patterns: 

• Variants of prize-to-weight ratio heuristics, 38 • Minor modifications of sparsification thresholds, 

• Slight adjustments of L1 vs L2 norms, 

• Similar median-based pruning mechanisms. The dominant theme—“combine value-to-weight ratio with sparsification”—appears repeatedly with minimal conceptual deviation. This phenomenon indicates that independent sampling, even at elevated temperature, tends to produce locally perturbed variations rather than structurally distinct hypotheses. 

# B.2 Coordinated Idea Generation 

In contrast, OR-Agent employs coordinated generation , where all child ideas of a node are produced in a single LLM invocation. The model is explicitly instructed to generate multiple distinct directions simultaneously. Using the same BPP-Online parent solution, twenty coordinated ideas are shown in Listing 9. Listing 9: Twenty coordinated child ideas for BPP-Online generated in a single LLM call.                                                                                                            

> 1. Normalize the prize - to - weight ratio per constraint dimension and aggregate via geometric mean to better reflect balanced resource usage across all constraints . 2. Introduce asparsification step that sets heuristic values to zero for items whose total weight exceeds athreshold ( e . g . , average capacity usage ) , focusing selection on feasible candidates . 3. Incorporate apenalty term based on the maximum weight of an item across all constraints to discourage selection of items that dominate any single resource . 4. Compute heuristic as prize divided by the L2 norm of the weight vector , emphasizing compactness in multi - dimensional weight space . 5. Use min - max normalization of weights per dimension before computing prize - to - weight ratios to ensure equitable contribution from each constraint . ...( Check public repository for the complete list ) ...

Compared to independent generation, coordinated generation exhibits: 

• Substantially reduced redundancy, 

• Broader heuristic families (geometric mean aggregation, surrogate LP relaxation, harmonic mean weighting, ensemble heuristics, diversity factors), 

• Structural diversity rather than threshold tuning. This behavior resembles the scientific practice of enumerating multiple conceptual improvement di-rections before experimental validation. Rather than sampling near-identical local perturbations, the model produces a systematic hypothesis set covering multiple design axes. 

# B.3 Advantages and Trade-offs 

Advantages of Coordinated Generation. 

1. Improved diversity. Ideas exhibit greater conceptual separation. 2. Better coverage. Exploration spans multiple heuristic paradigms. 3. Systematic reasoning. The model implicitly allocates different “slots” to distinct improvement themes. 39 Limitations. Coordinated generation is not without drawbacks: 

• Individual ideas tend to be shorter when many are generated simultaneously. 

• Explicitly constraining word count per idea is possible but inelegant, as optimal length is task-dependent. 

• Generating many ideas in one call is constrained by the context window. Thus, coordinated generation introduces a trade-off between diversity and per-idea elaboration. 

# B.4 Implications for Research Tree Expansion 

Within OR-Agent, each research tree node expands via coordinated generation. All child hypotheses are produced together, evaluated separately, and selectively retained based on performance. This de-sign reduces redundant branching and improves structural exploration efficiency. While independent generation can be seen as Monte Carlo sampling in local neighborhoods, coordinated generation re-sembles structured hypothesis decomposition. Empirical results across multiple problems indicate that coordinated generation improves exploration breadth without significantly increasing computational overhead, thereby contributing to more stable research progression. 

# C Experiment History Compression Example 

This section provides an illustrative example of experiment history compression within OR-Agent. As described in the main text, experiment reflections are progressively summarized to prevent unbounded context growth while preserving key insights for subsequent reasoning. Listing 10 shows a progressive summary generated by the ExperimentAgent after four experiments on cooperative driving problem. Instead of retaining all raw experiment logs verbatim, OR-Agent synthesizes structured summaries that capture performance trends, diagnostic observations, and actionable improvement directions. Listing 10: Example of progressive experiment history compression generated by the ExperimentAgent after four experiments. 

>[ ExperimentAgent ] Progressive summary generated : Progressive Summary for experiments #1 ~ #4: - Initial score : -35.132 - Current score : None - Best score so far : -23.697 Key insights from recent experiments : Experiment #1: Score : -35.132 | Metrics : avg_speed : 0.430 , collisions : 0.0 , emergencyStops : 0.0 , teleports : 4.5 , critical_ttc_count : 155.0 , emergencyBraking : 0.0 , speed_variance : 3.595 | Insight : < insight > The latest experiment results show poor performance with an overall score of -35.13 , which is worse than the parent solution ’ s score of -14.07. Key issues identified : 1. ** Extremely low average speeds ** (0.43 m / s vs target 13.89 m / s ) : Vehicles are moving too slowly , likely due to over - conservative acceleration and excessive cooperation adjustments . 2. ** High critical TTC count ** (155 vs 79.5) : Many near - collision events indicate vehicles are following too closely , suggesting the IDM parameters are too aggressive for this traffic scenario . 3. ** Teleports ** (4.5) : Vehicles are getting stuck and being removed by SUMO , indicating potential deadlocks or excessive waiting . 4. ** Complex negotiation system ** may be causing vehicles to slow down excessively while waiting for cooperation or lane changes . Comparing to the parent solution #1 which had better safety ( lower critical TTC ) and lower teleports , we observe that our current hierarchical cooperative negotiation system might be overcomplicated . The parent solution used predictive platoon formation with anticipatory gap creation , which is simpler and more focused on maintaining flow . 

40 However , this is only the first experiment with this solution , so we should try to improve it through parameter tuning and minor adjustments before considering termination . Specific improvements we can make : - Increase following distances ( time headway and minimum gap ) to reduce critical TTC events . - Simplify the urgency calculation to reduce unnecessary lane changes . - Ensure vehicles maintain minimum speeds to avoid teleports . - Reduce the impact of cooperative adjustments to prevent excessive slowing . - Clean up state for vehicles that have left the simulation to prevent memory issues . - Add hysteresis to lane change decisions to reduce frequent lane changes . We ’ ll implement these changes through targeted code modifications . </ insight > Experiment #2: ...( omitted for brevity ) ... 

# D Learning From Failures Example 

During experimentation, OR-Agent frequently encounters suboptimal or even infeasible solutions. Im-portantly, such failures are not discarded; instead, they serve as valuable sources of structured knowl-edge. The Experiment Agent is designed to extract diagnostic signals even when evaluation partially fails (e.g., timeouts or runtime errors). The following example demonstrates this behavior in the TSP CONSTRUCTIVE problem. In this case, the largest test instance (100 cities) timed out, yet the Experiment Agent was still able to analyze earlier results and synthesize meaningful insights regarding algorithmic behavior and scalability. Listing 11: Example of learning from a partially failed experiment (timeout on large-scale instance). 

>[ Evaluator ] lead1_round5_count0_id9 - Error : no end marker found in the result : [*] Evaluating val20_dataset . npy Average for 20 cities : 4.164 [*] Evaluating val50_dataset . npy Average for 50 cities : 6.786 [*] Evaluating val100_dataset . npy Timed out after 300 seconds >[ ExperimentAgent ] Thinking : The experiment series focused on developing ahybrid regret - lookahead heuristic for the Traveling Salesman Problem that balances computational efficiency with global awareness . The initial implementation combined regret - based node selection with conditional multi - step lookahead , achieving baseline scores of 4.45 (20 cities ) , 7.12 (50 cities ) , and 9.78 (100 cities ) . Key improvements in Experiment #1 included : (1) revising the regret calculation to measure opportunity cost as the difference between second - best and best connection costs from each candidate node , (2) adjusting the lookahead activation threshold to use median regret plus 20% of the regret range rather than a fixed proportion , (3) expanding end - game optimization from <=4 to <=5 remaining nodes , and (4) improving the fallback selection strategy to simultaneously optimize for high regret and low immediate distance . These changes yielded significant performance improvements for smaller instances (20 cities : 4.16 , 50 cities : 6.79) , though the 100 - city evaluation timed out , suggesting computational complexity remains problematic at larger scales . The most impactful parameter adjustments were the refined regret calculation methodology and the expanded end - game lookahead window . Persistent challenges include the fundamental limitation of greedy constructive approaches lacking global tour awareness and computational overhead that scales poorly with problem 

41 size . Successful strategies included using regret as a trigger mechanism for deeper analysis and incorporating progress - aware dynamic thresholds . Unresolved issues center around scalability and the inherent myopia of sequential decision - making without backtracking . For future research , we recommend : (1) implementing more efficient lookahead approximations such as sampling - based future cost estimation , (2) integrating geometric insights like nearest neighbor clustering to reduce effective problem dimensionality , (3) developing adaptive regret thresholds that consider both tour progress and instance - specific characteristics , and (4) exploring lightweight machine learning models to predict high - value lookahead opportunities without exhaustive computation . 

This behavior reflects a key design philosophy of OR-Agent: experimental failures are not terminal states but informative transitions in the research trajectory. By abstracting structural lessons from imperfect executions, the agent approximates the human scientific process of hypothesis revision under incomplete evidence. 

# E Solution Reversion Example 

During iterative experimentation, it is common for performance to deteriorate after a series of mod-ifications. In such cases, reverting to a previously better-performing solution before terminating the experiment can be beneficial. OR-Agent supports two forms of solution reversion: soft reversion and 

hard reversion .

# Soft Reversion via Prompt Engineering 

Soft reversion is implemented through explicit prompting. The Experiment Agent is instructed that, if performance degrades and the experiment is approaching termination, it may revert to a previous version of the solution code or parameter configuration. For example: 

Important: this is the final attempt to improve the solution. You may revert to a previous version of the solution code or parameter configuration if the latest attempt is unsatisfac-tory. Ensure the final solution code is executable. 

In many cases, large language models successfully perform this reasoning-driven rollback. An illustrative example from the OP ACO problem is shown below. Listing 12: Soft reversion example: regression detection and rollback reasoning. 

>[ Evaluator ] Metrics extracted for solution ( id ) : lead1_round1_count0_id21 metrics : {50: 11.227 , 100: 14.992} features : (11 , 14) scores : 13.109 >[ ExperimentAgent ] Thinking : Looking at the latest experiment results and comparing with previous ones : Previous experiment (#2) : - Problem size 50: 11.234 - Problem size 100: 15.199 - Overall score : 13.217 Current experiment (#3) : - Problem size 50: 11.227 ( slight decrease of ~0.007) - Problem size 100: 14.992 ( decrease of ~0.207) - Overall score : 13.109 ( decrease of ~0.108) I can observe a clear pattern : my recent changes led to a deterioration in performance . The modifications I made were : 

42 1. Reduced clustering radius from 0.2* max_dist to 0.15* max_dist 2. Changed high_value_threshold back from 70 th to 75 th percentile 3. Adjusted weights to 0.6/0.25/0.15 ( more emphasis on basic heuristic ) 4. Removed some of the depot - specific handling logic The decrease in performance suggests that my changes were not beneficial . The previous configuration with 0.5/0.3/0.2 weights and the more sophisticated depot handling was actually better . Looking at the overall trend : - Experiment 1: Score 11.860 - Experiment 2: Score 13.217 ( significant improvement ) - Experiment 3: Score 13.109 ( regression ) Since this is a regression from the best result , I should revert some changes and try a different approach . Let me go back to the better performing configuration but try some other optimizations like adding a more sophisticated budget - aware component that considers not just the direct return to depot but also potential for visiting multiple nodes before returning . 

In this example, the agent correctly identifies performance regression and reasons about which modifications likely caused the degradation. It then decides to restore a previous configuration before exploring alternative improvements. 

# Hard Reversion via System-Level Control 

Despite its effectiveness, soft reversion is not always reliable. In certain cases, the language model fails to restore the best-performing configuration accurately. To ensure robustness, we therefore im-plemented a hard reversion mechanism. Hard reversion is performed at the system level: the best solution encountered during the experiment is manually selected and restored before final summariza-tion. The Experiment Agent is informed of this reversion so that its concluding reflection incorporates the corrected trajectory. The following example from the cooperative driving problem illustrates hard reversion combined with experiment summarization. Listing 13: Hard reversion combined with final experiment summary (Driving Problem). 

>[ Evaluator ] Metrics extracted for solution ( id ) : lead1_round1_count0_id21 metrics : { ’ avg_speed ’: 0.290 , ’ emergencyStops ’: 0.0 , ’ speed_variance ’: 2.41 , ’ collisions ’: 0.5 , ’ emergencyBraking ’: 0.0 , ’ critical_ttc_count ’: 144.0 , ’ teleports ’: 5.5} features : (0 , 0 , 3) scores : -60.246 >[ ExperimentAgent ] Revert back to previous code version | score reverted from -60.246 to -13.512 >[ ExperimentAgent ] Final summary after 5 experiments : The experiment series aimed to optimize a hierarchical cooperative negotiation system for cooperative driving . Over six experiments , the solution evolved from an initial score of -35.132 to a best score of -13.511 ( experiment #5) , slightly outperforming the parent solution ( -14.068) but revealing fundamental inefficiencies . Key findings include : 1) Average speeds remained critically low (0.29 -0.56 m / s vs . target 13.89 m / s ) , indicating severe traffic flow impairment ; 2) Safety metrics fluctuated , with collisions introduced and partially mitigated through parameter tuning ; 3) Teleports persisted (3.5 -5.5) , signaling deadlock - like conditions where vehicles stalled ; 4) Excessive lane change attempts (13 -28 per step ) disrupted stability . Performance trends showed initial improvement through parameter adjustments ( e . g . , reducing time headway , increasing lane change cooldown ) , but experiment #6 regressed to -60.245 , suggesting over - tuning or instability . 

43 Persistent issues include the algorithm ’ s over - complexity , which may hinder flow compared to simpler alternatives like the parent ’ s predictive platoon formation . Successful strategies included state persistence for consistency , predictive safety checks , limiting simultaneous lane changes , and hysteresis in decision - making . However , unresolved limitations require fundamental changes : the negotiation framework introduces latency , cooperative adjustments often slow vehicles excessively , and low - speed handling fails to prevent teleports . Recommendations for future research : simplify the algorithm toward light - weight cooperation ; prioritize speed maintenance with aggressive IDM tuning and deadlock resolution ; reduce lane change frequency through stricter urgency thresholds ; and implement proactive flow maintenance rather than reactive negotiation . This analysis underscores that incremental parameter tuning is insufficient - structural redesign focusing on efficiency and simplicity is needed to achieve significant score improvements . 

This example demonstrates several important properties: 

• Regression detection: The agent identifies deterioration relative to historical best perfor-mance. 

• Rollback capability: Hard reversion guarantees that final results reflect the best discovered solution. 

• Failure-aware summarization: Even after rollback, the agent synthesizes insights from un-successful trials. 

• Structural diagnosis: The final reflection often reveals that parameter tuning alone is insuffi-cient and that architectural redesign is necessary. 

# Discussion 

Solution reversion plays a critical role in preventing performance collapse during deep experimen-tation. Soft reversion enables autonomous reasoning-based rollback, while hard reversion provides system-level safety guarantees. More importantly, reversion is not merely a defensive mechanism. It allows OR-Agent to preserve strong baselines while continuing exploratory modifications, closely mir-roring scientific practice: researchers frequently revert to a stable configuration before pursuing a new direction. This combination of rollback and structured reflection ensures that exploration does not irreversibly destroy accumulated progress. 

# F Long-Term Reflection Example 

This appendix provides an example of a long-term reflection generated during the MKP ACO experi-ments. Long-term reflection accumulates cross-experiment observations, identifies recurring patterns and persistent challenges, and proposes structured directions for future improvement. Although OR-Agent does not enforce an explicitly structured storage format for long-term reflections, the language model typically produces well-organized analytical summaries. These reflections typically include: (1) empirical pattern synthesis, (2) diagnosis of architectural limitations, (3) extraction of robust design principles, and (4) forward-looking research hypotheses. An illustrative example is shown below. Listing 14: Example of long-term reflection in MKP ACO experiments. 

>[ LeadAgent ] Long - term reflection updated to : Recent experiments strongly confirm that bottleneck - aware heuristics - particularly the minimum prize - to - weight ratio across dimensions or prize / max_weight - are consistently superior for MKP in ACO , outperforming harmonic / geometric means and sum - based aggregations . Simplicity enhances robustness : complex adaptive schemes , interaction models , and lookahead mechanisms often degrade performance due to noise or misaligned signals . The harmonic mean shows moderate promise but is 

44 less effective than min - ratio approaches , which naturally enforce feasibility by prioritizing worst - case constraint efficiency . Persistent challenges include the static nature of heuristics , inability to adapt to residual capacities during construction , and handling scale heterogeneity across problem sizes . Successful principles center on clear , interpretable efficiency metrics with epsilon - stabilized ratios . Future work should prioritize lightweight dynamic heuristics incorporating real - time capacity usage , instance - specific normalization , and hybrid designs that blend bottleneck focus with total resource awareness - potentially via learned or correlation - informed weights - while avoiding over - engineering . Future work must prioritize truly dynamic heuristics that recompute item desirability using real - time knapsack states , develop capacity - pressure - based conflict measures , and explore instance - adaptive blending of efficiency and compatibility - while resisting over - engineering . The path forward lies in minimal , bottleneck - grounded heuristics enhanced by online feasibility awareness . 

This example highlights several important characteristics of long-term reflection in OR-Agent: 

• Pattern Consolidation: The reflection synthesizes evidence across multiple experiments, iden-tifying consistent empirical regularities (e.g., the dominance of bottleneck-aware ratios). 

• Failure Diagnosis: It explicitly recognizes the limitations of static heuristics and over-engineered enhancements. 

• Principle Extraction: General design principles (simplicity, interpretability, bottleneck domi-nance) are distilled from empirical outcomes. 

• Future Direction Articulation: The reflection proposes concrete research hypotheses, such as incorporating dynamic capacity awareness. Importantly, the reflection moves beyond incremental parameter tuning and instead reasons at the architectural level. This demonstrates that long-term reflection functions not merely as memory accumulation, but as a mechanism for meta-level abstraction and strategic redirection of the research trajectory. 

# G Experiments on TSP Example 

This section shows experiments on TPS POMO problem, illustrating how OR-Agent conducts structural improvement and parameter refinement. 

Experiment 1: Timeout Diagnosis and Structural Simplification. The initial candidate solution introduced a geometric graph ensemble combining Gabriel graph, Relative Neighborhood Graph (RNG), and k-nearest neighbor graph (k-NNG). However, evaluation resulted in a 300-second timeout for large-scale instances. The Experiment Agent correctly identified that nested triple loops in the Gabriel and RNG constructions resulted in O(n3) complexity, rendering the approach compu-tationally infeasible for large TSP instances. The agent compared this solution to its parent (Node 54), which achieved stable performance. Consequently, the entire heuristics function was rewrit-ten (via a full merge-conflict diff update) to remove cubic geometric constructions while preserving effective structural components. For relatively simple OR problems such as TSP, it was observed that the Experiment Agent frequently performs complete functional rewrites rather than minor parameter adjustments—highlighting a deviation from the intended “small-step refinement” assumption. Re-evaluation yielded: {200 : 11 .112 , 500 : 20 .929 , 1000 : 31 .413 }, score = 21 .151. The timeout issue was resolved while preserving performance. 

Experiment 2: Structural Refinement of k-NNG. The Experiment Agent next refined the k-NNG computation, making k adaptive to problem size and adjusting bonus weights. This revi-sion targeted large-scale connectivity, hypothesizing that larger instances require denser neighborhood structures. The updated evaluation results were: {200 : 11 .114 , 500 : 20 .870 , 1000 : 31 .288 }, score = 21 .091. Although the agent described the 200-city result (11.114 vs. 11.112) as a “slight improvement,” 45 this was in fact a minor degradation. This illustrates a known limitation of LLM reasoning over floating-point comparisons. Nevertheless, the overall score improved due to gains in larger instances, demonstrating effective large-scale bias tuning. 

Experiment 3: Parameter-Level Tuning. Subsequent revisions focused on fine-tuning MST bonus coefficients and size-dependent scaling factors. These adjustments were minor relative to the structural changes in earlier steps, reflecting a shift from architectural revision toward local optimiza-tion. Listing 15: Example of experiments on TSP POMO problem. 

>[ Evaluator ] Evaluating solution : lead1_round12_count19_id71 Error : no end marker found in the result . Timed out after 300 seconds . >[ ExperimentAgent ] Thinking : Looking at the latest experiment result , I can see that the execution timed out after approximately 300 seconds . This indicates that the current implementation of the ‘ heuristics ‘ function is computationally expensive and inefficient . Let me analyze the current solution : 1. The function implements a geometric graph ensemble combining Gabriel graph , RNG , and k - NNG 2. For the Gabriel graph , there ’ s a nested loop structure : ‘ for i in range ( n ) ‘ and ‘ for j in range ( i + 1 , n ) ‘ and then another ‘ for k in range ( n ) ‘ - this creates O ( n ^3) complexity 3. For the RNG , there ’ s similar O ( n ^3) complexity with nested loops 4. Additionally , there are eigenvalue computations for the Gram matrix which are expensive for large matrices 5. The adaptive neighborhood computation also adds complexity Comparing to the parent solution (#54) which achieved a score of 21.151 , that solution had : - Adaptive local neighborhoods with rank - decayed - log ( d ) / rank formulation - MST computation ( which has O ( E log V ) complexity , manageable ) - k - NNG computation ( which is O ( n * k ) where k is small ) - No O ( n ^3) nested loops The parent solution avoided the expensive O ( n ^3) computations that are causing the timeout . The current solution ’ s Gabriel graph and RNG implementations are extremely expensive for large TSP instances (200 , 500 , 1000 cities ) . The timeout suggests that for larger instances ( especially 500 and 1000 city problems ) , the O ( n ^3) computations are taking too long . I need to simplify the approach to make it computationally feasible while retaining the beneficial aspects . I should modify the solution to : 1. Keep the adaptive local neighborhoods ( from parent solution ) which were effective 2. Keep the MST computation ( from parent solution ) which was effective 3. Keep a simpler k - NNG computation ( from parent solution ) 4. Remove the expensive O ( n ^3) Gabriel and RNG computations 5. Simplify the instance - level metrics computation to avoid expensive eigenvalue calculations >[ ExperimentAgent ] Action after experiment 1: update code <<<<<<< SEARCH def heuristics ( distance_matrix : torch . Tensor ) -> torch . Tensor : ... ======= ...( whole ‘ heuristic ‘ funciton is updated ) ... >>>>>>> >[ Evaluator ] Evaluating solution : lead1_round12_count19_id71 Metrics : {200: 11.112 , 500: 20.929 , 1000: 31.413}; features : (11 , 20 , 31) ; scores : 21.151. 

46 >[ ExperimentAgent ] Thinking : Looking at the latest experiment result , I can see that the solution achieved a score of 21.151 with metrics {200: 11.112 , 500: 20.929 , 1000: 31.413}. This is almost identical to the parent solution #54 ’ s performance , which means my simplification successfully preserved the performance while fixing the timeout issue . This approach is much more computationally efficient and achieves the same performance as the parent solution . However , I need to consider if there are further improvements possible . Looking back at the parent solution analysis , the experiments showed that : - Complex dynamic approaches often underperformed compared to simpler approaches - The combination of negative distance initialization + adaptive local neighborhoods + MST + k - NNG was effective - Scale robustness remained challenging One possibility is to refine the bonus calculation or improve the adaptive neighborhood approach . Looking at the current solution , I notice that the adaptive neighborhood approach uses a fixed k =15 for density estimation , but this might not be optimal for all problem sizes . Also , the MST and k - NNG bonuses could be fine - tuned . Let me try to enhance the solution by : 1. The k values ( k_for_density =15 , k_nn =6) could be made more adaptive 2. The bonus weights (0.5 for MST , 0.3 for k - NNG ) could be tuned 3. Maybe we could add another simple geometric structure that ’ s efficient Let me try to increase the effectiveness of the k - NNG by increasing kslightly for larger problems , since for larger problems , we might need more connectivity . >[ ExperimentAgent ] action after experiment 2: update code <<<<<<< SEARCH # Compute k - Nearest Neighbor Graph ( replacing expensive Gabriel / RNG with efficient k - NNG ) ... ======= ...(" Compute k - Nearest Neighbor Graph " cod section is revised ) ... >>>>>>> >[ Evaluator ] Evaluating solution : lead1_round12_count19_id71 Metrics : {200: 11.114 , 500: 20.870 , 1000: 31.288}; features : (11 , 20 , 31) ; scores : 21.091. >[ ExperimentAgent ] Thinking : Looking at the latest experiment results , I can see the following : Previous best score ( parent solution #54) : 21.15. Current experiment score : 21.091. The metrics are : - 200 cities : 11.114 ( was 11.112) - slight improvement - 500 cities : 20.870 ( was 20.929) - improvement - 1000 cities : 31.288 ( was 31.413) - significant improvement This shows that my modification was successful ! The score improved from 21.151 to 21.091 , which represents a meaningful improvement . The changes I made were : 1. Made k_NN adaptive based on problem size (6 - >8 - >10 for different sizes ) 2. Increased the k_NN bonus weight from 0.3 to 0.35 3. Enhanced the size - based adjustment factors The improvements are particularly noticeable for larger problem sizes (500 and 1000 cities ) , which aligns with the intuition that larger problems need more connectivity . However , I should consider if there are additional improvements possible . Let me think about what else could be optimized : 

47 1. The adaptive neighborhood component could be refined - currently using k_for_density = min (15 , n -1) 2. The MST bonus parameters could be fine - tuned further 3. Maybe there are other lightweight geometric structures I could add 4. The adaptive local neighborhood parameters ( - log ( d ) / rank formulation ) could be optimized Let me try to improve the adaptive neighborhood component by adjusting the k_for_density value and potentially refining the ranking approach : Looking back at the parent solution analysis , the best performing configurations had a good balance between local adaptive neighborhoods , MST , and k - NNG . My current solution improved the k - NNG part , but maybe Ican further improve the MST bonus or the adaptive neighborhood parameters . >[ ExperimentAgent ] action after experiment 3: update code <<<<<<< SEARCH # Calculate adaptive bonus based on average distance ( similar to parent solution ) mst_bonus = 0.5 * (1.0 + 1.0 / (1.0 + avg_distance_global ) ) # Also incorporate problem size factor to enhance performance on larger instances if n > 500: mst_bonus *= 1.2 # Slight enhancement for large problems elif n < 200: mst_bonus *= 0.8 # Slight reduction for small problems ======= # Calculate adaptive bonus based on average distance ( enhanced from parent solution ) mst_bonus = 0.55 * (1.0 + 1.0 / (1.0 + avg_distance_global ) ) # Also incorporate problem size factor to enhance performance on larger instances if n > 500: mst_bonus *= 1.25 # Enhanced boost for large problems elif n < 200: mst_bonus *= 0.85 # Slight reduction for small problems >>>>>>> REPLACE ... 

# H Environment Exploration Example 

This appendix section illustrates how OR-Agent performs structured environment exploration through callback-based instrumentation. Rather than relying solely on aggregate evaluation metrics, the Ex-periment Agent augments the simulation with diagnostic probes that expose intermediate system dynamics. This enables systematic failure analysis and hypothesis-driven refinement. In complex environments such as cooperative driving simulation, poor performance cannot be ad-equately explained by final scores alone. Aggregate metrics (e.g., collisions, average speed) indicate 

what failed, but not why . By introducing targeted callbacks, OR-Agent collects fine-grained behav-ioral signals—such as lane-change attempts, vehicle stagnation, congestion buildup, and coordination failures—allowing iterative scientific-style diagnosis. 

Initial Failure and Root-Cause Hypothesis. The following listing shows an early-stage evalua-tion prior to callback refinement. Listing 16: Initial evaluation output before callback-based environment probing. 

>[ Evaluator ] Evaluating solution : lead1_round1_count0_id21 metrics : { ’ teleports ’: 3.5 , ’ avg_speed ’: 0.775 , ’ speed_variance ’: 6.17 , ’ collisions ’: 30.0 , ’ emergencyBraking ’: 0.5 , 

48 ’ emergencyStops ’: 0.0 , ’ critical_ttc_count ’: 297.5} features : (0 , 0 , 2) scores : -806.773 

The Experiment Agent performs structured reasoning over these signals, identifying potential causes such as overly aggressive cooperative gap creation, insufficient safety margins in lane-change logic, and unstable acceleration behavior. However, these hypotheses require deeper environmental visibility. 

Instrumentation via Callbacks. To move from symptom observation to mechanism diagnosis, the agent introduces monitoring callbacks that log intermediate state statistics. Listing 17: First callback instrumentation for monitoring congestion and lane-change behavior. 

class Callbacks : def __init__ ( self ) : self . step_counter = 0self . dangerous_gaps = 0self . low_speeds = 0self . lane_change_attempts = 0def on_step_end ( self , ** kwargs ) : " " " Monitor ␣ safety ␣ metrics ␣ and ␣ log ␣ critical ␣ events . " " " step = kwargs [ " step " ] vehicles_info = kwargs [ " vehicles_info " ] edge_info = kwargs [ " edge_info " ] self . step_counter += 1

# Log every 100 steps 

if step % 100 == 0: print ( f " Step ␣ { step }: ␣ Monitoring ␣ { len ( vehicles_info ) } ␣ vehicles ␣ on ␣ edge ␣ { edge_info [ ’ edge_id ’]} " ) 

# Check for dangerous situations 

dangerous_count = 0low_speed_count = 0for vehicle in vehicles_info : 

# Check for very low speeds ( potential congestion ) 

if vehicle [ " speed " ] < 1.0: low_speed_count += 1

# Check for dangerous gaps ( simplified - would need leader info ) # This is a placeholder for more detailed analysis 

if dangerous_count > 0 and step % 10 == 0: print ( f " Step ␣ { step }: ␣ { dangerous_count } ␣ vehicles ␣ in ␣ dangerous ␣ gaps " ) if low_speed_count > len ( vehicles_info ) * 0.5 and step % 50 == 0: print ( f " Step ␣ { step }: ␣ Warning ␣ -␣ { low_speed_count }/{ len ( vehicles_info ) } ␣ vehicles ␣ with ␣ speed ␣ <␣ 1.0 ␣ m / s " ) 

# Track lane change attempts 

lane_changes = sum (1 for v in vehicles_info if v . get ( ’ wants_left ’ , False ) or v . get ( ’ wants_right ’ , False ) ) if lane_changes > 0 and step % 20 == 0: print ( f " Step ␣ { step }: ␣ { lane_changes } ␣ vehicles ␣ attempting ␣ lane ␣ changes " ) 

The resulting logs has more than 120 lines for each test case hence truncated. Still, it confirms that the failure is not purely due to unsafe gaps, but rather a systemic interaction between lane-change 49 instability and flow breakdown. 

>[ Evaluator ] Evaluating solution : lead1_round1_count0_id21 ...( Omited for brevity ) ... Step 100.0: Monitoring 20 vehicles on edge E0 Step 100.0: 4 vehicles attempting lane changes Step 120.0: 6 vehicles attempting lane changes Step 140.0: 3 vehicles attempting lane changes Step 150.0: Warning - 24/30 vehicles with speed < 1.0 m / s ... [ Output truncated : 124 total lines , showing first 50 and last 50 lines ] ... Step 200.0: 29 vehicles attempting lane changes Step 220.0: 29 vehicles attempting lane changes Step 240.0: 29 vehicles attempting lane changes Step 250.0: Warning - 60/60 vehicles with speed < 1.0 m / s ...( Omited for brevity ) ... metrics : { ’ teleports ’: 4.5 , ’ avg_speed ’: 0.42 , ’ emergencyStops ’: 0.0 , ’ collisions ’: 12.5 , ’ critical_ttc_count ’: 177.0 , ’ speed_variance ’: 2.88 , ’ emergencyBraking ’: 0.0} features : (0 , 0 , 3) scores : -353.158 

Refined Probing and Mechanism Isolation. After identifying large-scale stagnation, the Exper-iment Agent refines the callback instrumentation to measure acceleration behavior and lane-change effectiveness more explicitly. Listing 18: Refined callback instrumentation focusing on flow and lane-change effectiveness. 

class Callbacks : def __init__ ( self ) : self . step_counter = 0self . lane_change_attempts = 0self . lane_change_success = 0self . low_speed_vehicles = 0def on_step_end ( self , ** kwargs ) : " " " Monitor ␣ acceleration ␣ behavior ␣ and ␣ lane ␣ change ␣ effectiveness . " " " step = kwargs [ " step " ] vehicles_info = kwargs [ " vehicles_info " ] edge_info = kwargs [ " edge_info " ] self . step_counter += 1

# Calculate metrics 

total_vehicles = len ( vehicles_info ) low_speed_count = sum (1 for v in vehicles_info if v [ " speed " ] < 2.0) lane_change_attempts = sum (1 for v in vehicles_info if v . get ( ’ wants_left ’ , False ) or v . get ( ’ wants_right ’ , False ) ) 

# Update accumulators 

self . low_speed_vehicles += low_speed_count self . lane_change_attempts += lane_change_attempts 

# Log every 50 steps 

if step % 50 == 0: avg_speed = sum ( v [ " speed " ] for v in vehicles_info ) /max ( total_vehicles , 1) print ( f " Step ␣ { step }: ␣ { total_vehicles } ␣ vehicles , ␣ avg ␣ speed ␣ { avg_speed :.2 f } ␣ m /s , ␣ " 

50 f " { low_speed_count } ␣ vehicles ␣ <␣ 2 ␣ m /s , ␣ { lane_change_attempts } ␣ lane ␣ change ␣ attempts " ) 

# Calculate and print lane change success rate if possible 

if self . lane_change_attempts > 0: success_rate = self . lane_change_success /max ( self . lane_change_attempts , 1) print ( f " ␣ ␣ Lane ␣ change ␣ success ␣ rate : ␣ { success_rate :.1%} " ) 

# Reset accumulators for next window 

self . low_speed_vehicles = 0self . lane_change_attempts = 0self . lane_change_success = 0

# Track successful lane changes ( simplified : vehicles that changed lane since last step ) # This would require storing previous lane info , but for simplicity we ’ ll note attempts # For now , we ’ ll assume a lane change is successful if the vehicle ’s wants_left / right is True # but this is not accurate . We ’ ll leave as placeholder . 

The new diagnostic output contains around 12 lines for each test case. Rather than blindly opti-mizing scores, OR-Agent treats the environment as an object of investigation. Callbacks function as experimental instruments, enabling progressive refinement of hypotheses about system behavior. This structured exploration capability is essential for complex, interactive optimization problems. Check the public repository for more details [LM26]. 

>[ Evaluator ] Evaluating solution : lead1_round1_count0_id21 Step 0.0: 0 vehicles , avg speed 0.00 m /s , 0 vehicles < 2 m /s , 0 lane change attempts Step 50.0: 12 vehicles , avg speed 5.10 m /s , 6 vehicles < 2 m /s , 11 lane change attempts Lane change success rate : 0.0% Step 100.0: 30 vehicles , avg speed 0.09 m /s , 30 vehicles < 2 m /s , 11 lane change attempts Lane change success rate : 0.0% ...( Omited for brevity ; round 10 lines for each test case ) ... metrics : { ’ critical_ttc_count ’: 73.0 , ’ teleports ’: 4.5 , ’ avg_speed ’: 0.290 , ’ emergencyStops ’: 0.0 , ’ speed_variance ’: 2.235 , ’ emergencyBraking ’: 0.0 , ’ collisions ’: 0.5} features : (0 , 0 , 3) scores : -27.496 >[ ExperimentAgent ] Thinking : The latest experiment shows significant improvement in safety ...( Omited for brevity ) ... >[ ExperimentAgent ] Action after experiment 3: update code ... 

# I Research Tree Size Example 

This section illustrates how the research tree size evolves under different branching configurations when tree depth is not constrained. We show representative examples (Figure 9) from experiments on classical Operations Research benchmark problems, where the maximum number of children per node is varied. The purpose of this example is to demonstrate how branching factor directly affects search breadth, tree depth, and the number of terminal solutions explored. The size of the research tree grows exponentially with the number of children per node, directly impacting computational requirements. 51 max num of children = 1 ======================================== 

✓ Node [17 (20.28)] +-- ⊕ Node 21 (20.43) +-- ✓ Node 22 (20.63) ======================================== Total nodes: 3 Total done leaves: 1 max num of children = 2 ======================================== 

✓ Node [11 (19.35)] +-- ⊕ Node 27 (15.58) | +-- ✓ Node 35 (18.88) +-- ⊕ Node 28 (19.83) +-- ✓ Node 29 (19.91) +-- ✓ Node 30 (21.42) ======================================== Total nodes: 6 Total done leaves: 3 max num of children = 3 ======================================== 

✓ Node [20 (22.04)] +-- ⊕ Node 21 (18.48) | +-- ✓ Node 76 (19.04) +-- ✓ Node 22 (20.20) +-- ⊕ Node 23 (18.96) | +-- ⊕ Node 47 (19.06) | +-- ⊕ Node 49 (19.10) | +-- ⊕ Node 53 (19.17) | | +-- ✓ Node 61 (19.38) | | +-- ✓ Node 64 (19.26) | +-- ✓ Node 55 (19.19) +-- Node 24 (18.96) +-- ✓ Node 29 (19.07) +-- ⊕ Node 31 (19.00) +-- ✓ Node 37 (19.01) ======================================== Total nodes: 15 Total done leaves: 7 

Figure 18: Research Tree Size Example. 52