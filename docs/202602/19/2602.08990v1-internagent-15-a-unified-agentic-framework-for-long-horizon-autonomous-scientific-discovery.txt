Title: InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery

URL Source: https://arxiv.org/pdf/2602.08990v1

Published Time: Tue, 10 Feb 2026 03:45:15 GMT

Number of Pages: 64

Markdown Content:
# InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery 

InternScience Team, Shanghai Artificial Intelligence Laboratory 

https://github.com/InternScience/InternAgent 

Artificial intelligence is rapidly emerging as a powerful engine for scientific discovery. Modern machine learning and large language models support literature analysis, hypothesis generation, experimental planning, and data interpretation across biology, chemistry, and earth science. These advances have inspired AI Scientist systems that coordinate computational modeling, laboratory experimentation, and cross disciplinary reasoning to accelerate scientific progress. However, existing AI Scientist systems remain limited by domain specific designs, incomplete reasoning abilities, naive optimization pipelines, and insufficient support for long horizon autonomous operation. We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep re-search, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behav-ior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery. 

# Contents 

1 Introduction 32 InternAgent-1.5 5

2.1 System Overview 52.1.1 Architecture 62.1.2 Foundational Capabilities 72.2 Cross Disciplinary Graph Construction and Knowledge Capturing 72.2.1 Cross-Disciplinary Knowledge Graph 72.2.2 Flow Graph 92.2.3 Graph-Guided Output Synthesis 92.3 Experiment Execution and Multi-round Parallel Optimization 10 2.3.1 Generative Design for Experimental Optimization 10 2.3.2 Scenario 11 2.4 Structured Cognitive Memory for Long Horizon Scientific Discovery 11 2.4.1 Strategy-Procedural Memory 12 

> 1
> arXiv:2602.08990v1 [cs.AI] 9 Feb 2026

2.4.2 Task-Episodic Memory 13 2.4.3 Semantic-Knowledge Memory 13 

3 Experiments 13 

3.1 Experiments Setup 13 3.1.1 General Scientific Reasoning Abilities 13 3.1.2 Algorithm Discovery 14 3.1.3 Empirical Discovery 15 3.2 Evaluating Agentic Reasoning Abilities 16 3.3 Results for Algorithm Discovery Tasks 18 3.3.1 Scientific Algorithm 18 3.3.2 AI Algorithm 20 3.4 Discoveries of Scientific Mechanism 21 3.4.1 Earth Science 21 3.4.2 Life Science 23 3.4.3 Biological Science 25 3.4.4 Physical Science 25 3.5 Effectiveness of Structured Cognitive Memory 27 

4 Related Work 28 

4.1 Agentic AI for Scientific Discovery 28 4.2 Deep Research Agents 29 4.3 Memory Mechanism 29 

5 Conclusion 29 References 30 A Appendix 35 

A.1 Contributions and Acknowledgments 35 A.2 Earth Science example 36 

> 2

## 1. Introduction 

Table 1 | Comparison with state-of-the-art frameworks for autonomous scientific discovery. Method Domains Capabilities Algorithm Discovery Empirical Discovery Deep Research Solution Refinement Wet Lab Persistence Running 

AI Scientist [1, 2] âœ“ âœ— âœ— âœ“ âœ— âœ—

AlphaEvolve [3] âœ“ âœ— âœ— âœ“ âœ— âœ—

AI Co-Scientist [4] âœ— âœ“ âœ“ âœ— âœ“ âœ—

Robin [5] âœ— âœ“ âœ“ âœ— âœ“ âœ—

Kosmos [6] âœ— âœ“ âœ“ âœ— âœ“ âœ—

InternAgent 1.0 [7] âœ“ âœ— âœ— âœ— âœ— âœ—

InternAgent 1.5 âœ“ âœ“ âœ“ âœ“ âœ“ âœ“

Artificial intelligence is rapidly reshaping the landscape of scientific research, giving rise to the emerging paradigm of AI for Science [8, 9]. Recent progress in machine learning has driven advances across biology [10, 11, 12], chemistry [13, 5], and the physical and environmental sciences [14]. Large language models have expanded this frontier by supporting literature analysis [15, 16], hypothesis generation [17, 18, 7], experimental planning [7, 19, 20], and data interpretation [21, 22]. These capabilities have motivated a shift toward autonomous scientific systems capable of coordinating complex workflows that span computational modeling, wet -lab experimentation, and cross-disciplinary reasoning. A series of recent systems have demonstrated the potential of automated scientific agents. In algo-rithm optimization, AI Scientist [1, 2] and AlphaEvolve [3] integrate literature analysis, coding, and experimental evaluation into end-to-end research loops. In biomedicine, AI Co-Scientist [4] generates hypotheses and designs therapeutic experiments. In chemistry, systems such as ChemCrow [13] and Robin [5] connect large language models with domain specific toolchains for synthesis planning and molecular design. In earth science, EarthLink [14] integrates multisphere data and literature to sup-port mechanism level reasoning. These systems have shown impressive domain-specific performance but operate as isolated verticals with architectures that embed strong domain assumptions. To move beyond single domain expertise, systems such as Kosmos [6] introduce structured scientific world models to organize research across metabolomics, materials science, and genetics. Despite substantial progress, current systems of AI4S exhibit several characteristics that limit their ability to support autonomous cross-disciplinary discovery: 

â€¢ Domain -Specific Architectures: Many systems are organized around discipline -focused designs, which makes it difficult to perform unified reasoning across scientific fields. 

â€¢ Partial Foundational Capabilities: Existing frameworks vary in their support for the use of heterogeneous dry -lab and wet -lab experiments, leading to uneven coverage of core scientific competencies. 

â€¢ Linear Optimization Pipelines: Optimization procedures are often based on trajectory -local updates and therefore do not integrate information across broader search processes when refining scientific proposals. 

â€¢ Limited Long - Horizon Operation: Most systems do not maintain persistent memory over 

> 3

Figure 1 | Performance comparison of InternAgent-1.5 across GAIA [25], GPQA [26], HLE-full [27], and FrontierScience [28]. Foundational Capabilities     

> Flow -based Deep Research
> Structured Cognitive Memory
> Unified Discovery
> Subsystem
> Graph -augmented
> Solution Refinement

Intern 

Agent   

> Tasks
> Scientific Benchmarks
> AI
> Algorithm Discovery
> Science
> Emperical Discovery

â€¦Earth Physical Life 

Figure 2 | Overview of InternAgent-1.5 that summarizes its foundational capabilities, unified discovery pipeline, and supported scientific tasks in a high-level manner. extended research cycles, which restricts iterative refinement and long -term autonomous opera-tion. A comparative overview of existing systems is presented in Table 1, which summarizes these charac-teristics across domains and foundational capabilities. To address these challenges, we adopt an epistemological perspective grounded in the philosophy of science [23, 24] and categorize tasks into two fundamental domains: Algorithm Discovery , which 

transforms objectives into solutions in formal systems , and Empirical Discovery , which transforms observations into generalizations about the physical world . A framework capable of supporting both domains requires unified architectural principles, strong foundational capabilities, long -horizon iterative optimization, and the ability to operate across computational and experimental environments. Building on InternAgent 1.0 [7], we introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery. The system follows the observation that scientific inquiry across domains can be organized into a common structure that includes literature based hypothesis construction, methodological evaluation, and evidence driven refinement. InternAgent-1.5 operationalizes these processes through three coordinated subsystems, namely Generation , Verification , and Evolution .Each subsystem is driven by a foundational capability: deep research supports the Generation 

subsystem, solution refinement supports the Verification subsystem, and long horizon memory supports the Evolution subsystem. This design moves beyond structures restricted to single domain algorithm discovery and establishes a general framework suitable for both computational and empirical scientific tasks. A high level overview of InternAgent-1.5, including its core capabilities, subsystem organization, and supported discovery tasks, is presented in Fig. 2. 

> 4

InternAgent-1.5 is evaluated across standard benchmarks and open ended scientific discovery tasks. The system attains leading performance on agentic reasoning abilities, demonstrating the effective-ness of the foundational capabilities that drive the Generation and Verification subsystems. These capabilities, together with long horizon memory in the Evolution subsystem, support stable extended operation and enable consistent iterative refinement throughout long discovery cycles. Building on this capability structure, InternAgent-1.5 further performs competitively in both algorithm discovery 

and empirical discovery tasks, indicating that the unified framework scales from benchmark level reasoning to practical scientific workflows. In summary, the main contributions of this work are as follows: 

â€¢ A Unified Architecture for End-to-end Scientific Discovery: InternAgent-1.5 organizes the scientific discovery process into three coherent subsystems for Generation, Verification, and Evolution. These subsystems support the full cycle of hypothesis formulation, methodological evaluation, and evidence driven refinement through foundational capabilities for deep research, solution refinement, and long horizon memory. This organization provides a robust basis for reliable and scalable scientific discovery. 

â€¢ State-of-the-Art Foundational Capabilities: InternAgent-1.5 delivers strong foundational capabilities in deep research and solution refinement, supported by structured long horizon memory. Across benchmarks that measure cross disciplinary retrieval, structured reasoning, and scientifically grounded problem solving, the system achieves leading performance on HLE [27], GAIA [25], GPQA [26], FrontierScience [28], and SGI bench [29]. These results confirm that the foundational capabilities of InternAgent-1.5 are sufficiently reliable to support complex scientific workflows. 

â€¢ Sustained Autonomous Optimization: InternAgent-1.5 integrates a structured memory ar-chitecture with an iterative optimization process centered on the Verification and Evolution subsystems. This design supports the accumulation of contextual knowledge, the sustained refinement of hypotheses, and the stable improvement of methodological plans across many discovery cycles, moving toward scientific agents capable of extended self-improvement. 

â€¢ Breakthroughs in Algorithmic and Empirical Discovery: InternAgent-1.5 demonstrates strong performance in both algorithm discovery and empirical scientific discovery. It produces competitive algorithmic solutions in domains such as reinforcement learning and test time methodology, and generates high quality outputs for data oriented scientific tasks. In empirical settings, the system executes complete experimental workflows and identifies new insights in fields that include biology and earth sciences. These results illustrate the generality and practical effectiveness of the framework across computational and physical scientific environments. With these capabilities and results, we now present the design principles and technical foundations that enable InternAgent-1.5 to operate as a unified system for scientific discovery. 

## 2. InternAgent-1.5 

2.1. System Overview 

In this section, we present the system overview of InternAgent-1.5 as illustrated in Fig. 3. The system automates the full research cycle by integrating hypothesis formulation, methodological evaluation, and evidence driven refinement into a unified and continuously improving process. Its operation relies on foundational capabilities that support deep research, solution refinement, and long horizon memory. These capabilities are realized through agent driven reasoning and system level infrastructure and allow the system to maintain contextual continuity across iterations. With this capability structure,  

> 5General Tools Scientific Tools

Â·Â·Â· 

Generation Verification    

> Cross -Disciplinary Knowledge Graph
> Idea Generation and Evolution
> Generation Assessment Evolution

Toolset 

Â·Â·Â· 

Evolution                           

> Parallel Solution Optimization
> Deep Research
> Planner Executor Refiner
> Query Knowledge
> Query Knowledge
> Traj . 1
> Task1
> Task2
> Query
> Task1 Task2
> Task3
> Task2
> Execution Enviro nment
> Results Experiments
> Detailed
> Methodology
> Solution 0-1Solution 0-2
> Solution 1-1Solution 1-2
> Solution 3-1Solution 3-2
> Solution 3-1
> Â·Â·Â·
> Â·Â·Â·
> Â·Â·Â·
> Â·Â·Â·
> Computer
> Simulation
> Expert Model
> Execution
> Lab Equipment
> Operation
> Dry Lab Wet Lab
> Idea Result
> Idea Result
> Traj . N
> Â·Â·Â·
> Â·Â·Â·
> Â·Â·Â·
> Analyze
> Feedback
> Ideas &Methods

InternAgent   

> Structured Cognitive Memory
> Strategy Task Semantic

Figure 3 | Overview of InternAgent-1.5, illustrating its unified scientific discovery pipeline organized around the Generation, Verification, and Evolution subsystems. The system operates through founda-tional capabilities for deep research, solution refinement, and long horizon memory, which together enable sustained autonomous scientific discovery. InternAgent-1.5 coordinates multiple subsystems to support autonomous, scalable, and sustained scientific discovery. 

2.1.1. Architecture 

The architecture of InternAgent-1.5 is organized into three core subsystems, namely the Generation ,the Verification , and the Evolution . These subsystems form an integrated and iterative workflow. The Generation subsystem formulates hypotheses and methodological plans, the Verification subsystem evaluates these plans through computational or empirical procedures, and the Evolution subsystem incorporates the resulting evidence to update internal knowledge, strategies, and long term memory. This organization maintains a coherent flow of information and enables multi cycle autonomous operation. 

â€¢ Generation: The Generation subsystem constructs the conceptual foundation of scientific inquiry. It follows the generation and reflection paradigm of InternAgent 1.0 [7] and is driven by the foundational capability of deep research. It conducts large scale literature analysis, scientific reasoning, and contextual integration and may invoke scientific tools when processing domain specific data. It produces structured hypotheses and methodological plans and records key reasoning traces for subsequent processing. 

â€¢ Verification: The Verification subsystem evaluates the hypotheses and methodological plans produced by the Generation subsystem. Its operation is driven by the foundational capabil-ity of solution refinement, which structures the iterative search for improved procedures. It performs computational analyses, simulations, and laboratory style assessments as needed and uses historical information to guide evaluation choices. It supports parallel assessment of methodological variants and generates structured evidence for downstream refinement. 

â€¢ Evolution: The Evolution subsystem refines system understanding and long term strategies based on outcomes from the Generation and Verification subsystem. It is driven by the founda-tional capability of memory and unifies analytical feedback with persistent knowledge manage-

> 6

ment. It interprets verification results, identifies methodological limitations, updates procedural, episodic, and semantic information, and produces refined priors that guide subsequent cycles of the Generation and Verification subsystem. These subsystems rely on the foundational capabilities introduced above in order to function coherently across extended discovery horizons. The next section presents these capabilities in detail and describes the technical methods that implement them. 

2.1.2. Foundational Capabilities 

The operation of InternAgent-1.5 relies on a set of foundational capabilities that allow the Generation ,

Verification , and Evolution subsystems to function coherently across extended discovery cycles. These capabilities support literature based hypothesis construction, methodological evaluation, iterative refinement, and long horizon continuity. They are implemented through the technical methods introduced in Sections 2.2 to 2.4 and provide the requirements for end-to-end scientific discovery. 

The first capability is deep research. It supports the Generation subsystem by enabling large scale retrieval, integration, and structuring of cross disciplinary scientific knowledge. Section 2.2 introduces the search mechanisms and structured representations that realize this capability. 

The second capability is solution refinement. It supports the Verification subsystem by guiding the refinement of methodological plans and structuring the multi round search for improved procedures. Section 2.3 presents the optimization strategies that implement this capability. Scientific tools may be invoked within this subsystem when computational or empirical assessment is required. 

The third capability is long horizon memory. It supports the Evolution subsystem by maintain-ing persistent storage and retrieval of contextual information, reasoning traces, and experimental outcomes. Section 2.4 describes its structured organization and interaction rules. Across these capabilities, InternAgent-1.5 maintains the continuity, adaptability, and scalability needed for reliable and continuously improving scientific discovery. 

2.2. Cross Disciplinary Graph Construction and Knowledge Capturing 

Deep Research Capability within the Generation Subsystem 

To enable cross disciplinary knowledge construction and utilization, our design operates on both data and methodological levels. On the data side, the system integrates diverse scientific sources with the assistance of domain specific tools to parse, normalize, and structure scientific information into a large scale multidisciplinary knowledge graph. On the methodological side, it identifies relations and dependencies across domains through a structured extraction workflow that combines model driven analysis with tool assisted processing of specialized scientific data, enabling deep and effective cross disciplinary knowledge integration. 

2.2.1. Cross-Disciplinary Knowledge Graph 

To support accurate and comprehensive deep research, we maintain a cross-disciplinary knowledge graph (KG). Notably, it differs from traditional KGs [30, 31], which represent knowledge as triples including simple entities and relations; our KG instead captures a richer set of scientific elements. 

Graph Construction From parsed outputs of papers, survey articles, technical reports, and domain notes, we construct a heterogeneous graph with nodes representing documents, key concepts, methods, datasets, empirical settings, and problem statements. The parsing process incorporates domain specific scientific tools to assist in identifying specialized entities and scientific attributes that are difficult 

> 7

{

"Reaction": "Esterification (fat formation)", 

"Reactants": { 

"alcohol": "glycerol (propane -1,2,3 -triol)", 

"acid": "fatty acids (C 4â€“C28 alkanoic acids)" 

}, 

"Products": { 

"ester": "triacylglycerol (fat)", 

"by -product": "water" 

}, 

"Reversibility": "hydrolysis regenerates glycerol 

+ fatty acids", 

}

Esterification generally refers to the 

reaction of an alcohol with a carboxylic acid 

to give an ester and water . Common fats are 

esters ; they can be hydrolysed back to 

alcohol and acid . A typical fat is 

triacylglycerol, formed from glycerol 

(propane -1,2,3-triol) and fatty acids 

(alkanoic acids containing 4â€“28 carbon 

atoms) .

Cross -Disciplinary Knowledge  Knowledge Graph 

LLM, 

Bert, 

RAG, 

... 

Chemistry  Earth  Biology  Physics  AI 

... Figure 4 | The illustration for our cross-disciplinary knowledge graph. to extract through general text analysis alone. Edges encode typed relations such as â€œcitesâ€ and â€œby product.â€ This design lets a single research idea sit at the intersection of multiple methodological and application communities and converts a flat corpus into a structured map where cross field dependencies emerge as paths rather than isolated points. For example, given the text â€œEsterification generally refers to the reaction of an alcohol with a carboxylic acid to give an ester and water. Common fats are esters; they can be hydrolysed back to alcohol and acid. A typical fat is triacylglycerol, formed from glycerol (propane 1,2,3 triol) and fatty acids (alkanoic acids containing 4â€“28 carbon atoms).â€ the corresponding structured representation is shown in Fig. 4. FlowGraph   

> Please provide an in -depth literature review on the
> question: Will the Atlantic Meridional Overturning
> Circulation (AMOC) collapse or cross a critical threshold
> during the 21st century? Specifically, outline the current
> mainstream scientific consensus on the historical trends
> and future stability of the AMOC. At the same time,
> identify and analyze the substantial discrepancies among
> recent studies (including statistical analyses based on
> paleoclimate proxy data and simulations using complex
> climate models) in projecting the timing of a potential
> AMOC collapse. Explain the underlying reasons for these
> discrepancies, with particular emphasis on biases that
> may arise in how physical models represent freshwater
> forcing feedback mechanisms.

Answer     

> Synthesize
> AMOC
> consensus,
> collapse timing ,
> and modeling
> uncertainties .
> Analyze climate -
> model evidence
> for gradual
> versus abrupt
> AMOC change.
> Analyze proxy -
> based statistical
> evidence for early
> AMOC collapse.
> Review
> mainstream
> AMOC historical
> trends and
> future stability
> consensus.

Query 

> Introduce
> AMOC concept,
> drivers, and
> role in climate.
> Analyze model
> biases in
> freshwater
> forcing feedbacks
> affecting AMOC
> projections.

Figure 5 | The illustration for our flow graph. 

Knowledge Extraction and Retrieval We employ a schema-guided extraction workflow to build a knowledge graph from noisy, cross-domain text. First, candidate entities are identified via domain-agnostic named entity recognition and noun-phrase mining, and document-level co-citation and co-usage relations are used to establish initial concept links. A subsequent consolidation step refines node types and edge semantics, aligning textual evidence with citation evidence into a compact cross-disciplinary graph. To answer deep research queries, we integrate graph search with dense vector retrieval: graph search uncovers the nodes and paths that connect the query to relevant methods and domains, while dense retrieval captures semantically related items not directly linked in the graph. Finally, a ranking step merges these results and outputs path-structured evidence chains, which the deep research module then analyzes to reveal cross-disciplinary connections. 

82.2.2. Flow Graph 

In real scientific deep research tasks, knowledge often exhibits highly non-linear and dynamic depen-dencies. Conventional sequential research process struggle to capture these relationships effectively, which can lead to redundant information, over-reliance on early hypotheses, and inflexible reasoning processes. To address these challenges, we introduce Dynamic Structured Knowledge Flow as a core principle of deep research system, enabling systematic and adaptive organization of knowledge throughout the research process. Specifically, we capture the knowledge in research process as a directed acyclic graph (DAG) that explicitly represents tasks, subtasks, and their dependencies. 

Structured Knowledge Flow The research process is organized as a directed graph, which provides a structured representation of the reasoning process. Formally, the research process is defined as:  

> ðº

= (ð‘‰, ð¸ ), (1) where ð‘‰ denotes the set of nodes and ð¸ denotes the set of directed edges encoding dependencies among nodes. Each node ð‘£ ð‘– âˆˆ ð‘‰ corresponds to a subtask or a key conceptual unit arising during the reasoning process. To explicitly capture its functional role and execution status, each node is represented as a tuple:  

> ð‘£ ð‘–

= (ð‘¡ ð‘– , ð‘‘ ð‘– , ð‘  ð‘– , ð‘ ð‘– ), (2) where ð‘¡ ð‘– âˆˆ { search , solve , answer } specifies the task type associated with the node, ð‘‘ ð‘– describes the task content, ð‘  ð‘– tracks the execution state of the node, and ð‘ ð‘– stores the resulting knowledge context upon successful completion of the task. Directed edges in the graph encode structural dependencies or reasoning constraints between nodes. Specifically, each edge is defined as:  

> ð‘’ ð‘– ð‘—

= (ð‘£ ð‘– , ð‘£ ð‘— , ð‘Ÿ ð‘– ð‘— ) âˆˆ ð¸, (3) where ð‘’ ð‘– ð‘— indicates a directed relationship from node ð‘£ ð‘– to node ð‘£ ð‘— , and ð‘Ÿ ð‘– ð‘— characterizes the type of dependency between the two nodes, such as requires result from , provides evidence for , or constrains reasoning of . These relational edges ensure that information and intermediate results are propagated in a dependency-aware manner throughout the reasoning graph. 

Dynamic planning and refinement The knowledge flow is constructed incrementally: starting from a root query node, a planner identifies nodes that require further decomposition or context enrichment, generates successor nodes, and updates dependency edges accordingly. This iterative expansion continues until the flow sufficiently covers all subproblems necessary to address the research objective. This design not only enables efficient multi-agent collaboration but also supports adaptive refinement of the knowledge flow as new evidence emerges, ensuring coherent, systematic, and verifiable reasoning throughout the research process. 

2.2.3. Graph-Guided Output Synthesis 

Building upon the dynamically constructed knowledge flow, we describes how the abstract graph structure is instantiated into concrete research outputs. Once the planner completes the incremental construction of the flow graph, executable nodes are activated according to their dependency states and progressively populated with domain knowledge and intermediate reasoning results. Through iterative node execution, state updating, and context propagation along graph edges, the system refines the structured knowledge flow and synthesizes the final research answer. 

> 9

Cross-Disciplinary Knowledge Collector. To facilitate cross disciplinary insight, the Knowledge Collector gathers information from a diverse set of sources across multiple domains. These sources include outputs obtained through scientific tools and remote resources accessed via the Science Context Protocol (SCP) [32]. By integrating multi domain knowledge, the system can uncover unexpected connections and inspire ideas that may not emerge within a single discipline. Executable nodes with satisfied dependencies are assigned to agents, which decompose each subtask into a sequential reasoning and information retrieval process, iteratively assembling the knowledge required to resolve it. After a node is executed, its state is updated and the resulting knowledge context is propagated to dependent nodes, ensuring that subsequent reasoning benefits from the most up to date and contextually rich information. This design enables structured, adaptive, and collaborative knowledge acquisition throughout the research process. 

Reasoning capability enhancement We adopt a reasoning capability enhancement strategy that enables reasoning along multiple complementary pathways. For a given query, the model generates three forms of responses: a direct answer based solely on the input, a search augmented answer that incorporates evidence retrieved from external sources and scientific tools, and a self driven answer obtained through internal retrieval and refinement. These complementary outputs are aggregated to form the final response, balancing intrinsic reasoning, external evidence, and self consistency. This multi path strategy improves answer completeness and factual reliability while reducing reliance on any single reasoning pathway. 

2.3. Experiment Execution and Multi-round Parallel Optimization 

Solution Refinement Capability within the Verification Subsystem 

The transformation of a refined methodology into a verifiable scientific result requires an efficient and reliable validation loop. In both computational algorithm design and physical wet -lab experimentation, the search space of possible configurations is extremely large, and linear trial -and -error procedures often converge prematurely. This section introduces the multi -round parallel experiment optimization and execution framework, which enables InternAgent-1.5 to explore this space autonomously and progressively converge toward high-quality experimental proposals. 

2.3.1. Generative Design for Experimental Optimization 

Efficiently exploring a large and unstructured design space is a central challenge in automated scientific experimentation. Traditional strategies [33, 34] based on linear refinement or tree -structured search often face three fundamental limitations: Isolated Trajectories arise when insights discovered in one search path cannot inform parallel explorations. Unexploited Search History occurs when informative patterns across longer trajectories are not captured or reused. Limited Idea Composition 

restricts the integration of promising elements from different branches into improved solutions. We formalize the experimental optimization problem as identifying the optimal solution within a search space S, where each candidate solution ð‘  âˆˆ S represents a complete experimental proposal, including code logic, parameter configurations, and physical operation protocols. The objective is to find:  

> ð‘  âˆ—

= arg max  

> ð‘  âˆˆ S â„Ž

(T , ð‘  ), (4) where â„Ž(T , ð‘  ) denotes the evaluation metric of solution ð‘  on a given task T .To address the limitations of conventional search, InternAgent-1.5 adopts a Graph -Augmented Monte Carlo Search framework. This approach preserves the explorationâ€“exploitation balance of Monte Carlo Tree Search while replacing its rigid tree structure with a dynamic solution graph that aggregates 

> 10

information across all prior experiments. The search still follows the classical loop of selection, expan-sion, simulation, and backpropagation, but its effectiveness comes from a strengthened expansion phase powered by four graph-based operators: 

â€¢ Primary Expansion. Generates a new proposal using only its immediate parent. It performs localized adjustments such as parameter refinement or correction of logical inconsistencies, creating the core backbone of parent and child edges used in credit assignment. 

â€¢ Intra -branch Evolution. Conditions proposal generation on both the parent and the historical trajectory of ancestors within the same branch. By analyzing recent successes and failures, it reinforces productive design changes and avoids repeatedly exploring unpromising strategies, formalizing a localized form of self-reflection. 

â€¢ Cross -branch Reference. Introduces targeted transfer of design elements across different branches. When a branch stagnates, the system identifies high -performing nodes elsewhere in the solution graph and uses them as references, allowing the new proposal to incorporate robust structural patterns or methodological modules discovered in parallel explorations. 

â€¢ Multi -branch Aggregation. Synthesizes complementary strengths from multiple top -performing nodes across the solution graph. By decomposing these proposals into their essential components and recombining them, the operator produces hybrid designs that integrate successful ideas from previously independent search trajectories. Once a new proposal is generated through one of these operators, it is executed in the corresponding environment, either a computational simulator or a physical experimental system, to obtain an evaluation score. This score is backpropagated through the proposalâ€™s ancestral path to guide subse-quent exploration. By integrating graph -based information flow into the Monte Carlo search process, InternAgent-1.5 transforms experimental design into a collaborative and cumulative optimization pipeline, enabling rapid convergence toward high-quality scientific solutions. 

2.3.2. Scenario Code Opimization for Algorithm Discovery In algorithm discovery tasks, each proposal is an executable program specifying data -processing steps, model components, and evaluation settings. The search module generates new variants by refining computational logic or integrating effective structures identified in other branches. Each candidate is executed in a controlled environment that compiles the code and evaluates its performance on benchmark datasets. Quantitative metrics such as accuracy, runtime, and resource usage are returned to the optimization module and backpropagated through the proposalâ€™s lineage, enabling systematic refinement of algorithmic designs. 

Experimental Optimization for Empirical Discovery In empirical discovery tasks, each proposal specifies a full experimental procedure that may be executed either through computational simulation or on physical laboratory systems. The search process refines these procedures by adjusting parameters, modifying operational steps, or incorporating effective sub -protocols identified across branches. When a proposal is simulated, domain models predict experimental outcomes such as reaction yield or protein stability. When it is executed physically, SCP [32] coordinates automated devices to perform the protocol and collect measurements such as fluorescence intensity or assay signal quality. All quantitative results, whether simulated or physically measured, are returned to the optimization module for backpropagation, enabling iterative improvement of empirical workflows. 

2.4. Structured Cognitive Memory for Long Horizon Scientific Discovery 

Long Horizon Memory Capability within the Evolution Subsystem 

To support adaptive, efficient, and long horizon scientific discovery, InternAgent-1.5 incorporates a hierarchical memory subsystem referred to as Structured Cognitive Memory. This subsystem is    

> 11 Task 1: Round 2
> Traj . 2
> Experience 1
> Experience 2
> Experience N
> Â·Â·Â·
> Idea Graph
> Contrastive Learning
> Update
> Experience Library
> Experience Retrieve
> Novelty Selection
> Objective Evolve
> Task 1: Round 1
> Retrieve
> Generation Traj . 1 Execution
> Deep
> Research
> TEM
> Research Objective
> SPM
> FeedBack
> Cross -Disciplinary
> Knowledge
> SKM

Figure 6 | The illustration for our Structured Cognitive Memory. engaged throughout the entire discovery loop and maintains continuity across cycles, allowing the agent to operate coherently over extended durations. It consolidates procedural, episodic, and semantic information into a unified structure that enables short term refinement, mid term adaptation, and long term conceptual development. The overall framework of Structured Cognitive Memory is shown in Fig. 6. 

2.4.1. Strategy-Procedural Memory 

Strategy Procedural Memory (SPM) supports the deep research capability that InternAgent-1.5 invokes throughout the entire scientific discovery process whenever complex analytical reasoning is required. This capability involves integrating literature evidence, synthesizing contextual knowledge, constructing coherent multi-step reasoning plans, correcting failed strategies from earlier research workflows, and analyzing the root causes behind those failures. Instead of storing raw trajectories, the system distills reusable procedural structures from past reasoning processes, including both validated effective patterns and lessons learned from unsuccessful attempts. These procedural structures capture the key decision pivots, organizational patterns that have proven effective across earlier research workflows, as well as the diagnostic insights into failed strategies and their underlying reasons, serving as strategic priors that can be applied flexibly at different stages of the pipeline to avoid recurring pitfalls and optimize reasoning paths. Given a historical trajectory ð‘‡ , SPM constructs a compact representation as follows: 

zð‘‡ = ð‘“ proc (ð‘‡ ), (5) which captures essential procedural states extracted from the full execution trace. When a new deep research query ð‘ž arrives, InternAgent-1.5 retrieves trajectories with procedurally aligned structures: 

S( ð‘ž ) = topk  

> ð‘‡ âˆˆ M SPM

sim   ð‘“ proc (ð‘ž ), zð‘‡ 

 . (6) These retrieved strategic priors guide the planner toward globally coherent reasoning graphs, while constraining the executor to avoid redundant execution steps and unnecessary tool calls, thereby providing a stable and efficient procedural foundation for the downstream discovery process. 

> 12

2.4.2. Task-Episodic Memory 

Task Episodic Memory (TEM) supplies fine grained, within trajectory evidence that enables rapid adaptation during iterative experimentation. After each experiment, the system stores an episodic unit containing the attempted method ð‘š , extracted metrics ð‘¦ , and an improvement judgment. Each unit is encoded using a hybrid representation that combines semantic embeddings with sparse lexical features to support both conceptual and literal alignment. During hypothesis refinement, relevant episodes are retrieved through the following formula: 

R ( ð‘ž ) = topk 

> ð‘’ âˆˆ E

sim ( ð‘“ enc (ð‘ž ), ð‘“ enc (ð‘’ )) , (7) where ð‘ž denotes the current hypothesis. The retrieved episodes are injected directly into the generation context, helping the system avoid unsuccessful methodological directions, exploit successful ones, and refine hypotheses efficiently within each research trajectory. 

2.4.3. Semantic-Knowledge Memory 

Semantic Knowledge Memory (SKM) consolidates conceptual information across sessions and supports the long horizon evolution of research objectives. It consists of a Long Term Experience Library, which stores distilled methodological insights, and an Idea Graph that tracks the semantic topology of previously explored research directions.Specifically, upon the end of each experimental batch, the system employs a pairwise combination strategy for the generated methods to maximize the utilization of existing information. By leveraging contrastive learning between each combination according to their methods and experimental results, InternAgent-1.5 extracts both high-level methodological principles and low-level experimental heuristics to construct Long-term Experience Library. Given a research goal ðº , long term knowledge entries are retrieved via the following formula: 

K ( ðº ) = topk 

> ð‘˜ âˆˆ L

sim ( ð‘“ enc (ðº ), ð‘“ enc (ð‘˜ )) . (8) To promote continued exploration, each candidate objective ð‘ is assigned a novelty score: 

nov (ð‘ ) = 1 âˆ’ max   

> ð‘¥ âˆˆ G

sim ( ð‘“ enc (ð‘ ), ð‘“ enc (ð‘¥ )) , (9) which encourages the selection of objectives that extend beyond previously explored conceptual regions. In this way, SKM provides the semantic continuity and innovation pressure required for sustained multi-session scientific discovery. 

## 3. Experiments 

To evaluate InternAgent-1.5â€™s capabilities in the full process of scientific discovery from multiple aspects, we verify its effectiveness through cross-disciplinary benchmarks, autonomous algorithm development, and scientific mechanism discovery, as elaborated in Sec. 3.2, 3.3, and 3.4, respectively. In Sec. 3.4, InternAgent-1.5 demonstrates its applications in scenarios such as Earth Science 3.4.1, Life Science 3.4.2, Biological Science 3.4.3, and Physical Science Tasks 3.4.4. 

3.1. Experiments Setup 

3.1.1. General Scientific Reasoning Abilities SGI-Bench [29]. SGI-Bench is a scientist-aligned benchmark for Scientific General Intelligence (SGI), defined as the ability of an AI system to autonomously navigate the full scientific inquiry cycle of 

> 13

Deliberation, Conception, Action, and Perception. It operationalizes this goal with four task families spanning 10 scientific disciplines and 1,000+ expert-curated samples: Scientific Deep Research, Idea Generation, Dry/Wet Experiments, and Experimental Reasoning. Our results are reported on the DeepResearch and IdeaGeneration subsets. 

GAIA [25]. GAIA is a benchmark of real-world tasks that require coordinated abilities in reasoning, multimodal understanding, web navigation, and tool use. We report results on its 165-question validation set. 

HLE [27]. Humanityâ€™s Last Exam (HLE) is a large-scale multimodal benchmark of 2,500 expert-written questions covering mathematics, humanities, and the natural sciences. It is designed to probe frontier-level academic reasoning, where current LLMs still fall far short of human performance. 

Frontier Science [28]. FrontierScience is a benchmark that evaluates whether AI systems can perform expert -level scientific tasks, including study design, data interpretation, and hypothesis assessment. Following the protocol in the original paper, our results are averaged over 20 runs on the Olympiad subset and 30 runs on the Research subset using its standard evaluation set. 

GPQA-diamond [26]. GPQA is a collection of 448 expert-written multiple-choice questions in biology, chemistry, and physics, designed to test deep scientific reasoning rather than surface knowledge. We use its 198-question GPQA-diamond subset for evaluation. 

3.1.2. Algorithm Discovery Scientific Algorithm. To validate the ability of InternAgent-1.5 to discover algorithms in scientific data domains and to demonstrate its improvements over InternAgent -1.0 [7], we conducted experiments on six scientific data oriented algorithm discovery tasks. Notably, due to the limited capabilities of the coding agent in InternAgent -1.0 [7], the baseline repositories for all tasks were first manually consolidated into single -file implementations before being optimized by our system. In contrast, 

InternAgent-1.5 supports an end - to - end algorithm optimization workflow directly on the original codebases. 

â€¢ AutoRYP: The AutoRYP task is built on the Suzukiâ€“Miyaura reaction dataset containing 5,760 reaction entries [35]. A LoRA -finetuned LLaMA3 -8B embedding model [36] with an MLP predic-tor serves as the baseline. Model performance is assessed using the coefficient of determination (R 2). 

â€¢ AutoTPPR: The AutoTPPR task operates on the Perturb -seq single -cell transcription -response dataset [37]. GEARS [38], a GNN - and MLP -based framework for multi -omics representation learning, is adopted as the baseline. The Top-20 DE MSE is used as the evaluation metric. 

â€¢ AutoPower: The AutoPower task relies on the IEEE 39 -Bus benchmark for power -flow estima-tion [39]. SenseFlow [40], a physics -informed self -ensembling model, serves as the baseline method. Evaluation is performed using RMSE on PQ nodes. 

â€¢ AutoTSF: The AutoTSF task is defined on the ETTh1 multivariate time -series dataset from the ETT benchmark [41]. DLinear [42], an MLP -based decomposition and forecasting model, is used as the baseline. Mean Absolute Error (MAE) averaged over horizons 96, 192, 336, 720 serves as the metric. 

â€¢ AutoMD: The AutoMD task uses the MD17 dataset [43], which provides molecular energies and forces for seven small organic molecules. VisNet [44], an equivariant geometry -enhanced GNN, is adopted as the baseline. Force-MAE is used as the evaluation metric. 

â€¢ AutoEAP: The AutoEAP task is constructed from the UMI -STARR -seq enhancer -activity dataset [45]. DeepSTARR [46] provides the baseline for sequence -based enhancer -activity prediction. Housekeeper Pearson Correlation Coefficient (HK-PCC) is used for evaluation. 

> 14

AI Algorithm. To further evaluate the capabilities of InternAgent-1.5 on AI algorithm discovery, we assembled a diverse suite of tasks that span model training pipelines, memory optimization strategies, reinforcement learning methods, and data processing routines, which collectively represent several of the most active directions in current AI algorithm research. For each domain, we selected papers accepted by top AI conferences in 2025 as comparative baselines to validate whether InternAgent-1.5 can outperform the latest AI algorithms. 

â€¢ AutoTTS: The AutoTTS task is constructed on a benchmark evaluating test -time scaling strategies for enhancing LLM reasoning. Atom [47], a Markov -structured test -time scaling framework that refines reasoning through iterative candidate exploration and denoising, serves as the baseline. Model performance is assessed using standard accuracy -based reasoning metrics defined by the benchmark. 

â€¢ AutoMem: The AutoMem task is defined on long -term interaction and memory -management benchmarks for LLM agents. A -MEM [48], an agentic memory system inspired by the Zettelka-sten method and designed for dynamic note construction, semantic linking, and memory evolution, serves as the baseline. Evaluation focuses on long -horizon agent performance using metrics such as retrieval accuracy, contextual relevance, and behavioral consistency under extended interaction. 

â€¢ AutoLM: The AutoLM task examines self -distillation based data synthesis for mathematical reasoning. For comparison, we implement a complete self -distillation pipeline that performs synthetic question creation through few -shot prompting, reasoning -trajectory generation, and answer verification through majority voting. The resulting synthetic data are then used to train Intern-S1-mini [49]. The evaluation measures the mathematical -reasoning ability of the resulting model, using standard question-answering accuracy as the primary metric. 

â€¢ AutoTTRL: The AutoTTRL task is designed to autonomously discover reinforcement learning al-gorithms that do not require ground truth annotations on reasoning tasks ( i.e. , AIME 2024 [50]). We employ Test-Time Reinforcement Learning (TTRL) [51] as the baseline method, which utilizes a majority voting mechanism to provide effective reward estimation. Following TTRL, we generate 16 responses per question and calculate the average pass rate pass@1 = 1

> ð‘˜

Ãð‘˜ ð‘– =1 ð‘ ð‘– 

for evaluation, where ð‘ ð‘– denotes the correctness of the ð‘– -th response. 

3.1.3. Empirical Discovery Earth Science. To evaluate InternAgent-1.5 in the Earth Science domain, which involves petabyte-scale, multi-dimensional datasets and complex physical processes, we constructed two representative tasks: 

â€¢ Automated Climate Diagnostics: This task assesses the systemâ€™s ability to integrate multi-source knowledge for data analysis. The benchmark utilizes historical Surface Air Temperature (TAS) data (1970â€“2010) from 20 Global Climate Models (GCMs) in the CMIP6 archive [52] (including ACCESS-ESM1-5, CanESM5, etc.) and the ERA5 reanalysis dataset [53] as the observational ground truth. The goal is to autonomously identify global warming trends and regional biases. 

â€¢ Climate Downscaling Optimization: This task evaluates the systemâ€™s ability to innovate scientific methods. The objective is to enhance surface-temperature fields over China from coarse-resolution NCEP-NCAR-R1 data ( 2â—¦) [54] to fine-scale ERA5 resolution ( 0.25 â—¦). We employ standard Kriging interpolation [55] and linear Bias-Corrected Spatial Disaggregation (BCSD) [56] as baselines to test if the system can autonomously design a superior deep-learning-based solution. 

> 15

Life Science. To demonstrate the broad applicability of InternAgent-1.5 in early-stage drug discovery, we evaluate its capabilities to therapeutic target identification, a domain characterized by heteroge-neous multi-omics evidence, complex disease mechanisms, and strong requirements for mechanistic interpretability and experimental verifiability. We construct two representative target-discovery tasks that stress graph-structured planning, multi-modal tool orchestration, and iterative reflection: 

â€¢ Automated Biological Evidence Synthesis: The agent orchestrates end-to-end analyzes (ex-pression, genomic alteration, survival, pathway, and tractability) by integrating resources such as TCGA [57], OpenTargets [58] and KEGG [59], and synthesizes a coherent evidence chain. We reproduce OriGeneâ€™s discovery of GPR160 as an HCC target. 

â€¢ Hypothesis Generation and Target Prioritization: The agent constructs a multi-modal evi-dence graph (cohorts, proteomics, annotations, pathways, and literature) and iteratively refines mechanistic hypotheses to rank actionable candidates. We reproduce the identification of 

ARG2 as a mechanistically grounded target in CRC, together with experiment-ready validation suggestions. 

Biological Sciences. As a key capability in the Biological Sciences domain, the fluorescent -protein engineering task targets the improvement of existing fluorescent proteins for imaging applications. The system identifies the parent sequence and relevant structural context through literature -driven analysis, then performs dry -lab design by combining sequence inspection, folding prediction with ESMFold [60], and mutational -effect evaluation using sequenceâ€“function and stability predictors such as ProSST [61] to generate candidate variants. These designs are translated into wet -lab protocols through SCP [32], which coordinates automated DNA assembly, expression, purification, and fluorescence -intensity measurement. The resulting data are analyzed and fed back into the design layer, producing a structured experimental report that integrates predictions, protocols, and measured performance. 

Physical Science. To evaluate InternAgent-1.5 in chemical synthesis and drug discovery, we define two tasks requiring the integration of physical constraints and structural design: 

â€¢ Automated Reaction Outcome Prediction: Evaluated on the ChemCoTBench [62] forward prediction dataset, this task requires the agent to predict both the target major product and stoichiometric by-products. The system must analyze reactant properties and strictly apply atomic conservation logic. To ensure rigorous evaluation, we explicitly revised 26 problematic entries in this benchmark, providing a corrected ground truth for synthesis planning. 

â€¢ Generative Scaffold Hopping: This task aims to discover novel molecular entities that cir-cumvent patent barriers while preserving bioactivity. The agent is tasked with replacing the core scaffold of a molecule while maintaining key 3D shape and electrostatic features. The system must employ generative algorithms to propose bioisosteres and filter candidates based on medicinal chemistry metrics, such as Synthetic Accessibility and LogP, to ensure the proposed analogs are viable drug candidates. 

3.2. Evaluating Agentic Reasoning Abilities 

SGI-Bench. As shown in Table 2, InternAgent-1.5 (Gemini-3-pro+o4-mini) achieves the best perfor-mance on two SGI-Bench tracks, Deep Research and Idea Generation, significantly outperforming strong frontier models. On Deep Research track, InternAgent-1.5 reaches 37.74%, surpassing the second-best Gemini-3-pro 18.48%) by a large margin (+19.26%). On Idea Generation, InternAgent-1.5 attains 58.11%, exceeding the prior best GPT-5 55.40% (+2.71%). Overall, these results suggest that InternAgent-1.5â€™s iterative deep-research workflow that integrate structured planning, targeted 

> 16

Table 2 | Performance comparison on SGI-bench. The best results are bolded and the second best results are underlined. 

Method Deep Research Idea Generation 

Gemini-3-pro [63] 18.48 39.68 GPT-5 [64] 14.47 55.40 Claude-Sonnet-4.5 [65] 13.84 43.20 Qwen3-Max [66] 15.38 39.83 o4-mini [67] 11.95 40.78 

InternAgent-1.5 (Gemini-3-pro+o4-mini) 37.74 58.11 

Table 3 | Performance comparison on GAIA and HLE benchmarks. The best results are bolded and the second best results are underlined. Results not reported in the original papers are denoted as â€œ - ". 

GAIA val HLE Agent Base Model Level 1 Level 2 Level 3 Avg. Text only All 

React Model with Tools 

WebDancer [68] QwQ-32B 61.5 50.0 25.0 51.5 - -WebShaper [69] Qwen2.5-72B 69.2 63.4 16.6 60.1 - -MiroThinker [70] MiroThinker-v1.5-30B - - - 72.0 31.0 MiroThinker [70] MiroThinker-v1.5-235B - - - 80.8 39.2 -Tongyi-DR [71] Tongyi-DR-30B - - - 70.9 32.9 -

DeepResearch Agents 

OpenAI DR [72] - 74.29 69.06 47.60 67.36 - 26.60 ChatGPT-Agent [64] - - - - - - 41.60 

Kimi-Researcher [73] - - - - - - 26.90 Manus [74] - 86.50 70.10 57.70 73.30 - -Gemini DR [75] - - - - - - 26.90 OWL [76] Gemini 2.5 Pro 84.90 68.60 42.30 69.70 - -

Our Method 

InternAgent-1.5 Qwen3-235B 69.81 60.47 30.77 58.79 15.04 14.84 InternAgent-1.5 o4-mini 88.68 81.39 61.54 80.61 36.10 34.52 InternAgent-1.5 Gemini-3-pro+o4-mini 92.45 89.53 61.54 86.06 40.87 40.00 information gathering, and refinement yields substantial gains in evidence-driven research capability while also improving creative yet grounded idea generation. 

GAIA. As shown in Table 3, InternAgent-1.5 outperforms both closed-source Manus (73.30%) and leading open-source agentic models Mirothinker (80.8%) and Tongyi-DR (70.9%), even though they are specifically trained and evaluated only on the GAIA text-only subset. InternAgent-1.5 also shows strong robustness on Level 3 questions (61.54%). These results indicate that InternAgent-1.5â€™s iterative workflow combining knowledge planning, collection, and refinement is particularly effective for multi-hop and compositional reasoning. 

HLE. As shown in Table 3 and 4, InternAgent-1.5 achieves the best overall performance among all compared systems. It reaches 40.87% accuracy in the text-only setting and 40.00% on the full benchmark, outperforming strong baselines such as Gemini-3-pro-preview and GPT-5. The 

> 17

Table 4 | Domain-wise performance comparison on the Humanityâ€™s Last Exam (HLE). The best results are bolded and the second best results are underlined. 

Humanityâ€™s Last Exam Setting Model Math Bio/Med CS/AI Physics Human. Chem. Engineer. Other Avg. 

Text-Only 

Deepseek-R1 [77] 9.30 8.60 7.40 5.80 11.00 5.60 10.30 7.50 8.60 Gemini-3-pro-preview [63] 45.08 26.13 26.79 32.67 44.04 34.65 29.69 32.39 38.00 

InternAgent-1.5 48.96 30.63 29.46 34.16 44.56 30.69 28.13 37.50 40.87 All-Set 

o4-mini [67] 19.00 11.40 12.90 12.60 9.10 12.70 12.60 6.90 14.30 GPT-5 [64] 31.00 22.10 24.90 21.70 20.60 16.40 14.40 18.00 24.80 Gemini-3-pro-preview [63] 44.76 27.14 29.05 31.30 42.92 40.00 32.43 34.33 38.04 

InternAgent-1.5 48.09 30.36 30.71 33.04 42.47 34.55 30.63 38.63 40.00 

Table 5 | Performance comparison on FrontierScience of olympiad and research tasks across bio, chem, and phy domains. The best results are bolded and the second best results are underlined.                                                            

> Olympiad (avg N=20) Research (avg N=30) Method Bio Chem Phy All Bio Chem Phy All
> o4-mini [67] 47.00 Â±14.90 65.00 Â±6.40 53.40 Â±4.50 57.40 Â±3.30 9.67 Â±5.47 8.17 Â±4.37 0.83 Â±2.27 6.20 Â±2.54 InternS1-235B [78] 17.00 Â±12.69 52.88 Â±4.05 50.40 Â±3.88 48.05 Â±2.84 4.50 Â±4.35 11.00 Â±3.74 2.67 Â±3.35 6.06 Â±2.30 Mirothinker-v1.5-30B-A3B [70] 22.86 Â±4.52 69.64 Â±7.49 54.86 Â±3.18 57.57 Â±3.66 8.17 Â±6.39 8.50 Â±6.21 5.83 Â±4.10 7.50 Â±3.77 DeepSeek-V3.2-Thinking [79] 26.50 Â±7.26 72.25 Â±3.25 66.30 Â±2.63 64.70 Â±2.41 2.50 Â±3.10 16.33 Â±4.64 1.40 Â±2.70 6.84 Â±1.88 Qwen3-235B-A22B-Thinking [66] 24.00 Â±9.17 61.13 Â±6.05 57.10 Â±4.79 55.40 Â±3.68 10.17 Â±5.08 10.00 Â±6.32 1.58 Â±2.41 7.34 Â±3.37 Qwen3-30B-A3B-Thinking [66] 13.50 Â±9.10 47.25 Â±4.47 42.70 Â±3.65 41.60 Â±2.94 1.50 Â±2.93 2.00 Â±3.32 0.70 Â±1.79 1.41 Â±1.52
> Our Method
> InternAgent-1.5 46.00 Â±8.00 85.50 Â±3.67 76.80 Â±2.99 77.20 Â±3.06 10.33 Â±4.64 22.00 Â±6.00 3.67 Â±2.87 12.00 Â±2.49

improvements are consistent across most HLE sub-domains, highlighting the robustness of InternAgent-1.5 on long-horizon, cross-disciplinary reasoning tasks. 

FrontierScience. Table 5 compares the performance of various methods on Olympiad and Research tasks across biology, chemistry, and physics domains. InternAgent-1.5 achieves the best overall results in both Olympiad (77.20%) and Research (12.00%) settings, with particularly strong performance in Chemistry and Physics. It outperforms all baselines, including DeepSeek-V3.2-Thinking (64.70% Olympiad) and Mirothinker-v1.5 (7.50% Research), demonstrating its superiority in both structured problem-solving and open-ended scientific reasoning. 

GPQA. As shown in Table 6, InternAgent-1.5 achieves state-of-the-art performance on the GPQA-diamond benchmark with an average accuracy of 87.37%. It outperforms both strong base models and prior tool-augmented agents, with particularly strong results in Chemistry and Physics. These results demonstrate the effectiveness of our method for expert-level scientific reasoning. 

3.3. Results for Algorithm Discovery Tasks 

3.3.1. Scientific Algorithm 

We evaluated InternAgent-1.5 across six scientific domains and compared it against our previous work [7, 21], and state-of-the-art domain-specific baselines. As summarized in Table 7, InternAgent-1.5 consistently achieves superior performance across all tasks and demonstrates the efficacy of our architectural improvements. 

Chemical and Molecular Analysis. In the domain of chemical synthesis, the model demonstrates a strong capacity to interpret structured reaction information. For the AutoRYP task on the Suzuki-Miyaura dataset, InternAgent-1.5 achieves an ð‘… 2 of 36.6. This result significantly outperforms the LoRA finetuned LLaMA3 baseline of 27.6 and the Dolphin score of 31.8. Similarly, for the AutoMD 

> 18

Table 6 | Performance comparison on GPQA-diamond benchmark. The best results are bolded and the second best results are underlined. Results not reported in the original papers are denoted as â€œ - ".                                                 

> GPQA-diamond Agent Bio Chem Phys Avg.
> Base Models
> Qwen-3-8B [66] ---44.44 Qwen3-32B [66] ---49.49 Qwen3-235B [66] ---47.47 Intern-S1 [78] 89.47 59.49 93.02 78.26 Deepseek-R1 [77] 63.16 76.34 91.86 82.32 o4-mini [67] 78.95 63.44 94.19 78.28 GPT-5 [64] 84.21 76.34 95.35 85.35
> React Model with Tools
> WebShaper [69] 47.37 52.69 81.40 64.65 MiroThinker [80] 84.21 75.27 95.35 84.85 Tongyi DR [71] 78.95 67.74 95.35 80.30
> Our Method
> InternAgent-1.5 84.21 79.57 96.51 87.37

Table 7 | Performance comparison across six types of scientific algorithm tasks. 

Tasks Method AutoRYP AutoTPPR AutoPower AutoTSF AutoMD AutoEAP  

> ð‘… 2

MSE RMSE MAE Energy-MAE HK-PCC 

Baseline 27.6 0.197 0.00473 0.438 0.158 0.65 Dolphin [21] 31.8 0.173 0.00455 0.463 0.152 0.76 

InternAgent-1.0 [7] 35.4 0.146 0.00426 0.433 0.148 0.79 

InternAgent-1.5 36.6 0.143 0.00318 0.423 0.114 0.91 

task regarding Molecular Dynamics, our model effectively captures geometric features. It reduces the Energy MAE to 0.114 compared to 0.158 achieved by the equivariant GNN baseline VisNet. 

Physics and Engineering Systems. Our framework exhibits robust performance in modeling complex physical systems and temporal dependencies. In the AutoPower task for Power Flow Estimation, InternAgent-1.5 achieves an RMSE of 0.00318 on the IEEE 39-Bus dataset. This surpasses the physics informed SenseFlow model score of 0.00473. For the AutoTSF task involving Time Series Forecasting, the DLinear baseline proves to be a strong competitor with an MAE of 0.4382. Our method further reduces the error to 0.423 and demonstrates effective handling of multivariate trends in the ETTh1 dataset. 

Biological and Genomic Prediction. The most substantial improvements are observed in computa-tional biology tasks. In the AutoTPPR task for Transcription Prediction, the model achieves an MSE of 0.143. This outperforms the GNN based GEARS framework score of 0.197. Notably, in the AutoEAP task for Enhancer Activity Prediction, InternAgent-1.5 reaches a Pearson Correlation Coefficient of 0.91. This constitutes a drastic improvement over the DeepSTARR baseline of 0.65 and highlights the exceptional ability of the agent to map DNA sequences to quantitative activity levels. 

> 19

Table 8 | Results on complicated scientific algorithm design tasks. Note that our previous version Dolphin [21] and InternAgent-1.0 [7] cannot address these complicated tasks listed in the table. 

Tasks Method AutoTTS AutoMem AutoTTRL AutoLM Acc F1 Acc Acc 

Baseline 70.9 0.2338 Â±0.3452 23.3 0.880 

InternAgent 1.5 72.5 0.2785 Â±0.3643 23.9 0.904 

3.3.2. AI Algorithm Test-time Scaling. On the MMLU-CF dataset [81], we evaluate the reasoning capability using the architecture proposed by [47]. Our approach attains a score of 72.5, exceeding the baseline score of 70.9. This improvement indicates that InternAgent-1.5effectively enhances performance in knowledge-intensive tasks, demonstrating robust reasoning capabilities and superior adaptability in complex domain scenarios. 

Memory Mechanism. On the Locomo dataset [82], we evaluate under the AutoMem setup using Qwen2.5 -3B [66] as the base model to ensure alignment with the A -MEM [48] baseline. Using F1 as the evaluation metric, our approach attains an F1 of 0.2785, exceeding the baseline score of 0.2338. This improvement indicates that the proposed memory architecture and interaction mechanism enable more reliable long-horizon retention, retrieval, and integration of accumulated information. 

Reinforcement Learning. On the AutoRL task, we evaluate our approach across the reinforcement -learning control and decision -making benchmarks used in prior work. Using the same environments and return -based metrics as the TTRL baseline, our method achieves consistently higher returns and improved training stability. These results indicate that the proposed framework provides more effective trajectory refinement and decision -making guidance across diverse RL settings. 

Large Language Model. On the AutoLM task, we apply the full self distillation pipeline and fine tune Intern-S1-mini [78] on the synthesized mathematical reasoning data. To validate algorithmic performance under a minimal system configuration, all experiments are conducted with a 16k token context. We evaluate our approach on the MATH500 reasoning benchmark used in prior work. Accuracy improves from 0.880 to 0.904, indicating that the enhanced self distillation pipeline produces higher quality trajectories and verified answers, providing effective supervision for strengthening the modelâ€™s mathematical reasoning ability. 

> 20

3.4. Discoveries of Scientific Mechanism 

3.4.1. Earth Science 

Figure 7 | Automated Climate Analysis - Temperature Trends. InternAgent-1.5 autonomously generated this diagnostic for 20 CMIP6 models against ERA5 (1970-2010), showing the ranked bar chart of global-mean temperature trends ( â—¦C/decade). Building on the setup described in Sec. 3.1, we demonstrate how InternAgent-1.5 addresses the dual challenges of knowledge integration and high-fidelity modeling in Earth Science. 

Automated Data Analysis and Knowledge Integration. In the Automated Climate Diagnostics task, the system was prompted to evaluate CMIP6 climate model simulations against the ERA5 reanalysis. Rather than simply calculating statistics, InternAgent-1.5 employed its Knowledge Flow Planner to integrate climate modeling literature and physical reasoning. Guided by widely adopted diagnostic conventions, the system selected key evaluation metrics, including global-mean surface temperature trends ( â—¦C decade âˆ’1) and modelâ€“observation biases, and constructed an end-to-end analysis pipeline encompassing data retrieval, temporal alignment, and statistical estimation. The system successfully processed the multi model ensemble and generated a ranked bar chart in Fig. 7 that contextualizes model performance. To further assess the physical consistency of the simulated trends, InternAgent-1.5 also produced spatial maps of linear temperature change in Fig. 8, which enable interpretation at the regional scale. These diagnostics reveal canonical large scale warming patterns, including enhanced high latitude trends that match established characteristics of observed and simulated climate change. Taken together, the results show that InternAgent-1.5 supports climate analysis not only by automating computation but also by organizing diagnostics in a manner that aligns with domain specific interpretability. 

> 21

tas linear trend 1970-2010 (Â°C/decade) 

ERA5 ACCESS-ESM 1-5 BCC-CSM2-MR CAMS-CSM 1-0 CanESM 5 CAS-ESM2-0 CESM2-WACCM CMCC-CM2-SR5 CMCC-ESM2 E3SM-2-0 FGOALS-f3-L GFDL-ESM4    

> -1.0 -0.5 0.5 1.0 0.0

tas trend ( Â°C decade -1 ); stippling on ERA5â‰ˆ95% significant (a) 76.00 90.25 104.50 118.75 133.00 

> 20
> 28
> 36
> 44
> 52
> Reference

ERA5 (0.25Â° ref)     

> 76.00 90.25 104.50 118.75 133.00
> 20
> 28
> 36
> 44
> 52
> RMSE=4.017e-05

Bilinear (0.25Â°)     

> 76.00 90.25 104.50 118.75 133.00
> 20
> 28
> 36
> 44
> 52
> RMSE=4.078e-05

Ordinary Kriging (0.25Â°)     

> 76.00 90.25 104.50 118.75 133.00
> 20
> 28
> 36
> 44
> 52
> RMSE=2.861e-05

BCSD (0.25Â°)     

> 76.00 90.25 104.50 118.75 133.00
> 20
> 28
> 36
> 44
> 52
> RMSE=2.856e-05

GLM (0.25Â°)     

> 76.00 90.25 104.50 118.75 133.00
> 20
> 28
> 36
> 44
> 52
> RMSE=2.252e-05

InternAgent-1.5 (0.25Â°)            

> 0.00000 0.00005 0.00010 0.00015 0.00020 0.00025 0.00030 0.00035 0.00040
> pr (kg m 2s1)

Precipitation downscaling comparison (2017-06) 

(b) 

Figure 8 | (a) Automated Climate Analysis - Spatial Patterns. Spatial maps of linear temperature trends generated by InternAgent-1.5, identifying regional warming patterns across different CMIP6 models. (b) Precipitation downscaling compariso across different methods. 

Algorithmic Innovation and Optimization. For the Climate Downscaling Optimization task, InternAgent-1.5 addressed known limitations of widely used baseline methods, including Kriging [55] and BCSD [56], which can struggle to represent non-stationary biases and fine-scale spatial variability in surface temperature fields. 

22 The system autonomously conducted a literature review, recognizing that standard linear assumptions are insufficient for non-stationary biases. It proposed a deep-learning-based approach designed to cap-ture complex spatial dependencies, generated the implementation code, and refined the architecture through iterative validation. As shown in Fig. 8 and summarized in Table 9, the model optimized by InternAgent-1.5 achieves improved performance relative to established statistical baselines. While the baseline Kriging and BCSD methods yielded RMSEs of 3.1658 and 0.9049 respectively, our systemâ€™s solution reduced the RMSE to 0.8488 .Beyond improvements in bulk error statistics, qualitative comparison of spatial fields indicates that bilinear interpolation and kriging introduce substantial spatial smoothing and attenuate high-intensity precipitation cores. In contrast, the InternAgent-1.5 more faithfully reproduces fine-scale spatial gradients and localized convective maxima present in the ERA5 reference data. This suggests that the model effectively captures nonlinear and scale-interactive processes that are not resolved by conventional interpolation or stationary bias-correction methods. Collectively, these results confirms that InternAgent-1.5 can independently conceive and optimize new scientific tools rather than merely applying existing ones. Table 9 | Performance Comparison on Climate Downscaling Task. The deep learning method pro-posed and implemented by InternAgent-1.5 outperforms both traditional interpolation and statistical correction baselines in reconstructing high-resolution ( 0.25 â—¦) surface temperature fields. 

Method Type RMSE 

Kriging Interpolation Traditional Spatial 3.1658 BCSD Statistical Correction 0.9049 

InternAgent-1.5 AI-Optimized Deep Learning 0.8488 3.4.2. Life Science 

We present two representative case studies to illustrate how InternAgent-1.5 supports therapeutic target discovery in realistic biomedical scenarios. 

Automated Biological Evidence Synthesis. As a case study, we reproduced the discovery of GPR160 

as a novel therapeutic target in hepatocellular carcinoma (HCC) reported by OriGene [83]. We prompted InternAgent-1.5 to â€œidentify understudied yet mechanistically promising druggable targets in HCC using multi-modal evidence.â€ Using the Knowledge Flow Planner , the system autonomously decomposed the task into differential expression analysis, mutation and copy-number evaluation, survival association testing, pathway enrichment, and tractability assessment. It queried canonical resources including GEPIA, TCGA, GEO, and OpenTargets to generate an initial pool of 125 candidate genes, which was progressively narrowed to GPR160 through multi-round evidence compression and reflection. The system further produced expression profiles, Kaplanâ€“Meier survival curves, and KEGG pathway maps, revealing tumor-specific overexpression of GPR160 , its association with poor recurrence-free survival, and its potential involvement in immune-related signaling. This case demonstrates InternAgent-1.5â€™s ability to translate open-ended biomedical questions into structured and mechanistically grounded evidence chains. 

> 23

Figure 9 | Mitochondrial Arg2 immunometabolic pathway and therapeutic intervention points 

Hypothesis Generation and Target Prioritization. We further reproduced the identification of 

ARG2 as an overlooked yet mechanistically grounded target in colorectal cancer (CRC). The system constructed a multi-modal evidence graph integrating TCGA cohorts, Human Protein Atlas proteomics, UCSC genome annotations, pathway knowledge, and literature-derived molecular mechanisms. Built upon domain-specific reasoning templates, InternAgent-1.5 executed structured reasoning steps including disease gene consistency checks, pathwayâ€“phenotype alignment, pharmacological tractability analysis, and clinical differential expression testing. Through iterative reflection cycles, ARG2 emerged as the top-ranked candidate, accompanied by mechanistic explanations involving metabolic reprogramming and immunosuppressive microenvi-ronment remodeling. As illustrated in Fig. 9, which is automatically generated by InternAgent-1.5, mitochondrial ARG2-driven arginine depletion reduces nitric oxide (NO) availability, impairs T-cell effector function, and promotes tumor proliferation via enhanced polyamine biosynthesis, providing a unified metabolicâ€“immunological rationale for therapeutic intervention. The complete report is released in our open-source repository. InternAgent-1.5 further generated experiment-ready recommendations, including doseâ€“response assays, patient-derived organoid (PDO) validation, and immune profiling protocols, consistent with those used in the original study. Notably, ARG2 inhibition exhibited dose-dependent anti-tumor effects in HCT116 cells and multiple CRC PDO models, supporting the validity of the generated hypotheses. Together, these case studies show that InternAgent-1.5 can support end-to-end target discovery, bridging multi-omics evidence integration, mechanistic hypothesis generation, and experimental guidance. 

> 24

3.4.3. Biological Science 

Figure 10 | The engineering evolution from eGFP to sfGEP The experimental began with a targeted literature search by InternAgent-1.5 to identify fluorescent protein variants with strong brightness and folding stability. Evidence from peer reviewed studies pointed to sfGFP as a suitable candidate. This information, combined with predefined performance objectives, guided the design of the computational analyses and the experimental validation plan. To evaluate these candidates, a series of dry lab and wet lab procedures were carried out using tools and devices coordinated through SCP [32]. The workflow included fluorescence assays, stability measurements, and quality control checks, paired with dry lab predictions of structural stability and sequence function relationships. The results show that sfGFP achieves high functional brightness and reliable folding efficiency, consistent with findings reported in the literature. Based on all data returned by SCP coordinated tools and instruments, InternAgent-1.5 generated a final experimental report that summarizes the empirical outcomes and identifies variants appropriate for downstream use. Figure 10, which is automatically generated by InternAgent-1.5, presents an intermediate reasoning artifact automatically produced by InternAgent-1.5, which outlines how evidence from the literature is transformed into an engineering rationale and target selection for sfGFP, and the complete report is available in our open source repository. 

3.4.4. Physical Science 

Building on the setup described in Sec. 3.1, we demonstrate how InternAgent-1.5 addresses the dual challenges of strict atomic conservation and vast chemical search spaces in Physical Science. We report quantitative metrics on reaction prediction benchmarks and qualitative case studies in generative drug design. 

> 25

Table 10 | Performance on Forward Major Product ( Fwd major ) and By-product Prediction ( Fwd by ). Top-1 accuracy and Fingerprint Tanimoto Similarity (FTS) are reported. Models Fwd major Fwd by 

Top-1 FTS â†‘ Top-1 FTS â†‘

GPT-5.2 59 0.79 45 0.40 Claude4.5-sonnet-think 0.74 0.90 0.49 0.43 o3-mini 0.55 0.74 0.47 0.47 

Gemini-3-Pro-Thinking 0.81 0.91 0.45 0.36 InternAgent-1.5 0.86 0.94 0.62 0.42 

Automated Reaction Outcome Prediction. We evaluated the system on the ChemCoTBench [62] forward prediction task. Unlike standard language models that treat molecular strings as mere text [84], InternAgent-1.5 adopts a physicochemical-aware approach by proactively invoking RD-Kit [85] to compute critical molecular descriptors (e.g., LogP, TPSA) and standardize SMILES repre-sentations. This descriptor-guided reasoning allows the system to accurately deduce the main product while simultaneously employing atomic conservation logic to infer by-products such as water or halides. As detailed in Table 10, InternAgent-1.5 achieves a Top-1 accuracy of 0.86 and a Fingerprint Tanimoto Similarity (FTS) of 0.94 for major product prediction. These results significantly outperform recent reasoning-enhanced models such as o3-mini (Top-1 0.55) and Gemini-3-Pro-Thinking (Top-1 0.81). Furthermore, in the challenging by-product prediction task ( Fwd by ), our system achieves the highest Top-1 accuracy of 0.62, confirming its robustness in capturing complete reaction stoichiometry. Input     

> Scaffold Detection
> Scaffold Hopping
> &Rerank ing
> ...
> 1st 2nd 3rd
> Output
> Hit-to-Lead Optimization
> F

Figure 11 | Automated scaffold hopping and hit to lead optimization using InternAgent-1.5. 

The workflow begins by identifying the core scaffold in red from a DprE1 inhibitor hit. Through coordinated agent interaction, the system proposes structurally diverse bioisosteres and prioritizes piperidinopyrimidine variants shown in green. It then conducts a focused optimization step to address physicochemical limitations and generates a final candidate highlighted in blue, which features a modified heterocycle and fluorination. The resulting trajectory follows established medicinal chemistry practice and demonstrates the systemâ€™s ability to support rational drug design. 

Generative Scaffold Hopping. In the drug design domain, InternAgent-1.5 employs a generative multi-agent workflow that prioritizes 3D shape and electrostatic alignment over simple 2D topology. 

> 26

Crucially, the system integrates agent reasoning to refine candidates based on calculated metrics including Synthetic Accessibility (SA), Tanimoto Similarity, and LogP. When applied to a DprE1 inhibitor template known for solubility limitations [86], the agent successfully navigated away from the original pyrrolothiadiazole core, proposing plausible bioisosteres based on piperidinopyrimidine scaffolds (Figure 11, Outputs 1stâ€“3rd). Notably, the agent autonomously simulated a â€œhit-to-leadâ€ optimization phase. It replicated expert-driven evolution by replacing the lipophilic piperidine side chain with a polar morpholine ring and introducing a fluorine atom at the para-position of the phenyl ring (Output), modifications critical for enhancing metabolic stability and solubility [86]. 

3.5. Effectiveness of Structured Cognitive Memory 

Figure 12 | Experimental validation of memory effectiveness on algorithm discovery tasks. Research Direction 1: Graph -Based Attention                  

> Exploration.
> Objective: Investigate how graph -based attention
> mechanisms can be further integrated into
> Transformer networks for multivariate time series
> forecasting.
> Research Direction 2: Simplification and Optimization
> of Transformer Architectures .
> Objective: Analyze opportunities to simplify
> Transformer network architectures while maintaining
> or improving forecasting performance.
> Research Direction 3: Cross -Series Interaction and
> Hierarchical Structures.
> Objective: Delve into the integration of cross -series
> interaction mechanisms with hierarchical attention
> layers.
> Research Direction: Advanced
> Transformer -based Model Design for
> Multivariate Time Series Forecasting .
> Objective: Exploring architectural
> improvements of the most advanced
> Transformer network
> Research Direction 1: Streamlined Graph -Based
> Attention for Temporal Dynamics .
> Objective: Investigate the role of simplified graph -
> based attention mechanisms in enhancing temporal
> dynamics modeling within Transformer architectures.
> Research Direction 2: Adaptive Temporal Pattern
> Recognition.
> Objective: Explore adaptive mechanisms for
> recognizing and leveraging temporal patterns in
> Transformer architectures to improve model
> adaptability and forecasting accuracy.
> Research Direction 3: Cross -Series Interaction
> Enhancement through Simplified Iterative
> Techniques
> Objective: Explore simplified iterative techniques to
> enhance cross -series interaction mechanisms within
> Transformer architectures for improved
> transparency and performance.
> Initial Research Objective
> Evolve Evolve
> Research Objective (Evolve 1)
> Research Objective (Evolve 2)

â€¦

> Evolve

Figure 13 | The evolution process of research objectives on the AutoTSF task. Our system is designed to operate continuously over extended periods, and the Structured Cognitive Memory subsystem is a core component that enables sustained improvement across diverse scientific discovery tasks. To isolate how each module contributes to long-horizon capability, we evaluate Task-Episodic Memory (TEM), Semantic-Knowledge Memory (SKM), and Strategy-Procedural Memory (SPM) using modalities aligned with their functional roles. 

Task-Episodic Memory. We analyze TEM through performance trajectories over iterative research steps. With TEM active, curves rise smoothly, indicating stable short-horizon adaptation. Retrieved episodes provide fine-grained evidence from earlier trials, helping the model avoid ineffective method-ological choices and refine hypotheses efficiently. Removing TEM yields irregular or stagnant progres-sion, with frequent revisiting of unproductive strategies due to missing within-session outcomes. This contrast shows episodic grounding is critical for robust, sample-efficient adaptation during sustained operation. Fig.12 further presents long-horizon optimization experiments on scientific-discovery tasks, illustrating how TEM supports stable and persistent iterative improvement. 

27 Table 11 | Ablation study on the strategy-procedural memory on the GAIA benchmark. We report accuracy and the average of tool calls to evaluate both performance and efficiency. GAIA Accuracy (%) â†‘ Avg. # Tool Calls â†“                        

> Agent Level 1 Level 2 Level 3 Avg. Level 1 Level 2 Level 3 Avg. InternAgent 1.5 w/o SPM 92.45 84.88 53.85 82.42 12.06 23.51 55.65 22.69 InternAgent 1.5 92.45 89.53 61.54 86.06 9.13 21.22 37.33 18.52

Semantic-Knowledge Memory. To assess SKM, we use prompt-based case studies where the system proposes new research directions after multiple exploration batches. With SKM enabled, objectives reflect accumulated understanding of successful and unsuccessful methodological patterns. Retrieved long-term knowledge helps avoid saturated conceptual regions while preserving semantic continuity and measurable novelty. Fig. 13 illustrates iterative evolution of research objectives from an initial seed, showing that SKM maintains cross-batch coherence while allowing strategic redundancy to deepen promising sub-domains, balancing exploration breadth with exploitation depth. 

Strategy-Procedural Memory. As shown in Table 11, we evaluate SPM on benchmark tasks requiring multi-step reasoning and coordinated planning. The full system achieves higher success rates and more coherent plans than the SPM-ablated baseline. SPM provides procedural priors that improve planning structure and execution-level tool selection, reducing unnecessary branching and redundant calls. Without SPM, plans become longer and fragmented, with error propagation and imprecise tool-call parameters. Overall, SPM supports transferable procedural structure and improves planning efficiency and execution rigor. Overall, TEM, SKM, and SPM provide complementary support across short-term adaptation, long-term knowledge accumulation, and efficient reasoning execution for sustained improvement. 

## 4. Related Work 

4.1. Agentic AI for Scientific Discovery 

Recent progress in agentic AI has produced systems capable of carrying out increasingly autonomous forms of scientific reasoning. The AI Scientist [1] line of work demonstrates early examples of end to end research automation, with the initial system coordinating hypothesis generation and experiment design, and the later version [2] replacing fixed templates with a search based procedure that allows broader exploration of methodological space. AlphaEvolve [3]approaches scientific discovery from an evolutionary perspective by using language models to generate candidate algorithms and iteratively refine them through performance guided optimization. Other recent systems emphasize multi agent coordination within real scientific workflows. AI Co-Scientist [4] distributes literature analysis, hypothesis refinement, and methodological planning across specialized agents directed by a central model, while Robin integrates planning, data analysis, and validation into a closed loop system capable of discovering new compound candidates without manual intervention. Kosmos [6] further advances this direction by unifying literature retrieval, experiment design, and theory development into a continuously running discovery engine. Overall, these efforts illustrate the rapid emergence of autonomous scientific discovery systems and highlight the importance of long horizon reasoning, iterative experimentation, and persistent state management. These themes directly motivate the structured memory mechanisms developed in our work. 

> 28

4.2. Deep Research Agents 

Recent advances in Deep Research (DR) agents extend LLMs from retrieval-augmented generation to dynamic, tool-driven research workflows. Early systems such as WebGPT [87] and Toolformer [88] explored web and API integration, demonstrating how models can reason over retrieved information while selectively invoking external tools. Building on these ideas, industrial solutions e.g. , OpenAI DR [72], Gemini DR [75], Grok DR [89], and Perplexity DR [90], incorporate adaptive planning, iterative retrieval, and multimodal reasoning to support long-horizon research tasks. Recently, single-agent designs ( e.g. , Search-o1 [91], WebDancer [68], Tongyi DeepResearcher [92], MiroThinker [70]) enable end-to-end optimization within a unified reasoning loop, while multi-agent architectures ( e.g. ,AI Scientist [1], Agent Laboratory [93], and InternAgent [7]) offer greater modularity and scalability, which are particularly beneficial for complex research settings. Recent studies, e.g. , GeAR [94] and PANGU DeepDiver [95], further demonstrate the value of explicit structures and self-evolving mechanisms for multi-hop reasoning. 

4.3. Memory Mechanism 

Agent memory has become a central component of modern agent systems, enabling long -horizon reasoning, continual adaptation, and interaction with complex environments [96]. Recent advances cover token -level mechanisms [97] that extend contextual retention, parametric approaches that internalize accumulated experience into model parameters, and latent -memory systems [98] that store structured trajectories to guide future decisions. In parallel, short -term interaction memory has been explored in conversational and agent -simulation settings, where systems maintain ephemeral contextual traces to support local reasoning over brief episodes [99]. Long -term episodic memory has also been investigated through architectures that accumulate environment interactions across extended horizons and retrieve them for subsequent decisions [48], providing persistent records of agent experience. These techniques enhance an agentâ€™s mechanisms for incorporating prior information, although they are typically designed for interaction settings with limited temporal scope and therefore remain orthogonal to the multi-stage workflows considered in scientific discovery. 

## 5. Conclusion 

In this work, we presented InternAgent-1.5, a unified system for end -to -end scientific discovery. The framework integrates generation, verification, and evolution into a coherent architecture supported by foundational capabilities for deep research, solution refinement, and long horizon memory. This design enables consistent information flow across stages and provides a general substrate for cross -disciplinary scientific workflows. Comprehensive evaluations demonstrate that InternAgent-1.5 achieves exhibits strong performance in structured scientific reasoning. The system autonomously produces competitive algorithmic solutions, optimizes experimental proposals over extended trajectories, and executes multi -step computational and empirical workflows. Across algorithmic and empirical domains, InternAgent-1.5 consistently generates outputs that align with established scientific principles and reproduce findings observed in real scientific studies. Future work includes strengthening the coupling between computational reasoning and experimental validation, and accelerating the transition from generated hypotheses to verifiable results. Advancing these directions will further improve the efficiency and reliability of cross-disciplinary scientific discovery. 

> 29

## References 

[1] Chris Lu et al. â€œThe ai scientist: Towards fully automated open-ended scientific discoveryâ€. In: 

arXiv preprint arXiv:2408.06292 (2024). [2] Yutaro Yamada et al. â€œThe ai scientist-v2: Workshop-level automated scientific discovery via agentic tree searchâ€. In: arXiv preprint arXiv:2504.08066 (2025). [3] Alexander Novikov et al. â€œAlphaEvolve: A coding agent for scientific and algorithmic discoveryâ€. In: arXiv preprint arXiv:2506.13131 (2025). [4] Juraj Gottweis et al. â€œTowards an AI co-scientistâ€. In: arXiv preprint arXiv:2502.18864 (2025). [5] Ali Essam Ghareeb et al. â€œRobin: A multi-agent system for automating scientific discoveryâ€. In: 

arXiv preprint arXiv:2505.13400 (2025). [6] Ludovico Mitchener et al. â€œKosmos: An AI Scientist for Autonomous Discoveryâ€. In: arXiv preprint arXiv:2511.02824 (2025). [7] NovelSeek Team et al. â€œNovelSeek: When Agent Becomes the Scientistâ€“Building Closed-Loop System from Hypothesis to Verificationâ€. In: arXiv preprint arXiv:2505.16938 (2025). [8] Hanchen Wang et al. â€œScientific discovery in the age of artificial intelligenceâ€. In: Nature 

620.7972 (2023), pp. 47â€“60. [9] Richard Van Noorden and Jeffrey M Perkel. â€œAI and science: what 1,600 researchers thinkâ€. In: 

Nature 621.7980 (2023), pp. 672â€“675. [10] Josh Abramson et al. â€œAccurate structure prediction of biomolecular interactions with AlphaFold 3â€. In: Nature 630.8016 (2024), pp. 493â€“500. [11] John Jumper et al. â€œHighly accurate protein structure prediction with AlphaFoldâ€. In: nature 

596.7873 (2021), pp. 583â€“589. [12] Mihaly Varadi et al. â€œAlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy modelsâ€. In: Nucleic acids research 

50.D1 (2022), pp. D439â€“D444. [13] Andres M. Bran et al. â€œAugmenting large language models with chemistry toolsâ€. In: Nature Machine Intelligence 6.5 (2024), pp. 525â€“535. [14] Zijie Guo et al. â€œEarthLink: A Self-Evolving AI Agent for Climate Scienceâ€. In: arXiv preprint arXiv:2507.17311 (2025). [15] Bernard J Jansen, Soon-gyo Jung, and Joni Salminen. â€œEmploying large language models in survey researchâ€. In: Natural Language Processing Journal 4 (2023), p. 100020. [16] Xiangchao Yan et al. â€œSurveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writingâ€. In: Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 2025, pp. 12444â€“12465. [17] Biqing Qi et al. â€œLarge language models are zero shot hypothesis proposersâ€. In: arXiv preprint arXiv:2311.05965 (2023). [18] Biqing Qi et al. â€œLarge language models as biomedical hypothesis generators: a comprehensive evaluationâ€. In: arXiv preprint arXiv:2407.08940 (2024). [19] Shangheng Du et al. â€œAutoMLGen: Navigating Fine-Grained Optimization for Coding Agentsâ€. In: arXiv preprint arXiv:2510.08511 (2025). [20] Yusong Hu et al. â€œFlowSearch: Advancing deep research with dynamic structured knowledge flowâ€. In: arXiv preprint arXiv:2510.08521 (2025). 

> 30

[21] Jiakang Yuan et al. â€œDolphin: moving towards closed-loop auto-research through thinking, practice, and feedbackâ€. In: Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 2025, pp. 21768â€“21789. [22] Adib Bazgir, Yuwen Zhang, et al. â€œAgentichypothesis: A survey on hypothesis generation using llm systemsâ€. In: Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification, and Validation (2025). [23] Mario Bunge. Scientific Research. 2 Volumes . 1967. [24] Tony Hey, Stewart Tansley, Kristin Michele Tolle, et al. The fourth paradigm: data-intensive scientific discovery . Vol. 1. Microsoft research Redmond, WA, 2009. [25] GrÃ©goire Mialon et al. â€œGaia: a benchmark for general ai assistantsâ€. In: International Conference on Learning Representations (ICLR) . 2023. [26] David Rein et al. â€œGpqa: A graduate-level google-proof q&a benchmarkâ€. In: First Conference on Language Modeling . 2024. [27] Long Phan et al. â€œHumanityâ€™s last examâ€. In: arXiv preprint arXiv:2501.14249 (2025). [28] OpenAI. FrontierScience: Evaluating AIâ€™s Ability To Perform Expert-level Scientific Tasks . https: //openai.com/index/frontierscience/ . 2026. [29] Wanghan Xu et al. â€œProbing Scientific General Intelligence of LLMs with Scientist-Aligned Workflowsâ€. In: arXiv preprint arXiv:2512.16969 (2025). [30] Ines Chami et al. â€œLow-Dimensional Hyperbolic Knowledge Graph Embeddingsâ€. In: Annual Meeting of the Association for Computational Linguistics . 2020, pp. 6901â€“6914. [31] Zongsheng Cao et al. â€œDiffusionE: Reasoning on Knowledge Graphs via Diffusion-based Graph Neural Networksâ€. In: ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 2024, pp. 222â€“230. [32] Yankai Jiang et al. â€œSCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agentsâ€. In: arXiv preprint arXiv:2512.24189 (2025). [33] Paul Gauthier and Aider-AI Contributors. Aider: AI pair programming in your terminal . https: //github.com/Aider-AI/aider . Accessed: 2025-05-07. 2023. url : https://github. com/Aider-AI/aider .[34] Yixin Ou et al. â€œAutoMind: Adaptive Knowledgeable Agent for Automated Data Scienceâ€. In: 

arXiv preprint arXiv:2506.10974 (2025). [35] Damith Perera et al. â€œA platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flowâ€. In: Science 359.6374 (2018), pp. 429â€“434. [36] Abhimanyu Dubey et al. â€œThe llama 3 herd of modelsâ€. In: arXiv e-prints (2024), arXivâ€“2407. [37] Thomas M Norman et al. â€œExploring genetic interaction manifolds constructed from rich single-cell phenotypesâ€. In: Science 365.6455 (2019), pp. 786â€“793. [38] Yusuf Roohani, Kexin Huang, and Jure Leskovec. â€œPredicting transcriptional outcomes of novel multigene perturbations with GEARSâ€. In: Nature Biotechnology 42.6 (2024), pp. 927â€“935. [39] Ray Daniel Zimmerman, Carlos Edmundo Murillo-SÃ¡nchez, and Robert John Thomas. â€œMAT-POWER: Steady-state operations, planning, and analysis tools for power systems research and educationâ€. In: IEEE Transactions on power systems 26.1 (2010), pp. 12â€“19. [40] Zhen Zhao et al. â€œSenseFlow: A Physics-Informed and Self-Ensembling Iterative Framework for Power Flow Estimationâ€. In: arXiv preprint arXiv:2505.12302 (2025). 

> 31

[41] Haoyi Zhou et al. â€œInformer: Beyond Efficient Transformer for Long Sequence Time-Series Forecastingâ€. In: The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference . Vol. 35. 12. AAAI Press, 2021, pp. 11106â€“11115. [42] Ailing Zeng et al. â€œAre transformers effective for time series forecasting?â€ In: Proceedings of the AAAI conference on artificial intelligence . Vol. 37. 9. 2023, pp. 11121â€“11128. [43] Stefan Chmiela et al. â€œMachine learning of accurate energy-conserving molecular force fieldsâ€. In: Science advances 3.5 (2017), e1603015. [44] Yusong Wang et al. â€œEnhancing geometric representations for molecules with equivariant vector-scalar interactive message passingâ€. In: Nature Communications 15.1 (2024), p. 313. [45] Cosmas D Arnold et al. â€œGenome-wide quantitative enhancer activity maps identified by STARR-seqâ€. In: Science 339.6123 (2013), pp. 1074â€“1077. [46] Bernardo P de Almeida et al. â€œDeepSTARR predicts enhancer activity from DNA sequence and enables the de novo design of synthetic enhancersâ€. In: Nature genetics 54.5 (2022), pp. 613â€“ 624. [47] Fengwei Teng et al. â€œAtom of Thoughts for Markov LLM Test-Time Scalingâ€. In: The Thirty-ninth Annual Conference on Neural Information Processing Systems . 2025. [48] Wujiang Xu et al. â€œA-Mem: Agentic Memory for LLM Agentsâ€. In: The Thirty-ninth Annual Conference on Neural Information Processing Systems. 2025. url : https://openreview. net/forum?id=FiM0M8gcct .[49] Zheng Cai et al. InternLM2 Technical Report . 2024. arXiv: 2403.17297 [cs.CL] .[50] Jia Li et al. â€œNuminamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutionsâ€. In: Hugging Face repository 13.9 (2024), p. 9. [51] Yuxin Zuo et al. â€œTTRL: Test-Time Reinforcement Learningâ€. In: The Thirty-ninth Annual Con-ference on Neural Information Processing Systems. 2025. url : https://openreview.net/ forum ? id = VuVhgEiu20 & referrer = %5Bthe % 20profile % 20of % 20Bowen % 20Zhou % 5D(%2Fprofile%3Fid%3D~Bowen_Zhou8) .[52] Veronika Eyring et al. â€œOverview of the Coupled Model Intercomparison Project Phase 6 (CMIP6) experimental design and organizationâ€. In: Geoscientific Model Development 9.5 (2016), pp. 1937â€“1958. [53] Hans Hersbach et al. â€œERA5 monthly averaged data on single levels from 1979 to presentâ€. In: 

Copernicus Climate Change Service (C3S) Climate Data Store (CDS) 10 (2019), pp. 252â€“266. [54] Eugenia Kalnay et al. â€œThe NCEP/NCAR 40-year reanalysis projectâ€. In: Renewable energy .Routledge, 2018, Vol1_146â€“Vol1_194. [55] Tomislav Hengl, Gerard BM Heuvelink, and David G Rossiter. â€œAbout regression-kriging: From equations to case studiesâ€. In: Computers & geosciences 33.10 (2007), pp. 1301â€“1315. [56] Andrew W Wood et al. â€œLong-range experimental hydrologic forecasting for the eastern United Statesâ€. In: Journal of Geophysical Research: Atmospheres 107.D20 (2002), ACLâ€“6. [57] Katarzyna Tomczak, Patrycja CzerwiÅ„ska, and Maciej Wiznerowicz. â€œReview The Cancer Genome Atlas (TCGA): an immeasurable source of knowledgeâ€. In: Contemporary Oncolo-gy/WspÃ³Å‚czesna Onkologia 2015.1 (2015), pp. 68â€“77. [58] Denise Carvalho-Silva et al. â€œOpen Targets Platform: new developments and updates two years onâ€. In: Nucleic acids research 47.D1 (2019), pp. D1056â€“D1065. [59] Minoru Kanehisa. â€œThe KEGG databaseâ€. In: â€˜In silicoâ€™simulation of biological processes: Novartis Foundation Symposium 247 . Vol. 247. Wiley Online Library. 2002, pp. 91â€“103. 

> 32

[60] Alexander Rives et al. â€œBiological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequencesâ€. In: PNAS (2019). doi : 10.1101/622803 . url :

https://www.biorxiv.org/content/10.1101/622803v4 .[61] Mingchen Li et al. â€œProSST: Protein Language Modeling with Quantized Structure and Disen-tangled Attentionâ€. In: The Thirty-eighth Annual Conference on Neural Information Processing Systems . 2024. [62] Hao Li et al. â€œBeyond Chemical QA: Evaluating LLMâ€™s Chemical Reasoning with Modular Chemical Operationsâ€. In: arXiv preprint arXiv:2505.21318 (2025). [63] Gemini 3 Pro - Google DeepMind . url : https://deepmind.google/models/gemini/ pro/ .[64] OpenAI. ChatGPT (GPT-5) . 2025. url : https://chat.openai.com/ .[65] Introducing Claude Sonnet 4.5 . en. url : https://www.anthropic.com/news/claude-sonnet-4-5 .[66] An Yang et al. â€œQwen3 technical reportâ€. In: arXiv preprint arXiv:2505.09388 (2025). [67] Introducing OpenAI o3 and o4-mini . en-US. url : https : / / openai . com / index / introducing-o3-and-o4-mini/ .[68] Jialong Wu et al. â€œWebDancer: Towards Autonomous Information Seeking Agencyâ€. In: arXiv preprint arXiv:2505.22648 (2025). [69] Zhengwei Tao et al. â€œWebshaper: Agentically data synthesizing via information-seeking formal-izationâ€. In: arXiv preprint arXiv:2507.15061 (2025). [70] MiroMind Team et al. â€œMiroThinker: Pushing the Performance Boundaries of Open-Source Re-search Agents via Model, Context, and Interactive Scalingâ€. In: arXiv preprint arXiv:2511.11793 

(2025). [71] Tongyi DeepResearch Team et al. â€œTongyi DeepResearch Technical Reportâ€. In: arXiv preprint arXiv:2510.24701 (2025). [72] OpenAI. Deep Research System Card . https : / / cdn . openai . com / deep - research -system-card.pdf . 2025. [73] Moonshot AI. Kimi-Researcher: End-to-End Reinforcement Learning for Emerging Agentic Capa-bilities . https://moonshotai.github.io/Kimi-Researcher/ . Accessed: 2025-XX-XX. 2025. [74] Manus . https://manus.im/ . 2025. [75] Google. Introducing Gemini Deep Research . https://blog.google/products/gemini/ google-gemini-deep-research/ . 2024. [76] Mengkang Hu et al. OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation . 2025. arXiv: 2505.23885 [cs.AI] .[77] Daya Guo et al. â€œDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learningâ€. In: arXiv preprint arXiv:2501.12948 (2025). [78] Lei Bai et al. â€œIntern-s1: A scientific multimodal foundation modelâ€. In: arXiv preprint arXiv:2508.15763 (2025). [79] Aixin Liu et al. â€œDeepseek-v3. 2: Pushing the frontier of open large language modelsâ€. In: arXiv preprint arXiv:2512.02556 (2025). [80] MiroMind AI Team. MiroFlow: A High-Performance Open-Source Research Agent Framework .

https://github.com/MiroMindAI/MiroFlow . 2025. 

> 33

[81] Qihao Zhao et al. â€œMmlu-cf: A contamination-free multi-task language understanding bench-markâ€. In: Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 2025, pp. 13371â€“13391. [82] Adyasha Maharana et al. â€œEvaluating very long-term conversational memory of llm agentsâ€. In: 

arXiv preprint arXiv:2402.17753 (2024). [83] Zhongyue Zhang et al. â€œOriGene: A Self-Evolving Virtual Disease Biologist Automating Thera-peutic Target Discoveryâ€. In: bioRxiv (2025). [84] David Weininger. â€œSMILES, a chemical language and information system. 1. Introduction to methodology and encoding rulesâ€. In: Journal of chemical information and computer sciences 

28.1 (1988), pp. 31â€“36. [85] Greg Landrum. â€œRdkit documentationâ€. In: Release 1.1-79 (2013), p. 4. [86] Ondrej Kovar et al. â€œScaffold Hopping in Tuberculosis Drug Discovery: Principles, Applications, and Case Studiesâ€. In: Journal of Medicinal Chemistry 68.20 (2025), pp. 20903â€“20929. [87] Reiichiro Nakano et al. â€œWebgpt: Browser-assisted question-answering with human feedbackâ€. In: arXiv preprint arXiv:2112.09332 (2021). [88] Timo Schick et al. â€œToolformer: Language models can teach themselves to use toolsâ€. In: 

Advances in Neural Information Processing Systems (NeurIPS) (2023). [89] xAI. Grok-3 DeepSearch: Synthesizing Key Information to Distill Clarity from Complexity . https: //x.ai/news/grok-3 . 2025. [90] Perplexity. Perplexity Deep Research . https://www.perplexity.ai/ . 2025. [91] Xiaoxi Li et al. â€œSearch-o1: Agentic search-enhanced large reasoning modelsâ€. In: arXiv preprint arXiv:2501.05366 (2025). [92] Zile Qiao et al. â€œWebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agentsâ€. In: arXiv preprint arXiv:2509.13309 (2025). [93] Samuel Schmidgall et al. â€œAgent laboratory: Using llm agents as research assistantsâ€. In: arXiv preprint arXiv:2501.04227 (2025). [94] Zhili Shen et al. â€œGeAR: Graph-enhanced Agent for Retrieval-augmented Generationâ€. In: arXiv preprint arXiv:2412.18431 (2024). [95] Wenxuan Shi et al. â€œPangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Rein-forcement Learningâ€. In: arXiv preprint arXiv:2505.24332 (2025). [96] Yuyang Hu et al. â€œMemory in the Age of AI Agentsâ€. In: arXiv preprint arXiv:2512.13564 

(2025). [97] Yuhuai Wu et al. â€œMemorizing Transformersâ€. In: International Conference on Learning Repre-sentations .[98] Weizhi Wang et al. â€œAugmenting language models with long-term memoryâ€. In: Advances in Neural Information Processing Systems 36 (2023), pp. 74530â€“74543. [99] Joon Sung Park et al. â€œGenerative agents: Interactive simulacra of human behaviorâ€. In: 

Proceedings of the 36th annual acm symposium on user interface software and technology . 2023, pp. 1â€“22. 

> 34

## A. Appendix 

A.1. Contributions and Acknowledgments 

Lead Authors 

Shiyang Feng 1, Runmin Ma 1, Xiangchao Yan 1

Core Authors 

Yue Fan 1, Yusong Hu 1,6, Songtao Huang 1,2, Shuaiyu Zhang 1,2, Zongsheng Cao 1, Tianshuo Peng 1,4,Jiakang Yuan 1,2, Zijie Guo 1,2, Zhijie Zhong 1, Shangheng Du 1,5, Weida Wang 1,2, Jinxin Shi 1,5, Yuhao Zhou 1

Contributors 

Xiaohan He, Zhiyin Yu, Fangchen Yu, Bihao Zhan, Qihao Zheng, Jiamin Wu, Mianxin Liu, Chi Zhang, Shaowei Hou, Shuya Li, Yankai Jiang, Wenjie Lou, Lilong Wang, Zifu Wang, Jiong Wang, Wanghan Xu, Yue Deng, Dongrui Liu, Yiheng Wang 

Scientific Directors 

Wenlong Zhang 1, Fenghua Ling 1, Shufei Zhang 1, Xiaosong Wang 1, Shuangjia Zheng 3, Xun Huang 3,Siqi Sun 1,2, Shuyue Hu 1, Peng Ye 1,4, Chunfeng Song 1, Bin Wang 1, Conghui He 1, Yihao Liu 1, Xin Li 1,Qibin Hou 6, Tao Chen 2, Xiangyu Yue 1,4, Bin Wang 2, Liang He 1,5, Dahua Lin 1, Bowen Zhou 1

Corresponding Authors 

Bo Zhang 1, zhangbo@pjlab.org.cn Lei Bai 1, bailei@pjlab.org.cn 

Main Affiliations  

> 1

Shanghai Artificial Intelligence Laboratory  

> 2

Fudan University  

> 3

Lingang Laboratory  

> 4

The Chinese University of Hong Kong  

> 5

East China Normal University  

> 6

Nankai University 

> 35

A.2. Earth Science example 

Report Generated All by InternAgent-1.5 The Future of the Atlantic Meridional Overturning 

## Circulation (AMOC): A Review of Mainstream 

## Consensus, Divergent Projections, and the Risk of a 

## 21st-Century Collapse 

Introduction 

The Atlantic Meridional Overturning Circulation (AMOC) is a large-scale system of ocean 

currents vital to the global climate system. Characterized by a northward flow of warm, 

salty surface water and a southward return of cold, deep water, the AMOC acts as a 

planetary-scale conveyor, transporting an immense quantity of heat â€•estimated at 1.25 

petawatts â€•toward the high latitudes. This process is a primary reason for the relatively 

mild climates of the Northern Hemisphere, particularly in Western Europe. Beyond its 

role in heat redistribution, the AMOC is a crucial pump in global biogeochemical cycles, 

facilitating the sequestration of atmospheric carbon into the deep ocean and distributing 

nutrients essential for marine ecosystems. 

Given its critical role, the future stability of the AMOC in the face of anthropogenic climate 

change has become a subject of intense scientific debate and pressing concern. A 

significant slowdown or collapse would trigger a cascade of severe global consequences, 

including dramatic cooling across the Northern Hemisphere, a southward shift of the 

Intertropical Convergence Zone (ITCZ) that would alter tropical rainfall patterns, and 

accelerated sea-level rise along the U.S. East Coast [1][2][3]. This report addresses the 

central question of whether the AMOC will cross a critical tipping point in the 21st 

century. It examines a scientific landscape characterized by a deep-seated conflict: the 

mainstream consensus, as articulated by the Intergovernmental Panel on Climate 

Change (IPCC), which projects a gradual weakening, is increasingly challenged by a body 

of research suggesting the potential for a more abrupt and imminent collapse. 

This divergence in projections arises from fundamental uncertainties inherent in the 

methods used to study the AMOC. The conflict is driven by differing interpretations of 

evidence from two primary sources: statistical analyses of paleoclimate proxy records, 

36 which suggest early-warning signals may already be present, and simulations from 

complex physical climate models, which are known to have biases that may artificially 

enhance AMOC stability. This report synthesizes the current literature to explore this 

debate in detail. It will first define the AMOC and its significance, then review the 

mainstream consensus before investigating the divergent evidence for a potential tipping 

point. Subsequently, it will analyze the sources of these conflicting predictions, detail the 

potential consequences of an AMOC disruption, and conclude by summarizing the state 

of the science and outlining future research priorities. 

Understanding the AMOC: Definition, Drivers, and Global 

Significance 

The Atlantic Meridional Overturning Circulation (AMOC) is a large-scale system of ocean 

currents and a primary component of the global thermohaline circulation, often 

conceptualized as the   global conveyor belt.   This section provides a foundational 

overview of the AMOC, detailing the physical forces that drive its circulation and its 

multifaceted significance for the global climate and biogeochemical systems. 

Definition and Physical Drivers 

At its core, the AMOC is characterized by a northward flow of warm, salty surface and 

intermediate waters in its upper limb, which is balanced by a southward flow of cold, 

dense water in its deep lower limb. This immense circulatory system is fundamentally 

powered by density differences that arise from spatial gradients in seawater temperature 

(thermo) and salinity (haline). While often simplified as a single conveyor, the AMOC is a 

dynamically complex system sustained by an intricate interplay between thermodynamic 

processes and mechanical wind forcing. A basin-scale schematic illustrating these limbs, 

the main deep-water formation sites, and the roles of thermohaline and wind/tidal 

forcing is shown below. 37 Figure 1. AMOC schematic showing pathways, sinking sites, and thermohaline and 

wind forcing. 

The primary engine of the circulation is  thermohaline forcing . As warm, highly saline 

surface waters from the tropics and subtropics are transported northward into the high-

latitude North Atlantic, they encounter cold, dry polar air in the Nordic, Greenland, 

Labrador, and Irminger Seas. This leads to intense atmospheric cooling, with a net heat 

loss from the ocean that can exceed 200 W/mÂ² in winter. This dramatic cooling 

significantly increases the water  s density. The North Atlantic is inherently saltier than 

the North Pacific â€•a result of net atmospheric transport of water vapor from the Atlantic 

to the Pacific basin â€•which pre-conditions these surface waters, making them more 

susceptible to sinking upon cooling. The combination of high salinity and extreme 

cooling makes the surface water gravitationally unstable, causing it to sink to great 

depths and form North Atlantic Deep Water (NADW). This process of deep-water 

formation initiates the cold, deep lower limb of the AMOC, which then flows southward, 

perpetuating the overturning motion. 

This density-driven sinking is modulated by  surface buoyancy fluxes , which encompass 

the exchange of both heat and freshwater between the ocean and atmosphere. While 

heat loss is the main driver of densification, freshwater inputs from precipitation, river 

runoff (e.g., from Greenland), and the melting of sea ice and continental ice sheets can 

act as a countervailing force. An anomalous influx of freshwater can create a buoyant, 38 low-salinity lens at the surface, which stratifies the upper water column and can inhibit, 

or   cap,   the deep convection necessary for NADW formation. 

Deep-water formation does not occur uniformly across the basin but is concentrated in 

specific, geographically localized regions. The primary sites include: 

The Nordic Seas (Greenland and Norwegian Seas):  Here, densification is intensified 

by the formation of sea ice, which rejects concentrated brine into the surrounding 

water, further increasing its salinity and density. This newly formed dense water flows 

southward over key submarine sills, such as the Denmark Strait and the Faroe Bank 

Channel, cascading into the North Atlantic basin as dense overflow plumes that 

entrain surrounding waters and form the deepest components of NADW. 

The Subpolar Gyre (Labrador and Irminger Seas):  In this region, a different mode of 

formation, known as open-ocean convection, occurs. During severe winters, intense 

cooling and strong winds can erode the seasonal thermocline, mixing the water 

column to depths of 1,500 â€’2,000 meters. This process forms a slightly less dense 

variety of NADW. The combined output from these distinct formation sites merges to 

supply the deep, cold, southward-flowing limb of the AMOC. 

While thermohaline processes provide the sinking mechanism,  mechanical forcing from 

wind stress  is equally critical for sustaining the full AMOC circuit. Wind forcing 

contributes in two fundamental ways. First, it drives the large-scale horizontal gyre 

systems, including the powerful Gulf Stream and its eastward extension, the North 

Atlantic Current. These currents are not separate from the AMOC but are integral to it, 

efficiently transporting the vast quantities of warm, salty water that fuel the sinking 

processes in the north. Without this wind-driven transport, the thermohaline engine 

would be starved of its source waters. Second, the global overturning cell must be closed 

by the upwelling of deep water back to the surface, and a significant portion of the 

energy for this vertical movement is supplied by wind-driven mixing, particularly in the 

Southern Ocean, with smaller contributions from tidal mixing over rough topography. 

The AMOC must therefore be understood as a hybrid system, where density-driven 

sinking in the North Atlantic is critically enabled and balanced by wind-driven horizontal 

transport and global-scale vertical mixing. 

Global Climatic and Biogeochemical Significance 

The profound global significance of the AMOC stems from its deeply interconnected 

functions as a planetary-scale heat transporter, a regulator of biogeochemical cycles, and 

a driver of regional climate. 

1. 2. 39 Its most prominent function is the  redistribution of heat . By transporting warm surface 

waters northward, the AMOC  s upper limb releases an immense quantity of heat â€•

approximately 1.25 petawatts â€•to the atmosphere. This heat flux is a primary reason for 

the relatively mild climates in the Northern Hemisphere, particularly in Western Europe. 

A substantial weakening or collapse of this heat transport would trigger significant 

atmospheric cooling across the North Atlantic, fundamentally reshaping continental 

weather patterns and altering storm tracks. 

Beyond its thermodynamic role, the AMOC is a crucial pump in the  global carbon and 

nutrient cycles . It facilitates the sequestration of atmospheric carbon dioxide (CO â‚‚) into 

the deep ocean via both physical and biological mechanisms. The cooling of surface 

waters at high latitudes increases their capacity to dissolve CO â‚‚, enhancing the ocean  s

uptake of atmospheric carbon. When these carbon-rich waters sink to form NADW, the 

carbon is effectively isolated from the atmosphere for centuries to millennia. This 

mechanism is vital for mitigating the rate of atmospheric CO â‚‚ accumulation and the 

associated pace of global warming. Concurrently, the overturning circulation is essential 

for distributing nutrients. The deep, southward-flowing current becomes enriched with 

nutrients like nitrate and phosphate from the remineralization of sinking organic matter. 

When this nutrient-laden deep water eventually upwells in other ocean basins, such as 

the Southern Ocean and equatorial regions, it fertilizes the sunlit surface waters, fueling 

primary productivity and supporting the base of marine food webs on a global scale. A 

slowdown of the AMOC would thus diminish the ocean  s capacity to act as a carbon sink 

and disrupt marine ecosystems far from the Atlantic. 

Finally, the AMOC directly influences  regional climate phenomena and coastal sea 

levels  through complex dynamic adjustments and teleconnections. Variations in AMOC 

strength are strongly correlated with the latitudinal position of the Intertropical 

Convergence Zone (ITCZ), a critical band of tropical rainfall. A weaker AMOC is associated 

with a southward shift of the ITCZ, a phenomenon linked to severe and persistent 

droughts in the African Sahel and alterations to monsoon systems in South America and 

South Asia, thereby impacting agriculture and water security for billions. Furthermore, 

the circulation governs Atlantic sea level dynamics. The strong, northward-flowing Gulf 

Stream, as part of the AMOC  s upper limb, maintains a cross-basin pressure gradient that 

results in a lower sea level along the North American coast compared to the European 

side. A weakening of this current would cause the gradient to relax, leading to an 

anomalous and accelerated rate of sea-level rise along the eastern seaboard of the 

United States, independent of global mean sea-level rise from melting ice. The state of 

the AMOC also modulates Atlantic hurricane activity by influencing sea surface 40 temperatures and vertical wind shear, further highlighting its multifaceted impact on 

climatic hazards. 

The Mainstream Consensus: Historical Variability and 

Projected 21st-Century Weakening 

The mainstream scientific consensus, largely articulated in assessments by the 

Intergovernmental Panel on Climate Change (IPCC), draws a sharp distinction between 

the AMOC  s volatile history and its projected 21st-century trajectory. Paleoclimate 

evidence reveals a system capable of dramatic and abrupt shifts, particularly during 

glacial periods, which later settled into a state of relative stability during the Holocene. In 

stark contrast, analyses of the industrial era and projections for the coming century point 

toward an anomalous and sustained weakening directly attributed to anthropogenic 

climate change. While a gradual, progressive decline is a high-confidence projection, the 

potential for an abrupt collapse before 2100 remains a low-probability, high-impact risk 

clouded by acknowledged uncertainties in climate modeling. 

A Long-Term Perspective: From Glacial Volatility to Holocene 

Stability 

Paleoclimatological reconstructions provide an essential long-term context for 

understanding AMOC behavior, revealing a system that was far more volatile during the 

Late Pleistocene (c. 126,000 to 11,700 years ago) than in the current interglacial epoch. 

The last glacial period was marked by large-scale, rapid climate oscillations known as 

Dansgaard-Oeschger (D-O) events. The consensus view establishes a direct link between 

these events and significant fluctuations in AMOC strength. These fluctuations drove a 

â€œbipolar seesaw â€ effect, wherein a strengthened AMOC would transport more heat 

northward, leading to rapid warming over the North Atlantic and Greenland (by as much 

as 8 â€’15Â°C within decades), while the Southern Ocean simultaneously cooled. Conversely, 

periods of AMOC weakening reversed this pattern. The primary triggers for these abrupt 

slowdowns are strongly associated with massive freshwater discharges into the North 

Atlantic originating from collapsing ice sheets (known as Heinrich events) and large 

meltwater pulses, such as the one that initiated the Younger Dryas cold period 

approximately 12,800 years ago. This influx of buoyant freshwater stratified the surface 

ocean, critically inhibiting the deep convection in the Nordic and Labrador Seas that is 

essential for the formation of North Atlantic Deep Water (NADW) and the overall drive of 

the overturning circulation [1]. 41 Following the end of the last glacial period, the AMOC transitioned into a comparatively 

quiescent phase during the Holocene epoch, which began roughly 11,700 years ago. High-

resolution sedimentary proxy records, such as the ratio of protactinium-231 to thorium-

230 (231Pa/230Th), support a broad consensus that AMOC variability was significantly 

subdued compared to the preceding glacial era. These reconstructions suggest that the 

AMOC recovered from a weak state during the deglaciation period and stabilized to a 

strength comparable to pre-industrial levels around 6,500 years ago. Despite this general 

stability, the circulation did not lose its sensitivity to freshwater forcing. A notable 

weakening event, for instance, is associated with the 8.2 ka climate event â€•a prominent 

cooling episode in the Northern Hemisphere linked to the final drainage of proglacial 

lakes from the Laurentide Ice Sheet. This Holocene record serves to underscore a 

fundamental property of the AMOC: even within a stable background climate, its strength 

is fundamentally modulated by changes in the surface buoyancy fluxes of the North 

Atlantic [1][4]. 

The Modern Era: Detecting Trends Amidst Natural Fluctuation 

Assessing AMOC variability during the instrumental era is fraught with challenges, 

highlighted by a divergence between short-term direct observations, longer-term proxy 

reconstructions, and climate model simulations. Continuous, direct monitoring of the 

AMOC has only been possible since 2004 with the implementation of the RAPID-MOCHA 

array at 26.5Â°N. While this invaluable record has documented substantial interannual 

variability, including a notable temporary 30% decline in 2009 â€’2010, its short duration 

makes it difficult to definitively isolate a long-term, anthropogenically forced trend from 

natural, low-frequency oscillations like the Atlantic Multidecadal Oscillation (AMO), 

which operates on a timescale of roughly 70 years [5]. 

To extend the record further into the past, scientists rely on proxy-based reconstructions 

using indicators like sea surface temperature patterns. A growing body of such studies 

concludes that the AMOC has experienced a significant weakening of 15 â€’20% since the 

mid-20th century, with some analyses suggesting it is now in its weakest state in over a 

millennium. The anomalous â€œcold blob â€ of surface water observed in the subpolar 

North Atlantic is interpreted by some researchers as indirect evidence of this reduced 

northward heat transport [1]. This conclusion, however, is an area of active scientific 

debate. Other studies have challenged the robustness of these proxy reconstructions, 

pointing to potentially confounding atmospheric factors that could alternatively explain 

the observed temperature patterns. This debate underscores the inherent uncertainties 

in reconstructing a complex, three-dimensional ocean circulation from limited data. 42 Climate models have historically added another layer of complexity; comprehensive 

General Circulation Models (GCMs) often simulated a relatively stable AMOC throughout 

the 20th century, a finding at odds with the weakening trend suggested by proxies. The 

IPCC  s Sixth Assessment Report (AR6), utilizing the latest CMIP6 models, notes that while 

these models simulate interannual variability, they have struggled to fully capture 

observed changes and may possess a bias toward excessive stability [6]. 

Projected 21st-Century Weakening and the Risk of Collapse 

Despite the complexities of historical reconstruction, the mainstream scientific 

consensus, as formally articulated by the IPCC, projects with 

very high confidence  that 

the AMOC will weaken over the course of the 21st century. This conclusion is a robust 

outcome across nearly all global climate models and under various emissions scenarios. 

The physical rationale for this slowdown is rooted in fundamental climate principles: 

anthropogenic global warming leads to intensified surface warming and an increased 

influx of freshwater into the North Atlantic. This freshwater comes from the accelerating 

melt of the Greenland Ice Sheet and Arctic sea ice, as well as from changing precipitation 

patterns. Together, these factors reduce the density of surface waters, enhance ocean 

stratification, and inhibit the deep-water formation in high-latitude seas that propels the 

entire circulation [7]. 

The multi-model ensemble mean from the Coupled Model Intercomparison Project 

Phase 6 (CMIP6) projects an AMOC decline of 34% to 45% by the year 2100 under high-

emissions scenarios (e.g., SSP5-8.5). This corresponds to a transport reduction of 

approximately 6 to 8 Sverdrups (Sv) and represents a more pronounced weakening than 

was projected in the previous generation of CMIP5 models [8]. While a considerable 

spread exists across individual models, with some projecting a more moderate 

weakening of up to 30%, the general agreement across different modeling frameworks 

reinforces the core consensus of a significant slowdown [9]. 

A critical distinction within this consensus lies between a gradual weakening and a 

potential abrupt collapse. The IPCC  s Special Report on the Ocean and Cryosphere in a 

Changing Climate (SROCC) and its Sixth Assessment Report (AR6) both assess an abrupt 

AMOC collapse before 2100 as 

very unlikely . However, the IPCC qualifies this assessment 

with only 

medium confidence , a crucial caveat that reflects acknowledged model 

limitations and the profound, catastrophic impacts such an event would entail [10][11]. 

This confidence level signifies that while a collapse is not the expected outcome this 

century, the possibility cannot be entirely ruled out. The decision to lower the confidence 43 level from 

high  (in AR5) to 

medium  (in AR6) was partly informed by growing evidence of 

the system  s modern instability [6]. This uncertainty is largely rooted in recognized biases 

within climate models, which may be inherently predisposed to excessive AMOC stability 

and thus underestimate its sensitivity to freshwater forcing â€•the very mechanism 

implicated in past abrupt changes. Specific structural biases identified in CMIP6 models 

include an underrepresentation of the deep-water transport component known as Lower 

North Atlantic Deep Water (lNADW) and coarse model resolutions that limit the accurate 

simulation of critical processes like deep-water overflows and narrow boundary currents 

[12]. This divergence between the gradual decline projected by most comprehensive 

models and the more imminent tipping points suggested by some statistical analyses 

underscores that the future of the AMOC remains a critical and active area of climate 

research [13]. As summarized in Table 1, key IPCC assessments, model projections, 

confidence levels, drivers, and primary uncertainties are organized for quick reference. 

Table 1: Summary of IPCC projections, collapse risk, confidence levels, drivers, and 

model uncertainties 44 Aspect  Summary  Quantitative 

projection / 

magnitude 

AR6 

confidence 

AR5 

confidence 

Projected 21st-

century 

weakening 

With very high 

confidence, the 

AMOC will weaken 

over the course of 

the 21st century; a 

gradual, progressive 

decline is a robust 

outcome across 

nearly all global 

climate models and 

emissions scenarios. 

CMIP6 multi-model 

ensemble mean: ~34 â€’

45% decline by 2100 

under high-emissions 

scenarios (SSP5-8.5), 

corresponding to ~6 â€’8

Sv transport reduction; 

some models project a 

more moderate 

weakening (~30%). 

Very high 

confidence 

Not 

specified 

Abrupt 

collapse 

before 2100 

An abrupt AMOC 

collapse before 2100 

is assessed as very 

unlikely; however, 

this remains a low-

probability, high-

impact risk that 

cannot be fully ruled 

out. 

Probability: very 

unlikely (IPCC AR6); 

characterized as low-

probability, high-

impact. 

Medium 

confidence 

(that 

collapse 

before 2100 

is very 

unlikely) 

High 

confidence 

(collapse 

very 

unlikely) 

Confidence 

level 

comparison 

(AR6 vs AR5) 

AR6 assigns very high 

confidence to 

projected 

weakening, but 

lowers confidence 

that an abrupt 

collapse before 2100 

is very unlikely 

compared with AR5. 

Confidence change: 

collapse confidence 

reduced from â€œhigh â€

(AR5) to â€œmedium â€

(AR6). 

Very high 

(weakening); 

Medium 

(collapse 

very unlikely) 

High 

(collapse 

very 

unlikely) 45 Aspect  Key drivers  Primary uncertainties / 

model biases 

Notes / sources 

Projected 21st-

century 

weakening 

Anthropogenic global 

warming; intensified 

surface warming; 

increased freshwater 

input from Greenland Ice 

Sheet and Arctic sea-ice 

melt; altered 

precipitation patterns 

reducing surface-water 

density and inhibiting 

North Atlantic Deep 

Water (NADW) formation. 

Limited instrumental 

record (continuous RAPID-

MOCHA observations 

since 2004); proxy 

reconstruction 

uncertainties; climate-

model biases toward 

excessive AMOC stability; 

underrepresentation of 

Lower North Atlantic 

Deep Water (lNADW); 

coarse resolution limiting 

representation of deep-

water overflows and 

boundary currents. 

IPCC AR6; CMIP6 multi-

model results; 

references cited in text 

(e.g., [7], [8], [6]). 

Abrupt 

collapse 

before 2100 

Paleoclimate analogues: 

massive freshwater 

discharges from 

collapsing ice sheets 

(Heinrich events) and 

large meltwater pulses 

(e.g., Younger Dryas); 

contemporary drivers 

include accelerating 

Greenland melt, Arctic 

sea-ice loss, and 

increased precipitation-

driven stratification. 

Structural model 

limitations may 

underestimate collapse 

risk; excessive AMOC 

stability bias; limited 

representation of lNADW; 

coarse spatial resolution; 

short observational 

record; emerging 

evidence of modern 

AMOC instability. 

IPCC AR6 and SROCC 

assessments; 

discussion in text (e.g., 

[10], [11], [6]). 46 The Tipping Point Debate: Divergent Evidence for an 

Imminent Collapse 

While the mainstream consensus from large-scale climate modeling efforts, such as the 

Coupled Model Intercomparison Project Phase 6 (CMIP6), largely projects a gradual 

weakening of the Atlantic Meridional Overturning Circulation (AMOC) through the 21st 

century without a full collapse, a significant and growing body of research presents a 

starkly different conclusion. These studies suggest that the AMOC may be much closer to 

a critical tipping point, with an abrupt transition plausible within decades rather than 

centuries [1][14]. This divergence from the prevailing view stems from two primary, and 

increasingly intersecting, lines of investigation that challenge the stability inherent in 

many Earth System Models (ESMs). The first relies on statistical analyses of observational 

and paleoclimate data to detect early-warning signals (EWS) of an impending critical 

transition. The second leverages specialized climate models, designed to overcome 

stability biases, to identify physically-based precursors to collapse, which are then 

sought in observationally-constrained datasets. 

Statistical Early-Warning Signals in Observational and Paleoclimate 

Data 

The most compelling argument for an approaching AMOC collapse is rooted in the 

statistical analysis of time-series data, which applies the theoretical framework of 

dynamical systems nearing a critical transition. According to this theory, as a complex 

system like the AMOC loses resilience, it exhibits a phenomenon known as   critical 

slowing down   (CSD). This process, where the system  s recovery rate from minor 

perturbations diminishes, produces detectable statistical fingerprints, most notably an 

increase in variance and lag-1 autocorrelation (AR1) [10]. 

> Primary
> drivers and
> uncertainties
> (synthesis)
> Anthropogenic warming
> and freshwater forcing
> dominate long-term
> weakening; abrupt
> changes historically
> linked to extreme
> freshwater perturbations.
> Proxy â€’model
> disagreement; limited
> observations; known
> structural and resolution-
> related model biases
> affecting AMOC sensitivity
> to freshwater forcing.
> Paragraph references
> for drivers and
> uncertainties (e.g., [1],
> [4], [5], [6], [12]).

47 A prominent 2023 study by Ditlevsen applied this framework to sea surface temperature 

(SST) data from the subpolar gyre, a region whose thermal properties are dynamically 

coupled to AMOC heat transport and can serve as an effective proxy. The analysis 

uncovered a statistically significant increase in both variance and autocorrelation in the 

SST record, with the trend accelerating notably since the 1970s. By extrapolating these 

trends based on the theoretical behavior of a system approaching a saddle-node 

bifurcation, the study produced an alarming forecast: an AMOC collapse is most likely to 

occur mid-century, with a 95% confidence interval spanning 2025 â€’2095 and a central 

estimate around 2057 [10]. As illustrated in Figure 2, the collapse timing estimates and 

their associated uncertainties diverge sharply between the different methodologies of 

statistical EWS analyses, specialized model experiments, and the broader CMIP6 

ensembles. 

Figure 2. Horizontal comparison of AMOC collapse timing and probability estimates. 

This statistical evidence is corroborated by research seeking to reconcile observed 

signals with model behavior. Shin et al. (2025) identified a consistent warning signal â€•

increasing AR1 of both SST and salinity â€•in the eastern Subpolar North Atlantic (SPNA). A 

crucial finding of their work is that while ESMs can produce these signals, they only do so 

under future warming scenarios that far exceed Paris Agreement targets. The fact that 

these EWS are already present in observational records under current climate conditions 

strongly implies that ESMs may systematically overestimate AMOC stability. The study 

directly links these statistical signals to physical oceanographic changes, pointing to a 

significant freshening event in the eastern SPNA between 2014 and 2019 as empirical 48 evidence of a critical decline in stability driven by freshwater flux [15]. The validity of 

using such statistical indicators is further bolstered by paleoclimate evidence. As 

mentioned previously, the AMOC is a known tipping element responsible for past abrupt 

climate shifts, such as Dansgaard-Oeschger events [16]. Moreover, a 2022 analysis by 

Michel et al. of thousand-year-long paleoclimate reconstructions confirmed that rising 

autoregressive properties served as detectable EWS for past climatic transitions in the 

North Atlantic, lending strong theoretical and historical credence to applying these 

methods to modern observational data [17]. 

Physically-Based Precursors from Specialized Models 

A second, pivotal line of evidence bridges the gap between idealized model physics and 

real-world observations. This approach uses specialized model experiments to identify 

an optimal, physically-based EWS that is more robust than purely statistical indicators. A 

2024 simulation using the Community Earth System Model (CESM) successfully induced a 

classic AMOC collapse by applying a gradual but sustained influx of freshwater to the 

North Atlantic over 1,700 years. Although the magnitude of the forcing required was 

considered unrealistically high, this was deemed a necessary measure to counteract the 

model  s intrinsic stability bias. The primary achievement of this experiment was not to 

predict the timing of collapse but to identify a robust physical precursor: the strength of 

the southward freshwater transport at the Atlantic  s southern boundary (34Â°S) [1]. 

Building directly upon this model-derived insight, Smolders et al. (2024) used this 

specific salinity-based indicator to analyze several ocean reanalysis datasets â€•ORAS5, 

SODA, and GLORYS â€•which integrate vast observational data into a dynamically 

consistent framework. Their analysis detected a pronounced CSD pattern in the 

southward freshwater transport, particularly at depths below 500 meters along the 34Â°S 

transect. By extrapolating the observed decline in the system  s restoring rate, the study 

produced a startling forecast that directly contradicts the low-probability, post-2100 

collapse assessments from the IPCC and most CMIP6 models. They calculated a mean 

collapse time of 2050, with a 59% (Â±17%) probability of the tipping event occurring 

before that year [18]. 

Methodological Debates and Countervailing Evidence 

The stark projections from studies like Ditlevsen (2023) and Smolders et al. (2024) have 

ignited significant scientific debate, with scrutiny focused on both the statistical methods 

and the fidelity of the climate models themselves. In a direct critique, Reschenhofer 49 (2024) argues that the statistical techniques used to detect CSD, particularly the use of 

rolling windows on short and potentially non-stationary time series, lack robustness. 

Citing a structural break in the data in the late 1990s that could invalidate the core 

assumptions of EWS methods, Reschenhofer applied an alternative statistical approach 

(a Hodrick-Prescott filter and cumulative plotting) to the same reanalysis data and found 

no evidence of an impending collapse. This alternative analysis showed no trend towards 

a minimum in freshwater transport and no sign of increasing variance or autocorrelation 

[19]. 

On the modeling front, the apparent stability of most CMIP6 models is itself a subject of 

intense discussion. There is a growing concern that this stability might be an artifact of 

shared model deficiencies, such as the misrepresentation of freshwater transport 

pathways that artificially divert freshwater away from the critical deep-water formation 

zones in the North Atlantic [1]. Indeed, studies have shown that a model previously 

stable under a CO â‚‚-doubling scenario can be induced to collapse after 300 years once 

such biases are corrected. Furthermore, when CMIP6 simulations are extended beyond 

2100 under high-emissions scenarios, a consensus for collapse does emerge; one 

analysis found that all nine models examined exhibited a complete shutdown of the deep 

northern overturning cell, confirming that a tipping point is a consistent feature of these 

models, albeit on a longer timescale [1]. 

However, other model-based analyses reveal powerful countervailing factors that could 

promote AMOC stability. Research by Sinet et al. (2025) demonstrated that meltwater 

from the West Antarctic Ice Sheet can exert a stabilizing influence, potentially 

counteracting or even preventing a collapse induced by meltwater from the Greenland 

Ice Sheet [20]. In a broad analysis of 34 CMIP6 models, Baker et al. (2025) concluded that 

a 21st-century collapse is unlikely, pointing to robust stabilizing feedbacks, such as wind-

driven upwelling in the Southern Ocean, that sustain the global overturning circuit [14]. 

This highlights the central conflict in the current scientific landscape: while tailored 

statistical and model-based analyses designed to detect tipping points suggest a 

proximate threat, broader suites of climate models indicate a system with powerful, 

though perhaps not insurmountable, resilience. 

Sources of Divergence: Biases in Paleoclimate Proxies and 

Climate Models 

The substantial divergence in scientific predictions regarding the 21st-century stability of 

the Atlantic Meridional Overturning Circulation (AMOC) is rooted in fundamental 50 challenges and biases inherent in the two primary methodologies used for its study: the 

statistical analysis of paleoclimate proxies and simulations with complex climate models. 

While proxies provide a crucial long-term perspective, their interpretation is fraught with 

uncertainty, leading to conflicting assessments of past variability and the presence of 

early-warning signals (EWS). Conversely, while climate models offer a physically-based 

framework for future projections, many contain systemic biases that may artificially 

enhance AMOC stability, masking its true vulnerability. This section examines these 

sources of divergence, first by detailing the limitations of paleoclimate reconstructions 

and then by analyzing systemic errors within climate models. 

Challenges in Statistical Analyses of Paleoclimate Proxies 

The wide spectrum of conclusions drawn from paleoclimate records â€•from forecasts of 

imminent collapse to findings of relative stability â€•stems from a cascade of issues related 

to data quality, interpretation, and statistical methodology. These challenges begin with 

the proxies themselves, which are indirect, noisy, and geographically limited 

representations of the vast and complex AMOC system [21]. 

Inherent Proxy Limitations and Signal Ambiguity:  A foundational constraint is the 

limited spatial and temporal resolution of proxy archives. Data from a single sediment or 

ice core provides point-based information that can be dominated by local or regional 

climate variability, potentially masking or mimicking a basin-scale AMOC signal [21]. 

Furthermore, the temporal resolution of key archives like marine sediment cores, where 

a single data point may represent an average over decades or centuries, is often too 

coarse to resolve the high-frequency changes in variance and autocorrelation that are 

hallmarks of critical slowing down, a key theoretical basis for EWS [21]. The integrity of 

the climate signal is also frequently compromised by processes within the archive. 

Bioturbation in marine sediments, for instance, acts as a low-pass filter, which can 

artificially increase autocorrelation and generate a spurious EWS that is an artifact of the 

archive, not a true climate signal [22]. Hiatuses, changes in sediment focusing, signal 

diffusion, and layer thinning in ice cores can further corrupt the fidelity of the record [22] 

[23]. 

Perhaps the most significant source of conflicting interpretations is that most proxies 

respond to multiple environmental factors. For example, foraminiferal Î´Â¹â¸O is a 

composite signal reflecting sea temperature, global ice volume, and local salinity. A 

change in this proxy could be plausibly attributed to an AMOC-induced temperature shift, 

a meltwater signal from the Greenland Ice Sheet, or a local freshening event, allowing 51 different research teams to arrive at divergent conclusions from the same data [24]. 

Similarly, while alkenone-based sea surface temperatures (SSTs) are more direct, a 

temperature change in the subpolar gyre is still a combined result of AMOC heat 

transport, atmospheric patterns, and local gyre dynamics [10][24]. 

Uncertainties in Data Interpretation and Reconstruction:  The process of converting 

raw proxy data into a time series introduces further uncertainty. The construction of an 

age model to establish a chronology is an inherently uncertain process, with error 

margins in methods like radiocarbon dating. An inaccurate age model that stretches or 

compresses time can artificially create or obscure statistical trends in EWS metrics like 

lag-1 autocorrelation (AR1), which are highly sensitive to the temporal spacing of data 

points [25]. Likewise, the calibration of a proxy measurement to a physical variable like 

SST is a statistical exercise where the choice of model (e.g., simple linear regression vs. 

complex Bayesian models) can yield different reconstructions [26]. A critical issue is the 

frequent underestimation of uncertainty from the calibration process itself; if the 

 unexplained variance   is not rigorously propagated, the resulting time series may 

appear deceptively precise, biasing EWS analysis by masking significant fluctuations [26]. 

This culminates in the central challenge of separating a faint EWS signal from a dominant 

noise background, which is often â€œred â€ noise possessing its own autocorrelation that 

can be indistinguishable from a genuine EWS [10][17][27]. 

The Role of Methodological Assumptions:  The final and most dramatic source of 

divergence lies in the statistical methods applied to the reconstructed time series. The 

search for EWS is governed by assumptions that can predetermine the outcome. For 

instance, the theoretical framework often assumes stationarity, but climate records 

contain long-term forcings and abrupt shifts (structural breaks) that can be 

misinterpreted by a rolling-window analysis as a simultaneous increase in variance and 

autocorrelation, creating a â€œphantom â€ EWS [19]. The choice of detrending method to 

address this â€•from a rigid linear fit to a flexible Gaussian kernel â€•creates a trade-off 

where one might leave artifacts (false positives) and the other might remove the genuine 

signal (false negatives) [19]. The selection of the primary indicator (e.g., subpolar gyre 

SSTs vs. southward freshwater transport) also preconditions the outcome [1][10]. 

Perhaps most significantly, alarming predictions of a collapse within a specific timeframe 

(e.g., 2025-2095) are not derived solely from observing a trend but from fitting that trend 

to a specific theoretical model of a system approaching a bifurcation, such as a saddle-

node bifurcation [10]. This powerful assumption allows for extrapolation to a precise 

collapse point, but the conclusion is entirely conditional on the chosen model being 

correct. This methodological leap explains the vast gulf between studies forecasting an 52 imminent collapse and those analyzing the same underlying data to find no significant 

trend at all [19][10]. 

Biases and Uncertainties in Complex Climate Models 

Divergence in AMOC projections also arises from fundamental biases and incomplete 

physics within the complex climate models used for forecasting. While large-scale efforts 

like the Coupled Model Intercomparison Project Phase 6 (CMIP6) generally project a 

gradual AMOC weakening without an abrupt collapse, this apparent consensus on 

stability is increasingly scrutinized as a potential artifact of shared model deficiencies, 

particularly in the representation of freshwater feedbacks and cryospheric interactions 

[1]. 

Intrinsic Stability Biases from Misrepresented Feedbacks:  A critical source of artificial 

AMOC stability in many Earth System Models (ESMs) is a systemic misrepresentation of 

the salt-advection feedback. The stability of the AMOC is governed by the meridional 

freshwater transport by the overturning circulation, quantified as  . Observational 

data indicate  is negative, signifying a net freshwater export that creates a 

positive, destabilizing feedback. However, a majority of CMIP models erroneously 

simulate a positive  , implying a net freshwater import. This inverts the sign of a 

key feedback, hard-wiring an artificial stability into the model climate that dampens its 

response to perturbations [28]. This bias is deeply embedded, as even high-resolution 

models initialized in a more realistic state eventually drift toward the biased, positive 

regime [28]. 

The origin of this pervasive bias is often teleconnected to remote processes, particularly 

a systematic positive freshwater flux bias over the Indian Ocean. In many models, 

excessive precipitation in this region creates an anomalously fresh surface layer that is 

transported into the South Atlantic via the Agulhas Leakage. This influx artificially 

freshens the entire Atlantic basin, contributing to the erroneous positive  and 

making the modeled AMOC more resilient to freshening from North Atlantic sources like 

Greenland meltwater [28][29]. Bifurcation analyses confirm that this remote bias shifts 

the AMOC â€™s collapse threshold to an â€œunrealistic parameter regime, â€ which explains 

why many CMIP6 models appear stable and require unrealistically high freshwater 

forcing to induce a collapse in experimental settings. When such freshwater transport 

biases are corrected, a previously stable model can be made to collapse under more 

realistic forcing levels [1]. 

> F_ovS
> F_ovS
> F_ovS
> F_ovS
> F_ovS 53

Divergence from Incomplete Ocean-Ice Sheet Interactions:  Significant divergence also 

stems from how models represent the complex and often competing feedbacks from 

melting ice sheets. The transition from idealized â€œhosing â€ experiments with prescribed 

freshwater forcing to fully coupled, dynamic ice sheet models has revealed critical new 

dynamics [1]. For instance, meltwater from the Antarctic Ice Sheet (AIS) can introduce a 

powerful delaying effect on AMOC decline. Under a high-emissions scenario, AIS 

discharge can lead to profound cooling in the Southern Ocean and an expansion of sea 

ice. This powerful negative feedback counteracts anthropogenic warming and, through 

climatic teleconnections, slows the rate of AMOC change in the Northern Hemisphere, 

potentially delaying a projected collapse by approximately 35 years [30]. Models lacking 

this dynamic coupling are therefore likely to produce more pessimistic near-term stability 

assessments. 

Complicating this further, meltwater from different poles can exert opposing influences. 

While Greenland Ice Sheet (GIS) melt is a well-documented destabilizing agent, 

meltwater from the West Antarctic Ice Sheet (WAIS) can have a stabilizing effect, 

increasing the AMOC  s resilience to GIS-induced freshening [20]. A model  s projection is 

therefore exquisitely sensitive to the relative timing and magnitude of melt from each 

pole; models that focus predominantly on GIS melt will invariably identify a more 

vulnerable AMOC [20]. Finally, a foundational uncertainty lies in how models generate 

and route meltwater. Basal melting, driven by geothermal heat flux, represents a 

continuous freshwater source that is often omitted or inadequately parameterized [31]. 

Furthermore, meltwater is often distributed uniformly over large ocean grid cells in 

models, whereas in reality, it is channeled through subglacial networks to specific points. 

These differences in the spatial distribution and depth of freshwater injection can 

profoundly alter the local oceanic response, particularly stratification in critical deep-

water formation regions, creating another axis of divergence in AMOC projections [30]. 

Potential Consequences of an AMOC Disruption 

A significant slowdown or complete collapse of the Atlantic Meridional Overturning 

Circulation (AMOC) would not be a singular event but would instead precipitate a 

cascade of profound, interconnected, and globally significant consequences. As a 

primary engine for the planetary redistribution of heat, nutrients, and carbon, the 

AMOC  s stability is intrinsically linked to regional temperature regimes, hydrological 

cycles, and the biogeochemical functioning of the world  s oceans. A disruption would 

induce a complex and regionally heterogeneous reorganization of Earth  s systems, 54 underscoring the critical importance of resolving the scientific debate surrounding its 

future trajectory. 

Climatic Reorganization and Atmospheric Dynamics 

The most direct consequence of a substantial AMOC slowdown is a dramatic thermal 

rebalancing between the hemispheres. The circulation is responsible for transporting an 

estimated 1.25 petawatts of heat poleward, which moderates the climate of the North 

Atlantic region. A collapse would disrupt this heat transport, inducing severe cooling 

across the Northern Hemisphere, with a particular concentration in Northwestern 

Europe. Model projections indicate an average surface air temperature drop of 3.4Â°C in 

Great Britain and between 4Â°C and 10Â°C across parts of Northern Europe. Some 

simulations suggest that winter cooling could be as extreme as 10Â°C to 30Â°C within a 

century, creating conditions where winter sea ice could extend into the territorial waters 

of the British Isles and Denmark [1][32]. This would create a distinctive regional cooling 

pattern, often referred to as the North Atlantic   cold blob,   in stark contrast to the 

background signal of global warming. Concurrently, as this heat is no longer transported 

north, a slight warming is expected in the Southern Hemisphere, a phenomenon known 

as the   bipolar seesaw   [1][32]. 

The impacts of this thermal reorganization would extend far beyond the North Atlantic. 

Climate models indicate the emergence of a   climatic dipole   in South America, where 

tropical regions would experience cooling while extratropical areas to the south would 

face warming and increased aridity [2][33]. The same AMOC weakening that cools Europe 

is projected to cause more frequent and intense summer heatwaves in South America  s

La Plata Basin [33]. The magnitude of these climatic shifts is strongly linked to the degree 

of AMOC decline; models simulating a more moderate weakening project a pattern of 

minimum warming in the subpolar North Atlantic rather than absolute cooling, 

highlighting the AMOC  s state as a major source of uncertainty in regional climate 

projections [2]. 

A weakened AMOC would also fundamentally restructure global precipitation patterns, 

primarily by causing a southward shift of the Intertropical Convergence Zone (ITCZ), the 

planet  s main tropical rain belt, a teleconnection mentioned in previous sections. This 

atmospheric response occurs because the AMOC  s northward heat transport helps to 

position the ITCZ farther north; a weakening of the circulation relaxes this pull. This 

southward displacement, a classic signature of a weakened AMOC observed in 

paleoclimate records from periods like the Younger Dryas, would trigger severe and 55 persistent droughts in regions such as the African Sahel and lead to a significant 

reduction in average rainfall and snowfall over Europe and the mid-latitudes, threatening 

water security for millions [1][3]. This atmospheric reorganization creates a complex 

mosaic of effects. While some regions would face drought, Northeast Brazil is projected 

to receive more intense extreme precipitation events, and increased rainfall over the 

southern Amazon could potentially counteract forest dieback trends [2]. The AMOC  s

influence extends to global monsoon systems as well; a slowdown is expected to alter the 

Indian Monsoon, and variability in the AMOC  s South Atlantic component (SAMOC) has 

been shown to modulate monsoon intensity with a lead time of 15-20 years [34]. 

Furthermore, altered atmospheric pressure and temperature gradients would 

reconfigure storm tracks. Projections from the IPCC and other studies suggest a potential 

strengthening of the North Atlantic storm track. One controversial hypothesis posits that 

a full collapse could dramatically increase the thermal gradient between the cold 

subpolar region and its surroundings, fueling more powerful mid-latitude cyclonic 

 superstorms   with near-hurricane-force winds in winter [1]. 

Environmental and Biogeochemical Consequences 

The AMOC  s disruption would also have a direct, dynamic impact on regional sea levels. 

As discussed previously, the strong northward flow of the Gulf Stream maintains a 

pressure gradient across the Atlantic. A slowdown of this current would cause this 

gradient to relax, triggering an anomalous and accelerated rate of sea-level rise along the 

U.S. East Coast that is distinct from, and additive to, global mean sea-level rise [2]. 

Furthermore, the AMOC is a crucial component of the global carbon cycle. As previously 

outlined, it acts as a vital pump that sequesters atmospheric CO â‚‚ when warm surface 

waters cool and sink in the high-latitudes, effectively isolating carbon in the deep ocean 

for centuries to millennia [35]. A slowdown would severely diminish this sequestration 

capacity, leaving more anthropogenic CO â‚‚ in the atmosphere and creating a positive 

feedback loop that exacerbates global warming. This reduction in ocean carbon uptake, 

termed the   AMOC carbon feedback,   is projected to range from 7.5 to 32 petagrams of 

carbon (PgC) by 2100. Studies indicate a quasi-linear reduction of approximately 0.2 PgC 

per year for every 10 Sverdrup (Sv) decline in AMOC strength, with potentially trillions of 

dollars in unaccounted-for climate damages and an aggravation of ocean acidification 

[36][35]. 

The AMOC is also essential for distributing the nutrients that support marine life. The 

deep, southward-flowing limb of the circulation, which becomes enriched with nitrate 56 and phosphate from sinking organic matter, transports this vital supply to other basins 

where it upwells and fertilizes surface waters [2]. A disruption would sever this critical 

nutrient supply chain, leading to a significant decrease in primary productivity. This 

would have cascading effects through the food web, affecting zooplankton populations 

and the stocks of commercially vital fish like cod and haddock, triggering a large-scale 

reorganization of marine habitats and threatening biodiversity [36]. 

Finally, the AMOC is engaged in a complex feedback loop with the cryosphere. While 

freshwater from melting ice sheets is a primary driver of AMOC weakening, the state of 

the AMOC in turn influences ice sheet stability [37]. Meltwater from the West Antarctic Ice 

Sheet, for example, can have a dual effect, potentially preventing, facilitating, or even 

triggering a recovery of the AMOC depending on the specific characteristics of the 

freshwater influx. This highlights the risk of cascading tipping events where the stability 

of the Greenland Ice Sheet, the West Antarctic Ice Sheet, and the AMOC are coupled. 

While a full collapse within this century is considered unlikely, a significant weakening of 

20-81% is deemed possible under extreme scenarios, with these non-linear interactions 

representing a major source of uncertainty in climate projections [14][37]. 

Conclusion and Future Research Priorities 

The stability of the Atlantic Meridional Overturning Circulation (AMOC) in the 21st century 

remains a subject of significant scientific debate and uncertainty. The current body of 

research is characterized by a fundamental divergence: while many comprehensive Earth 

System Models (ESMs) project a gradual, non-critical weakening, a growing number of 

studies using paleoclimate proxies and statistical analyses of observational data suggest 

a critical tipping point could be crossed this century. This report has synthesized the 

evidence underpinning these conflicting views, revealing that the divergence stems from 

deep-seated limitations in both modeling and observational approaches. To reconcile 

these findings and produce a more reliable, uncertainty-quantified assessment of AMOC 

risk, a comprehensive research strategy is imperative. Future priorities must be directed 

towards systematically addressing the weaknesses in both domains, enhancing the 

fidelity of past reconstructions and improving the physical realism of future projections 

[1]. 

The specific limitations inherent to each methodology, and the research required to 

address them, are summarized in Table 2. 

Table 2: Comparison of paleoclimate reconstructions and climate models/ESMs, 

their biases, and recommended fixes 57 Category  Primary 

Inputs/Processes 

Typical 

Limitations/Biases 

Consequences 

for AMOC 

Inference 

Recommended 

Methodologica 

lFixes 

Paleoclimate 

Reconstruction 

Paleoclimate 

proxy records 

from sediment 

and ice cores 

(e.g., 

foraminiferal 

Î´Â¹â¸O), data 

archives, 

statistical 

methods for EWS 

detection, age 

models, 

calibration 

methods. 

Geographically 

sparse and 

temporally coarse 

records; bias from 

local variability 

masking basin-scale 

signals. Post-

depositional artifacts 

(bioturbation) 

generating spurious 

EWS. Age model 

uncertainties 

creating or erasing 

trends. Calibration 

methods 

underestimating 

variance and noise. 

Non-stationarity and 

structural breaks 

misinterpreted as 

EWS. 

Interpretive 

ambiguity; risk 

of incorrect 

conclusions 

from artifacts. 

Distorted trend 

detection from 

age model 

errors. Difficulty 

distinguishing 

faint EWS from 

complex   red 

noise  

backgrounds. 

Overly alarming 

predictions 

from simplified 

model 

extrapolations. 

Expand high-

resolution core 

archives in key 

regions. 

Develop and 

validate novel 

circulation 

proxies. Use 

multi-proxy 

reconstructions 

to disentangle 

drivers. Use 

integrated 

Bayesian 

chronological 

modeling to 

propagate age 

uncertainty. 

Develop and 

validate EWS 

algorithms 

robust to non-

stationarity. 58 Climate 

Models/ESMs 

Earth System 

Models (ESMs) 

with physical 

oceanography, 

cryospheric 

interactions, salt-

advection 

feedback, air-sea 

fluxes, and ice 

sheet dynamics. 

Artificial AMOC 

stability from 

misrepresented salt-

advection feedback 

(erroneous positive 

), often 

linked to remote 

biases (e.g., Indian 

Ocean precipitation). 

Omission of dynamic 

Antarctic melt and its 

stabilizing 

teleconnections. 

Underestimation of 

total freshwater 

budgets (e.g., 

omitting basal 

melting). 

Oversimplified 

meltwater routing 

(uniform 

distribution). 

> F_ovS

AMOC is 

rendered overly 

resilient to 

perturbations. 

Projections of 

vulnerability are 

biased and 

incomplete. 

Localized 

impact of 

meltwater on 

stratification 

and deep 

convection is 

underestimated. 

Improve 

atmospheric 

parameterizatio 

n to eliminate 

remote 

freshwater 

biases. Use 

higher-

resolution 

models for 

inter-basin 

exchange. 

Validate 

physical 

realism of 

feedback loops 

(sign/magnitud 

e of  ). 

Implement 

fully coupled, 

two-way 

interactive ice 

sheet models 

for both poles 

in all major 

modeling 

projects. 

Develop and 

implement 

high-resolution 

subglacial 

hydrology and 

plume models 

for realistic 

meltwater 

routing. 

> F_ovS

59 Advancing Paleoclimate Reconstructions of AMOC Variability 

Improving the empirical foundation for AMOC behavior requires a concerted effort to 

enhance the quality and interpretation of paleoclimate proxy records. This involves 

improving data archives, refining analytical frameworks, and developing more robust 

statistical methods for detecting early-warning signals (EWS). 

First, to move beyond divergent conclusions rooted in geographically sparse and 

temporally coarse records, a critical priority is to expand the paleoclimatic archive 

network. Existing records can be biased by local variability that may either mask or 

mimic a true basin-scale AMOC signal, underscoring the need for new, high-resolution 

sediment and ice cores from dynamically significant regions like deep-water formation 

zones [21]. Concurrently, research must focus on developing and validating novel proxies 

that offer more direct measurements of circulation, moving beyond traditional indicators 

like foraminiferal Î´Â¹â¸O that conflate temperature, ice volume, and salinity effects [24]. 

The adoption of multi-proxy reconstructions is essential for reducing this interpretive 

ambiguity [10][24]. Furthermore, non-climatic noise within archives â€•such as 

bioturbation or discontinuities from submarine landslides â€•can generate spurious EWS 

[22][23]. A key research avenue is the development of proxy system and diagenetic 

models to quantify and correct for these post-depositional artifacts [22]. 

Second, the interpretive frameworks that translate raw proxy measurements into a time 

series must be fundamentally improved. To address the profound uncertainty introduced 

by age models â€•where errors in temporal spacing can create or erase EWS trends â€•future 

work must prioritize integrated Bayesian chronological modeling techniques that allow 

for robust propagation of age-model uncertainty [25]. Similarly, proxy calibration 

methods must rigorously account for all sources of uncertainty. Many reconstructions 

differ due to choices in calibration and the underestimation of â€œunexplained variance â€

[26]. Research should shift towards protocols, such as hierarchical Bayesian models, that 

produce probabilistic ensembles of time series rather than single â€œbest-fit â€

reconstructions [27]. Such ensembles are crucial for realistically assessing the signal-to-

noise ratio and distinguishing a faint EWS from the complex â€œred noise â€ background 

inherent to paleoclimatic data [10][17]. 

Third, the statistical methodologies used for EWS detection must be made more robust. 

As current methods can misinterpret abrupt â€œstructural breaks â€ in non-stationary 

records as â€œphantom â€ EWS, a priority is to develop and validate detection algorithms 

robust to these conditions, benchmarking them against synthetic data and ESM output 

where the ground truth is known [19]. The selection of analytical parameters, such as 60 detrending methods, must become more objective and data-driven [19]. Confidence in 

any detected signal would be increased by a shift to multi-indicator approaches, seeking 

EWS consistently across different proxies representing distinct components of the AMOC 

system [1][10]. Finally, the field must bridge the gap between statistical trend detection 

and physical forecasting. Instead of relying on simplified bifurcation models for 

extrapolation, future work should develop integrated frameworks that provide 

probabilistic risk assessments, conditioning any forecast on deep uncertainties and using 

physics-based models for plausible constraints [10]. 

Refining Climate Model Projections of AMOC Stability 

To scrutinize the apparent consensus on 21st-century AMOC resilience â€•a consensus that 

may be an artifact of shared model deficiencies â€•research must address systemic biases 

in model physics and cryospheric interactions [1]. 

First, a paramount priority is to correct fundamental model biases that artificially 

stabilize the AMOC. The most critical of these is the misrepresentation of the salt-

advection feedback. A majority of ESMs erroneously simulate a net meridional freshwater 

import into the Atlantic (a positive  ), which inverts the sign of a key destabilizing 

feedback and renders the model AMOC overly resilient. In contrast, observations indicate 

a net freshwater export (a negative  ), implying a positive, self-reinforcing 

feedback [29]. As this bias is often teleconnected to remote processes like excessive 

precipitation over the Indian Ocean, future work must improve atmospheric 

parameterizations and use higher-resolution ocean models to resolve inter-basin 

exchange [28]. New validation protocols are needed to assess the realism of such 

feedback loops. Systematically performing bifurcation analyses on ESMs is also essential 

to reveal whether a model  s stability is genuine or an artifact of biases [29][38]. 

Second, the representation of ocean-ice sheet interactions must be significantly 

advanced. A crucial priority is to accelerate the transition from idealized freshwater 

â€œhosing â€ experiments to the universal implementation of fully coupled, two-way 

interactive ice sheet models for both Greenland and Antarctica. This is essential because 

meltwater from different poles can exert opposing influences. While Greenland Ice Sheet 

(GIS) melt is a known destabilizing agent, recent modeling shows that Antarctic Ice Sheet 

(AIS) melt can induce a powerful stabilizing feedback by cooling the Southern Ocean, 

which through teleconnections can delay an AMOC collapse by decades [30]. Melt from 

the West Antarctic Ice Sheet (WAIS) in particular may increase AMOC resilience to GIS 

freshening [20]. Since the net effect is sensitive to the relative melt from each pole, future 

> F_ovS
> F_ovS 61

model intercomparison projects must mandate simulations with fully coupled, bi-

hemispheric cryosphere components to avoid incomplete and biased projections [1][20]. 

Third, the physical realism of how meltwater is generated and routed must be improved. 

Current models often underestimate the total freshwater budget by omitting sources like 

basal melting driven by geothermal heat flux, a continuous process largely absent from 

climate projections [31]. Furthermore, meltwater is often distributed uniformly over vast 

ocean grid cells, diluting its impact on stratification. In reality, melt is discharged at 

specific, localized points. Consequently, a key objective is to develop and implement 

high-resolution subglacial hydrology and plume models that can realistically route 

meltwater from the ice bed to the ocean, enabling a more accurate assessment of its 

impact on the local convective processes that drive the AMOC [30]. 

References 

[1]  ðŸŒ  Atlantic meridional overturning circulation - https://en.wikipedia.org/wiki/Atlantic 

_meridional_overturning_circulation 

[2]  ðŸŒ  How a Weakening Atlantic Ocean Circulation Is Rewriting ... - https://ocean2climat 

e.org/2025/12/28/how-a-weakening-atlantic-current-is-rewriting-south-americas-weathe 

r/ 

[3]  ðŸŒ  [Commentary] The risk of a potential circulation change in ... - https://india.monga 

bay.com/2024/11/commentary-the-risk-of-a-potential-circulation-change-in-the-atlantic-

adds-to-climate-worries/ 

[4]  ðŸŒ  Low variability of the Atlantic Meridional Overturning ... - https://www.nature.com/ 

articles/s41467-025-61793-z 

[5]  ðŸŒ  Recent Progress in Understanding and Predicting Atlantic ... - https://pmc.ncbi.nl 

m.nih.gov/articles/PMC6991968/ 

[6]  ðŸŒ  Figure 3.30 - https://www.ipcc.ch/report/ar6/wg1/figures/chapter-3/figure-3-30/ 

[7]  ðŸŒ  Summary for Policymakers  â€• Special Report on the Ocean ... - https://www.ipcc.c 

h/srocc/chapter/summary-for-policymakers/ 

[8]  ðŸŒ  CMIP6 Models Predict Significant 21st Century Decline of ... - https://repository.libr 

ary.noaa.gov/view/noaa/30634 

[9]  ðŸŒ  Atlantic Ocean Current Expected to Undergo Limited ... - https://ese.caltech.edu/ne 

ws/atlantic-ocean-current-expected-to-undergo-limited-weakening-with-climate-change 62 [10]  ðŸŒ  Warning of a forthcoming collapse of the Atlantic ... - https://www.nature.com/arti 

cles/s41467-023-39810-w 

[11]  ðŸŒ  expert reaction to paper warning of a collapse of the ... - https://www.sciencemed 

iacentre.org/expert-reaction-to-paper-warning-of-a-collapse-of-the-atlantic-meridional-o 

verturning-circulation/ 

[12]  ðŸŒ  Comparing observed and modelled components of the ... - OS - https://os.coperni 

cus.org/articles/20/589/2024/ 

[13]  ðŸŒ  Substantial Risk of 21st Century AMOC Tipping even under ... - https://arxiv.org/ht 

ml/2407.19909v1 

[14]  ðŸŒ  Continued Atlantic overturning circulation even under ... - https://www.nature.co 

m/articles/s41586-024-08544-0 

[15]  ðŸ“„  Reconciled warning signals in observations and models imply approaching AMOC 

tipping point - https://arxiv.org/pdf/2503.22111v1 

[16]  ðŸŒ  FEATURE ARTICLE  Â· Is the Atlantic Overturning Circulation ... - https://tos.org/oce 

anography/article/is-the-atlantic-overturning-circulation-approaching-a-tipping-point 

[17]  ðŸŒ  Early warning signal for a tipping point suggested by a ... - https://pmc.ncbi.nlm.n 

ih.gov/articles/PMC9440003/ 

[18]  ðŸ“„  Probability Estimates of a 21st Century AMOC Collapse - https://arxiv.org/pdf/240 

6.11738v1 

[19]  ðŸ“„  A new indicator for the AMOC strength still gives no indication of an imminent 

collapse - https://arxiv.org/pdf/2402.16600v1 

[20]  ðŸ“„  West Antarctic Meltwater can Prevent an AMOC Collapse - https://arxiv.org/pdf/25 

02.17104v1 

[21]  ðŸŒ  Global and regional sea-surface temperature changes over ... - https://cp.copernic 

us.org/articles/21/1895/2025/ 

[22]  ðŸŒ  Sustained North Atlantic warming drove anomalously intense ... - https://pmc.ncb 

i.nlm.nih.gov/articles/PMC11251152/ 

[23]  ðŸŒ  Multi-proxy constraints on Atlantic circulation dynamics ... - https://pmc.ncbi.nlm. 

nih.gov/articles/PMC10089918/ 

[24]  ðŸŒ  Î´ 18 O water isotope in the iLOVECLIM model (version ... - https://gmd.copernicus. 

org/articles/6/1505/2013/gmd-6-1505-2013-relations.html 63 [25]  ðŸŒ  Quantifying age and model uncertainties in palaeoclimate ... - https://pmc.ncbi.nl 

m.nih.gov/articles/PMC6501663/ 

[26]  ðŸŒ  Statistical Uncertainty in Paleoclimate Proxy Reconstructions - https://pubmed.nc 

bi.nlm.nih.gov/35860010/ 

[27]  ðŸŒ  Simple noise estimates and pseudoproxies for the last 21 ... - https://essd.coperni 

cus.org/articles/11/1129/ 

[28]  ðŸŒ  Persistent climate model biases in the Atlantic Ocean  s ... - OS - https://os.coperni 

cus.org/articles/20/549/2024/ 

[29]  ðŸŒ  [2308.11751] The effect of model freshwater flux biases on ... - https://arxiv.org/ab 

s/2308.11751 

[30]  ðŸ“„  Antarctic ice sheet - climate feedbacks under high future carbon emissions - http 

s://arxiv.org/pdf/2005.09731v1 

[31]  ðŸ“„  Liquid Water on Cold Exo-Earths via Basal Melting of Ice Sheets - https://arxiv.org/ 

pdf/2212.03702v1 

[32]  ðŸŒ  Revisiting climate impacts of an AMOC slowdown - https://pmc.ncbi.nlm.nih.gov/ 

articles/PMC11578178/ 

[33]  ðŸŒ  Future climate change shaped by inter-model differences ... - https://www.nature. 

com/articles/s41467-021-24015-w 

[34]  ðŸŒ  Atlantic Meridional Overturning Circulation and Its Impact ... - https://www.aoml. 

noaa.gov/phod/research/moc/moc_monsoons/index.php 

[35]  ðŸŒ  Weakening AMOC reduces ocean carbon uptake and ... - https://pmc.ncbi.nlm.nih. 

gov/articles/PMC11892582/ 

[36]  ðŸŒ  How Does AMOC Affect Marine Life? â†’ Question - https://climate.sustainability-dir 

ectory.com/question/how-does-amoc-affect-marine-life/ 

[37]  ðŸŒ  Meltwater from West Antarctic ice sheet tipping affects ... - https://pmc.ncbi.nlm.n 

ih.gov/articles/PMC12617517/ 

[38]  ðŸŒ  Physics of AMOC multistable regime shifts due to freshwater ... - https://esd.coper 

nicus.org/articles/16/1221/2025/ 64