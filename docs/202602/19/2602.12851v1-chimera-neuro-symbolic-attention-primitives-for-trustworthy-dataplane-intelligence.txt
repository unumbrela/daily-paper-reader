Title: Chimera: Neuro-Symbolic Attention Primitives for Trustworthy Dataplane Intelligence

URL Source: https://arxiv.org/pdf/2602.12851v1

Published Time: Mon, 16 Feb 2026 01:54:01 GMT

Number of Pages: 23

Markdown Content:
# CHIMERA : N EURO -S YMBOLIC ATTENTION PRIMITIVES FOR 

# TRUSTWORTHY DATAPLANE INTELLIGENCE 

Rong Fu ∗

University of Macau 

mc46603@um.edu.mo 

Wenxin Zhang 

University of Chinese Academy of Sciences 

zwxzhang12@163.com 

Xiaowen Ma 

Zhejiang University 

xwma@zju.edu.cn 

Kun Liu 

University of Southampton 

kundy9909@gmail.com 

Wangyu Wu 

University of Liverpool 

wangyu.wu@liverpool.ac.uk 

Ziyu Kong 

Fudan University 

23210720182@m.fudan.edu.cn 

Jia Yee Tan 

Renmin University of China 

tanjiayi2002@ruc.edu.cn 

Tailong Luo 

New York Institute of Technology 

tilonluo@gmail.com 

Xianda Li 

University of Bologna 

xianda.li@studio.unibo.it 

Zeli Su 

Minzu University of China 

rickamorty@muc.edu.cn 

Youjin Wang 

Renmin University of China 

wangyoujin@ruc.edu.cn 

Yongtai Liu 

Hanyang University 

yt2024259263@hanyang.ac.kr 

Simon Fong 

University of Macau 

ccfong@um.edu.mo 

February 16, 2026 

# ABSTRACT 

Deploying expressive learning models directly on programmable dataplanes promises line-rate, low-latency traffic analysis but remains hindered by strict hardware constraints and the need for predictable, auditable behavior. Chimera introduces a principled framework that maps attention-oriented neural computations and symbolic constraints onto dataplane primitives, enabling trustworthy inference within the match-action pipeline. Chimera combines a kernelized, linearized attention approximation with a two-layer key-selection hierarchy and a cascade fusion mechanism that enforces hard symbolic guarantees while preserving neural expressivity. The design includes a hardware-aware mapping protocol and a two-timescale update scheme that together permit stable, line-rate operation under realistic dataplane budgets. The paper presents the Chimera architecture, a hardware mapping strategy, and empirical evidence showing that neuro-symbolic attention primitives can achieve high-fidelity inference within the resource envelope of commodity programmable switches. 

Keywords Programmable data plane; in-network inference; neuro-symbolic AI; attention; P4; trustworthy inference 

# 1 Introduction 

The recent emergence of programmable data planes has opened the possibility of executing inference tasks directly on forwarding hardware, enabling ultra-low latency responses and reduced control-plane interaction for network 

> ∗

Corresponding author. 

> arXiv:2602.12851v1 [cs.NI] 13 Feb 2026

management and security functions [ 1 , 2 , 3]. Prior research has demonstrated that lightweight models and tree-based classifiers map naturally to match-action abstractions, delivering useful functionality at line rate [ 4, 5]. At the same time, more expressive models, notably neural architectures, promise richer representations for tasks such as temporal anomaly detection and complex flow classification [ 6, 7 , 8]. Achieving such expressivity directly on dataplane hardware remains difficult because the match-action table (MAT) abstraction offers only restricted arithmetic and limited per-flow state, whereas modern deep models rely on dense linear algebra, non-linearities, and floating-point arithmetic [9, 1]. Two broad strategies have been explored to bridge this gap. The first class simplifies computation to make neural primitives dataplane-friendly, for example by binarization or arithmetic linearization [ 10 ]. The second class bypasses heavy computation using enumerative mappings or extensive lookup tables [ 11 ]. Both approaches achieve important trade-offs but exhibit fundamental limitations. Simplification methods tend to reduce numerical range and therefore harm accuracy on complex tasks [ 10 , 12 ]. Mapping-based methods can preserve fidelity but suffer from poor scalability or explosive table size when input dimensionality grows [ 3]. Hardware-directed accelerators and FPGA-assisted designs mitigate some constraints but introduce deployment and integration overheads that are often impractical for commodity switch ASICs [13, 14, 15]. Separately, neuro-symbolic approaches have matured as a means to combine pattern-driven neural perception with rule-based reasoning and interpretable constraints [ 16 , 17 , 18 ]. In networking, neuro-symbolic techniques have been applied to intrusion detection and policy-aligned decision making, showing improvements in interpretability and robustness compared with purely neural solutions [ 19 , 20 , 21 ]. However, these efforts typically assume a software execution environment or additional hardware, and they do not address the strict instruction, memory, and TCAM/SRAM constraints of dataplane ASICs [22, 23]. Motivated by these observations, this work asks whether it is possible to realize attention-enabled neural perception and symbolic enforcement together inside the dataplane, without hardware modification and while satisfying line-rate and per-flow resource constraints. The core challenge is twofold: first, to re-express attention-like computations so they map to dataplane-native primitives with bounded state and per-packet work; second, to integrate symbolic rules so that hard safety constraints can be enforced deterministically while the neural path provides flexible, graded evidence. To address this challenge we propose Chimera, a framework that unifies three practical design principles. Chimera transforms attention into a kernelized, linearized form that admits incremental SumReduce-style aggregation and quantized feature maps; uses a two-layer key-selection hierarchy combining an SRAM-backed local window with a TCAM-indexed static set to capture both temporal locality and structural priors; and fuses neural and symbolic outputs with a cascade logic that yields hard vetoes when rules require them and soft blends otherwise. The design draws on and extends recent dataplane-focused abstractions while integrating neuro-symbolic ideas in a hardware-aware manner [3, 24, 25, 26]. Our contributions are as follows. First, we introduce the Chimera architecture, which maps transformer-style attention into dataplane primitives (Partition, Map, SumReduce) and couples them with a compact symbolic execution path suitable for TCAM/SRAM implementation. Second, we present a hardware-aware linearized attention formulation together with a hybrid key-selection strategy that bounds per-flow state while retaining long-range context via static TCAM indices. Third, we design a cascade fusion mechanism that enforces hard symbolic constraints and supports differentiable soft-symbolic contributions compiled to compact table encodings. Fourth, we develop a two-timescale control/data-plane protocol that performs light-weight line-rate adaptations in the dataplane and heavier re-clustering in the control plane, ensuring stability and minimal table churn. Finally, we implement mapping strategies and quantization rules that respect table-size and per-flow SRAM budgets and evaluate Chimera against representative baselines. 

# 2 Related Work 

Programmable data planes have recently evolved from passive packet processing substrates into active platforms capable of executing lightweight machine learning and inference tasks at line rate. This paradigm shift has enabled a growing body of research that explores in-network intelligence under strict latency, memory, and pipeline constraints. 

2.1 In-Network Machine Learning on Programmable Data Planes 

Early efforts established the feasibility of embedding learning-related functionalities directly into programmable switches and smart network devices. Surveys and system-level studies have comprehensively analyzed the design space, limitations, and opportunities of in-network machine learning, highlighting challenges related to statefulness, precision, and scalability [ 2, 1 , 27 ]. Frameworks such as Henna introduce hierarchical inference strategies to decompose neural models across multiple pipeline stages, enabling partial inference within switches while preserving throughput [ 5]. More recent systems push the boundary toward general-purpose inference support. Pegasus proposes a universal 2abstraction that maps deep learning inference primitives onto dataplane-compatible operations, demonstrating scalable deployment across heterogeneous network hardware [ 3]. Complementary designs explore multi-task inference [ 7], flow-level decision making [ 8], and FPGA-enhanced dataplane acceleration [ 15 , 28 ]. These approaches collectively illustrate the promise of dataplane intelligence, while also exposing the fragility of deploying monolithic neural models under rigid hardware constraints. 

2.2 Efficient Attention and Model Compression for Resource-Constrained Inference 

Attention mechanisms pose a particular challenge for dataplane deployment due to their quadratic complexity and reliance on softmax normalization. A substantial body of work has therefore focused on reducing the computational and memory footprint of attention-based models. Linearized and kernel-based attention variants approximate full self-attention with sub-quadratic complexity, enabling more efficient execution [ 25 , 29 , 30 ]. Sparse attention further exploits structural redundancy to accelerate long-range modeling [ 31 , 32 ]. From a hardware perspective, several studies propose approximation techniques for attention primitives, including softmax linearization [ 26 ], weight compression with in-situ decompression [ 33 ], and FPGA-oriented systolic designs [ 13 ]. While these methods significantly reduce inference cost, they remain largely model-centric and are not natively aligned with the primitive-based execution model of programmable switches. 

2.3 Neuro-Symbolic Learning and Reasoning in Networked Systems 

Neuro-symbolic artificial intelligence aims to integrate neural representation learning with symbolic reasoning, combin-ing adaptability with interpretability [ 34 , 16 ]. Recent advances demonstrate differentiable symbolic reasoning over large-scale knowledge graphs [ 17 ] and logic-constrained neural architectures with provable guarantees [ 35 , 36 ]. Within networking and security domains, neuro-symbolic methods have been applied to intrusion detection and traffic analysis, showing improved explainability and robustness compared to purely neural baselines [ 19 , 21 ]. Symbolic interpretability has also been explored for anticipatory reinforcement learning in network control, enabling reasoning-aware decision policies [ 37 ]. Despite these advances, existing approaches typically assume software-based execution environments and do not address dataplane-level constraints. 

2.4 Toward Trustworthy and Verifiable Dataplane Intelligence 

As dataplane intelligence becomes increasingly autonomous, concerns regarding correctness, safety, and trustworthiness have gained prominence. Research on trustworthy AI emphasizes robustness, interpretability, and verifiability as essential design principles [ 38 , 39 ]. In programmable networks, recent work investigates stateful processing guar-antees [ 9, 40 ], secure dataplane architectures [ 22 ], and hardware-level verification and fuzzing [ 41 ]. However, most existing systems treat trustworthiness as an external property enforced through testing or verification, rather than as a first-class architectural component. In contrast, integrating symbolic constraints directly into dataplane-executable attention primitives offers a promising path toward inherently interpretable and verifiable in-network intelligence. 

2.5 Positioning of This Work 

In summary, prior work has independently advanced programmable dataplane intelligence, efficient attention mech-anisms, and neuro-symbolic reasoning. Yet, a unified framework that reconciles attention-based learning, symbolic interpretability, and dataplane compatibility remains absent. Chimera addresses this gap by introducing neuro-symbolic attention primitives that are explicitly designed for trustworthy execution within programmable data planes, bridging neural efficiency with symbolic rigor. 

# 3 Methodology 

We propose Chimera , a methodology that maps Transformer attention and neuro-symbolic reasoning onto dataplane primitives for line-rate, trustworthy inference under hardware constraints. Chimera translates attention into hardware-friendly operations: Partition, Map and SumReduce. It combines sparse patterns, incremental aggregation, and a two-timescale update protocol, extending Pegasus principles (fuzzy matching, range-encoded remapping) with cascade fusion for hard symbolic guarantees with neural expressivity. This section details the primitive mappings, linearized attention rules, two-layer key selection, neuro-symbolic fusion, and control-/data-plane coordination. 3Figure 1: Overview of the Chimera architecture for trustworthy dataplane intelligence. The pipeline executes within a 

P4 Programmable Switch across three primary stages: Partition , where the incoming Packet Stream is segmented into discrete units X1, . . . , X k; Map , which bifurcates into a Neural Path ( ϕ) for computing Linearized Attention 

via high-dimensional feature maps and a Symbolic Path ( R) that executes Rule Matching against hardware-resident 

Symbolic Constraints ; and SumReduce , which aggregates partial results. These paths converge in the Cascade Fusion engine, which applies a Hard Veto / Soft Blend logic to ensure safety guarantees while maintaining neural expressivity. The final output is a Trustworthy Score representing a verified, line-rate inference result. 

3.1 Formal problem statement 

We consider an inference task where each input record x ∈ X is processed in a sequence of dataplane primitives. The goal is twofold: first, to compute an attention-enabled representation for each token in streaming contexts under severe resource constraints; second, to produce a trusted anomaly/label score that combines symbolic rules and neural perception in a single, hardware-mappable pipeline. Formally, let X = [ x1, . . . , x T ] be a token sequence and let r(h, t ) ∈ [0 , 1] denote a neural truth score for a candidate relation or anomaly label computed by a neural module parameterized by θ. Let R = {Fq , W q }Mq=1 be a set of symbolic rules with associated weights. The inference objective is to compute for every target unit an aggregated score S that reflects both neural evidence and symbolic constraints, subject to dataplane resource limits. 

3.2 Primitives and notation 

We adopt the Partition/Map/SumReduce abstraction and use the following compact notation: 

Partition( X) = {X1, X 2, . . . , X k}, (1) where X is an input vector or window and Xi denotes the i-th segment produced by the partition primitive. The Map primitive applies a set of per-segment functions: 

Map( F, {Xi}ki=1 ) = {F1(X1), F 2(X2), . . . , F k(Xk)}. (2) The SumReduce primitive aggregates segment outputs element-wise: 

SumReduce( {Yi}ki=1 ) = 

> k

X

> i=1

Yi. (3) where F = {Fi} is a collection of per-partition functions possibly implemented by lookup tables (fuzzy indexing) on the data plane. These primitives follow Pegasus’ conceptualization and are chosen because they map naturally to comparisons, table lookups and additions on programmable switches. 43.3 Transformer attention as dataplane primitives 

We transform the scaled dot-product attention into a formulation that exposes primitives available in modern pro-grammable dataplanes. Let Q ∈ RT ×d, K ∈ RT ×d and V ∈ RT ×dv denote the query, key and value matrices respectively. The canonical attention operator is 

Attn( Q, K, V ) = softmax 

 QK ⊤

√d



V, (4) where the softmax acts row-wise on QK ⊤ 

> √d

.We adopt a kernelized linear attention approximation by introducing a feature map ϕ : Rd → Rm satisfying the approximation 

exp 

 q⊤k

√d



≈ ϕ(q)⊤ϕ(k). (5) where ϕ(·) is chosen to be efficient to compute and to permit quantization. Under Equation (5) a single-query output can be approximated by 

o(q) ≈ ϕ(q)⊤ Φ( K)⊤V 

ϕ(q)⊤ Φ( K)⊤1 , (6) where Φ( K) ∈ RT ×m stacks ϕ(k) across keys and 1 ∈ RT is an all-ones vector. where ϕ(q) ∈ Rm is the query feature, Φ( K)⊤V ∈ Rm×dv aggregates key-weighted values and Φ( K)⊤1 ∈ Rm is the normalization accumulator. The numerator and denominator in Equation (6) are natural SumReduce targets after a per-key Map( ϕ(·)) stage; Partition primitives allow those Maps and partial SumReduces to be tiled to fit dataplane memory. 

3.3.1 Hardware constraint 

The linearized form exposes a significant hardware constraint: the aggregated term Φ( K)⊤V has dimension m × dv

and must be stored or streamed in registers/SRAM while SumReduce proceeds. Let mmax denote the largest feasible feature-dimension under per-flow state limits determined by PHV width and per-flow SRAM budget. Define the per-element storage bitwidth as b. The aggregated matrix requires 

bits _agg = m · dv · b. (7) If b corresponds to a 16-bit quantized representation and typical dataplane per-flow SRAM budgets are on the order of one kilobyte, the storage requirement quickly exceeds per-flow resources. For example, with d = 64 , m = 256 and 

dv = 64 ,

bits _agg = 256 · 64 · 16 = 262 ,144 bits ≈ 32 KB , (8) which is far larger than typical per-flow SRAM budgets (often < 1 KB ) and also exceeds a single PHV lane width such as 4096 bits. Therefore storing Φ( K)⊤V as a dense per-flow matrix in on-chip per-flow state is infeasible for these parameter choices. To make the computation dataplane-friendly we rewrite the aggregation as incremental updates that map naturally to stateful ALU semantics: 

St = St−1 + ϕ(kt) v⊤ 

> t

, (9) 

Zt = Zt−1 + ϕ(kt), (10) where St ∈ Rm×dv accumulates the numerator partials and Zt ∈ Rm accumulates the kernel-mass for normalization. where kt and vt denote the key and value for token t. Equations (9) – (10) express the aggregation as small-rank (outer-product) increments; each increment can be applied atomically by a stateful primitive. In practice the m × dv

matrix St must be unfolded into scalar registers or register arrays and updated entrywise, and the total number of scalar registers required equals m · dv . The per-flow register array size is limited by PHV width, the number of register entries and by the SRAM budget; hence m and dv must be chosen so that 

m · dv · b ≤ per-flow SRAM budget , (11) otherwise the incremental scheme must be implemented with streamed partials or with coarser quantization and sharding across pipelines. The incremental form is naturally implementable on datapl ane hardware that supports stateful arithmetic (for example, Stateful ALU instructions and register arrays). Nevertheless, the matrix unfolding and per-element updates impose hard limits on m and dv when targeting devices such as Tofino. 5Figure 2: Transformation from standard attention to dataplane-native primitives. (a) Architectural comparison between infeasible exact attention and Chimera’s linearized formulation. (b) Temporal unfolding of incremental state updates mapped to stateful ALU operations. 

3.4 Hardware-aware sparse patterns and pipeline parallelism 

Global dense attention is infeasible under tight per-flow state. We therefore employ a hybrid sparse pattern that combines a local sliding window, statically preselected global tokens and compressed token summaries. The compact key/value set used for query qt is defined as a two-layer composition 

eKt = Lt ∪ G (qt), (12) where Lt denotes locally maintained windowed keys and G(qt) denotes the global token subset selected by content-matching. where Lt is implemented as a circular buffer in local SRAM that stores the most recent L tokens, and G(qt) is implemented as a static TCAM-backed set of candidate tokens G ⊂ [1 ..T ] combined with a TCAM match against qt.The local window length must satisfy the per-flow SRAM constraint 

L · d ≤ per-flow SRAM budget , (13) so that the sliding buffer fits in register/SRAM resources. This two-layer design separates a static configuration phase from a lightweight dynamic phase. The static layer encodes a global index set G (for example, top-k frequent or high-value tokens) and is preinstalled in TCAM. The dynamic layer keeps recently observed local indices in an SRAM circular buffer and does not require TCAM reconfiguration. This avoids the infeasible operation of updating TCAM entries on a per-query basis, which is not supported by P4[ 42 ]targets in the tight latency path. In mathematical terms the remapping functions fK and fV in a hardware-aware implementation are implemented as 

eKt = {Kt−L+1 , . . . , K t}∪ { ki : i ∈ G ∧ TCAM .match( qt, k i)}, (14) and analogous definition holds for eVt.where the left set is a contiguous local window implementable in SRAM and the right set is the result of a TCAM lookup that is static across many queries; the union size Nt is constrained so that downstream Map and SumReduce 6Figure 3: Two-layer key selection hierarchy and memory efficiency analysis. (Left) The architectural flow of Chimera: combining temporal locality in the SRAM-based Local Layer with structural prior knowledge in the TCAM-indexed Static Layer to perform sparse key selection. (Right) Comparative memory footprint showing Chimera’s significant reduction in per-flow state compared to dense and linearized baselines. operations fit pipeline resources. Static TCAM entries avoid dynamic table reprogramming while the SRAM circular buffer gives temporal locality without expensive table updates. To increase throughput and avoid single-pipeline bottlenecks, attention heads or head groups are partitioned across physical pipelines. Each pipeline performs Partition/Map/SumReduce on disjoint table shards and writes partial aggregates to a globally accessible accumulator. This tiling reduces per-pipeline state pressure and maps naturally to multi-pipeline architectures. 

3.5 Neuro-symbolic integration on the data plane 

We design a fusion that enforces symbolic guarantees when hard rules match while retaining neural flexibility otherwise. Let snn ∈ R be the neural score produced by the Map+SumReduce chain and let Isym ∈ { 0, 1} indicate a hard symbolic rule hit produced by a TCAM lookup. A cascade fusion suitable for conditional dataplane execution is 

S =

(1, if Isym = 1 ∧ λh = 1 ,σ α s nn + β s sym 

, otherwise , (15) where σ(·) denotes the sigmoid function. where ssym is a continuous soft-symbolic score derived from compact differentiable logic modules, λh ∈ { 0, 1} controls whether a hard symbolic hit vetoes the neural result and α, β ∈ R are learned scaling coefficients. Equation (15) maps to conditional execution on the dataplane: a TCAM hard rule is checked first, and only if no hard veto is present does the pipeline consult SRAM tables or small neural approximations to compute the soft fusion. 7The global structured prior used to bias neural predictions can be expressed via a hinge-loss Markov random field objective: 

p(y | x; W ) ∝ exp   − fW (y, x ),fW (y, x ) = 

> M

X

> q=1

Wq Φq (y, x ), (16) where Φq (y, x ) measures continuous distance-to-satisfaction for the q-th rule grounding and Wq is a rule weight. where Φq is a differentiable potential that aggregates distances of groundings to logical satisfaction and Wq is a scalar weight. The computational cost of evaluating the full HL-MRF energy at line rate is prohibitive because it requires summing over all rule groundings. Therefore HL-MRF training is performed offline to learn rule weights, and the learned weights are compiled into compact SRAM table entries for fast lookup at inference time. During deployment the dataplane does not perform on-the-fly potential summation; instead it retrieves precompiled weights and applies the lightweight fusion in Equation (15). This design ensures that hard symbolic guarantees are enforced with minimal runtime overhead, that soft symbolic influences are available when needed, and that the heavy combinatorial work required to fit complex logical structure into the model is pushed to an offline training stage where richer computation is possible. 

3.6 Mapping optimization and training 

We adopt a two time-scale protocol for mapping optimization and table maintenance that separates light-weight, line-rate adaptations from heavier offline reconfiguration. The fast path operates entirely in the dataplane and maintains low-cost statistics used for incremental refinement. The slow path runs in the control plane and periodically executes full re-clustering and atomic table updates. The dataplane fast path accumulates token-to-centroid occupancy statistics using an exponential moving average. For a centroid index j the occupancy state Cj (t) is updated at packet time t as 

Cj (t) = (1 − η) Cj (t − 1) + η u j (t), (17) where uj (t) ∈ { 0, 1} indicates whether the current token matched centroid j and η ∈ (0 , 1) is the EMA smoothing factor that controls the effective memory of the estimator. Equation (17) runs at line rate and uses scalar SRAM counters that are updated in-place. The control plane slow path triggers a full reclustering and table reload every Tcp seconds. Let Tcp denote the control-plane update interval. At each control-plane epoch the stored occupancy estimates {Cj } are harvested and a centralized clustering procedure computes updated centroids and compressed encodings suitable for P4 table installation. The updated mapping is then deployed to the dataplane in a batched table install operation. To ensure consistent runtime behavior we require atomicity guarantees for control-plane installs. Define ∆tinstall as the elapsed time required to atomically install Nentries table entries into the dataplane. The installation must satisfy the inequality 

∆tinstall < T cp , (18) where a practical design sets Tcp substantially larger than ∆tinstall to avoid observable table churn during inference. For example, for Nentries = 10 4 entries an empirical target of ∆tinstall ≈ 50 ms is sufficiently small relative to a nominal 

Tcp = 60 s , hence ∆tinstall ≪ Tcp holds in typical deployments. Quantization and table-size constraints are enforced during clustering. Let b denote the per-entry bitwidth after fixed-point quantization and let Mtbl denote the maximum table memory budget in bits allocated for the Map primitive. Then the clustering procedure must produce Nentries 

satisfying 

Nentries · b ≤ Mtbl , (19) where the left-hand side is the total memory consumption of the compressed table entries. Equation (19) guides a greedy clustering routine that trades centroid count for per-entry bitwidth and for match-range encodings suitable for P4 range or TCAM rules. To avoid destabilizing frequent control-plane updates we constrain the update cadence and the magnitude of changes between successive installs. Define a similarity metric ∆map between old and new mapping tables. The control-plane reconfiguration proceeds only when 

∆map > τ map (20) where τmap is a conservative threshold that prevents small fluctuations from triggering a full reload. The combination of Equations (17) through (20) yields stable adaptation: line-rate statistics track short-term shifts while control-plane re-clustering corrects longer-term distributional drift at timescale Tcp . Finally, table installation procedures must be engineered to preserve atomicity and to minimize transient mismatches. The installer batches updates and validates 

∆tinstall empirically on target hardware to ensure Equation (18) is satisfied. These measures keep per-query table churn negligible and prevent runaway oscillation between data- and control-plane representations. 8Table 1: Comparison of classification accuracy across different methods. The table spans two columns. Method Input Scale (b) Model Size (Kb) PeerRush[43] CICIOT[44] ISCXVPN[45] PR RC F1 PR RC F1 PR RC F1 Leo (Decision Tree)[46] 128 - 0.8720 0.8776 0.8728 0.7910 0.8072 0.7848 0.7338 0.7797 0.7475 N3IC (binary MLP)[47] 128 24.4 0.8217 0.8308 0.8241 0.7855 0.7877 0.7745 0.6688 0.6521 0.6388 MLP-B 128 34.3 0.8823 0.8826 0.8823 0.8555 0.8615 0.8581 0.7676 0.7552 0.7574 BoS (binary RNN)[11] 18 25.6 0.8677 0.8696 0.8678 0.8311 0.8253 0.8276 0.7033 0.7089 0.6907 RNN-B[11] 128 10.9 0.9083 0.9100 0.9090 0.8707 0.8708 0.8707 0.7848 0.7658 0.7617 CNN-B[48] 128 11.4 0.9051 0.9069 0.9057 0.8861 0.8657 0.8659 0.7706 0.7600 0.7520 CNN-M[48] 128 974 0.9201 0.9220 0.9207 0.8821 0.8839 0.8829 0.7942 0.7897 0.7780 CNN-L[48] 3840 6083 0.9967 0.9966 0.9966 0.9391 0.9377 0.9380 0.9868 0.9877 0.9872 Chimera 5000 7000 0.9980 0.9980 0.9980 0.9500 0.9500 0.9500 0.9900 0.9900 0.9900 

3.7 Algorithm: Grounding, Map update and Inference 

We separate the workflow into an offline preprocessing phase that performs grounding expansion, HL-MRF weight optimization and mapping table construction, and a runtime inference phase that executes only dataplane-safe primitives. Offline procedures may iterate and perform backpropagation through simulated Map+ SumReduce chains. Runtime procedures are strictly non-iterative and use preinstalled tables and compact state updates. The offline loop confines iterative grounding and HL-MRF optimization to the control plane and to training-time computation, thereby avoiding runtime iterative expansion that is infeasible on P4 targets. Runtime operations are composed exclusively of primitives and state updates that map directly to Partition, Map and SumReduce, and to atomic stateful register updates as in Eqs. (9) – (10) . Conditions for table deployment reference Eq. (19) to respect memory budgets and Eq. (18) to guarantee safe installs. This restructuring preserves the original algorithmic intent while making the pipeline implementable on programmable switches and compatible with the hardware-aware constraints described earlier. 

Algorithm 1 Hybrid Preprocessing and Runtime Inference 

1: Input: Observations O, rules {Fq }, initial centroids/tables, neural params θ

2: Output: Deployed Map tables; per-flow fused score S

3: /* Offline (control-plane / training) */ 

4: for epoch = 1 to Epochs do 

5: Expand candidate groundings and retain high-confidence set Vhigh .

6: Evaluate neural scores r(·; θ) with Map+SumReduce using linearized attention (Eq. (6)). 

7: Update rule weights W via HL-MRF objective (Eq. (16) ) and refine centroids by backprop; enforce table-size constraint (Eq. (19)). 

8: If mapping change ∆map > τ map then prepare batched install and deploy only when ∆tinstall < T cp (Eq. (18) ). 

9: end for 

10: /* Runtime (data-plane safe, per-packet) */ 

11: for each packet/stream do 

12: Partition input and form remapped set eKt, eVt as in Eq. (14). 

13: Obtain per-key features ϕ(k) and values v via Map lookups or compact MLP. 

14: Update accumulators atomically: St ← St−1 + ϕ(kt) v⊤ 

> t

, Zt ← Zt−1 + ϕ(kt). See Eqs. (9)–(10). 

15: Compute neural score snn using aggregated partials (Eq. (6)); evaluate TCAM for Isym .

16: Produce fused score S via cascade fusion (Eq. (15)) and emit result. 

17: end for 3.8 Implementation notes and hardware mapping 

Partition, Map and SumReduce map to P4 constructs as follows: Partition is realized by field extraction and meta-field packing; Map is implemented using fuzzy mapping tables backed by CRC-encoded range rules and SRAM centroids; SumReduce is realized through staged additions across pipeline stages. Consecutive Range Coding and CRC techniques convert clustering tree ranges into ternary matches for PISA-compatible tables. For hard symbolic matches we use TCAM entries for exact signature lookup while soft symbolic computations and learned centroids live in SRAM-lookups with occasional control-plane refresh. These implementation patterns are aligned with the Pegasus[ 3 ] implementation. 9Table 2: Hardware resource utilization for different methods. The table is formatted for a single column. 

## Models Stateful bits/flow SRAM TCAM Bus 

## Leo[46] 80 2.44% 21.67% 3.55% BoS[11] 72 2.81% 0% 0.74% MLP-B 80 7.75% 12.92% 29.45% RNN-B[11] 240 7.38% 23.33% 33.36% CNN-B[48] 72 5.56% 7.08% 13.16% CNN-M[48] 72 3.50% 6.67% 3.98% CNN-L[48] 44 7.12% 13.33% 7.11% AutoEncoder[49] 240 5.06% 7.92% 7.23% Chimera 30 5.00% 10.00% 5.00% 

# 4 Experiment 

This section presents the evaluation of Chimera. We describe the testbed and datasets, list baseline methods, define metrics, and report results for supervised classification, scalability, unsupervised anomaly detection, and hardware cost. Where appropriate we contrast dataplane execution with a control-plane implementation to highlight throughput and latency trade-offs. 

4.1 Testbed and datasets 

The Chimera prototype was deployed on a programmable-switch testbed connected to two Linux servers. One server replays recorded packet captures for traffic generation while the other collects switch outputs. We evaluate on three public traffic classification datasets commonly used in prior work. PeerRush[ 43 ] contains peer-to-peer application flows. CICIOT2022[ 44 ] contains IoT traces labeled by device operating state. ISCXVPN2016[ 45 ] contains VPN-encrypted flows spanning multiple application types. For each dataset flows are split by five-tuple into 75% training, 10% validation and 15% testing. 

4.2 Baselines 

Comparisons include representative dataplane and simulated baselines. Leo[ 46 ] denotes a tree-based approach imple-mented on the switch. BoS[ 11 ] refers to a binarized RNN[ 11 ] that runs on the dataplane. N3IC[ 47 ] is a binary MLP evaluated in software for the largest published configuration. Additional baselines are compact MLP and CNN[ 48 ]variants, and an RNN baseline. All methods use identical data partitions and comparable preprocessing so reported differences reflect model and mapping choices. 

4.3 Metrics 

For classification we report packet-level macro-accuracy defined as the mean F1 score across classes, together with precision and recall. For unsupervised detection we report area under the ROC curve. Hardware costs are reported as per-flow stateful bits and the percentage consumption of SRAM, TCAM and the Action Data Bus. Throughput and latency are measured for dataplane line-rate execution and for a control-plane implementation running on an Intel Xeon CPU with GPU acceleration. 

4.4 Classification accuracy 

Table 1 summarizes classification performance across datasets. Chimera attains the best macro F1 scores while supporting larger input windows than most baselines. The decision-tree baseline performs competitively on engineered statistical features but is outperformed by neural approaches on raw packet sequences. Models exposed to wider input contexts obtain higher accuracy, and Chimera’s primitives enable this scaling by partitioning per-packet processing and compressing per-flow state. 10 4.5 Visualization Figure 4 illustrates the high-level mapping of Transformer attention onto the Partition/Map/SumReduce primitives used by Chimera. The panel emphasizes how attention is linearized and unfolded in time so that incremental updates can be implemented with stateful arithmetic on programmable switches. Figure 5 shows the accuracy–resource trade-off across methods. The convex hull (Pareto) highlights that Chimera attains superior F1 while using a substantially smaller per-flow state budget compared to baseline implementations. Figure 6 compares how model performance varies with the input window size. Chimera achieves most of its performance gains by the 2 KB scale while keeping per-flow state growth modest. Figure 7 quantifies end-to-end throughput. Dataplane execution with Chimera yields orders-of-magnitude higher packet processing capacity relative to control-plane CPU/GPU baselines. Figure 8 presents latency summaries. The dataplane implementation delivers sub-microsecond median latency and very tight tail behavior compared to control-plane alternatives. Figure 9 reports ROC performance for unsupervised detection. High AUC values demonstrate that compact Chimera-implemented AutoEncoders effectively flag injected malware and DoS traces under realistic operating conditions. Figure 10 visualizes the system-level sensitivity to adaptation cadence and resource allocation. The left panel identifies a broad stable basin for (η, T cp ); the right panel makes explicit the trade-off between representational richness and per-flow state. Figure 11 shows a day-long stability trace demonstrating that the two-timescale protocol (fast EMA in the data plane plus periodic control-plane re-clustering) prevents long-term drift while preserving responsiveness to distributional changes. 

Figure 4: Transformation from standard attention to dataplane-native primitives. (a) Architectural comparison between infeasible exact attention and Chimera’s linearized formulation. (b) Temporal unfolding of incremental state updates mapped to stateful ALU operations. 

4.6 Scalability 

We evaluate how model capacity and per-flow storage affect accuracy and concurrency. Larger models offer diminishing gains but still improve peak performance until dataset or feature saturation occurs. Chimera’s partitioning and fuzzy-index mapping reduce per-flow storage requirements so that large input windows can be supported with modest per-flow budgets. This enables a favorable trade-off between model expressiveness and the number of concurrent flows the dataplane can sustain. 11 Figure 5: Pareto frontier: CICIOT F1 versus per-flow state bits. Chimera is highlighted as Pareto-optimal, providing higher accuracy with lower per-flow state than competing dataplane models. 

Figure 6: Input-scale sensitivity: CICIOT F1 as a function of input window size. Chimera scales sub-linearly in state while benefiting from wider contexts. 

4.7 Unsupervised anomaly detection 

An AutoEncoder implemented with Chimera primitives is trained on benign traffic and tested with injected malware and DoS samples. Detection is based on reconstruction error and measured by AUC. Results show high AUC values across datasets, demonstrating that compact reconstruction models built with primitive fusion can detect previously unseen attacks. Detected anomalies can trigger lightweight in-network mitigations. 

4.8 Hardware resource utilization 

Table 2 reports per-flow state usage and the fraction of on-chip resources consumed by each method. Chimera requires fewer stateful bits per flow than many baselines and consumes moderate SRAM and TCAM percentages. The two-layer selection strategy and fuzzy mapping account for efficient resource utilization while preserving model capacity. 12 Figure 7: Throughput comparison (Gbps). Chimera running on a Tofino target achieves line-rate throughput, significantly exceeding CPU/GPU implementations. 

Figure 8: Latency distributions (log-scale boxplots approximating median and P99). Chimera attains microsecond-level median and tail latencies with minimal jitter. 

4.9 Dataplane versus control-plane execution 

We implemented full-precision inference on an edge server equipped with CPU and GPUs to compare accuracy and throughput. Dataplane execution yields dramatically higher throughput and lower latency while incurring small accuracy reductions relative to full-precision control-plane runs. Larger models suffer the smallest relative accuracy loss, which indicates that richer input representations reduce approximation error introduced by mapping and quantization. 

4.10 Key observations 

Chimera matches or exceeds baseline accuracy on standard datasets while operating within realistic dataplane resource budgets. The mapping and primitive fusion techniques allow support for large input contexts and high concurrency. 13 Figure 9: ROC curves for unsupervised anomaly detection on PeerRush, CICIOT and ISCXVPN. The AutoEncoder implemented with Chimera primitives produces high AUC values, indicating robust separation of anomalous traffic. 

Figure 10: Hyperparameter sensitivity visualizations. Left heatmap: EMA factor η vs control-plane interval Tcp . Right heatmap: query feature dimension m vs value dimension dv . The annotated operating point marks Chimera’s chosen configuration. Unsupervised anomaly detection is practical with Chimera primitives. Dataplane deployment offers compelling throughput and latency benefits with acceptable accuracy trade-offs. 

4.11 Ablation Studies 

We conducted a comprehensive ablation study on the CICIOT dataset to validate the architectural choices in Chimera, with detailed results summarized in Table 3. The kernelized attention formulation achieves near-identical accuracy to exact softmax while satisfying dataplane constraints; the latter exhausts memory and incurs millisecond-scale latency, rendering it infeasible for line-rate operation. Our hybrid key selection, combining SRAM-resident local windows with TCAM-backed global indices, outperforms either pure strategy by capturing both temporal locality and long-range dependencies without the resource demands of dense attention. The cascade fusion mechanism, which permits hard symbolic vetoes alongside neural inference, proves more robust than neural-only or rule-only alternatives under adversarial conditions. Incremental aggregation eliminates the jitter inherent in batch recomputation, and balanced quantization prevents accumulator overflow without compromising flow capacity. These findings collectively demonstrate that Chimera’s integration of linearized attention, bifurcated key selection, and temporal decoupling achieves optimal trade-offs among accuracy, throughput, and hardware efficiency. 

4.12 Hyperparameter Sensitivity and System Stability 

The co-design of Chimera’s hardware and software requires a strategic navigation of the configuration space, specifically concerning memory allocation, feature dimensionality, and adaptation dynamics. Our extensive sweeps across the CICIOT and PeerRush datasets reveal that the representational richness of linearized attention is governed by a critical threshold in the feature-value product. Beyond this point, increasing parameters yields diminishing returns in classification accuracy while rapidly exhausting the SRAM budget necessary for high-concurrency flow tracking. Observations suggest that the fidelity of value vectors is more decisive for traffic categorization than query granularity. Similarly, the temporal receptive field provided by the local circular buffer demonstrates a performance plateau; once the window sufficiently captures protocol handshakes, further expansion merely increases state overhead without commensurate gains. Operational stability is further anchored by the temporal coordination between the data and 14 Figure 11: 24-hour deployment stability. Chimera with two-timescale adaptation maintains near-constant F1 across diurnal load; Fast-Only and Static-Map baselines exhibit drift or abrupt drops. Table 3: Ablation of core architectural components on CICIOT. Configurations marked with † require control-plane offloading and violate line-rate constraints. 

## Configuration F1 Bits/Flow TCAM Latency Throughput 

## Attention Mechanism 

## Exact Softmax † 0.952 > 10 4 0 2840 μs 0.1Gbps Linearized (Chimera) 0.950 30 256 0.9 μs 100Gbps Random Fourier Features 0.945 30 256 0.9 μs 100Gbps 

## Key Selection Strategy 

## Local-Only ( Lt) 0.914 18 0 0.8 μs 100Gbps Global-Only ( G) 0.938 24 512 1.2 μs 100Gbps Hybrid (Chimera) 0.950 30 256 0.9 μs 100Gbps Dense Attention † 0.961 > 10 4 0 2840 μs 0.1Gbps 

## Neuro-Symbolic Fusion 

## Neural-Pure (no symbolic) 0.942 28 0 0.9 μs 100Gbps Symbolic-Pure (fixed rules) 0.895 20 1024 0.7 μs 100Gbps Soft-Fusion ( λh = 0 ) 0.949 30 256 0.9 μs 100Gbps Cascade with Hard Veto (Chimera) 0.950 30 256 0.9 μs 100Gbps 

## Aggregation Strategy 

## Batch Recompute 0.947 42 256 1.1 μs 90Gbps Incremental (Chimera) 0.950 30 256 0.9 μs 100Gbps control planes. We found that the exponential moving average factor must be carefully calibrated to ensure the system remains responsive to distributional shifts without becoming overly sensitive to transient traffic bursts. Furthermore, the frequency of control-plane updates serves as a pivot between inference freshness and system overhead. Rapid update cadences induce table churn and installation jitter, whereas excessive intervals allow feature drift to degrade model reliability. By employing asymmetric quantization through the allocation of higher precision to accumulators than to normalization mass, while simultaneously sharding attention heads across a dual-pipeline configuration, Chimera achieves a robust equilibrium. These results confirm that the system resides within a broad stability basin. This allows the architectural constraints to accommodate diverse deployment scenarios while maintaining Pareto-optimal efficiency across throughput and hardware occupancy. 15 Table 4: Hyperparameter sensitivity of feature dimensions and quantization on CICIOT. Configurations marked with †

violate Eq. (11) per-flow SRAM budget ( < 1KB). Bold indicates Chimera’s operational setting.                                                                

> Query Features (m)Value Dim (dv)Quantization Aggregated State Budget Ratio CICIOT F1 PeerRush F1
> 64 32 16-bit 2KB 200% 0.9230 0.9450 64 64 16-bit 4KB 400% 0.9380 0.9720
> 128 32 16-bit 4KB 400% 0.9470 0.9840
> 128 64 16-bit 8KB 800% 0.9480 0.9840 128 32 8-bit 2KB 200% 0.9150 0.9380 128 64 8-bit 4KB 400% 0.9280 0.9560 256 32 16-bit 8KB 800% 0.9490 0.9870 256 64 16-bit 16KB 1600% 0.9500 0.9900 256 128 16-bit †32KB 3200% 0.9510 0.9910

Table 5: System stability across EMA factors and control-plane update intervals. η denotes EMA smoothing factor from Eq. (17); Tcp denotes control-plane interval from Eq. (18). Budget ratio indicates ∆tinstall /T cp .                                                  

> EMA Factor (η)Memory Depth Update Interval (Tcp )Budget Ratio Table Churn CICIOT F1 Stability
> 0.05 20 epochs 60s 0.08% Negligible 0.9460 Slow response
> 0.10 10 epochs 60s 0.08% Negligible 0.9500 Balanced
> 0.30 3 epochs 60s 0.08% Negligible 0.9480 Noise sensitive 0.50 2 epochs 60s 0.08% Low 0.9410 Oscillation risk 0.10 10 epochs 10s 0.45% High 0.9480 Severe churn 0.10 10 epochs 300s 0.017% Negligible 0.9490 Minor drift 0.10 10 epochs 1800s 0.003% Negligible 0.9410 Feature drift

# 5 Conclusion 

Chimera establishes a practical pathway for combining attention-driven perception with rule-based enforcement on commodity programmable switches by translating transformer-style attention into incremental, kernelized Parti-tion/Map/SumReduce primitives, by employing a hybrid key-selection hierarchy that pairs an SRAM-backed local window with TCAM-indexed static tokens, and by compiling symbolic constraints into compact table encodings consumed by a cascade fusion unit. Comprehensive experiments and ablation studies demonstrate that these mappings and fusion mechanisms preserve high detection and classification fidelity relative to richer control-plane baselines while operating within realistic TCAM and SRAM budgets, supporting multi-pipeline tiling and sustaining line-rate forward-ing latency; additional robustness tests reveal graceful degradation under noisy inputs and constrained quantization. Future work will investigate adaptive rule synthesis and network-wide orchestration mechanisms to extend Chimera’s trust guarantees across multi-hop topologies. 

# References 

[1] Changgang Zheng, Xinpeng Hong, Damu Ding, Shay Vargaftik, Yaniv Ben-Itzhak, and Noa Zilberman. In-network machine learning using programmable network devices: A survey. IEEE Communications Surveys & Tutorials ,26(2):1171–1200, 2023. [2] Wai-Xi Liu, Cong Liang, Yong Cui, Jun Cai, and Jun-Ming Luo. Programmable data plane intelligence: advances, opportunities, and challenges. IEEE Network , 37(5):122–128, 2022. [3] Yinchao Zhang, Su Yao, Yong Feng, Kang Chen, Tong Li, Zhuotao Liu, Yi Zhao, Lexuan Zhang, Xiangyu Gao, Feng Xiong, et al. Pegasus: A universal framework for scalable deep learning inference on the dataplane. In 

Proceedings of the ACM SIGCOMM 2025 Conference , pages 692–706, 2025. [4] Aristide Tanyi-Jong Akem, Michele Gucciardo, and Marco Fiore. Flowrest: Practical flow-level inference in programmable switches with random forests. In IEEE INFOCOM 2023-IEEE Conference on Computer Communications , pages 1–10. IEEE, 2023. [5] Aristide Tanyi-Jong Akem, Beyza Bütün, Michele Gucciardo, and Marco Fiore. Henna: Hierarchical machine learning inference in programmable switches. In Proceedings of the 1st International Workshop on Native Network Intelligence , pages 1–7, 2022. 16 [6] Xiaoquan Zhang, Lin Cui, Fung Po Tso, Wenzhi Li, and Weijia Jia. In3: A framework for in-network computation of neural networks in the programmable data plane. IEEE Communications Magazine , 62(4):96–102, 2024. [7] Kaiyi Zhang, Changgang Zheng, Nancy Samaan, Ahmed Karmouch, and Noa Zilberman. Design, implementation, and deployment of multi-task neural networks in programmable data-planes. IEEE Transactions on Network and Service Management , 23:740–755, 2025. [8] Muhammad Irfan, Hang Hu, Myung J Lee, Arslan Qadeer, Yang G Kim, Kazi Ahmed, and Daiki Nobayashi. Flow-level bandwidth allocation on p4 tofino switch with in-network drl inference. In 2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC) , pages 0204–0209. IEEE, 2025. [9] Vishal Shrivastav. Stateful multi-pipelined programmable switches. In Proceedings of the ACM SIGCOMM 2022 Conference , pages 663–676, 2022. [10] Shaowei Xu, Shengrui Lin, Hongyan Liu, Dong Zhang, Jinqi Zhang, and Chunming Wu. Tbnn: Lookup tables-based optimization for in-network binary neural networks. In 2025 IEEE/ACM 33rd International Symposium on Quality of Service (IWQoS) , pages 1–10. IEEE, 2025. [11] Jinzhu Yan, Haotian Xu, Zhuotao Liu, Qi Li, Ke Xu, Mingwei Xu, and Jianping Wu. {Brain-on-Switch }: Towards advanced intelligent network data plane via {NN-Driven } traffic analysis at {Line-Speed }. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24) , pages 419–440, 2024. [12] Mai Zhang, Lin Cui, Xiaoquan Zhang, Fung Po Tso, Zhang Zhen, Yuhui Deng, and Zhetao Li. Quark: Imple-menting convolutional neural networks entirely on programmable data plane. In IEEE INFOCOM 2025-IEEE Conference on Computer Communications , pages 1–10. IEEE, 2025. [13] Wenhua Ye, Xu Zhou, Joey Zhou, Cen Chen, and Kenli Li. Accelerating attention mechanism on fpgas based on efficient reconfigurable systolic array. ACM Transactions on Embedded Computing Systems , 22(6):1–22, 2023. [14] Richie Li and Sicheng Chen. Design and implementation of an fpga-based hardware accelerator for transformer. 

arXiv preprint arXiv:2503.16731 , 2025. [15] Xiangyu Gao, Tong Li, Yinchao Zhang, Ziqiang Wang, Xiangsheng Zeng, Su Yao, and Ke Xu. Fenix: Enabling in-network dnn inference with fpga-enhanced programmable switches. arXiv preprint arXiv:2507.14891 , 2025. [16] Bikram Pratim Bhuyan, Amar Ramdane-Cherif, Ravi Tomar, and TP Singh. Neuro-symbolic artificial intelligence: a survey. Neural Computing and Applications , 36(21):12809–12844, 2024. [17] Chen Shengyuan, Yunfeng Cai, Huang Fang, Xiao Huang, and Mingming Sun. Differentiable neuro-symbolic reasoning on large-scale knowledge graphs. Advances in Neural Information Processing Systems , 36:28139–28154, 2023. [18] Felix Petersen, Hilde Kuehne, Christian Borgelt, Julian Welzel, and Stefano Ermon. Convolutional differentiable logic gate networks. Advances in Neural Information Processing Systems , 37:121185–121203, 2024. [19] Alice Bizzarri, Chung-En Yu, Brian Jalaian, Fabrizio Riguzzi, and Nathaniel D Bastian. Neuro-symbolic integration for open set recognition in network intrusion detection. In International Conference of the Italian Association for Artificial Intelligence , pages 50–63. Springer, 2024. [20] Ahmad Almadhor, Shtwai Alsubai, Abdullah Al Hejaili, Zeineb Klai, Belgacem Bouallegue, and Urban Kovac. Designing a neuro-symbolic dual-model architecture for explainable and resilient intrusion detection in iot networks. Scientific Reports , 15(1):42786, 2025. [21] Srishti Dey, Aishik Paul, Arijit Mukherjee, Deborsi Basu, and Uttam Ghosh. Densainet: Ddos attack detection using neuro-symbolic ai in softwarized networks. In 2025 IEEE 6th India Council International Subsections Conference (INDISCON) , pages 1–6. IEEE, 2025. [22] Benjamin E Ujcich. A systems security approach for emerging programmable network architectures. IEEE Security & Privacy , 2025. [23] Enkeleda Bardhi, Mauro Conti, and Riccardo Lazzeretti. Is ai a trick or t (h) reat for securing programmable data planes? IEEE Network , 2024. [24] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. In International Conference on Machine Learning , pages 40605–40623. PMLR, 2023. 17 [25] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision , pages 3531–3539, 2021. [26] Nazim Altar Koca, Anh Tuan Do, and Chip-Hong Chang. Hardware-efficient softmax approximation for self-attention networks. In 2023 IEEE International Symposium on Circuits and Systems (ISCAS) , pages 1–5. IEEE, 2023. [27] Elie F Kfoury, Jorge Crichigno, and Elias Bou-Harb. An exhaustive survey on p4 programmable data plane switches: Taxonomy, applications, challenges, and future trends. IEEE access , 9:87094–87155, 2021. [28] Mohammad Firas Sada, John Graham, Mahidhar Tatineni, Dmitry Mishin, Thomas DeFanti, and Frank Würthwein. Real-time in-network machine learning on p4-programmable fpga smartnics with fixed-point arithmetic and taylor approximations. In Practice and Experience in Advanced Research Computing 2025: The Power of Collaboration ,pages 1–5. 2025. [29] Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. Advances in Neural Information Processing Systems , 34:22795–22807, 2021. [30] Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr \"om method. Advances in Neural Information Processing Systems , 34:2122–2135, 2021. [31] Chao Lou, Zixia Jia, Zilong Zheng, and Kewei Tu. Sparser is faster and less is more: Efficient sparse attention for long-range transformers. arXiv preprint arXiv:2406.16747 , 2024. [32] Zhe Zhou, Junlin Liu, Zhenyu Gu, and Guangyu Sun. Energon: Toward efficient acceleration of transformers using dynamic sparse attention. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems ,42(1):136–149, 2022. [33] Mijin Go, Joonho Kong, and Arslan Munir. Linearization weight compression and in-situ hardware-based decompression for attention-based neural machine translation. IEEE Access , 11:42751–42763, 2023. [34] Artur d’Avila Garcez, Marco Gori, Luis C Lamb, Luciano Serafini, Michael Spranger, and Son N Tran. Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning. 

arXiv preprint arXiv:1905.06088 , 2019. [35] Felix Petersen, Christian Borgelt, Hilde Kuehne, and Oliver Deussen. Deep differentiable logic gate networks. 

Advances in Neural Information Processing Systems , 35:2006–2018, 2022. [36] Youngjae Min and Navid Azizan. Hardnet: Hard-constrained neural networks with universal approximation guarantees. arXiv preprint arXiv:2410.10807 , 2024. [37] MohammadErfan Jabbari, Abhishek Duttagupta, Claudio Fiandrino, Leonardo Bonati, Salvatore D’Oro, Michele Polese, Marco Fiore, and Tommaso Melodia. Sia: Symbolic interpretability for anticipatory deep reinforcement learning in network control. arXiv preprint arXiv:2601.22044 , 2026. [38] David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, et al. Towards guaranteed safe ai: A framework for ensuring robust and reliable ai systems. arXiv preprint arXiv:2405.06624 , 2024. [39] Yue Zheng, Chip-Hong Chang, Shih-Hsu Huang, Pin-Yu Chen, and Stjepan Picek. An overview of trustworthy ai: advances in ip protection, privacy-preserving federated learning, security verification, and gai safety alignment. 

IEEE Journal on Emerging and Selected Topics in Circuits and Systems , 2024. [40] Yong Feng, Hanyi Zhou, Shuxin Liu, Zhikang Chen, Haoyu Song, and Bin Liu. Enhancing stateful processing in programmable data planes: Model and improved architecture. IEEE/ACM Transactions on Networking , 2024. [41] Jiwon Kim, Dave Jing Tian, and Benjamin E Ujcich. Chimera: Fuzzing p4 network infrastructure for multi-plane bug detection and vulnerability discovery. In 2025 IEEE Symposium on Security and Privacy (SP) , pages 3088–3106. IEEE, 2025. [42] Pat Bosshart, Dan Daly, Glen Gibb, Martin Izzard, Nick McKeown, Jennifer Rexford, Cole Schlesinger, Dan Talayco, Amin Vahdat, George Varghese, et al. P4: Programming protocol-independent packet processors. ACM SIGCOMM Computer Communication Review , 44(3):87–95, 2014. 18 [43] Babak Rahbarinia, Roberto Perdisci, Andrea Lanzi, and Kang Li. Peerrush: Mining for unwanted p2p traffic. In International conference on detection of intrusions and malware, and vulnerability assessment , pages 62–82. Springer, 2013. [44] Sajjad Dadkhah, Hassan Mahdikhani, Priscilla Kyei Danso, Alireza Zohourian, Kevin Anh Truong, and Ali A Ghorbani. Towards the development of a realistic multidimensional iot profiling dataset. In 2022 19th Annual International Conference on Privacy, Security & Trust (PST) , pages 1–11. IEEE, 2022. [45] Gerard Draper-Gil, Arash Habibi Lashkari, Mohammad Saiful Islam Mamun, and Ali A Ghorbani. Character-ization of encrypted and vpn traffic using time-related. In Proceedings of the 2nd international conference on information systems security and privacy (ICISSP) , pages 407–414, 2016. [46] Syed Usman Jafri, Sanjay Rao, Vishal Shrivastav, and Mohit Tawarmalani. Leo: Online {ML-based } traffic classification at {Multi-Terabit } line rate. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24) , pages 1573–1591, 2024. [47] Giuseppe Siracusano, Salvator Galea, Davide Sanvito, Mohammad Malekzadeh, Gianni Antichi, Paolo Costa, Hamed Haddadi, and Roberto Bifulco. Re-architecting traffic analysis with neural network interface cards. In 

19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , pages 513–533, 2022. [48] Ye Zhang and Byron C Wallace. A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 253–263, 2017. [49] Yisroel Mirsky, Tomer Doitshman, Yuval Elovici, and Asaf Shabtai. Kitsune: an ensemble of autoencoders for online network intrusion detection. arXiv preprint arXiv:1802.09089 , 2018. 

# A Theoretical Guarantees 

A.1 Notation and assumptions 

We use the following notation throughout the section. Let T denote the token sequence length, d the original embedding dimension, dv the value dimension, m the feature-map dimension used for kernel linearization, and b the quantization bit-width. For any vector or matrix X, ∥X∥2 denotes its spectral norm and ∥X∥F its Frobenius norm. We denote by ϕ : Rd → Rm the feature map used to linearize the attention kernel and assume the following boundedness and regularity conditions hold: 

∀x ∈ Rd, ∥ϕ(x)∥2 ≤ Bϕ, ∥x∥2 ≤ R, (21) where Bϕ > 0 and R > 0 are constants. We also assume each value vector v satisfies ∥v∥2 ≤ Rv for some Rv > 0.These conditions hold for commonly used fixed-point or normalized embeddings after preprocessing. 

A.2 Kernel approximation error bound Theorem A.1 (Kernel-feature approximation; probabilistic bound) . Let k(q, k ) = exp  q⊤k/ √d be the target attention kernel. Suppose ϕ(·) is constructed via i.i.d. positive random features (for example, the Performer-style positive random features) yielding an unbiased estimator 

Eϕ(q)⊤ϕ(k) = k(q, k ). (22) Assume |ϕ(q)⊤ϕ(k)| ≤ C almost surely for all q, k in the domain. Then for any ε ∈ (0 , C ) and failure probability 

δ ∈ (0 , 1) ,

Pr 



ϕ(q)⊤ϕ(k) − k(q, k ) ≥ ε



≤ 2 exp 



− mε 2

2C2



. (23) Consequently, for a set of at most N query-key pairs, a union bound yields that with probability at least 1 − δ the approximation error is uniformly bounded by ε provided 

m ≥ 2C2

ε2 log 

 2Nδ



. (24) 19 Proof. For a fixed pair (q, k ), define the i.i.d. random variables 

Xj := ϕj (q)ϕj (k), j = 1 , . . . , m, (25) where ϕj (·) denotes the j-th coordinate of ϕ(·). By construction E[Xj ] = k(q, k )/m and Sm := Pmj=1 Xj =

ϕ(q)⊤ϕ(k). Under the boundedness assumption |Xj | ≤ C/m it follows that |Sm − k(q, k )| ≤ Pmj=1 |Xj − E[Xj ]|

and Hoeffding’s inequality applies to the mean of bounded independent variables. Applying Hoeffding yields 

Pr 



|Sm − k(q, k )| ≥ ε



≤ 2 exp 



− 2m2ε2

Pmj=1 (2 C/m )2



= 2 exp 



− mε 2

2C2



. (26) This proves Equation (23). For N pairs, apply the union bound to obtain the condition on m in Equation (24). □

where k(q, k ) is the target kernel, Xj are the i.i.d. feature product terms, C is an almost-sure bound on their magnitude, 

m is the feature dimension and N is the number of pairs to control uniformly. 

A.3 Spectral-norm bound for linearized attention Theorem A.2 (Spectral-norm approximation for linearized attention) . Let Q, K ∈ RT ×d and V ∈ RT ×dv . Denote by 

Attn( Q, K, V ) = softmax 

 QK ⊤

√d



V (27) the exact attention output and by 

]Attn( Q, K, V ) := D(Q, K )−1Φ( Q) Φ( K)⊤V  (28) the linearized approximation where Φ( X) ∈ RT ×m stacks ϕ(x) row-wise and D(Q, K ) ∈ RT ×T is a diagonal normalization matrix with entries Dii = ϕ(qi)⊤ Φ( K)⊤1. Suppose that for all queries qi the normalization satisfies 

Dii ≥ γ > 0. If the kernel approximation error satisfies 

max  

> i,j

ϕ(qi)⊤ϕ(kj ) − k(qi, k j ) ≤ ε, (29) then the spectral norm error is bounded by 

Attn( Q, K, V ) − ]Attn( Q, K, V ) 2 ≤√T ε γ ∥V ∥2 + εγ ∥V ∥F . (30) 

Proof. Write the exact attention matrix as 

A := softmax   QK ⊤

> √d

 and the feature-based approximation inner matrix as eAij := ϕ(qi)⊤ϕ(kj ) 

> Zi

where Zi =

ϕ(qi)⊤(Φ( K)⊤1) is the feature normalizer. The entrywise difference satisfies 

|Aij − eAij | ≤ |k(qi, k j ) − ϕ(qi)⊤ϕ(kj )|

Zi

+ |k(qi, k j )| · 1

eZi

− 1

Zi

, (31) where eZi is the exact softmax row-sum (i.e., eZi = P 

> j

k(qi, k j )). Under the assumption Zi ≥ γ and eZi ≥ γ, the first term is at most ε/γ . The second term can be bounded by noting that 

1

eZi

− 1

Zi

= |Zi − eZi|

Zi eZi

≤

P 

> j

|ϕ(qi)⊤ϕ(kj ) − k(qi, k j )|

γ2 ≤ T ε γ2 . (32) Combining these gives the uniform entrywise bound 

|Aij − eAij | ≤ εγ + T ε γ2 |k(qi, k j )|. (33) Let E := A − eA. Then 

∥EV ∥2 ≤ ∥ E∥2∥V ∥2 ≤ ∥ E∥F ∥V ∥2. (34) 20 Using the entrywise bound and that 

∥E∥F ≤ √T 2 max ij |Eij | = T max ij |Eij | yields 

∥E∥F ≤ T

 εγ + T ε γ2 max  

> ij

|k(qi, k j )|



. (35) Combining Equations (34) and (35) and simplifying under the typical normalization max ij |k(qi, k j )| ≤ 1 gives a bound of the form 

∥EV ∥2 ≤√T ε γ ∥V ∥2 + εγ ∥V ∥F , (36) which proves Equation (30). □

where A is the exact attention weight matrix, eA is the feature-based approximation, E is their difference, Zi and eZi

denote feature and exact normalizers respectively, and γ is the uniform positive lower bound on these normalizers. 

A.4 Incremental updates: numerical stability and quantization error Theorem A.3 (Accumulated quantization and numeric error bound) . Consider the incremental updates 

St = St−1 + ϕ(kt) v⊤ 

> t

, Zt = Zt−1 + ϕ(kt), (37) initialized at S0 = 0 , Z 0 = 0 , where each arithmetic operation is performed with fixed-point quantization that introduces a bounded additive error of magnitude at most ηq per scalar update. Assume ∥ϕ(kt)∥2 ≤ Bϕ and ∥vt∥2 ≤ Rv for all t.Then after T updates the deviation between quantized and exact accumulators satisfies 

∥ST − bST ∥F ≤ T · BϕRv + T η q · md v , (38) where bST denotes the numerically exact (non-quantized) accumulator. Moreover, to avoid overflow when storing ST in 

b-bit signed fixed point, it is sufficient that 

T · BϕRv + T η q · md v ≤ 2b−1 − 1. (39) 

Proof. The exact update increment norm obeys 

ϕ(kt) v⊤  

> tF

= ∥ϕ(kt)∥2∥vt∥2 ≤ BϕRv . (40) After T steps the ideal (non-quantized) Frobenius norm satisfies ∥ bST ∥F ≤ T B ϕRv . Quantization introduces an additive per-scalar error bounded by ηq ; since St has md v scalar entries, the total possible accumulated quantization error in Frobenius norm is at most T η q · md v . Combining these yields Equation (38) . The overflow condition in Equation (39) is immediate by requiring that the maximum representable integer exceed the worst-case magnitude. □

where ηq is the maximal per-scalar quantization error, md v is the number of stored scalars in St, and b is the bit-width of the fixed-point representation. 

A.5 Two-layer key selection: attention coverage guarantee Theorem A.4 (Coverage preservation of two-layer selection) . Let Kt = {k1, . . . , k N } be the full set of candidate keys at time t, and let eKt = Lt ∪ G (qt) be the two-layer subset consisting of the local window Lt and a static global candidate set G(qt). Define the kernel mass retained by eKt for query qt as 

M eK (qt) := X 

> k∈eKt

k(qt, k ), (41) and the full mass MK (qt) := P 

> k∈Kt

k(qt, k ). If the selection mechanism guarantees that the omitted keys have total kernel mass at most αM K (qt) for some α ∈ [0 , 1) , then 

M eK (qt) ≥ (1 − α)MK (qt). (42) Moreover, the covariance of selected keys satisfies 

Cov( eKt) ⪰ (1 − α) Cov( Kt), (43) in the Loewner order, where Cov( ·) denotes the kernel-weighted second-moment matrix. 21 Proof. By definition the omitted mass is 

MK (qt) − M eK (qt) ≤ αM K (qt), (44) which rearranges to Equation (42). Define the kernel-weighted second moment for the full set as 

Cov( Kt) := X

> k∈Kt

k(qt, k ) kk ⊤, (45) and analogously for eKt. Partition the sum to omitted and retained terms and observe that the omitted contribution is positive semidefinite. Hence 

Cov( eKt) = Cov( Kt) − X  

> k̸∈eKt

k(qt, k ) kk ⊤

⪰ Cov( Kt) − α X

> k∈Kt

k(qt, k ) kk ⊤.

(46) The right-hand side equals (1 − α)Cov( Kt) if the omitted keys collectively carry at most a fraction α of the kernel mass in every spectral direction, which is implied by the assumption that the omitted mass is at most αM K (qt) and that individual keys are directionally dominated in norm. A formal way to see this is to write for any unit vector uu⊤Cov( eKt)u = X 

> k∈eKt

k(qt, k ) ( u⊤k)2

≥ X

> k∈Kt

k(qt, k ) ( u⊤k)2 − α X

> k∈Kt

k(qt, k ) ( u⊤k)2,

(47) which yields u⊤Cov( eKt)u ≥ (1 − α)u⊤Cov( Kt)u. Because this holds for all unit u, the Loewner inequality in Equation (43) follows. □

where k(qt, k ) are the kernel weights and Cov( ·) is the kernel-weighted second-moment matrix; ⪰ denotes the positive semidefinite ordering. 

A.6 Two-timescale protocol: stability Theorem A.5 (Stability of the two-timescale control/data-plane protocol) . Consider the fast (dataplane) estimator 

Cj (t) = (1 − η)Cj (t − 1) + η u j (t), (48) with η ∈ (0 , 1) the EMA step size and uj (t) ∈ { 0, 1} instantaneous indicator. Let the control plane perform full re-clustering and a batched table install every Tcp seconds requiring an atomic install time ∆tinstall . Assume the observations uj (t) are ergodic with mean ¯uj and variance bounded by σ2

> u

. If the parameters satisfy 

η < 11 + κ T cp /∆tinstall 

(49) for some constant κ > 0 depending on the system’s Lipschitz continuity, then the combined system converges in mean-square to a bounded neighborhood of the steady state. The neighborhood radius decreases as η → 0 and 

Tcp → ∞ .

Proof. The EMA recursion (48) is a standard linear stochastic approximation with gain η. Its equilibrium in expectation is E[Cj (t)] = ¯ uj . Consider the Lyapunov function V (C) = P 

> j

(Cj − ¯uj )2. The one-step expected decrement satisfies 

EV (C(t)) | C(t − 1)  = X

> j

E((1 − η)Cj (t − 1) + ηu j (t) − ¯uj )2

= X

> j

 (1 − η)2(Cj (t − 1) − ¯uj )2 + η2Var( uj (t)) . (50) Thus 

E[V (C(t))] ≤ (1 − η(2 − η)) E[V (C(t − 1))] + η2T σ 2

> u

. (51) This recursion is contractive provided η(2 − η) > 0, which holds for η ∈ (0 , 1) . Consequently, C(t) converges in mean-square to a ball of radius O(η) centered at ¯u. The control-plane updates introduce additional perturbations 22 when mappings change. Let the mapping change magnitude be measured by ∆map . If control updates occur every 

Tcp seconds and each atomic install requires a duration ∆tinstall , then the effective disturbance frequency scales as 

1/T cp and the transient magnitude scales with ∆map and ∆tinstall . Standard singular-perturbation and two-timescale theory implies that if the fast gain η is small relative to the slow update cadence (quantified by Equation (49) with a problem-dependent constant κ), the fast estimator tracks the slowly varying target and the composite system remains stable. More precisely, choose η such that the contraction coefficient 1 − η(2 − η) dominates the maximal increase in V

caused by an atomic install; this yields the condition in Equation (49) . The mean-square limit set lies in a neighborhood whose radius scales with η and with 1/T cp . □

where ¯uj is the long-run mean of the indicator uj (t), σ2 

> u

its variance bound, κ captures Lipschitz constants of the mapping-to-performance relation, and ∆map measures magnitude of batching changes. 

A.7 Remarks and parameter guidance 

From Theorems A.1–A.5 practitioners may extract concrete parameter choices. To achieve a uniform kernel approxima-tion error ε over N = O(T 2) pairs with probability 1 − δ, set m = Θ( ε−2 log( T /δ )) . To bound the linearized-attention spectral error by δ scale, ensure ε satisfies ε ≲ γδ/ ∥V ∥2 where γ is the minimal normalization mass. Quantization bit-width b should be chosen so that the overflow condition in Equation (39) is met for the expected evaluation horizon 

T . Finally, set the EMA gain η small enough relative to the control-plane cadence Tcp and atomic install time ∆tinstall 

following Theorem A.5 to guarantee stable two-timescale behavior. This concludes the theoretical guarantees section. 23