Title: Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models

URL Source: https://arxiv.org/pdf/2602.10282v1

Published Time: Thu, 12 Feb 2026 01:13:37 GMT

Number of Pages: 16

Markdown Content:
This is a preprint. Work in progress. 

# LINEAR -LLM-SCM: BENCHMARKING LLM S FOR 

# COEFFICIENT ELICITATION IN LINEAR -G AUSSIAN 

# CAUSAL MODELS 

Kanta Yamaoka 1,2, Sumantrak Mukherjee 1, Thomas G¨ artner 3, David Selby 1,Stefan Konigorski 3,5, Eyke H ¨ ullermeier 1,4,6, Viktor Bengs 1, Sebastian Vollmer 1,21Data Science and its Applications, German Research Centre for Artificial Intelligence (DFKI), Germany 

> 2

Dept. of Computer Science, University of Kaiserslautern–Landau (RPTU), Germany 

> 3

Digital Health - Machine Learning Research Group, Hasso Plattner Institute for Digital Engineering, Germany 

> 4

Institute of Informatics, University of Munich (LMU), Germany 

> 5

Hasso Plattner Institute for Digital Health at Mount Sinai, Icahn School of Medicine at Mount Sinai, USA  

> 6

Munich Center for Machine Learning (MCML), Germany 

{kanta.yamaoka, sumantrak.mukherjee,sebastian.vollmer }@dfki.de 

## ABSTRACT 

Large language models (LLMs) have shown potential in identifying qualita-tive causal relations, but their ability to perform quantitative causal reasoning— estimating effect sizes that parametrize functional relationships—remains under-explored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework de-composes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several chal-lenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and seman-tic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced 1 the benchmarking framework so that re-searchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly. 

## 1 INTRODUCTION 

Robust intelligence requires an agent to have an internal “world model” (Ha & Schmidhuber, 2018), an internal causal mechanism to be able to infer causal structures and their effect relationships (Pearl, 2019). Recent work (Richens & Everitt, 2024) suggests that any agent capable of solving complex decision tasks must effectively learn a causal model of its data-generating process. Given the recent advancement of large language models (LLMs), they exhibit the ability to encode a vast corpus of human knowledge, such as clinical knowledge (Singhal et al., 2023), one may also speculate whether LLMs are encoding literature with causal information and therefore constructing causal models. Per LLMs’ Causal Hierarchy , recently classified by Zhang et al. (2023), today’s LLMs have shown promise in Type 1 tasks (identifying causal relationships using domain knowledge), but they strug-gle with Type 2 (discovering new knowledge from data) and Type 3 (quantitative estimation of consequences) tasks. LLMs can often (but not always) reconstruct the qualitative structure of a directed acyclic graph (DAG) as Type 1. However, Type 3, their ability to perform quantitative pa-rameterization, estimating the specific effect sizes within those structures, remains under-explored in continuous variables, necessitating theoretical developments and empirical results. 

> 1

https://github.com/datasciapps/parameterize-dag-with-llm 

1

> arXiv:2602.10282v1 [cs.LG] 10 Feb 2026

This is a preprint. Work in progress. In this paper, we introduce Linear-LLM-SCM, a benchmarking framework designed to evaluate the quantitative causal capabilities of LLMs. Our framework programmatically decomposes DAGs into local parent-child structures, tasking LLMs with eliciting regression coefficients for linear Gaus-sian SCMs. Our framework itself is agnostic to LLMs and DAGs; in our open-sourced testbed, researchers can use their own DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains with minimal effort. We investigate the following research questions (RQs): • RQ1: Given a pre-specified DAG, can LLMs elicit plausible regression coefficients in com-parison to the real-world ground truths? • RQ2: How robust is this parameterization when facing adversarial conditions, such as DAG misspecification (spurious edges) or changes in variable units? • RQ3: What are the common failure modes encountered when using LLMs for parameteri-zation? One may wonder why we assess DAG misspecification when assuming pre-specified structures. Real-world graphs are often uncertain: experts may omit or include edges, and data-driven discovery can yield multiple plausible DAGs in the face of uncertainty (Padh et al., 2025). Today’s causal discovery largely focuses on point estimates (Vowels et al., 2023), while uncertainty-aware Bayesian structure learning remains challenging due to combinatoric complexity. Even in the future LLM-based causal discovery becomes more feasible, it may produce misspecified DAGs due to limited uncertainty quantification (Papamarkou et al., 2024). The downstream causal effect estimation, which comes after possibly misspecified DAGs and which we aim to investigate in our framework, also needs robustness in this regard. Employing such adversarial perturbations also help mitigate potential data leakage (Yang et al., 2023) in LLMs for evaluation in causality. Our key contributions can be summarized as follows: • We introduce Linear-LLM-SCM, a framework that allows us to evaluate LLMs’ ability to estimate linear-Gaussian SCM coefficients when the DAG structure is given, by decompos-ing the graph into local parent–child structures. • We open-source an evaluation pipeline that supports plug-and-play DAGs, variable meta-data (units/ranges), as well as LLMs, and reports coefficient-distance and ordering metrics against ground truth. 2

• Broadly, our work provides a practical benchmarking framework of causal effect estimation given an assumed DAG, which helps further research of reasoning and generalizability of LLMs as motivated by Pearl (2019) and Richens & Everitt (2024). 

## 2 BACKGROUND AND RELATED WORK 

Structural causal models (SCMs) formalize causal systems as a pair (G, F), where G is a directed acyclic graph (DAG) encoding causal structure and F = {fi} is a set of structural equations, with each endogenous variable generated as Xi = fi(Pa( Xi), E i). Recent work has explored large lan-guage models (LLMs) for causal reasoning, primarily focusing on causal discovery and qualitative inference (Long et al., 2022; 2023; Kiciman et al., 2024). However, several studies raise concerns about LLMs’ causal capabilities, arguing that they mainly succeed at identifying causal relation-ships using prior knowledge (Type 1) , while struggling with discovering causal structure from data 

(Type 2) and quantitatively estimating intervention effects (Type 3) (Zhang et al., 2023; Zeˇ cevi´ cet al., 2023; Yang et al., 2023; Jin et al., 2023). Most existing work on LLM-based causal effect estimation focuses on discrete domains or requires numerical observational data and specialized architectures (Chen et al., 2023; Feder et al., 2023; Zhang et al., 2024). Closest to our setting, Bynum & Cho (2025) combined LLMs with pre-specified causal graphs but estimate effects via sampling-based conditional distributions rather than directly eliciting structural parameters. In contrast, we study whether LLMs can directly estimate continuous 

> 2https://github.com/datasciapps/parameterize-dag-with-llm

2This is a preprint. Work in progress. 

Algorithm 1 Linear-LLM-SCM Benchmarking Framework 

Require: DAG structure G = (V, E), Variable descriptions D, Value ranges R, Phenomenon overview P, LLM M

Ensure: Aggregated set of linear coefficients ˆβ 

> 1:

Initialize ˆβ ← ∅  

> 2:

for each target variable Xj ∈ V in topological order do  

> 3:

1. Identify direct parents P a (Xj ) ⊂ V using edges E ▷ Decompose into local structures  

> 4:

2. Construct prompt Sj incorporating:  

> 5:

• Domain expert persona and Phenomenon overview P 

> 6:

• Short descriptions D and Units for Xj and P a (Xj ) 

> 7:

• Hard constraints/Value ranges R 

> 8:

3. Call LLM: Response ← M (Sj ) requesting JSON format  

> 9:

4. Parse Response : Extract numerical ˆβ for P a (Xj ) and intercept β0 ▷ This part has an iterative feedback mechanism, see Algorithm 2  

> 10:

5. ˆβ ← ˆβ ∪ { ˆβi,j } 

> 11:

end for  

> 12:

return Aggregated parameterized SCM ˆβ for an entire graph structure G

linear causal effect parameters for pre-specified SCMs using only causal structure and variable se-mantics, without observational data. A detailed review of related work is provided in Appendix A.1. 

## 3 LINEAR -LLM-SCM B ENCHMARKING FRAMEWORK 

We provide pre-specified DAG structures that decompose real-world phenomena into parent-child interactions. The system iterates through the DAG, calling LLMs via prompt templates to elicit functional mappings for each parent-child set, then aggregating results to obtain full effect param-eters. This assumes LLMs learned quantitative or qualitative information from their large training corpus. For quantitative aspects, LLMs may have encountered scientific literature with effects for-malized as SCMs, Bayesian Networks, or linear regressions/ODEs/PDEs. For qualitative aspects, LLMs encountered natural language causal statements (e.g., “Coffee consumption positively regu-lates alertness after 1 hour”) in scientific and social media texts. 3.1 OVERVIEW OF THE BENCHMARKING FRAMEWORK 

The Algorithm 1 describes the steps of our benchmarking framework. The framework requires the following input: DAG structure (nodes and directed edges), node descriptions with units (e.g., “GC”: “Glucose (micromolar, μM)”), and variable constraints (lower and upper bounds). The framework programmatically iterates through all parent-child sets. For each set, the program crafts a prompt with domain expert persona (“You are an expert in XYZ...”), the target variable, its direct parents, and their constraints. The prompt visibility includes only the current node and direct parents, plus shared phenomenon information. For example, for a DAG about a phenomenon, Cachexia, the prompt includes the following texts: “You are going to identify internal dynamics of a phenomenon, Cachexia. Cachexia is a complicated metabolic syndrome related to underlying illness and charac-terized by muscle mass loss with or without fat mass loss that is often associated with anorexia, an inflammatory process, insulin resistance, and increased protein turnover.” For details of each prompt to call to LLMs, we will discuss in Section 3.2. The units of variables are specified in descriptions of direct parents and target variables, sourced from original literature or author judgment. We currently do not elicit confidence intervals for coef-ficients; residual variance is included as the last term in the functional specification. While Linear SCMs typically include uncertainty quantification, we have not yet implemented this in our prompt. We obtain the functional mapping of direct parents and target variables as text expressions within JSON objects returned by LLMs. While LLMs provide plausibility descriptions for debugging, we typically parse only the parameterized equation from the proposed lin str eq field. The system programmatically extracts beta coefficients from the LLM structural equation format. 3This is a preprint. Work in progress. 3.2 PROMPT FOR NODE LEVEL FUNCTION PARAMETRIZATION 

Designing the interaction between LLMs and DAGs is critical. Our current approach traverses the DAG node-by-node; for each variable, the LLM receives descriptions of only its direct parents to de-termine functional parameterization. While one could pass the entire graph via text representations (e.g., DAGitty by Textor et al. (2017) or Mermaid by Sveidqvist, Knut (2014)) or feed entire DAGs into visual language models, we chose the parent-child template based on Occam’s Razor. This focus on local structures also aligns with recent benchmarks (Bynum & Cho, 2025; Nafar et al., 2025). Our prompt includes the names and short descriptions of the target variable and its direct parents, alongside formatting instructions. Figure 1 shows an example of a prompt for a parent-child local structure. The prompt is consisting of three parts: First, expert persona, a summary of the phe-nomenon of interest, and variable units are introduced to the LLM. Next, the parameterization task, linear equation template, and variable ranges appear. Finally, the LLM is informed of the output format, which starts with the thought process and ends with the parameterization result as a string. This process corresponds to line 4 of Algorithm 1. You are a leading Finance researcher and an Structural Causal Model (SCM) expert in Consumer Behavior.   

> This dataset focuses on factors influencing credit card behavior and expenditure patterns, providing insights into consumer finance decisions.
> Given the direct causes, you must propose a linear structural equation for the target variable $Y$. Do not use non-linear functions (e.g., exponential, sigmoid). The coefficients of the linear equation are continuous variables in space $\mathbb{R}$.
> The target variable is $Y =$ "Share": "The ratio of monthly credit card expenditure to yearly income (Ratio).".
> The direct causes (Parents) are: "Card": "Whether the application for credit card was accepted or not (Categorical/Binary).", "Age": "The age in years plus twelfths of a year (Unit: Years + fractions).".
> Propose the complete linear equation: $Y = \beta_0*1 + \beta_Card*Card + \beta_Age*Age + E_Y$. You *must* use the actual raw variable names (e.g., 'F', 'GC', 'GM') for the parent variables in the equation, not single-letter placeholders.
> The following hard constraints (value ranges) are known and must be respected:
> - Variable Share is bounded within [0.0, 1.0].
> - Variable Card is bounded within [0, 1].
> - Variable Age is bounded within [18, 100].
> 1. Explicitly define the error term $E_Y$ (e.g., standard normal noise, $E_Y \sim N(0, \sigma^2)$).
> 2. For each coefficient (\beta_0, \beta_Card, \beta_Age), explain its finance plausibility, its expected sign (positive/negative), and justify your chosen magnitude (unit-effect).
> Given the DAG for variable "Share", please provide a plausible linear parameterisation.
> Output format: Only respond in JSON format, with the following keys:
> - plausibility: str (Finance Plausibility)
> - proposed_lin_str_eq: str (Proposed Linear Structural Equation, *do not use placeholder betas like \\beta_0, use concrete numerical values*)
> Persona
> Phenomenon
> Variables & units
> (only parent-child)
> Output format
> instruction
> Task instruction
> Range constraints
> (only parent-child)

Figure 1: An example of a prompt for a local parent-child structure in a DAG. 3.3 ITERATIVE FEEDBACK 

When eliciting functional mappings per target variable, the LLM-SCM framework includes an iter-ative feedback refinement mechanism (Algorithm 2). This process requires pre-specified hard con-straints for each variable. This matters because parameterization happens at the direct parent-child level and coefficients are used downstream; therefore the refinement enforces such constraints. For each variable, evaluate its possible value range based on parent constraints (C1) and the proposed parameterization, then compare it with the node’s constraints (C2) to check whether C2 includes C1. If not, re-parameterize using the last proposal, repeating up to the loop budget (e.g., 5). 3.4 METRICS FOR EVALUATING PARAMETERIZATION 

Our framework performs comparisons between ground-truth effect parameters and the effect pa-rameterization from LLMs. For DAGs with learned parameter ground truth (explained in the next section), we calculate metrics (M1)–(M4). For (M1)–(M3), we compute the L2 norm over all lin-ear coefficients across nodes; the contribution aggregation differs by metric. These metrics capture distances between LLM-elicited parameter vectors per node βLLM,j and ground truth vectors per node βGT,j . For each vector, we denote j’s each direct parents’ edges effect size (scaler) using index i: βLLM,j,i and βGT,j,i . (M4) captures relative effect-size ordering per node. Concretely, if a node j has parents a and b and the effect sizes are βLLM,j,a = −0.8 < β LLM,j,b = 0 .5 while 4This is a preprint. Work in progress. 

Algorithm 2 Iterative Feedback for Refinement with Hard Constraints 

Require: Target variable Xj , Parent hard constraints RP a (Xj ), Node hard constraints C2, Loop budget n

Ensure: Accepted linear coefficients ˆβ

1: for iteration = 1 to n do 

2: Call LLM M with prompt Sj to get proposal P ▷ Includes parameterization ˆβ

3: Calculate possible value range C1 of Xj based on RP a (Xj ) and P

4: if C2 includes C1 then ▷ Validation check 

5: return ˆβ (Accept proposal) 

6: else 

7: Reject proposal 

8: Update prompt Sj with previous proposal and validation results 

9: end if 

10: end for 

11: return Last available ˆβ ▷ Budget n reached 

βGT,j,a = −2 < β GT,j,b = 3 , for this node we increment the sum by one before moving to the next node, finally obtaining (M4). The metrics are summarized as follows: • (M1) L2 Norm distance between LLM-elicited vs GT: 

M1 =

sX

> j∈V

X

> i∈P a (j)

(βLLM,j,i − βGT,j,i )2 (1) • (M2) L2 Norm distance with node-wise effect normalization: 

M2 =

vuutX

> j∈V

X

> i∈P a (j)

 βLLM,j,i 

∥βLLM,j ∥2

− βGT,j,i 

∥βGT,j ∥2

2

(2) • (M3) L2 Norm distance excluding edges with single parents: 

M3 =

vuut X  

> j∈{ V:|P a (Xj)|>1}

X 

> i∈P a (Xj)

 βLLM,j,i 

∥βLLM,j ∥2

− βGT,j,i 

∥βGT,j ∥2

2

(3) • (M4) Effect size relative ordering matches per target variable: 

M4 = X  

> j∈{ V:|P a (Xj)|>1}

I  ordering of {βLLM ,i,j }i∈P a (Xj ) = ordering of {βGT ,i,j }i∈P a (Xj )



(4) 

## 4 EXPERIMENTAL SETUP 

We assume linear functional elicitation for pre-specified DAGs using LLMs given their nodes and directed edges. In other words, we will leave more complicated non-linear functional elicitation to our future work. This is due to verifiability. In Bayesian Network literature, if we consider the linear assumption, it is possible to obtain linear settings including learned parameters as well as DAG structures (Leonelli, 2025) for real-world settings; If we consider non-linear parameterization with non-synthetic example, obtaining the ground truths would be difficult. In our functional elicitation, we assume direct parameterization in a symbolic fashion using LLMs, but we do not sample many effect examples from LLMs unlike the most similar work to our knowledge (Bynum & Cho, 2025). For DAGs and their parameterization, we assume continuous variables but not discrete ones. 4.1 LLM M ODEL SELECTION 

For our experiment, general purpose pre-trained LLMs (Gemini 2.5 Flash and Llama 3 family) as in Table 1. We tried models with different model sizes and architectures either mixture-of-experts 5This is a preprint. Work in progress. Table 1: Benchmark model specifications for our study. MoE stands for mixture of experts. Dense means dense-transformer and this also indicates the model is non-MoE-based. The tick ✓indicates yes, and the cross × indicates no. 

MODEL NAME MODEL SIZE ARCHITECTURE OPEN WEIGHTS 

Gemini 2.5 Flash Unknown MoE ×

Llama 3.1 8B 8B Dense ✓

Llama 3.3 70B 70B Dense ✓

Table 2: DAGs with ground-truths included for our study from BnRep repository. Literature in-dicates sources introducing either the DAG structure or DAG effect parameterization. VR. (Value Ranges) indicates whether value ranges are available in the original literature. 

NAME LITERATURE VR. NODES DOMAIN 

cachexia1 G¨ orgen & Leonelli (2020); Eisner et al. (2011) Y 6 Genetics expenditure Tsagris (2022); Greene (2003) N (Crafted) 12 Economics foodsecurity Leonelli et al. (2020); Barons et al. (2018) Y 4 Social Sciences algal2 Jackson-Blake et al. (2022) Y 9 Env. Science lexical Baumann & Sekanina (2022) Y 8 Social Sciences liquefaction Hu et al. (2023) Y 10 Earth Sciences stocks Sener & Demir (2024) Y 13 Economics (MoE), where an input is routed into different experts, obtaining output in an ensemble fashion (Ja-cobs et al., 1991), vs non-MoE ones. For Gemini 2.5 Flash, the model consists of sparse mixture-of-experts (MoE) (Gemini Team, Google, 2025). Representative models from the open-weights community, Llama 3.1 8B and Llama 3.3 70B have dense transformer, where all parameters are used for inference (Llama Team, Meta, 2024). 4.2 DAG S WITH GROUND TRUTH EFFECT PARAMETERS 

In our experiment, we use real-world DAGs from a Bayesian Network repository, BnRep (Leonelli, 2025). In the Appendix, Figure 4 describes how we selected DAGs and effect ground-truths from a large DAG repository. First, we kept the ones with a continuous Linear Gaussian setting which corresponds to the linear SCM setting. For convenience, we limit ourselves on DAGs with no more than 15 nodes. Moreover, we excluded DAGs where variable names consist of letters and numbered suffixes, for example, “X1” and “X2.” Table 2 lists the 7 resulting DAGs that we use in our experiments. 4.3 ADVERSARIAL CONDITIONS 

Recall that our RQ2 was: How robust is this parameterization when facing adversarial conditions, such as DAG misspecification (spurious edges) or changes in variable units? In this section, we perform robustness checks with two additional and adversarial conditions, (I) changes in the unit values or (II) DAG misspecification. 4.3.1 (I) T WEAKING UNITS TO CHECK ROBUSTNESS 

For the first type of adversarial conditions, namely changes in units, we use Cachexia1 DAG from Table 2 because the DAG comes with units and the DAG structure is relatively simpler among others. We captured aggregated trends on each model with temperature set to zero to make the behavior as deterministic as possible. We used 25 runs for each condition. In our experiments regarding (i), we employed the following two conditions: (A) the Cachexia DAG with real bounds and units, and (B) the Cachexia DAG with real bounds and tweaked units from micromolar to nanomolar. 6This is a preprint. Work in progress. 4.3.2 (II) S IMULATED DAG M ISSPECIFICATION TO CHECK ROBUSTNESS 

We created multiple adversarial mutated examples. For each sampler, we added a spurious edge between two variables with no actual connections in the ground truths. Based on this, we performed parameterization to check how robust each LLM is. Based on the original Expenditure DAG (avail-able in the Appendix, Figure 3), we created the following four DAG exemplars while keeping their acyclic nature. There are four mutated examples originating from the original DAG, namely adding the following spurious edges: (S1) Owner → Expenditure , (S2) Majorcards → Dependents , (S3) 

Owner → Share , and (S4) Majorcards → Selfemp .

## 5 RESULTS AND DISCUSSION 

5.1 RQ1: D IRECT ESTIMATION RESULTS 

Table 3 presents direct parameter estimation results across three LLM models on all 7 DAGs from Table 2. Throughout, values after ± indicate 95% CIs. For M1 metrics, which are simple L2 norm values, there is high variability among different DAG variants in the same model family. The M1 variability across models is also observed. This is because M1 is not scale invariant with respect to the variable ranges. Given this, we focus on metrics M2 (normalized), and M3 (focus on effect distance from multiple parents of a node), M4 (scale invariant, counting metrics). In M2, for each DAG, the best model varies. From M3, in 6/7 DAGs Gemini 2.5 Flash outperforms others. From M4, in 6/7 DAGs, Gemini 2.5 Flash is the best. In comparisons of Llama 3.1 8B and Llama 3.3 70B, in general, Llama 3.3 70B is in most cases better than the other, indicating larger parameter size can result in improvement in quantitative causal parameterization. Note that Llama 3.1 8B failed to generate parsable equations for DAG algal2 . We did not optimize for this case to avoid harming other model-DAG conditions. We also used temperature set to zero, which in theory should make models deterministic. Nevertheless, larger models (Gemini 2.5 Flash, Llama 3.3 70B) exhibit high stochasticity across metrics, with non-Gaussian distributions at n=25 samples, possibly due to hardware and software factors beyond our control via external APIs. This variability is concerning for safety-critical domains, e.g., medicine, where incorrect parameteriza-tion can propagate through downstream effects and hinder adoption, corroborating research on LLM non-determinism (Klishevich et al., 2025). 5.2 RQ2: R OBUSTNESS RESULTS 

To assess the robustness of LLM-based parameter estimation, we examine how model predictions respond to adversarial conditions. Values after ± indicate 95% CIs. Table 4 and Table 5 quantify performance degradation when facing unit value changes and DAG misspecification respectively. 5.2.1 (I) U NIT TWEAKING ROBUSTNESS 

Table 4 evaluates model robustness when unit values for the Cachexia1 domain are tweaked from micromolar to nanomolar scale. This robustness check tests whether models maintain consistent causal effect estimates when facing realistic variations in measurement units, which is critical for practical deployment where domain experts may choose different units. Similar to the previous direct estimation result, focusing on M2-M4, sometimes, counter-intuitively tweaked units resulted in better parameterization. Admittedly, to conclude this, this would require more empirical coverage and we will extend this adversarial example of unit tweaking and we try to avoid overspeculation. 5.2.2 (II ) DAG M ISSPECIFICATION ROBUSTNESS 

Table 5 examines model performance when the Expenditure DAG is intentionally misspecified by adding spurious edges, where O indicates the original DAG while S1-S4 indicate mutated variants. As we discussed earlier, our analysis is based on the more reliable metrics, M2-M4. First, M2, which is normalized by node (vector at the node level is normalized), is the lowest with original DAG conditions compared to other mutated conditions. This is the case among all the models. Next, if we compare each value of M2 and M3 of the same model and the same DAG, they have the same values. Recall that contributions of M3 are filtered to keep the parent effects for a variable only 7This is a preprint. Work in progress. Table 3: Direct estimation results (Averaged n = 25 , Temp 0). Values after ± indicate 95% CIs. 

M 1: L2 distance; M 2: normalized L2; M 3: normalized L2 excluding single-parent edges; M 4:Effect relative order count. For M 1–M 3, lower is better ( ↓); for M 4, higher is better ( ↑). See Table 2 for DAG descriptions. 

MODEL DAG M1 ↓ M2 ↓ M3 ↓ M4 ↑

Gemini 2.5 Flash cachexia1 13.783 ± 1.826 2.449 ± 0.240 1.074 ± 0.114 1.000 ± 0.000 

Llama 3.1 8B cachexia1 16 .527 ± 0.000 1.036 ± 0.000 1.036 ± 0.000 0.000 ± 0.000 

Llama 3.3 70B cachexia1 13 .843 ± 0.938 1.994 ± 0.213 1.352 ± 0.071 0.440 ± 0.199 

Gemini 2.5 Flash expenditure 148084 .559 ± 22359 .281 0.998 ± 0.226 0.998 ± 0.226 7.520 ± 0.200 

Llama 3.1 8B expenditure 2463.377 ± 0.000 2.053 ± 0.000 2.053 ± 0.000 7.000 ± 0.000 

Llama 3.3 70B expenditure 27137 .540 ± 12574 .354 1.548 ± 0.234 1.548 ± 0.234 6.560 ± 0.199 

Gemini 2.5 Flash foodsecurity 22 .801 ± 0.038 2.017 ± 0.006 0.236 ± 0.050 0.500 ± 0.214 

Llama 3.1 8B foodsecurity 22.727 ± 0.006 0.447 ± 0.000 0.447 ± 0.000 0.000 ± 0.000 

Llama 3.3 70B foodsecurity 22 .914 ± 0.009 2.039 ± 0.001 0.395 ± 0.004 0.000 ± 0.000 

Gemini 2.5 Flash algal2 4.094 ± 0.256 0.514 ± 0.098 0.514 ± 0.098 2.000 ± 0.000 

Llama 3.1 8B algal2 Model output equations not parsable by the program. 

Llama 3.3 70B algal2 4.657 ± 0.004 0.559 ± 0.054 0.559 ± 0.054 2.000 ± 0.000 

Gemini 2.5 Flash lexical 42 .246 ± 0.650 2.036 ± 0.053 2.036 ± 0.053 2.040 ± 0.265 

Llama 3.1 8B lexical 35 .854 ± 0.000 3.252 ± 0.000 2.565 ± 0.000 1.000 ± 0.000 

Llama 3.3 70B lexical 35.847 ± 0.001 2.274 ± 0.037 2.274 ± 0.037 0.778 ± 0.374 

Gemini 2.5 Flash liquefaction 12 .294 ± 5.556 0.844 ± 0.034 0.844 ± 0.034 2.800 ± 0.160 

Llama 3.1 8B liquefaction 9.999 ± 0.000 2.012 ± 0.002 2.012 ± 0.002 3.000 ± 0.000 

Llama 3.3 70B liquefaction 12 .093 ± 0.221 1.271 ± 0.026 1.271 ± 0.026 3.000 ± 0.000 

Gemini 2.5 Flash stocks 0.829 ± 0.048 1.059 ± 0.122 1.006 ± 0.063 3.478 ± 0.242 

Llama 3.1 8B stocks 0.893 ± 0.005 1.589 ± 0.003 1.589 ± 0.003 2.880 ± 0.130 

Llama 3.3 70B stocks 1.060 ± 0.026 0.934 ± 0.059 0.934 ± 0.059 2.600 ± 0.226 

Table 4: Robustness under unit tweak (Averaged n = 25 , Temp 0). Values after ± indicate 95% CIs. M 1: L2 distance; M 2: normalized L2; M 3: normalized L2 excluding single-parent edges; 

M 4: Effect relative order count. For M 1–M 3, lower is better ( ↓); for M 4, higher is better ( ↑). Units: (L)= μM, (T)=nM. 

MODEL CND. UNITS M1 ↓ M2 ↓ M3 ↓ M4 ↑

Gemini 2.5 Flash A L 13 .783 ± 1.826 2.449 ± 0.240 1.074 ± 0.114 1.000 ± 0.000 

B T 12.399 ± 1.774 1.769 ± 0.372 0.917 ± 0.136 1.120 ± 0.206 

Llama 3.1 8B A L 16.527 ± 0.000 1.036 ± 0.000 1.036 ± 0.000 0.000 ± 0.000 

B T 17 .198 ± 0.000 0.905 ± 0.002 0.905 ± 0.002 0.000 ± 0.000 

Llama 3.3 70B A L 13 .843 ± 0.938 1.994 ± 0.213 1.352 ± 0.071 0.440 ± 0.199 

B T 13.441 ± 0.199 2.439 ± 0.160 1.107 ± 0.115 0.640 ± 0.250 

when there are multiple parents. M2 and M3 values being the same makes sense due to the original expenditure DAG as in Figure 3 in the Appendix. All the variables that have at least one parent in the DAG have multiple parents already, so the filtering mechanism in M3 does not exclude any contributions from the parent effects. In this particular DAG, M2 and M3 are the same, but in other DAGs generally this does not happen. In M4, overall in each model, the relative effect order counts are the best for the original DAGs in Gemini 2.5 Flash and Llama 3.1 8B, except Llama 3.3 70B, where the original DAG is the second best per M4, while S4 is the best. However, there is overlap of 95% intervals among the two conditions. To summarize, generally, adversarial conditions yield lower M4 values, quantifying performance degradation given DAG misspecification with spurious edges. 8This is a preprint. Work in progress. Table 5: Robustness under DAG misspecification for the expenditure DAG (Averaged n = 25 , Temp 0). Values after ± indicate 95% CIs. M 1: L2 distance; M 2: normalized L2; M 3: normalized L2 excluding single-parent edges; M 4: Effect relative order count. For M 1–M 3, lower is better ( ↓); for M 4, higher is better ( ↑). Misspec: (O)=Original DAG, (S1)-(S4)=Spurious edges added.                                                                                                                                                                                                 

> MODEL MISSPEC. M1 ↓M2 ↓M3 ↓M4 ↑
> Gemini 2.5 Flash O148084 .559 ±22359 .281 0.998 ±0.226 0.998 ±0.226 7.520 ±0.200
> S1 81587.682 ±20626 .822 1.372 ±0.282 1.372 ±0.282 6.200 ±0.299
> S2 138770 .541 ±21002 .249 1.235 ±0.267 1.235 ±0.267 6.400 ±0.253
> S3 120240 .669 ±20548 .802 1.355 ±0.253 1.355 ±0.253 6.560 ±0.255
> S4 106012 .716 ±23004 .372 1.988 ±0.189 1.988 ±0.189 7.320 ±0.271
> Llama 3.1 8B O2463.377 ±0.000 2.053 ±0.000 2.053 ±0.000 7.000 ±0.000
> S1 20151 .134 ±0.000 2.483 ±0.000 2.483 ±0.000 6.000 ±0.000
> S2 2463.377 ±0.000 2.063 ±0.001 2.063 ±0.001 6.000 ±0.000
> S3 2463.377 ±0.000 2.106 ±0.000 2.106 ±0.000 6.000 ±0.000
> S4 2463.377 ±0.000 2.413 ±0.000 2.413 ±0.000 7.000 ±0.000
> Llama 3.3 70B O27137 .540 ±12574 .354 1.548 ±0.234 1.548 ±0.234 6.560 ±0.199
> S1 25843 .000 ±13086 .141 1.703 ±0.217 1.703 ±0.217 5.520 ±0.230
> S2 29202 .638 ±13299 .788 1.918 ±0.186 1.918 ±0.186 5.920 ±0.251
> S3 24257 .613 ±11979 .590 2.038 ±0.118 2.038 ±0.118 5.360 ±0.192
> S4 12524.332 ±7698 .439 2.222 ±0.138 2.222 ±0.138 6.680 ±0.271

In comparisons of the models, Llama 3.1 8B has almost no variability within each DAG conditions. This is already mentioned in previous direct estimation results, and also the case in S1-S4 too. This indicates in a deterministic model, adding spurious edges do not introduce stochasticity in the coefficients. This indicates the stochasticity of the models may not come from confusion, but could be attributed to the model itself. 

Limitations and Future Work The parameterization degradation in the lens of M2-M4 with spu-rious edge addition suggests that LLMs are prone to being distracted by irrelevant context (Shi et al., 2023). Our research scope was to provide a benchmarking framework; we do not address how to mitigate this LLMs’ susceptibility to perturbations, which we leave as future work. Another limita-tion is our focus on linear assumption in SCMs. We also admit the empirical coverage for the unit tweaking is limited and will plan to scale into different unit specifications in different DAGs. 

## 6 CONCLUSION 

The Linear-LLM-SCM framework benchmarks the quantitative causal reasoning of LLMs by de-composing DAGs into parent-child structures for coefficient elicitation. Among Gemini 2.5 Flash, Llama 3.1 8B, Llama 3.3 70B, results indicate that Gemini 2.5 Flash achieves the highest perfor-mance across the tested DAGs when evaluated using normalized distance (M3) and relative effect size ordering (M4). These findings demonstrate the necessity of scale-invariant metrics, e.g., (M4), as L2 distance (M1) is influenced by variable ranges. A challenge identified is the stochasticity in Gemini 2.5 Flash and Llama 3.3 70B, which produced inconsistent results at a temperature of zero. This behavior contrasts with Llama 3.1 8B, which shows consistency but lower accuracy. Such vari-ance presents risks for deployment in fields like healthcare or public policy, where decision-making requires deterministic parameterization since such inaccuracies might propagate through the down-stream causal structure’s and affect the validity of other downstream nodes’ effects. Robustness testing shows susceptibility to DAG misspecification, in terms of adding spurious edges, resulting in performance degradation and lower effect ordering accuracy. Models struggle to assign zero or near-zero coefficients to these edges, indicating a lack of robustness to DAG structural uncertainty. The framework currently assesses linear-Gaussian models and is limited by its linear assumption. Future research should extend these benchmarks to non-linear functional forms and investigate methods to mitigate the impact of structural noise on performance. Overall, we plan to increase empirical coverage in the adversarial examples to further understand when the current LLMs performs well, poorly, how to mitigate, or what kind of architectural changes are required toward robust intelligence 9This is a preprint. Work in progress. capable of quantitative causal reasoning. We hope our open-source framework will support further empirical investigations of the quantitative causal reasoning of LLMs in the community. 

## 7 ACKNOWLEDGEMENT 

We acknowledge funding for the project AI4Nof1 by the state of Rhineland Palatinate, Germany. We would like to thank Valentin Margraf, Jonas Hanselle, Serafima Lebedeva and Niklas Nertinger for their valuable feedback during weekly research meeting. 10 This is a preprint. Work in progress. 

## REFERENCES 

Martine J. Barons, Sophia K. Wright, and Jim Q. Smith. Eliciting Probabilistic Judgements for Integrating Decision Support Systems. International Series in Operations Research & Manage-ment Science , pp. 445–478, 2018. URL https://ideas.repec.org//h/spr/isochp/ 978-3-319-65052-4_17.html .Andreas Baumann and Katharina Sekanina. Accounting for the relationship be-tween lexical prevalence and acquisition with Bayesian networks and population dynamics. Linguistics Vanguard , 8(1):209–224, December 2022. ISSN 2199-174X. doi: 10.1515/lingvan-2021-0038. URL https://www.degruyterbrill. com/document/doi/10.1515/lingvan-2021-0038/html?srsltid= AfmBOooG7SLjI542EXDJKooxAgjLOJENZTNrDwLgkg9JF_2uF6Ab6-2f .Lucius E.j. Bynum and Kyunghyun Cho. Language models as causal effect generators. In Chris-tos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng (eds.), Proceed-ings of the 2025 Conference on Empirical Methods in Natural Language Processing , pp. 2096– 2115, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main.107. URL https://aclanthology. org/2025.emnlp-main.107/ .Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. DISCO: Distilling counterfactuals with large language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 5514–5528, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.302. URL 

https://aclanthology.org/2023.acl-long.302/ .Roman Eisner, Cynthia Stretch, Thomas Eastman, Jianguo Xia, David Hau, Sambasivarao Dama-raju, Russell Greiner, David S. Wishart, and Vickie E. Baracos. Learning to predict cancer-associated skeletal muscle wasting from 1H-NMR profiles of urinary metabolites. Metabolomics ,7(1):25–34, March 2011. ISSN 1573-3890. doi: 10.1007/s11306-010-0232-9. URL https: //doi.org/10.1007/s11306-010-0232-9 .Amir Feder, Yoav Wald, Claudia Shi, Suchi Saria, and David Blei. Data augmenta-tions for improved (large) language model generalization. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural In-formation Processing Systems , volume 36, pp. 70638–70653. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/df88b275bef31ac96c85f0c4013734fc-Paper-Conference.pdf .Gemini Team, Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/ abs/2507.06261 .William H Greene. Econometric analysis. Pretence Hall , 2003. Christiane G¨ orgen and Manuele Leonelli. Model-Preserving Sensitivity Analysis for Families of Gaussian Distributions. Journal of Machine Learning Research , 21(84):1–32, 2020. ISSN 1533-7928. URL http://jmlr.org/papers/v21/18-668.html .David Ha and J¨ urgen Schmidhuber. World models. CoRR , abs/1803.10122, 2018. URL http: //arxiv.org/abs/1803.10122 .Jilei Hu, Bin Xiong, Zheng Zhang, and Jing Wang. A continuous Bayesian network regression model for estimating seismic liquefaction-induced settlement of the free-field ground. Earth-quake Engineering & Structural Dynamics , 52(11):3216–3237, September 2023. ISSN 0098-8847, 1096-9845. doi: 10.1002/eqe.3804. URL https://onlinelibrary.wiley.com/ doi/10.1002/eqe.3804 .Leah A. Jackson-Blake, Franc ¸ois Clayer, Sigrid Haande, James E. Sample, and S. Jannicke Moe. Seasonal forecasting of lake water quality and algal bloom risk using a continuous Gaussian 11 This is a preprint. Work in progress. Bayesian network. Hydrology and Earth System Sciences , 26(12):3103–3124, June 2022. ISSN 1027-5606. doi: 10.5194/hess-26-3103-2022. URL https://hess.copernicus.org/ articles/26/3103/2022/ .Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation , 3(1):79–87, 1991. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng LYU, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, and Bernhard Sch¨ olkopf. Cladder: Assessing causal reasoning in language models. In A. Oh, T. Nau-mann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems , volume 36, pp. 31038–31065. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/631bb9434d718ea309af82566347d607-Paper-Conference.pdf .Emre Kiciman, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large language models: Opening a new frontier for causality. Transactions on Machine Learning Research , 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=mqoxLkX210 . Featured Certification. Eugene Klishevich, Yegor Denisov-Blanch, Simon Obstbaum, Igor Ciobanu, and Michal Kosinski. Measuring determinism in large language models for software code review, 2025. URL https: //arxiv.org/abs/2502.20747 .Manuele Leonelli. bnrep: A repository of bayesian networks from the academic literature. Neu-rocomputing , 624:129502, 2025. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom. 2025.129502. URL https://www.sciencedirect.com/science/article/pii/ S0925231225001742 .Manuele Leonelli, Eva Riccomagno, and Jim Q. Smith. Coherent combination of probabilistic outputs for group decision making: an algebraic approach. OR Spectrum , 42(2):499–528, June 2020. ISSN 1436-6304. doi: 10.1007/s00291-020-00588-8. URL https://doi.org/10. 1007/s00291-020-00588-8 .Xiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan Yang, Yuhang Zhou, Fuxiao Liu, Tian-rui Guan, Haoliang Wang, Tong Yu, Julian McAuley, Wei Ai, and Furong Huang. Large lan-guage models and causal inference in collaboration: A comprehensive survey. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Findings of the Association for Computational Linguistics: NAACL 2025 , pp. 7668–7684, Albuquerque, New Mexico, April 2025. Association for Compu-tational Linguistics. ISBN 979-8-89176-195-7. doi: 10.18653/v1/2025.findings-naacl.427. URL 

https://aclanthology.org/2025.findings-naacl.427/ .Llama Team, Meta. The llama 3 herd of models. CoRR , abs/2407.21783, 2024. doi: 10.48550/ ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783 .Stephanie Long, Tibor Schuster, and Alexandre Pich´ e. Can large language models build causal graphs? In NeurIPS 2022 Workshop on Causality for Real-world Impact , 2022. URL https: //openreview.net/forum?id=LQQoJGw8JD1 .Stephanie Long, Alexandre Pich´ e, Valentina Zantedeschi, Tibor Schuster, and Alexandre Drouin. Causal discovery with language models as imperfect experts. In ICML 2023 Workshop on Struc-tured Probabilistic Inference & Generative Modeling , 2023. URL https://openreview. net/forum?id=RXlvYZAE49 .Sunil Mohan and Theofanis Karaletsos. How well do llms understand drug mechanisms? a knowl-edge + reasoning evaluation dataset, 2025. URL https://arxiv.org/abs/2511.06418 .Aliakbar Nafar, Kristen Brent Venable, Zijun Cui, and Parisa Kordjamshidi. Extracting probabilis-tic knowledge from large language models for bayesian network parameterization, 2025. URL 

https://arxiv.org/abs/2505.15918 .12 This is a preprint. Work in progress. Kirtan Padh, Zhufeng Li, Cecilia Casolo, and Niki Kilbertus. Your assumed dag is wrong and here’s how to deal with it. In Biwei Huang and Mathias Drton (eds.), Proceedings of the Fourth Conference on Causal Learning and Reasoning , volume 275 of Proceedings of Machine Learning Research , pp. 1239–1267. PMLR, 07–09 May 2025. URL https://proceedings.mlr. press/v275/padh25a.html .Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, Jos´ e Miguel Hern´ andez-Lobato, Aliaksandr Hubin, Alexander Immer, Theofanis Karaletsos, Mohammad Emtiyaz Khan, Agustinus Kristiadi, Yingzhen Li, Stephan Mandt, Christopher Nemeth, Michael A Osborne, Tim G. J. Rudner, David R¨ ugamer, Yee Whye Teh, Max Welling, Andrew Gordon Wilson, and Ruqi Zhang. Position: Bayesian deep learning is needed in the age of large-scale AI. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning ,volume 235 of Proceedings of Machine Learning Research , pp. 39556–39586. PMLR, 21–27 Jul 2024. URL https://proceedings.mlr.press/v235/papamarkou24b.html .Judea Pearl. The seven tools of causal inference, with reflections on machine learning. Commun. ACM , 62(3):54–60, February 2019. ISSN 0001-0782. doi: 10.1145/3241036. URL https: //doi.org/10.1145/3241036 .Jonathan Richens and Tom Everitt. Robust agents learn causal world models. In The Twelfth In-ternational Conference on Learning Representations , 2024. URL https://openreview. net/forum?id=pOoKI3ouv1 .Ersin Sener and Ibrahim Demir. Gaussian Bayesian network model of healthcare, food and en-ergy sectors in the pandemic: T¨ urkiye case. Heliyon , 10(1):e23798, January 2024. ISSN 2405-8440. doi: 10.1016/j.heliyon.2023.e23798. URL https://www.sciencedirect.com/ science/article/pii/S2405844023110061 .Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Sch¨ arli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning ,volume 202 of Proceedings of Machine Learning Research , pp. 31210–31227. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/shi23a.html .Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Sch¨ arli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise Ag¨ uera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models en-code clinical knowledge. Nature , 620(7972):172–180, Aug 2023. ISSN 1476-4687. doi: 10.1038/ s41586-023-06291-2. URL https://doi.org/10.1038/s41586-023-06291-2 .Sveidqvist, Knut. Mermaid: Generate diagrams from markdown-like text, December 2014. URL 

https://github.com/mermaid-js/mermaid .Johannes Textor, Benito van der Zander, Mark S Gilthorpe, Maciej Li´ skiewicz, and George TH Elli-son. Robust causal inference using directed acyclic graphs: the r package ‘dagitty’. International Journal of Epidemiology , 45(6):1887–1894, 01 2017. ISSN 0300-5771. doi: 10.1093/ije/dyw341. URL https://doi.org/10.1093/ije/dyw341 .Michail Tsagris. The FEDHC Bayesian Network Learning Algorithm. Mathematics , 10(15):2604, July 2022. ISSN 2227-7390. doi: 10.3390/math10152604. URL https://www.mdpi.com/ 2227-7390/10/15/2604 .Matthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden. D’ya Like DAGs? A Survey on Structure Learning and Causal Discovery. ACM Computing Surveys , 55(4):1–36, April 2023. ISSN 0360-0300, 1557-7341. doi: 10.1145/3527154. URL https://dl.acm.org/doi/ 10.1145/3527154 .13 This is a preprint. Work in progress. Zeyu Wang. CausalBench: A Comprehensive Benchmark for Evaluating Causal Reasoning Capa-bilities of Large Language Models. In Kam-Fai Wong, Min Zhang, Ruifeng Xu, Jing Li, Zhongyu Wei, Lin Gui, Bin Liang, and Runcong Zhao (eds.), Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10) , pp. 143–151, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. sighan-1.17/ .Sewall Wright. Correlation and causation. Journal of agricultural research , 20(7):557–585, 1921. Linying Yang, Oscar Clivio, Vik Shirvaikar, and Fabian Falck. A critical review of causal infer-ence benchmarks for large language models. In AAAI 2024 Workshop on ”Are Large Language Models Simply Causal Parrots?” , 2023. URL https://openreview.net/forum?id= mRwgczYZFJ .Matej Zeˇ cevi´ c, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. Causal parrots: Large language models may talk causality but are not causal. Transactions on Machine Learn-ing Research , 2023. ISSN 2835-8856. URL https://openreview.net/forum?id= tv46tCzs83 .Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jen-nings, Chao Ma, Tom Minka, Nick Pawlowski, and James Vaughan. Understanding causality with large language models: Feasibility and opportunities, 2023. URL https://arxiv.org/ abs/2304.05524 .Jiaqi Zhang, Joel Jennings, Agrin Hilmkil, Nick Pawlowski, Cheng Zhang, and Chao Ma. Towards causal foundation model: on duality between causal inference and attention, 2024. URL https: //arxiv.org/abs/2310.00809 .

## A APPENDIX 

A.1 BACKGROUND AND RELATED WORK 

The section first describes the theoretical foundation of structural causal models. Then, we will examine general LLMs’ abilities and inabilities reported in the causality, which is a broad area of study including causal discovery and causal effect estimation. Then we will focus on existing efforts of using LLMs for causal effect estimation in the causality literature. A.2 STRUCTURAL CAUSAL MODELS AND EFFECT PARAMETERIZATION 

A Structural Causal Model is formally defined by a pair (G, F), where G is the Directed Acyclic Graph (DAG) representing the causal structure, and F = {fi} is the collection of structural equa-tions. Each endogenous variable Xi is determined by a function fi of its direct causes (parents) Pa (Xi) and an independent exogenous noise term Ei, such that Xi = fi(Pa (Xi), E i). In our prob-lem settings, we focus on linear causal effects to narrow down the problem space, and therefore, parameterization here refers to finding coefficients of these linear functions F.Conventional causal inference tasks include finding causal relationships, direction of edges and iden-tifying such parameters using data and intervention in the real-world. However, in our work, we only aim to estimate such parameters for linear causal effects using potentially encoded knowledge from large language models (LLMs) for each variable in a pre-specified DAG for a real-world phe-nomenon. A.2.1 LLM S GENERAL POTENTIAL AND CRITICISM FOR CAUSALITY 

For causal discovery, Long et al. (2022; 2023) used LLMs to identify causal connections between node pairs in DAGs, reporting opportunities despite inconsistencies and prompt sensitivities. Kici-man et al. (2024) found LLM-based methods outperform covariance-based algorithms in pairwise causal discovery and excel at natural language counterfactual reasoning. There is also work raising limitations of the current LLMs in this regard. For example, Zhang et al. (2023) proposed LLMs’ Causal Hierarchy, which consists of the three types: Type 1: Identifying 

14 This is a preprint. Work in progress. 

causal relationships using domain knowledge , Type 2: Discovering new knowledge from data , Type 3: Quantitative estimation of consequences of actions . They claim LLMs can perform Type 1 tasks via training data but not Type 2 and Type 3 due to token generation limitations. Our work evalu-ates LLMs’ capability for Type 3 tasks. Another study by Zeˇ cevi´ c et al. (2023) raises doubts about LLMs’ causal capabilities, conjecturing they merely learned causal facts from training data rather than causal mechanisms, Causal Parrots . They note LLMs are not explicitly trained for causal tasks and may simply parrot causal statements without true reasoning. Their criticism focuses on causal discovery; since we assume given causal structures and only estimate effects, this may not directly apply to our settings. Criticisms also include ground truth leakage in causal discovery Yang et al. (2023). In the broader context of causal inference, Jin et al. (2023) introduced CausalCOT, a prompt-ing strategy for the whole causal reasoning lifecycle, concluding this task is highly challenging for LLMs. A.2.2 LLM S FOR CAUSAL EFFECT ESTIMATION 

While some literature indicates initial success reports for treatment effect estimation in discrete 

domains, including counterfactual generation, causal effect estimation in continuous domains (e.g., SCMs) remains largely unexplored. Most existing approaches for continuous causal estimation provide numerical observational data to the model, which often requires specialized architectures or tokenization schemes; by contrast, we study coefficient elicitation without observational data, using only DAG structure and variable semantics. Bynum & Cho (2025) combined LLMs with structural causal modeling for pre-specified DAGs. However, they use sampling-based approaches to estimate effects rather than directly eliciting func-tional mappings. Their SD-SCMs represent effects as Conditional Probability Distributions rather than linear structural equations (Wright, 1921), focus on discrete-domain counterfactuals, while our work addresses continuous-domain parametrization for simulating real-world phenomena. Liu et al. (2025) surveys LLM-based causal inference, finding only three papers on treatment effect es-timation—two in discrete domains (Chen et al., 2023; Feder et al., 2023) and one (Zhang et al., 2024) addressing both domains but requiring numerical observational data with specialized atten-tion mechanisms. Unlike these approaches, we assume pre-specified causal DAG structures without observational data. Nafar et al. (2025) similarly benchmark effect estimation from pre-specified DAGs but assume discrete domains with Conditional Probability and sampling distributions, using eighty public DAGs in finance and health. Other benchmarks (Wang, 2024; Mohan & Karaletsos, 2025) examine LLMs on causality tasks but focus on causal relationship identification rather than quantitative effect estimation in continuous SCMs. A.2.3 SAMPLES OF DAG S USED IN THE EXPERIMENTS 

One may easily find DAGs we used in our experiments from references, but to save time of the readers, we attached the two example DAGs, Figure 2 and Figure 3. GM VBAFGC 

Figure 2: The DAG structure of cachexia1 from BnRep repository. A.3 DAG S SELECTION FLOWCHART 

15 This is a preprint. Work in progress. Owner Active Dependents Income Reports Card Majorcards Share Expenditure Age Months Selfemp 

Figure 3: The DAG structure of expenditure from BnRep repository. BnRep DAGs 

n = 214 

Type == "Gaussian" 

n = 16 

Excluded 

Type == " Discrete" or " Hybrid "

n = 198 

DAGs 

included 

n = 7 

Variable names with 

"{alphabet}{digits}" format 

or duplicates 

or #node > 15 

n = 9 

Figure 4: Inclusion and exclusion flowchart for DAG ground-truths from BnRep DAG repository 16