---
title: "Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models"
authors: "Kanta Yamaoka, Sumantrak Mukherjee, Thomas Gärtner, David Antony Selby, Stefan Konigorski, Eyke Hüllermeier, Viktor Bengs, Sebastian Josef Vollmer"
date: 2026-02-10
pdf: "https://arxiv.org/pdf/2602.10282v1"
tags: ["query:sr"]
score: 6.0
evidence: 提取函数关系和结构方程
tldr: 本研究针对大语言模型（LLM）在连续域定量因果推理能力的不足，提出了 Linear-LLM-SCM 评测框架。该框架在已知有向无环图（DAG）的前提下，评估 LLM 对线性高斯结构因果模型（SCM）参数估计的准确性。通过将 DAG 分解为局部父子集并提示模型生成回归方程，研究发现 LLM 在系数估计中存在显著的随机性、对错误 DAG 结构的敏感性以及语义扰动下的不稳定性。该研究揭示了 LLM 作为定量因果参数化工具的局限性，并开源了相关框架。
motivation: 尽管 LLM 在定性因果关系识别上表现出色，但其在连续域中估计定量因果效应大小的能力尚未得到充分探索。
method: 提出 Linear-LLM-SCM 框架，将 DAG 分解为局部节点集，引导 LLM 生成结构方程并与真实参数对比。
result: 实验揭示了 LLM 在系数估计中存在高度随机性，且极易受到伪边缘和结构或语义扰动的影响。
conclusion: 当前 LLM 在作为定量因果参数化工具方面存在明显局限，其估计结果的稳定性和准确性仍有待提升。
---

## Abstract
Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework decomposes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several challenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and semantic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced the benchmarking framework so that researchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly.