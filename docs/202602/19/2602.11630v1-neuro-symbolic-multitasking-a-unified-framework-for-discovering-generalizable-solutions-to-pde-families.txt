Title: Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families

URL Source: https://arxiv.org/pdf/2602.11630v1

Published Time: Fri, 13 Feb 2026 01:44:58 GMT

Number of Pages: 25

Markdown Content:
> 1

# Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families 

Yipeng Huang, Dejun Xu, Zexin Lin, Zhenzhong Wang, Member, IEEE, Min Jiang, Senior Member, IEEE 

Abstract —Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical struc-ture but varying in specific parameters. Traditional numerical methods, such as the finite element method, need to indepen-dently solve each instance within a PDE family, which incurs massive computational cost. On the other hand, while recent advancements in machine learning PDE solvers offer impressive computational speed and accuracy, their inherent “black-box” nature presents a considerable limitation. These methods primar-ily yield numerical approximations, thereby lacking the crucial interpretability provided by analytical expressions, which are essential for deeper scientific insight. To address these limitations, we propose a neuro-assisted multitasking symbolic PDE solver framework for PDE family solving, dubbed NMIPS. In particular, we employ multifactorial optimization to simultaneously discover the analytical solutions of PDEs. To enhance computational efficiency, we devise an affine transfer method by transferring learned mathematical structures among PDEs in a family, avoid-ing solving each PDE from scratch. Experimental results across multiple cases demonstrate promising improvements over existing baselines, achieving up to a ∼35.7% increase in accuracy while providing interpretable analytical solutions. 

Index Terms —Symbolic regression, evolutionary multitask op-timization, physics-informed machine learning. 

I. INTRODUCTION Solving Partial Differential Equations (PDEs) lies at the core of numerous scientific and engineering disciplines, providing the mathematical language to describe phenomena ranging from fluid dynamics to quantum mechanics [1]–[4]. Within this broad field, a particularly common and challenging sce-nario involves families of PDEs: these are sets of equations that share an identical underlying mathematical structure but vary in specific parameters. For instance, consider the need to simulate heat transfer in a new alloy across a range of operational temperatures, or to model the flow of a chemical mixture under various concentrations. In such cases, the fun-damental governing PDE remains the same, but the specific material properties or environmental conditions, represented by parameters, change [2]. 

Yipeng Huang, Dejun Xu, Zexin Lin, Min Jiang and Zhenzhong Wang are with the Department of Artificial Intelligence, Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan, Ministry of Culture and Tourism, School of Informatics, Xiamen University, and Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen 361005, Fujian, China (Corresponding authors: Zhenzhong Wang and Min Jiang, e-mail: zhenzhongwang@xmu.edu.cn and minjiang@xmu.edu.cn) .Analogous 

Fig. 1. Numerical solutions’ landscapes for PDEs with different parameters reveal that equations belonging to the same family demonstrate analogous analytical solutions. 

Early methodologies for solving PDE families have heavily relied on numerical techniques, such as the Finite Element Method (FEM) [5] or Finite Volume Method (FVM) [6]. These approaches typically operate by discretizing the domain to iteratively solve each individual PDE instance. Despite their robustness and established theoretical foundations, which offer rigorous guarantees regarding stability and convergence, these methods suffer from prohibitive computational costs. This limitation renders them ill-suited for high-throughput scenarios or real-time applications, as every parameter variation neces-sitates a complete re-computation from scratch. More recently, the landscape of PDE solving has been sig-nificantly transformed by advancements in machine learning-based solvers, creating distinct paradigms. Among these archi-tectures, Physics-Informed Neural Networks (PINNs) [3], [7], depicted in Fig. 2 (a), directly embed physical laws into the network to enforce consistency with the governing equations. However, PINNs remain instance-specific, requiring retrain-ing for new conditions. To overcome this, Neural Operators (NOs) [2], [8], [9] shown in Fig. 2 (b) have emerged as generalizable alternatives. These methods leverage universal approximation capabilities to learn mappings from input pa-rameters directly to PDE solutions. Consequently, NOs achieve substantial computational efficiency; once trained, they utilize their generalization capabilities to perform direct inference across varying parameter configurations without iterative re-optimization. While these machine learning-based PDE solvers offer substantial computational speed and accuracy, their inherent “black-box” nature presents a considerable limitation. These methods only provide numerical approximations, thereby lack-ing the crucial interpretability inherent to analytical expres-

> arXiv:2602.11630v1 [cs.AI] 12 Feb 2026 2

sions [10], [11]. Such interpretability is essential for deeper scientific insight and broader applicability. For instance, in analyzing complex fluid dynamics or reaction-diffusion sys-tems, obtaining explicit symbolic expressions (such as specific convection terms or diffusion coefficients) allows researchers to directly identify the physical mechanisms and conservation laws governing the system dynamics [12]. To bridge this gap, symbolic regression (SR) methods offer a powerful alternative capable of discovering closed-form mathematical formulas, providing highly interpretable analytical solutions for PDEs. However, a key limitation of ex-isting symbolic regression techniques, represented by Genetic Programming Symbolic Regression (GPSR) [13] in Fig. 2 (c), lies in their singular focus; they are incapable of generalizing across cases, treating only one PDE instance at a time. This makes them inefficient for solving entire PDE families, as each parameter variation necessitates an independent and time-consuming discovery process. Crucially, instances within a PDE family share an identical mathematical skeleton. This is empirically illustrated in Fig. 1, where the spatiotemporal landscapes of the 1D Advection equation are compared under different coefficients ( β = 0 .8

and β = 1 .0). Despite the parametric variation, the numerical solutions exhibit similar wave propagation patterns, and their corresponding analytical expressions retain a consistent sym-bolic structure, differing only in specific scalar coefficients. This implies that their solution landscapes exhibit intrinsic structural correlations [14], [15]. Explicitly leveraging these shared characteristics could bypass redundant discovery pro-cesses, accelerating the discovery of related equations. Keeping this in mind, we propose a novel Multitasking Symbolic Regression framework (i.e., Fig. 2 (d)) to simul-taneously solve multiple PDEs within a given family, moving beyond the one-off nature of traditional symbolic regression. The proposed multitasking symbolic regression framework is devised based on the multifactorial optimization strategy [16]. In particular, to promote the process of reusing shared knowledge among instances within a PDE family, we leverage an affine transfer method, which exploits and transfers the shared mathematical structural features that inherently exist between PDEs belonging to the same family, enabling more rapid and effective discovery of their analytical solutions. The main contributions are follows: 1) We introduce the first framework capable of simulta-neously solving entire families of PDEs to yield inter-pretable analytical solutions. Unlike existing methods that address individual PDE instances, our approach leverages the inherent structural similarities within a PDE family, offering a paradigm shift towards efficient and comprehensive solution discovery. 2) We employ a multifactorial optimization strategy to technically solve the PDE families simultaneously. By mapping distinct PDE instances into a unified opti-mization landscape, this mechanism enables a single population to concurrently evolve analytical solutions for multiple equations. 3) To promote the knowledge transfer, we integrate a novel affine transfer method to exploit the similarity between the instances within a PDE family. This technique trans-fers learned mathematical structural features between related PDEs, dramatically accelerating the process of finding analytical solutions compared to learning each PDE from scratch. 4) Experimental results across multiple cases demonstrate significant improvements over existing baselines, achiev-ing up to a ∼35.7% increase in accuracy while achieving promising efficiency. Furthermore, our proposed method showcased superior generalization capabilities in noisy data settings. The rest of this paper is organized as follows. Section II briefly presents the basic concepts of PDEs and reviews existing studies. Section III details the proposed approach. In Section IV, the empirical results of the proposed algorithm and various state-of-the-art algorithms are presented. Finally, conclusions are drawn in Section V. II. PRELIMINARIES 

A. PDE Families 

PDEs serve as the foundation for describing physical phe-nomena across science and engineering. However, practical applications often focus on a PDE family rather than a single equation [17]. A PDE family consists of equations that share an identical underlying mathematical structure but differ in specific physical parameters, such as viscosity or diffusion coefficients. In such families, the fundamental behavior of the system remains consistent, while the specific properties vary with the parameter settings. Formally, a PDE family is defined by a general functional form G that explicitly includes a parameter vector p. A PDE belonging to such a family is expressed as: 

G(x1, . . . , x n, u, ∇u, . . . , ∇ku; p) = 0 . (1) Here, G outlines the relationships between the independent variables, the unknown function, and its derivatives, while each unique parameter combination in p specifies a distinct instance within the family. Therefore, solving a PDE family involves finding solutions u(x; p) across a specified range of parameter values. For a comprehensive mathematical background on general PDEs and illustrative examples, please refer to the supplemental material (see Section I). 

B. Related Work 

a) Traditional Numerical Methods. These established techniques primarily rely on domain discretization. The Finite Difference Method (FDM) approximates derivatives using finite difference formulas [18]–[20]. Similarly, the FEM and FVM discretize the problem domain into elements or control volumes, utilizing polynomial basis functions to approximate solutions [21]–[23]. In contrast, spectral methods employ global basis functions, such as Fourier series or Chebyshev polynomials, to approximate solutions across the entire domain [24]–[26]. While robust, the optimal choice among these paradigms depends heavily on the specific PDE characteris-tics, and they often face computational bottlenecks in high-resolution or multi-query scenarios. 3

Physics-informed Neural Networks Neural Operator Genetic Programming Symbolic Regression Our Method 

Generalizable & Black-box Instance-specific & Black-box Instance-specific & Interpretable Multitasking & Interpretable 

+

x t

> Physical Laws

-

x t

+

x t

> Genetic Programming
> Genetic Programming
> Search Space
> Search Space
> Unified Search Space
> Genetic Programming
> Transfer

a bdc

> Physical Laws
> Fig. 2. Schematic comparison of different paradigms for solving PDEs. (a) Physics-informed Neural Networks embed physical laws by constraining the network training with PDE residuals. Despite integrating physics, they yield uninterpretable numerical approximations and require retraining for each instance, operating as instance-specific black-box models. (b) Neural Operator achieves generalizability by learning mappings between function spaces; however, they essentially remain black-box models that output numerical solutions without analytical forms. (c) Genetic Programming Symbolic Regression offers interpretability by searching for analytical expressions, yet it is constrained to solving one specific problem at a time. (d) Our method constructs a unified search space and leverages a transfer mechanism between tasks, realizing a paradigm that is both multitasking capable and interpretable.

b) Deep Learning-based PDE Solvers. To address com-putational limitations, deep learning methodologies have emerged as promising alternatives. Finite-dimensional oper-ators typically employ deep convolutional neural networks to parameterize the solution, though they often suffer from mesh dependency [27], [28]. PINNs integrate observational data with the governing PDE by embedding the PDE itself into the neural network’s loss function, leveraging automatic differentiation for gradient computations [29]–[31]. NOs, on the other hand, are designed to approximate the mapping from a PDE to its solution directly, yielding fast and discretization-invariant solvers [32]–[35]. These methods have primarily been applied to solve individual PDEs. However, recent work by Cai et al . [36] extended the neural operator DeepONet [37] to address systems of PDEs. While these numerical and deep learning approaches pro-vide effective approximations, the interpretability of their obtained solutions often remains a significant limitation. They typically function as black-box models or numerical approxi-mations, failing to reveal the underlying analytical structures governing the physical systems. This limitation highlights the need for methods capable of discovering explicit analytical solutions, a challenge addressed by symbolic regression. 

c) Symbolic regression. SR distinguishes itself from tradi-tional solvers by searching for explicit mathematical expres-sions that describe observed data. In recent years, deep learn-ing methods have made significant progress in this field. Un-like traditional discrete evolutionary search, these approaches leverage differentiable optimization and neural network guid-ance to improve search efficiency and robustness. For in-stance, Petersen et al . proposed a deep symbolic regression method based on policy gradients, which generates symbolic expressions through risk-seeking strategies and effectively recovers complex functional relationships [38]. Subsequently, Udrescu and Tegmark introduced the AI Feynman framework, which combines neural network approximations with physics-inspired decomposition strategies [39]. Moreover, end-to-end transformer models have enabled efficient search for high-dimensional expressions [40], and multi-modal fusion methods have further enhanced recoverability [41]. However, despite their computational efficiency, these deep learning approaches often fall short in interpretability and precise control over symbolic structures. In contrast, GPSR naturally offers advantages in inter-pretability and operator flexibility, making it particularly suit-able for discovering analytical solutions to PDEs. GP can directly generate analytical expressions and explore complex structures through evolutionary operators. In recent years, a series of improvements has further enhanced the performance and generalization of GP. For instance, Huang et al . introduced semantic linear genetic programming to accelerate conver-gence [42], while Bartlett et al . developed an exhaustive frame-work providing theoretical guarantees for formula recovery in small-scale problems [43]. Furthermore, to enhance search diversity and handle multi-objective complexities, Zhong et al .proposed multiform GP frameworks [44], while Wang et al .integrated reinforcement learning with GP to achieve multi-task semantic optimization [45]. Specific to the domain of PDEs, researchers have increas-ingly leveraged these capabilities to solve physical equations. 4z z 

> zz
> z
> MLP 2
> MLP 1
> Affine Transformation
> Affine Transformation
> Selection
> Evaluation
> Terminal Check
> Constant Optimization

+

x +

x t  

> Initialization Final Solution
> Yes
> No

a

b  

> z
> zzz
> z

+

x -

x t    

> Crossover & Mutation
> Path 1
> Path 2
> Knowledge Transfer
> zzz
> zz
> z
> zzzz
> Chrom 1
> Chrom 2
> ET 1
> ET 2
> Fig. 3. The overall framework of the proposed NMIPS. (a) Multifactorial PDE solving framework: This part constructs a unified encoding space to execute operations such as crossover, mutation, and selection. It embeds a knowledge transfer module to enable the efficient solving of multiple tasks within a PDE family. (b) Affine Transformation-based Knowledge Transfer: This mechanism extracts statistics of each population and uses multi-layer perceptrons to learn scaling and shifting parameters. It performs an affine transformation across populations to achieve efficient knowledge sharing between tasks.

Oh et al . used GP to search for symbolic functions satisfying boundary conditions, demonstrating that with judicious search space design, GP can discover analytical solutions for low-dimensional PDEs [46]. To further ensure the parsimony and physical meaningfulness of the discovered equations, Cao 

et al . introduced a simplification-pruning operator, yielding highly accurate and concise symbolic expressions [11]. Most recently, Cao et al . further extended GP to multi-physics systems via T-NNGP, a hybrid algorithm combining numerical methods with deep learning for efficient solving [10]. Nevertheless, despite these advancements, applying GP di-rectly to the solution of PDEs remains challenging. On one hand, constraints inherent in PDEs involve higher-order deriva-tives and boundary conditions, whereas GP fitness functions typically rely solely on pointwise fitting errors. On the other hand, the high-dimensional spatiotemporal nature of PDEs makes traditional GP search prone to the curse of dimension-ality. Most critically, however, a prevailing challenge remains: current methods generally struggle to simultaneously obtain analytical solutions for entire families of PDEs. They typically solve for a single instance with fixed parameters, lacking the generalization capability to handle parametric variations found in PDE families. Therefore, effectively applying GP to PDEs requires not only the integration of automatic differentiation and physical constraints but also mechanisms to support multi-task or generalizable learning, which is precisely the core problem addressed in this work. III. M ETHODOLOGY 

A. Multifactorial Optimization for Multi-Task PDE Solving 

Assume a PDE family consists of K PDE-solving tasks, 

Ti : {L i(u(xi)) = 0}Ki=1 , with different parameters, to be solved concurrently. Here, {L i}ni=1 is a set of differen-tial operators, u(x) is a vector of unknown functions, and 

x = (x, t ) is a vector of spatial-temporal variables. The primary objective is to obtain a set of analytical solutions 

{u(x1), u(x2), . . . , u(xK )}within a single optimization pro-cess to satisfy Li(u(xi)) = 0 , i = 1 , 2, . . . , K . To achieve this, we propose NMIPS, a method that leverages multifacto-rial optimization to simultaneously tackle these multiple PDE tasks. The overall architectural framework is visually depicted in Fig. 3, and the detailed algorithmic procedure is outlined in Algorithm 1. In the following sections, we will introduce the various components of this multifactorial optimization formulation, including its chromosome structure, evolutionary operations, and population evaluation and selection. 

a) Chromosome Design. To represent the analytical so-lutions of PDEs, we employ expression trees (ETs) to en-code these mathematical formulas. Structurally, ETs consist of internal function nodes and leaf nodes. Function nodes denote operations like addition ( +), sine ( sin ), etc., while leaf nodes represent terminals such as variables (e.g., x, t ) and constants. Specifically, our chromosome structure is based on the gene expression representation with automatically defined functions (C-ADF) [47], [48]. As illustrated in Fig. 4, a C-ADF chromosome comprises one main function and multiple ADFs. The main function yields the final output of the solution, while 5

Algorithm 1 NMIPS 

Input: Population size N , Transfer interval Gtf , Random mating probability rmp .

Output: The best analytical solutions for K PDE tasks.  

> 1:

Construct a unified chromosome encoding space compatible with all PDE tasks;  

> 2:

Initialize a unified population within the space and calculate factorial costs with constant optimization;  

> 3:

Assign the factorial rank, skill factor, and scalar fitness values for all individuals;  

> 4:

Set generation counter g = 1 ; 

> 5:

while stopping conditions are not satisfied do  

> 6:

for i = 1 to N do  

> 7:

if rand () < rmp then  

> 8:

Generate offspring ui by one-point crossover and uniform mutation;  

> 9:

else  

> 10:

Generate offspring ui by DE-based mutation and crossover;  

> 11:

end if  

> 12:

Evaluate factorial cost of ui with constant optimization based on its skill factor;  

> 13:

end for  

> 14:

Perform one-to-one selection to update the population;  

> 15:

Update scalar fitness for all individuals;  

> 16:

if g mod Gtf == 0 then  

> 17:

Perform knowledge transfer (see Algorithm 2);  

> 18:

end if  

> 19:

Update best solutions for each task;  

> 20:

g = g + 1 ; 

> 21:

end while  

> 22:

return The best analytical solutions found for all K tasks; 

the ADFs serve as reusable sub-functions called by the main function or other ADFs. Both the main function and ADFs are represented by the Karva expression [47]. A Karva expression describes a solution using a fixed-length string composed of functions and terminals, which can be deterministically converted into an ET using a breadth-first traversal method. Each Karva expression is divided into two parts: a head and a tail . The head may contain both function and terminal symbols, whereas the tail consists only of terminals. To ensure that each chromosome can be converted into a valid ET, the lengths of the head ( h)and tail ( l) are constrained by the following relationship: 

l = h · (v − 1) + 1 , (2) where v represents the maximum number of arguments among all functions in the defined function set. A specific example of a chromosome featuring one main function and one ADF, along with their decoded expressions, is provided in Fig. 5. As observed from this example, a final solution can be decoded as 2 · (2 · a2 · c + a)2 · b · c.To adapt C-ADF to a gene expression representation for multitasking, an extension to its encoding scheme is essential. A key challenge is that different tasks across domains may possess unique sets of functions and terminals. To address this, we employ integers to represent both functions and terminals. This approach allows a single integer value in the chromosome to represent various symbols specific to different tasks. Specifically, suppose there are K tasks to be solved. Let Fi

and Ti denote the function set and terminal set of the i-th task, respectively. To ensure proper conversion of any chromosome      

> head tail head tail head tail

······                          

> G1+ * G 2a b c a c * + * t1t1t1t2
> Main Function ADF 1(G1)
> head tail tail head
> G1
> +
> a
> ac
> b
> *
> c
> *
> +*
> t1t1t2
> G1(G1(a, c ) + a, b * c )(t1+t1) * ( t1* t 2)
> Main Function ADF Na ADF 1
> t1G1
> Fig. 4. Structure of the C-ADF.
> head tail head tail head tail

······                                          

> G1+ * G 2a b c a c * + * t1t1t1t2
> Main Function ADF 1(G1)
> head tail tail head
> G1
> +
> a
> ac
> b
> *
> c
> *
> +*
> t1t1t2
> G1(G1(a, c ) + a, b * c )(t1+t1) * ( t1* t 2)
> Functions ADFs Terminals Input arguments
> Main Function ADF Na ADF 1
> t1G1
> G1+ * G 2a b c a c * + * t1t1t1t2
> Main Function ADF 1(G1)
> head tail tail head
> G1
> +
> a
> ac
> b
> *
> c
> *
> +*
> t1t1t2
> G1(G1(a, c ) + a, b * c )(t1+t1) * ( t1* t 2)
> t1G1
> Fig. 5. Example of a chromosome includes one main function and one ADF.

to a valid ET, the lengths of the head ( h) and tail ( l) are governed by an adapted constraint: 

l = h ·

(

max 

> a∈F1∪F2∪... ∪FK

ξ(a) − 1

)

+ 1 , (3) where ξ(a) returns the number of arguments of function a.After determining the chromosome length, the next crucial aspect is the representation of individual elements within each chromosome using integers. Four distinct integer ranges are defined to represent the different element types: functions, ADFs, terminals, and input arguments for ADFs. These ranges are specified as [0 , A −1] , [A, B −1] , [B, C −1] , and [C, D −1] .The upper bounds A, B, C, and D are calculated as follows: 

A = max 

> i∈{ 1,...,K }

|Fi|, (4) 

B = A + Na, (5) 

C = B + max 

> i∈{ 1,...,K }

|Ti|, (6) 

D = C + Ng , (7) where |Fi| returns the number of elements in set Fi, Na is the number of ADFs in each chromosome, and Ng is the number of input arguments for each ADF. With this common chromosome representation, the decod-ing process that transforms a general chromosome into a task-specific chromosome for evaluation is conducted in two distinct phases: 1) Element type identification determines the corresponding type represented by each integer dimension of the chromosome. This is achieved by checking which predefined integer range the integer belongs to. 2) Task-specific mapping scales the integer value according to the maximum number of elements for the identified type within the specific task. The scaling formula is given by: 

x′ 

> i

= xi − Lx

Ux − Lx

· Nx, (8) where [Lx, U x − 1] is the integer range of the element type identified in the first phase, and Nx is the maximum number of elements for this identified type in the context of the current task. Through this mapping, x′ 

> i

is converted to an integer between 0 and Nx − 1, which then serves as an index to map 6

x′ 

> i

to a meaningful symbol (function or terminal) specific to the task being evaluated. For population initialization, we construct a unified encod-ing space accommodating all target PDE tasks. Subsequently, a population, denoted as P OP , of size N is randomly ini-tialized. Each candidate solution for a PDE is encoded as a vector, with individual elements represented by integers. To differentiate the semantic meaning of these elements, the integer space is partitioned into the four distinct segments detailed above. All elements are randomly sampled from their respective segments according to their predefined types. 

b) Population Reproduction. The reproduction process aims to encourage the discovery and implicit transfer of useful genetic material across tasks, allowing beneficial traits found for one task to improve the search for others. To achieve this, NMIPS employs two crossover and mutation strategies: 1) one-point crossover and uniform mutation, and 2) the differential evolution-based mutation and crossover. The two crossover and mutation strategies are randomly selected based on a probability of rmp .For one-point crossover and uniform mutation, a randomly selected individual zr1 is paired with the current parent individual zi. A single-point crossover operation is performed between them, producing an intermediate offspring yi. This process facilitates the exchange of potentially useful genetic material. Subsequently, a uniform mutation is applied to yi

to generate the final offspring ui, which further enhances population diversity and aids in escaping local optima. The offspring’s skill factor τ is randomly inherited from either zi

or zr1 with equal probability. Alternatively, if the one-point crossover and uniform mu-tation are not performed, the differential evolution strategy constructs a differential vector using the best individual zbest 

from the current population and two other distinct random individuals. This vector is then scaled by a mutation rate ϵ.A gene-level crossover is then performed between the target individual zi and this mutation vector. This process generates an offspring that not only inherits partial information from the parent but also incorporates significant new genetic diversity. Following these reproduction processes, the newly gener-ated offspring population is merged with the parent population to form a temporary population, denoted as P OP temp , where the skill factor and scalar fitness of each individual are subsequently updated. 

c) Population Evaluation. The evaluation process must consider not only the data point fitting error but also the un-derlying physical consistency dictated by PDEs. Specifically, for a given individual z on task Ti, its objective function value, denoted as fi(z), serves as the factorial cost in multifactorial optimization. This fi(z) is computed by combining both the data point fitting error and the PDE fitting error for task Ti.For the data point fitting error, the NMIPS utilizes the root mean square error (RMSE) as an evaluation metric to quantify the candidate solution’s performance in fitting the training samples. Given a set of data points {(xk, t k, u k)}Nd 

> k=1

sampled from a high-precision numerical simulation of a particular PDE, where (xk, t k) represents the coordinate point and uk

is the corresponding true solution value at that point. If the generated candidate analytical solution expression is ˆu(x, t ),then the formula for calculating the data point fitting error on these sampled points is as follows: 

Ldata =

√√√√ 1

NdNd∑

> k=1

(ˆ u(xk, t k) − uk)2, (9) where Nd denotes the total number of sampled data points used for training. To ensure the candidate expression satisfies the physical constraints of the original PDE, we incorporate the PDE fitting error. This comprehensive error term comprises three distinct components: the PDE residual error, the initial condition error, and the boundary condition error. Let the form of the PDE be expressed as: 

F (x, t, u, ∂u 

∂x , ∂u 

∂t , . . . ) = 0 , (10) where F (u, ∂u  

> ∂x

, ∂u  

> ∂t

, . . . ) represents the PDE expression con-structed from the unknown function u and its partial deriva-tives of various orders. After substituting the candidate ex-pression ˆu(x, t ) into the target PDE, the PDE residual can be calculated on a set of test points {(xj , t j )}Nf

> j=1

, defined as: 

LPDE residual =

√√√√ 1

NfNf

∑

> j=1

|F (xj , t j , ˆu, ∂ ˆu

∂x , ∂ ˆu

∂t , . . . )|2, (11) where Nf denotes the number of collocation points sampled within the spatiotemporal domain to evaluate the physics residual. Furthermore, to guarantee the consistency of the candidate expression at the initial temporal moment and spatial bound-aries, initial condition error and boundary condition error terms are introduced separately. Taking the initial condition 

u(x, 0) = f (x) as an illustrative example, its corresponding error can be formulated as: 

LIC =

√√√√ 1

Nic Nic ∑

> k=1

(ˆ u(xk, 0) − f (xk)) 2, (12) where Nic denotes the number of sampling points distributed along the initial boundary. Similarly, the boundary condition error LBC can be constructed, thereby collectively forming the complete PDE fitting error component. To ensure a fair assessment of the discovered structures, we optimize numerical constants prior to assigning the final fit-ness. Specifically, constants within each expression are treated as learnable parameters and fine-tuned via a gradient-based optimizer to minimize the combined loss from data and PDE constraints. We employ automatic differentiation to construct computational graphs, enabling the precise computation of high-order derivatives required for both constant tuning and residual evaluation. 

d) Population Selection. Based on the factorial costs across all tasks, the following multifactorial metrics are derived to determine which individuals from the temporary population (P OP temp ) advance to the next generation:  

> •

Factorial rank ( rj

> z

): For an individual z, its factorial rank 

rj 

> z

on task Tj is its position when the entire population is 7

sorted in ascending order of factorial cost fj (z) for task 

Tj . A lower rank indicates better performance.  

> •

Skill factor ( τz): The skill factor τz of an individual 

z indicates the task on which it performs best (i.e., achieves the lowest factorial rank). It is defined as τz =arg min j {rj

> z

}. 

> •

Scalar fitness (φz): The scalar fitness φz is a task-independent metric used to compare individuals across different tasks. It is defined as φz = 1 /[min j∈{ 1,...,K } rj

> z

].This metric consolidates multi-task performance into a single value, where a higher scalar fitness implies bet-ter overall performance across its best-performing task. Formally, an individual za is said to multifactorially dominate zb if φza > φ zb .To select individuals from the population for survival into the next generation, we adopt a one-to-one selection strategy, commonly employed in the differential evolution algorithm. This mechanism critically depends on a scalar fitness metric derived during the population evaluation phase. Specifically, each newly generated offspring ui undergoes a direct compar-ison against its corresponding parent zi. The update rule for the i-th individual in the next generation is defined as: 

zi =

{

ui, if φui > φ zi or zi ∈ S redundant 

zi, otherwise (13) where Sredundant represents the set of redundant individuals (e.g., duplicate solutions or non-viable candidates). If ui

exhibits a superior scalar fitness compared to zi, or if zi

is identified as a redundant individual, then ui successfully survives and replaces zi in the subsequent generation. Con-versely, if neither of these conditions is met, zi is retained in the population. This selective strategy serves a dual purpose: it not only aids in preserving high-quality solutions but also dynamically contributes to enhancing population diversity by promptly replacing less fit or redundant individuals. 

B. Affine Transformation for Knowledge Transfer 

To bolster the knowledge sharing across diverse PDE pa-rameter configurations, we propose an affine transformation-based knowledge transfer module. As illustrated in Fig. 3 (b), this module operates on a dual-population mechanism, designed to align the statistical distributions of two distinct populations (denoted as Z1 and Z2) via learnable scaling and shifting operations. Affine transformations are notably adept at facilitating this alignment while effectively preserving the topological structure of the original features. Consequently, we leverage this approach to convey valuable genetic material and solution structures from the source task’s search space to that of the target task, thereby enhancing structural diversity. Let the two populations from different tasks or evolutionary branches be represented as Z1 = {z(i)1 }Ni=1 and Z2 =

{z(j)2 }Nj=1 . To capture the global distribution characteristics, we compute the mean vector μk and variance vector σ2 

> k

for each population k ∈ { 1, 2}. These statistics serve as inputs for the subsequent alignment step. To map the statistics of Z1 towards Z2 and vice versa, we employ two parallel multi-layer perceptrons (MLP 1 and 

Algorithm 2 Affine Transformation-based Knowledge Trans-fer                                                                                             

> Input: Current population Z, Population size N.
> Output: Transferred population Znew .
> 1: Divide Zinto task-specific groups G1and G2according to skill factor τ;
> 2: Sort G1, G 2descendingly by scalar fitness φ;
> 3: Z1←Top N/ 2individuals from G1;
> 4: Z2←Top N/ 2individuals from G2;
> 5: for k= 1 to 2do
> 6: Calculate mean μkand variance σ2
> kof Zk;
> 7: Obtain affine parameters γk, β kby Eq. (14);
> 8: end for
> 9: Z′
> 1←Apply affine transform on Z1using μ1,σ1, γ 1, β 1;
> 10: Z′
> 2←Apply affine transform on Z2using μ2,σ2, γ 2, β 2;
> 11: Repair genes in Z′
> 1∪ Z ′
> 2via modulo arithmetic to valid integer ranges;
> 12: Ztemp =Z1∪ Z 2∪ Z ′
> 1∪ Z ′
> 2;
> 13: Re-evaluate factorial costs and update φfor all z∈ Z temp ;
> 14: Sort Ztemp descendingly by scalar fitness φ;
> 15: Znew ←Top Nindividuals from Ztemp ;
> 16: return Znew ;

MLP 2). These networks output two learnable parameters for affine transformation, a scaling factor γ and an offset factor 

β, which modulate the mapping between Z1 and Z2. Such transformations have been proven to effectively promote the resolution of PDEs within PINNs [49]. By feeding the ex-tracted moments into the MLPs, the parameters are optimized in a data-driven manner to minimize distribution discrepancy. Formally, the mapping process for the k-th population is defined as: 

[γk, β k] = MLP k(μk, σ2

> k

; θk), (14) where θk represents the trainable weights of the MLP. With the learned parameters γk and βk, we apply an affine transformation to the normalized individuals. For an individual 

zk ∈ Z k, the transformed individual z′ 

> k

is given by: 

z′ 

> k

= (1 + γk) ⊙ zk − μk

√σ2 

> k

+ ε + βk, (15) where ⊙ denotes element-wise multiplication, and ε is a small constant for numerical stability. This operation effectively projects the individuals into a new search space that fuses the structural properties of the original population with the statistical characteristics of the counterpart. To ensure the transformed distribution effectively aligns with the target distribution, we employ MSE as the opti-mization objective. We minimize the element-wise difference between the transformed source population and the target population, which acts as a proxy for minimizing the Wasser-stein distance between the two distributions. The loss function 

Lalign is defined as: 

Lalign = 1

N

> N

∑

> i=1

∥z′ 

> i

− z(i)

> target

∥2, (16) where z(i) 

> target

represents the i-th individual in the target population. Finally, to leverage the diversity gained from this bidirectional transfer and drive a renewed selection process, the transformed populations are merged with the original 8

pools. The aggregated population Ztemp is formed by con-catenating the transformed sets, denoted as: 

Ztemp = Z1 ∪ Z 2 ∪ Z ′ 

> 1

∪ Z ′

> 2

. (17) A rigorous selection process is then applied to Ztemp to identify superior individuals for the next evolutionary cycle. This knowledge transfer enhances population diversity and significantly improves the evolutionary efficiency of the target task. The complete procedure is detailed in Algorithm 2. IV. EXPERIMENTS 

A. Experiments setup 1) Datasets: To evaluate the effectiveness of NMIPS in solving parameterized PDEs, we constructed a benchmark dataset consisting of six representative PDEs: the 1D Advec-tion equation, 1D Burgers’ equation, 1D Advection-Diffusion equation, 2D Advection equation, 2D Navier-Stokes equation, and 3D Advection equation. For each PDE, we systematically varied scalar and functional parameters to generate multiple tasks. For each PDE, while keeping the equation structure unchanged, four different sets of functional parameters were sampled to form the corresponding PDE Family. For each task, 1100 points were randomly sampled from the computational domain to construct the training and evaluation datasets. For PDEs with known analytical solutions, data were generated using closed-form expressions. For PDEs lacking general analytical solutions, high-precision numerical methods were employed to compute reference solutions. 

2) Baselines: To validate the performance of NMIPS, we compared it with five state-of-the-art baselines: PhySO [50], DSR [38], SP-GPSR [13], GNOT [51], and Geo-FNO [52]. These baselines span various approaches, including symbolic regression, genetic programming, and neural operators. For more detailed experimental settings, please refer to the supplemental material (see Section II). This includes detailed descriptions of each equation in the dataset, specific configurations of the baseline methods, as well as the hyper-parameter settings (e.g., population size, mutation rate) and the symbol library used for NMIPS. 

B. Results and Discussion 1) Results on the 1D Advection Equation: To evaluate the predictive accuracy of NMIPS and compare it with existing approaches, we conducted experiments on the 1D advection equation with varying advection coefficients, β. The mean squared error (MSE) of the predicted physical fields is pre-sented in Table II. The table includes results from NMIPS under different combinations (Num. 0, 1, 2, 3) of β of two PDEs and five other baseline methods. The results reveal several key insights. First, NMIPS consistently demonstrates superior performance across a range of advection coefficients. Specifically, NMIPS achieves an average MSE of 5.47E-01, which is lower than all other baselines. This indicates that NMIPS can more accurately model the physical field evolution of the 1D advection equation compared to existing state-of-the-art methods. Second, while the performance of all models generally fluctuates with changes in β, NMIPS              

> TABLE I DISCOVERED EXPRESSIONS FOR THE 1D B URGERS ’EQUATION BY DIFFERENT SYMBOLIC REGRESSION METHODS
> Method Expression
> NMIPS u= 0 .014( t−x) + 0 .024
> PhySO u= sin(16809 .645 e−105 .420 t)
> DSR u= sin( et·(1 .0−eeex
> ))
> SP-GPSR u= 3 .496 e−5.511 e−e−10 .0t

exhibits a more stable and consistently low error rate. The overall average MSE confirms that NMIPS not only achieves competitive performance but also demonstrates a superior level of generalization across different physical parameter settings. To provide a visual assessment of model fidelity, the spatial-temporal error distributions for the specific case of β = 1 .000 

are presented in supplemental material (see Fig. S-7). These visualizations corroborate the quantitative results, confirming that NMIPS maintains significantly higher fidelity to the exact dynamics compared to baseline methods, which exhibit more pronounced deviations. 

2) Results on the 1D Burgers’ Equation: To evaluate the predictive accuracy of NMIPS on the 1D Burgers’ equation, we conducted experiments with varying viscosity coefficients, 

ν. As shown in Table III, NMIPS achieves the best overall performance with an average MSE of 1.53E-02. This result outperforms the closest competitors, SP-GPSR (1.63E-02) and DSR (1.69E-02), while maintaining a significant lead over other comparative methods. A key observation is our method’s stable performance across different values of ν. For instance, at ν = 0.001 ,our method achieves a minimum MSE of 3.97E-02, out-performing all competitors. Although some baselines (e.g., PhySO at ν = 0 .007 ) may achieve a lower MSE at a spe-cific point, our method maintains consistent accuracy across all tested ν values, as evidenced by our overall average. Furthermore, qualitative analysis further corroborates these quantitative findings. The spatial-temporal error maps in Fig. 6 (ν = 0.001 ) show that NMIPS yields significantly lower pointwise error—visualized as darker regions—particularly in high-gradient shock wave zones. The symbolic expressions in Table I offer insight into this performance gap. While NMIPS identifies a concise and physically interpretable linear relation (u ∝ t − x), the baseline methods converge on overly complex expressions containing nested exponentials. This indicates that NMIPS effectively captures the true underlying dynamics, avoiding the overfitting issues commonly observed in other symbolic regression approaches. 

3) Results on the 1D Advection-Diffusion Equation: To evaluate the capabilities of NMIPS on the 1D Advection-Diffusion equation, we conducted experiments across various combinations of the advection coefficient ( α) and diffusion coefficient ( β). As detailed in the supplemental material (see Table S-I), NMIPS achieves the lowest average MSE of 1.19E-01. This performance surpasses all baseline methods, including the closest competitors, SP-GPSR and DSR, demonstrating su-perior overall accuracy. The specific challenge of this equation lies in regimes where advection dominates diffusion. In such 9                                                                                                                      

> TABLE II MSE OF DIFFERENT METHODS ON 1D A DVECTION EQUATION
> Num βNMIPS PhySO DSR SP-GPSR GNOT Geo-FNO
> 0123
> 00.100 /5.64E-01 5.87E-01 5.75E-01 6.24E-01 5.93E-01 6.08E-01 8.18E-01 9.52E-01 10.400 4.34E-01 /4.33E-01 4.33E-01 4.64E-01 4.51E-01 4.44E-01 4.48E-01 5.39E-01 20.700 1.46E-01 1.48E-01 /1.60E-01 1.59E-01 1.59E-01 1.51E-01 5.25E-01 2.14E-01 31.000 1.02E+00 1.04E+00 1.02E+00 /1.14E+00 1.03E+00 1.11E+00 6.55E-01 1.17E+00
> Avg 5.47E-01 5.97E-01 5.58E-01 5.77E-01 6.11E-01 7.19E-01
> TABLE III MSE OF DIFFERENT METHODS ON 1D B URGERS ’EQUATION
> Num νNMIPS PhySO DSR SP-GPSR GNOT Geo-FNO
> 0123
> 00.001 /3.98E-02 3.97E-02 4.04E-02 7.52E-02 4.77E-02 4.51E-02 2.00E-01 5.72E-02 10.004 6.39E-03 /7.01E-03 6.50E-03 5.59E-03 6.31E-03 7.19E-03 8.37E-02 1.22E-02 20.007 7.14E-03 6.20E-03 /6.37E-03 5.40E-03 5.92E-03 5.68E-03 5.23E-02 1.13E-02 30.010 1.02E-02 6.94E-03 7.00E-03 /8.98E-03 7.90E-03 7.31E-03 7.92E-02 1.08E-02
> Avg 1.53E-02 2.38E-02 1.69E-02 1.63E-02 1.04E-01 2.29E-02
> Fig. 6. A spatial-temporal error analysis of different methods for the 1D Burgers’ equation.

scenarios, rapid transport creates steep gradients that are not smoothed by diffusion, leading to shock-like features that are notoriously difficult to model. For the challenging setting of 

α = 0 .700 and β = 0 .001 , NMIPS achieves a competitive MSE of 1.47E-01, significantly outperforming PhySO (1.71E-01) and Geo-FNO (1.56E-01). The error heatmaps in the supplemental material (see Fig. S-8) corroborate the quantitative findings, further underscor-ing NMIPS’s strong generalization capabilities in effectively balancing the dynamics of advection and diffusion. 

4) Results on the 2D Advection Equation: To evaluate the predictive accuracy of NMIPS on the 2D advection equa-tion, we conducted experiments with various combinations of advection coefficients, βx and βy . As presented in Table IV, NMIPS achieves the best overall performance with a low average MSE of 2.58E-01. This result surpasses closely competing symbolic regression methods such as PhySO and SP-GPSR, while significantly outperforming deep learning baselines like GNOT and Geo-FNO. The results further highlight the robustness of NMIPS across different flow regimes. For the computationally challenging case characterized by high advection in both dimensions (βx = 0 .700 , β y = 0 .969 ), our method achieves a minimum MSE of 2.41E-01, demonstrating its ability to model complex multidimensional dynamics effectively. In contrast, baselines such as GNOT exhibit extreme performance fluctuations; while GNOT achieves low error in specific low-advection settings (e.g., Num 0), its error spikes drastically in other cases, resulting in poor overall generalization. Conversely, NMIPS maintains consistent stability across all parameter combinations, validating its superior adaptability. Additionally, the error heatmap for a specific case is presented in the supplemental material (see Fig. S-9). 

5) Results on the 2D Navier-Stokes Equation: To evaluate the predictive accuracy of NMIPS on the 2D Navier-Stokes equation, we conducted experiments across a range of viscos-ity coefficients ( ν). As presented in the supplemental material (see Table S-II), NMIPS demonstrates leading performance. With an average MSE of 5.25E-02, NMIPS outperforms all other baselines, including PhySO, Geo-FNO, and GNOT, and maintains a competitive edge over SP-GPSR and DSR. This confirms our model’s effectiveness in accurately capturing the complex dynamics of viscous fluid flow. The robustness of NMIPS is particularly evident in specific flow regimes. For instance, at ν = 0.020 , it achieves a minimum MSE of 4.36E-02, establishing a clear performance margin over PhySO (1.14E-01) and GNOT (1.07E-01). For a deeper investigation into the source of this accuracy, we provide the discovered symbolic expressions (see Table S-III) and their detailed analysis in the supplemental material. Additionally, the error heatmap for a representative case is visualized in the supplemental material (see Fig. S-10), further corroborating the high fidelity of our approach. 

6) Results on the 3D Advection Equation: To evaluate the predictive accuracy of NMIPS on the 3D advection equation, we conducted experiments with varying advection coefficients 10                                                                                                                                    

> TABLE IV MSE OF DIFFERENT METHODS ON 2D A DVECTION EQUATION
> Num βxβyNMIPS PhySO DSR SP-GPSR GNOT Geo-FNO
> 0123
> 00.100 0.842 /2.86E-01 2.55E-01 2.92E-01 2.81E-01 2.63E-01 2.81E-01 1.38E-01 4.36E-01 10.400 0.349 2.55E-01 /2.60E-01 2.53E-01 2.56E-01 2.88E-01 2.51E-01 5.17E-01 4.04E-01 20.700 0.969 2.50E-01 2.41E-01 /2.45E-01 2.53E-01 2.72E-01 2.65E-01 5.19E-01 3.92E-01 31.000 0.186 2.50E-01 2.56E-01 2.57E-01 /2.67E-01 2.60E-01 2.62E-01 5.03E-01 4.19E-01
> Avg 2.58E-01 2.64E-01 2.71E-01 2.64E-01 4.19E-01 4.13E-01
> TABLE V MSE OF DIFFERENT METHODS ON 3D A DVECTION EQUATION
> Num βxβyβzNMIPS PhySO DSR SP-GPSR GNOT Geo-FNO
> 0123
> 00.100 0.983 0.548 /1.23E-01 1.24E-01 1.30E-01 1.32E-01 1.41E-01 1.33E-01 4.07E-01 2.11E-01 10.400 0.579 0.573 1.27E-01 /1.27E-01 1.46E-01 1.36E-01 1.39E-01 1.31E-01 3.57E-01 2.02E-01 20.700 0.818 0.951 1.23E-01 1.27E-01 /1.26E-01 1.46E-01 1.40E-01 1.41E-01 3.63E-01 1.93E-01 31.000 0.697 0.204 1.38E-01 1.33E-01 1.34E-01 /1.40E-01 1.40E-01 1.34E-01 3.59E-01 1.84E-01
> Avg 1.30E-01 1.39E-01 1.40E-01 1.35E-01 3.72E-01 1.97E-01

(βx, βy , and βz ). As shown in Table V, NMIPS consis-tently achieves superior performance. With an average MSE of 1.30E-01, it outperforms all baselines. The robustness of NMIPS is particularly evident in high-advection regimes. For the challenging case characterized by high coefficients across all three dimensions ( βx = 0 .700 , β y = 0 .818 , β z = 0 .951 ), NMIPS maintains a low MSE of 1.23E-01. This performance is notably better than that of PhySO (1.46E-01) and DSR (1.40E-01), highlighting the model’s strong generalization capabilities under complex transport conditions. This quantitative superiority is further supported by the symbolic analysis provided in the supplemental material (see Table S-IV). NMIPS identifies a physically plausible form that correctly captures the advective relationship, whereas baseline methods fail to identify the correct symbolic form. 

7) Results on Computational Costs: To evaluate computa-tional efficiency, we compared NMIPS with symbolic regres-sion baselines across six benchmark PDE tasks. As shown in the supplemental material (see Fig. S-11), the vertical axis is plotted on a logarithmic scale to clearly highlight the orders-of-magnitude differences in runtime among the methods. The figure clearly demonstrates that NMIPS holds a signifi-cant computational advantage, finishing as the fastest approach in the majority of tasks, including the 1D Burgers’, 1D Advection-Diffusion, 2D Navier-Stokes, and 3D Advection equations. The difference is particularly stark for the 1D tasks. For the 1D Burgers’ and 1D Advection-Diffusion equations, NMIPS completes in approximately 250-300 seconds. In con-trast, competitors like PhySO, DSR, and SP-GPSR require runtimes in the thousands of seconds (ranging from 2000s to 4000s), representing a full order-of-magnitude speedup for our approach. While PhySO and SP-GPSR proved faster on the 1D Advection and 2D Advection tasks, respectively, NMIPS remained highly competitive. More importantly, NMIPS con-sistently avoids the extreme computational costs that plague other symbolic methods in complex scenarios. Overall, these results validate the computational efficiency of NMIPS. When combined with the high accuracy and interpretability demonstrated in previous sections, NMIPS es-tablishes its practicality and ability to deliver a more balanced trade-off between accuracy, speed, and interpretability than competing symbolic regression approaches. 

C. Ablation Studies 

To assess the contribution of the knowledge transfer mod-ule, we performed an ablation study by removing the affine transformation (w/o AT), a key component for transferring knowledge between different PDEs. The results in Table VI provide a clear comparison of our full model against the ablated version across six distinct physical equations. The results consistently demonstrate the critical role of the affine transformation module. On average, our full model (w/ AT) consistently outperforms the ablated version (w/o AT) across all six equations. While performance in a few specific, individual parameter settings can fluctuate (e.g., Num 2 for 1D Burgers’), the average MSE metrics confirm the module’s consistent and positive contribution. Quantitatively, the affine transformation provides its most significant performance gain on the 2D Advection equation, reducing the average MSE from 3.13E-01 to 2.58E-01, a substantial 17.6% improve-ment. Similarly, notable gains are seen on the 2D Navier-Stokes (a 13.4% reduction from 6.06E-02 to 5.25E-02) and 3D Advection (an 11.6% reduction from 1.47E-01 to 1.30E-01) tasks. This strongly suggests that the module’s ability to align latent features becomes increasingly crucial as the complexity of the physical system increases. Even on tasks where the average improvement is more modest, such as the 1D Burgers’ (a 2.5% reduction from 1.57E-02 to 1.53E-02) and 1D Advection-Diffusion (a 2.5% reduction from 1.22E-01 to 1.19E-01), the module still enhances overall performance. These findings strongly confirm that the affine transformation module successfully facilitates knowledge transfer, enhancing the model’s predictive accuracy and generalization. 11 

TABLE VI MSE ON DIFFERENT EQUATIONS WITH AND WITHOUT AFFINE TRANSFORMATION 

Num 1D Advection 1D Burgers’ 1D Advection-Diffusion 2D Advection 2D Navier-Stokes 3D Advection w/ AT w/o AT w/ AT w/o AT w/ AT w/o AT w/ AT w/o AT w/ AT w/o AT w/ AT w/o AT 

0 5.75E-01 7.31E-01 3.99E-02 4.30E-02 2.66E-01 2.77E-01 2.78E-01 3.35E-01 1.23E-01 1.30E-01 1.26E-01 1.32E-01 1 4.33E-01 4.58E-01 6.63E-03 6.82E-03 1.08E-02 1.08E-02 2.56E-01 3.40E-01 4.62E-02 5.86E-02 1.33E-01 1.30E-01 2 1.51E-01 1.47E-01 6.57E-03 6.08E-03 1.48E-01 1.49E-01 2.45E-01 2.81E-01 2.48E-02 3.59E-02 1.26E-01 1.32E-01 3 1.03E+00 1.09E+00 8.06E-03 6.98E-03 5.13E-02 5.12E-02 2.54E-01 2.96E-01 1.64E-02 1.75E-02 1.35E-01 1.92E-01 

Avg 5.47E-01 6.06E-01 1.53E-02 1.57E-02 1.19E-01 1.22E-01 2.58E-01 3.13E-01 5.25E-02 6.06E-02 1.30E-01 1.47E-01 

Fig. 7. Noise robustness comparison of different methods. 

D. Sensitivity Studies 

This section analyzes the performance of the proposed method in predicting physical fields under various noise levels. We add Gaussian noise to both the fields and the interface position data at each sampling point. The noise ϵ is drawn from a normal distribution N (0 , σ 2), where the standard deviation 

σ is set to 5%, 10%, and 15% of the original field magnitude, yielding noisy data ˜u = u + ϵ.The results, presented in Fig. 7, compare the MSE of our proposed algorithm with five other baselines. The re-sults clearly demonstrate that our method exhibits superior noise robustness. As shown in the graph, our method (solid orange line) consistently achieves the lowest MSE across all tested noise levels, from 0% to 15%. Its performance remains exceptionally stable, with the error curve staying almost flat, showing only negligible degradation as data cor-ruption increases. This highlights our model’s strong ability to filter out noisy signals and capture the underlying physical principles. In contrast, while other top-performing symbolic regression baselines like PhySO and SP-GPSR also show a high degree of stability, their MSEs are consistently higher than ours at every noise level. DSR, another baseline, shows a more noticeable upward trend in error, indicating a greater sensitivity to increasing noise. Furthermore, the neural operator models, GNOT and Geo-FNO, appear significantly less robust, exhibiting MSE values on a much higher scale. These models suffer from both lower accuracy and higher volatility. Notably, GNOT’s error peaks dramatically at the 10% noise level. These experimental results validate the remarkable resilience of our method. The ability to maintain consistent accuracy despite data corruption is a critical advantage for real-world applications where data is inherently prone to noise. V. CONCLUSION Our work introduced a multitasking symbolic regression framework to solve families of PDEs, not only offering speedup but also outputting analytical solutions for inter-pretability. In particular, the method leverages the shared mathematical structure within a family of PDEs to find an interpretable, analytical solution for all instances simultane-ously. A novel affine transfer strategy is embedded in the framework to enable effective and rapid transfer of learned features between related PDEs, dramatically accelerating the discovery of analytical solutions compared to learning each instance from scratch. Our extensive experimental results confirm the superior performance of our approach. We have demonstrated not only significant improvements in efficiency but also enhanced accuracy and generalization capabilities, particularly in challenging scenarios with limited or noisy data. By delivering a framework that is both computationally efficient and highly interpretable, our work represents a major step toward enabling the widespread discovery of analytical solutions for complex scientific and engineering problems. REFERENCES [1] K. J. Bergen, P. A. Johnson, M. V. de Hoop, and G. C. Beroza, “Machine learning for data-driven discovery in solid earth geoscience,” Science ,vol. 363, no. 6433, p. eaau0323, 2019. [2] N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stu-art, and A. Anandkumar, “Neural operator: Learning maps between function spaces with applications to pdes,” Journal of Machine Learning Research , vol. 24, no. 89, pp. 1–97, 2023. [3] J. Oldenburg, F. Borowski, A. ¨Oner, K.-P. Schmitz, and M. Stiehm, “Geometry aware physics informed neural network surrogate for solving navier–stokes equation (GAPINN),” Advanced Modeling and Simulation in Engineering Sciences , vol. 9, no. 1, p. 8, 2022. [4] C. A. Amaral, V. L. Oliveira, J. P. Salazar, and E. I. Duzzioni, “Quantum machine learning and quantum-inspired methods applied to computa-tional fluid dynamics: a short review,” arXiv preprint arXiv:2510.14099 ,2025. [5] A. E. Tekkaya and C. Soyarslan, Finite Element Method . Berlin, Heidelberg: Springer Berlin Heidelberg, 2014, pp. 508–514. [6] R. Eymard, T. Gallou¨ et, and R. Herbin, “Finite volume methods,” 

Handbook of numerical analysis , vol. 7, pp. 713–1018, 2000. [7] S. Cai, Z. Mao, Z. Wang, M. Yin, and G. E. Karniadakis, “Physics-informed neural networks (PINNs) for fluid mechanics: A review,” Acta Mechanica Sinica , vol. 37, no. 12, pp. 1727–1738, 2021. [8] Z. Li, N. B. Kovachki, K. Azizzadenesheli, B. liu, K. Bhattacharya, A. Stuart, and A. Anandkumar, “Fourier neural operator for parametric partial differential equations,” in International Conference on Learning Representations , 2021. [9] Q. Cao, S. Goswami, and G. E. Karniadakis, “Laplace neural operator for solving differential equations,” Nature Machine Intelligence , vol. 6, no. 6, pp. 631–640, 2024. [10] L. Cao, Z. Lin, K. C. Tan, and M. Jiang, “Interpretable solutions for multi-physics pdes using T-NNGP,” in Proceedings of the AAAI Conference on Artificial Intelligence , vol. 39, no. 13, Apr. 2025, pp. 14 212–14 220. 12 

[11] L. Cao, Y. Liu, Z. Wang, D. Xu, K. Ye, K. C. Tan, and M. Jiang, “An interpretable approach to the solutions of high-dimensional partial differential equations,” Proceedings of the AAAI Conference on Artificial Intelligence , vol. 38, no. 18, pp. 20 640–20 648, Mar. 2024. [12] S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Discovering governing equations from data by sparse identification of nonlinear dynamical systems,” Proceedings of the National Academy of Sciences , vol. 113, no. 15, pp. 3932–3937, 2016. [13] L. Cao, Z. Zheng, C. Ding, J. Cai, and M. Jiang, “Genetic programming symbolic regression with simplification-pruning operator for solving differential equations,” in Neural Information Processing . Springer Nature Singapore, 2024, pp. 287–298. [14] A. Quarteroni, A. Manzoni, and F. Negri, Reduced basis methods for partial differential equations: an introduction . Springer, 2015, vol. 92. [15] M. Penwarden, S. Zhe, A. Narayan, and R. M. Kirby, “A metalearning approach for physics-informed neural networks (PINNs): Application to parameterized pdes,” Journal of Computational Physics , vol. 477, p. 111912, 2023. [16] A. Gupta, Y.-S. Ong, and L. Feng, “Multifactorial evolution: Toward evolutionary multitasking,” IEEE Transactions on Evolutionary Compu-tation , vol. 20, no. 3, pp. 343–357, 2015. [17] M. Takamoto, T. Praditia, R. Leiteritz, D. MacKinlay, F. Alesiani, D. Pfl¨ uger, and M. Niepert, “PDEBench: An extensive benchmark for scientific machine learning,” in Advances in Neural Information Processing Systems , vol. 35. Curran Associates, Inc., 2022, pp. 1596– 1611. [18] J. O. Robertsson, D.-J. van Manen, C. Schmelzbach, C. Van Renterghem, and L. Amundsen, “Finite-difference modelling of wavefield con-stituents,” Geophysical Journal International , vol. 203, no. 2, pp. 1334– 1342, 2015. [19] S. Wahyudi and F. A. Ramadhan, “Study of various thermal conductivity layers in bioheat transfer against thermal distribution on human skin with finite difference method (FDM),” International Journal of Integrated Engineering , vol. 13, no. 7, pp. 307–314, Sep. 2021. [20] H. Ullah, T. Hayat, S. Ahmad, and M. S. Alhodaly, “Entropy generation and heat transfer analysis in power-law fluid flow: Finite difference method,” International Communications in Heat and Mass Transfer , vol. 122, p. 105111, 2021. [21] A. Marzok, “Extended finite volume method for problems with arbitrary discontinuities,” Computational Mechanics , vol. 76, no. 4, pp. 1211– 1232, Oct. 2025. [22] N. Muhammad, “Finite volume method for simulation of flowing fluid via openfoam,” The European Physical Journal plus , vol. 136, no. 10, p. 1010, 2021. [23] J. A. Haider and S. Ahmad, “Dynamics of the rabinowitsch fluid in a reduced form of elliptic duct using finite volume method,” International Journal of Modern Physics B , vol. 36, no. 30, p. 2250217, 2022. [24] C. Bernardi and Y. Maday, “Spectral methods,” Handbook of numerical analysis , vol. 5, pp. 209–485, 1997. [25] K. J. Burns, G. M. Vasil, J. S. Oishi, D. Lecoanet, and B. P. Brown, “Dedalus: A flexible framework for numerical simulations with spectral methods,” Phys. Rev. Res. , vol. 2, p. 023068, Apr 2020. [26] B. Meuris, S. Qadeer, and P. Stinis, “Machine-learning-based spectral methods for partial differential equations,” Scientific Reports , vol. 13, no. 1, p. 1739, 2023. [27] A. Kag and V. Saligrama, “Condensing cnns with partial differential equations,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2022, pp. 610–619. [28] S. Nikolopoulos, I. Kalogeris, and V. Papadopoulos, “Non-intrusive surrogate modeling for parametrized time-dependent partial differential equations using convolutional autoencoders,” Engineering Applications of Artificial Intelligence , vol. 109, p. 104652, 2022. [29] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang, “Physics-informed machine learning,” Nature Reviews Physics ,vol. 3, no. 6, pp. 422–440, 2021. [30] A. B. Buhendwa, S. Adami, and N. A. Adams, “Inferring incompress-ible two-phase flow fields from the interface motion using physics-informed neural networks,” Machine Learning with Applications , vol. 4, p. 100029, 2021. [31] J. Yu, L. Lu, X. Meng, and G. E. Karniadakis, “Gradient-enhanced physics-informed neural networks for forward and inverse pde prob-lems,” Computer Methods in Applied Mechanics and Engineering , vol. 393, p. 114823, 2022. [32] P. Jin, S. Meng, and L. Lu, “MIONet: Learning multiple-input operators via tensor product,” SIAM Journal on Scientific Computing , vol. 44, no. 6, pp. A3490–A3514, 2022. [33] A. Rahman, R. J. George, M. Elleithy, D. Leibovici, Z. Li, B. Bonev, C. White, J. Berner, R. A. Yeh, J. Kossaifi, K. Azizzadenesheli, and A. Anandkumar, “Pretraining codomain attention neural operators for solving multiphysics pdes,” Advances in Neural Information Processing Systems , vol. 37, pp. 104 035–104 064, 2024. [34] M. Wei and X. Zhang, “Super-resolution neural operator,” in Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2023, pp. 18 247–18 256. [35] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, “Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators,” Nature machine intelligence , vol. 3, no. 3, pp. 218–229, 2021. [36] S. Cai, Z. Wang, L. Lu, T. A. Zaki, and G. E. Karniadakis, “DeepM&Mnet: Inferring the electroconvection multiphysics fields based on operator approximation by neural networks,” Journal of Computational Physics , vol. 436, p. 110296, 2021. [37] L. Lu, P. Jin, and G. E. Karniadakis, “DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators,” arXiv preprint arXiv:1910.03193 ,2019. [38] B. K. Petersen, M. Landajuela, T. N. Mundhenk, C. P. Santiago, S. K. Kim, and J. T. Kim, “Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients,” in International Conference on Learning Representations , 2021. [39] S.-M. Udrescu and M. Tegmark, “AI Feynman: A physics-inspired method for symbolic regression,” Science advances , vol. 6, no. 16, p. eaay2631, 2020. [40] P.-a. Kamienny, S. d'Ascoli, G. Lample, and F. Charton, “End-to-end symbolic regression with transformers,” Advances in Neural Information Processing Systems , vol. 35, pp. 10 269–10 281, 2022. [41] Y. Li, J. Liu, M. Wu, L. Yu, W. Li, X. Ning, W. Li, M. Hao, Y. Deng, and S. Wei, “MMSR: symbolic regression is a multi-modal information fusion task,” Information Fusion , vol. 114, p. 102681, 2025. [42] Z. Huang, Y. Mei, and J. Zhong, “Semantic linear genetic programming for symbolic regression,” IEEE Transactions on Cybernetics , vol. 54, no. 2, pp. 1321–1334, 2024. [43] D. J. Bartlett, H. Desmond, and P. G. Ferreira, “Exhaustive symbolic regression,” IEEE Transactions on Evolutionary Computation , vol. 28, no. 4, pp. 950–964, 2024. [44] J. Zhong, J. Dong, W.-L. Liu, L. Feng, and J. Zhang, “Multiform genetic programming framework for symbolic regression problems,” 

IEEE Transactions on Evolutionary Computation , vol. 29, no. 2, pp. 429–443, 2025. [45] C. Wang, Q. Chen, B. Xue, and M. Zhang, “Multi-task semantic genetic programming with reinforcement learning for multi-output symbolic regression,” IEEE Transactions on Evolutionary Computation , pp. 1– 1, 2025. [46] H. Oh, R. Amici, G. Bomarito, S. Zhe, R. Kirby, and J. Hochhalter, “Ge-netic programming based symbolic regression for analytical solutions to differential equations,” arXiv preprint arXiv:2302.03175 , 2023. [47] C. ˆ andida Ferreira, “Gene expression programming: A new adaptive algorithm for solving problems,” Complex Systems , vol. 13, pp. 87–129, 2001. [48] J. Zhong, L. Feng, W. Cai, and Y.-S. Ong, “Multifactorial genetic programming for symbolic regression problems,” IEEE Transactions on Systems, Man, and Cybernetics: Systems , vol. 50, no. 11, pp. 4492–4505, 2020. [49] L. Mandl, A. Mielke, S. M. Seyedpour, and T. Ricken, “Affine transfor-mations accelerate the training of physics-informed neural networks of a one-dimensional consolidation problem,” Scientific Reports , vol. 13, no. 1, p. 15566, 2023. [50] W. Tenachi, R. Ibata, and F. I. Diakogiannis, “Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws,” The Astrophysical Journal , vol. 959, no. 2, p. 99, dec 2023. [51] Z. Hao, Z. Wang, H. Su, C. Ying, Y. Dong, S. Liu, Z. Cheng, J. Song, and J. Zhu, “GNOT: A general neural operator transformer for operator learning,” in Proceedings of the 40th International Conference on Machine Learning , ser. Proceedings of Machine Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scar-lett, Eds., vol. 202. PMLR, 23–29 Jul 2023, pp. 12 556–12 569. [52] Z. Li, D. Z. Huang, B. Liu, and A. Anandkumar, “Fourier neural operator with learned deformations for pdes on general geometries,” Journal of Machine Learning Research , vol. 24, no. 388, pp. 1–26, 2023. 1

# Supplemental Material: Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families 

## Yipeng Huang, Dejun Xu, Zexin Lin, Zhenzhong Wang, Member, IEEE, 

## Min Jiang, Senior Member, IEEE 

This supplemental material includes the mathematical background and problem formulation, detailed experimental setup, additional experimental results, scientific insights, and discussion. I. MATHEMATICAL BACKGROUND AND PROBLEM FORMULATION 

Partial Differential Equations (PDEs) are mathematical equations that involve an unknown function of multiple independent variables and their partial derivatives with respect to those variables. They are fundamental to describing a vast array of physical phenomena across science and engineering, from the propagation of waves and the flow of heat to the dynamics of fluids and the behavior of quantum particles. Unlike ordinary differential equations (ODEs) [1], which involve functions of a single independent variable, PDEs capture complex interactions and variations across space and time. Formally, a partial differential equation for an unknown function u(x1, x 2, . . . , x n) can be expressed in a general form as: 

F

(

x1, . . . , x n, u, ∂u 

∂x 1

, . . . , ∂u 

∂x n

, ∂2u

∂x 21

, . . . , ∂ku

∂x kn

)

= 0 . (1) Here, u represents the unknown function we aim to solve for, depending on multiple independent variables. The terms x1, x 2, . . . , x n are the independent variables, often representing spatial coordinates (e.g., x, y, z )or time ( t). Partial derivatives like ∂u  

> ∂x i

denote the first-order partial derivative of u with respect to xi, and  

> ∂ku
> ∂x ki

denotes the k-th order partial derivative of u with respect to xi; higher-order partial derivatives involving combinations of variables (e.g., ∂2u 

> ∂x i∂x j

) may also be present. Finally, F is a given function (or operator) that defines the specific relationships between the independent variables, the unknown function, and its partial derivatives, thereby specifying the particular PDE. PDEs are classified based on various properties, including linearity, order (the highest order of derivative present), and type (e.g., elliptic, parabolic, hyperbolic), which often correlate with the physical phenomena they describe. A PDE family refers to a collection of Partial Differential Equations that share an identical underlying mathematical structure but differ in the specific values of one or more parameters embedded within the equation. This concept is crucial in many practical scenarios where a physical system’s fundamental behavior remains constant, but its specific properties or environmental conditions vary. Formally, a PDE family can be defined as a set of PDEs described by a general functional form F that explicitly includes one or more parameters. Let p = ( p1, p 2, . . . , p m) be a vector of m parameters. Then, a PDE belonging to such a family can be written as: 

G(x1, . . . , x n, u, ∇u, ∇2u, . . . , ∇ku; p) = 0 . (2)  

> Yipeng Huang, Dejun Xu, Zexin Lin, Min Jiang and Zhenzhong Wang are with the Department of Artificial Intelligence, Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan, Ministry of Culture and Tourism, School of Informatics, Xiamen University, and Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen 361005, Fujian, China (Corresponding authors: Zhenzhong Wang and Min Jiang, e-mail: zhenzhongwang@xmu.edu.cn and minjiang@xmu.edu.cn) .2

In this definition, G represents the fixed mathematical structure or functional form of the PDE that is common across the entire family, outlining the relationships between the independent variables, the unknown function, and its derivatives. The term p = ( p1, p 2, . . . , p m) is the parameter vector, where each unique combination of values for p1, . . . , p m specifies a distinct instance of the PDE within the family. These parameters frequently embody physical constants (e.g., diffusion coefficients, reaction rates, material properties like viscosity or thermal conductivity) or external environmental factors. The notation 

∇u, ∇2u, . . . , ∇ku collectively signifies all partial derivatives of u up to order k.For example, consider the time-dependent diffusion equation in one spatial dimension: 

∂u 

∂t = D ∂2u

∂x 2 . (3) Here, u(x, t ) is the unknown function representing concentration or temperature, x and t are independent variables (space and time), and D is the diffusion coefficient, which serves as the parameter p. When studying diffusion in different materials, D will assume various values (e.g., D1, D 2, . . . , D m). Each such value defines a distinct PDE within the diffusion equation family, all sharing the same core mathematical structure. Solving a PDE family thus entails finding solutions for u(x, t ; D) across a specified range of 

D values. II. DETAILED EXPERIMENTAL SETUP 

A. Datasets Details 1) 1D Advection Equation: The 1D Adevction equation describes the transport of a conserved quantity along the spatial domain under a constant velocity. The equation is formulated as: 

∂u 

∂t + β ∂u 

∂x = 0 , (4) where β denotes the constant advection velocity. The system is subject to an initial condition u(x, t =0) = u0(x). Notably, this equation admits an analytical solution: u(x, t ) = u0(x − βt ).For our implementation, we construct the initial condition through a superposition of N sinusoidal waves: 

u0(x) = 

> N

∑

> i=1

Ai sin( kix + ϕi), (5) where ki = 2πn i 

> Lx

, with ni being random integers sampled from the interval [1 , n max ]. Here, N represents the number of sinusoidal components, Lx denotes the computational domain size, Ai are random float numbers uniformly chosen in [0 , 1] , and ϕi represents randomly selected phases from the interval [0 , 2π].In our experimental setup, we configure N = 2 and nmax = 8 . The dataset is generated directly from the analytical solution by sampling points in the spatial domain x ∈ [0 , 1] and temporal domain t ∈ [0 , 2] . We generate Ntasks = 4 distinct tasks, where each task j is assigned a specific advection velocity β from the set {0.1, 0.4, 0.7, 1.0}, which is linearly spaced in the range [0 .1, 1.0] . Fig. S-1 provides a visualization of the solutions for the 1D Advection equation under different parameter settings. 

2) 1D Burgers’ Equation: The 1D Burgers’ equation combines nonlinear convection and viscous dissipation, and serves as a canonical model for studying turbulence and shock formation in fluid dynamics. The equation is formulated as: 

∂u 

∂t + ∂

∂x 

(u2

2

)

= ν

π∂2u

∂x 2 , (6) where ν denotes the kinematic viscosity, and u(x, t ) is the velocity field. The system is subject to periodic boundary conditions and an initial condition u(x, t = 0) = u0(x). Unlike the advection equation, this equation does not have a simple general analytical solution for an arbitrary u0(x). Therefore, the dataset 3  

> Fig. S-1. Visualization of solutions for the 1D Advection equation under different parameters.
> Fig. S-2. Visualization of solutions for the 1D Burgers’ equation under different parameters.

is generated using a numerical solver. We employ a finite difference method on a grid with M = 100 

spatial points and T = 1000 time steps to solve the equation. For our implementation, we adopt the same initial condition as in the 1D advection case. The dataset is generated by first solving the PDE numerically over the spatial domain x ∈ [0 , 1] and temporal domain 

t ∈ [0 , 2] . Then, 1100 data points are randomly sampled from the resulting solution grid. We generate 

Ntasks = 4 distinct tasks, where each task j is assigned a specific kinematic viscosity ν from the set 

{0.001 , 0.004 , 0.007 , 0.01 }. Fig. S-2 provides a visualization of the solutions for the 1D Burgers’ equation under different parameter settings. 

3) 1D Advection-Diffusion Equation: The 1D Advection-Diffusion equation characterizes the interplay between advective transport and diffusive spreading of a scalar field. The equation is formulated as: 

∂u 

∂t + β ∂u 

∂x = α ∂2u

∂x 2 , (7) where β denotes the advection velocity, α represents the diffusion coefficient, and u(x, t ) is the conserved quantity. The system is subject to an initial condition u(x, t = 0) = u0(x). This equation does not have a simple general analytical solution for an arbitrary u0(x), so the dataset is generated using a numerical solver. We employ the Crank-Nicolson method, a finite difference scheme, on a grid with Nx = 100 

spatial points and Nt = 100 time steps to solve the equation. For our implementation, we adopt the same initial condition as in the 1D advection case. The dataset is generated by first solving the PDE numerically over the spatial domain x ∈ [0 , 1] and temporal domain t ∈

[0 , 2] . Then, 1100 data points are randomly sampled from the resulting solution grid. We generate Ntasks = 4 

distinct tasks. Each task j is assigned a specific advection velocity β from the set {0.1, 0.4, 0.7, 1.0},which is linearly spaced in the range [0 .1, 1.0] . Concurrently, each task is also assigned a unique, randomly sampled diffusion coefficient α from the interval U(0 .001 , 0.005) . Fig. S-3 provides a visualization of the solutions for the 1D Advection-Diffusion equation under different parameter settings. 4  

> Fig. S-3. Visualization of solutions for the 1D Advection-Diffusion equation under different parameters.
> Fig. S-4. Visualization of solutions for the 2D Advection equation under different parameters.

4) 2D Advection Equation: The 2D Advection equation describes transport along a planar flow field and tests solver performance under multi-dimensional spatial interactions. The equation is formulated as: 

∂u 

∂t + βx

∂u 

∂x + βy

∂u 

∂y = 0 , (8) where βx, βy denote the advection velocities in the x and y directions, respectively. The system is subject to an initial condition u(x, y, t = 0) = u0(x, y ). Notably, this equation admits an analytical solution: 

u(t, x, y ) = u0(x − βxt, y − βyt).5 

> Fig. S-5. Visualization of solutions for the 2D Navier-Stokes equation under different parameters.

For our implementation, we construct the initial condition as a product of two sinusoidal waves: 

u0(x, y ) = A1 sin( k1x + ϕ1) · A2 sin( k2y + ϕ2), (9) where ki = 2 πn i, with ni being random integers sampled from the interval [1 , n max ]. This corresponds to a computational domain size of Lx = 1 and Ly = 1 . Here, the amplitudes A1 and A2 are fixed at 1.0,and ϕi represents randomly selected phases from the interval [0 , 2π].In our experimental setup, we configure nmax = 8 . The dataset is generated directly from the analytical solution by sampling 1100 points in the spatial domain x, y ∈ [0 , 1] and temporal domain t ∈ [0 , 1] .We generate Ntasks = 4 distinct tasks. Each task j is assigned a specific advection velocity βx from the set {0.1, 0.4, 0.7, 1.0}, which is linearly spaced in the range [0 .1, 1.0] . Concurrently, each task is also assigned a unique, randomly sampled advection velocity βy from the interval U(0 .1, 1.0) . Fig. S-4 provides a visualization of the solutions for the 2D Advection equation under different parameter settings. 

5) 2D Navier-Stokes Equation: The 2D Navier-Stokes equation models 2D incompressible vorticity evolution, combining nonlinear convection and viscous diffusion to study vortex dynamics and turbulence. The equation is formulated as: ∂ω 

∂t + u · ∇ ω = ν∆ω, (10) where ω is the vorticity, u = ( u, v ) is the velocity field, and ν is the kinematic viscosity. The system is subject to periodic boundary conditions and an initial condition ω(x, y, t = 0) = ω0(x, y ). For this specific case, known as the Taylor-Green vortex, the non-linear advection term u · ∇ ω simplifies to zero, allowing for a known analytical solution. 6 

> Fig. S-6. Visualization of solutions for the 3D Advection equation under different parameters.

For our implementation, we use the specific initial condition for the Taylor-Green vortex: 

ω0(x, y ) = sin(2 πx ) sin(2 πy ). (11) This initial condition leads to the exact analytical solution: 

ω(t, x, y ) = sin(2 πx ) sin(2 πy ) exp( −8π2νt ). (12) In our experimental setup, the dataset is generated directly from this analytical solution by sam-pling 1100 points in the spatial domain x, y ∈ [0 , 1] and temporal domain t ∈ [0 , 2] . We generate 

Ntasks = 4 distinct tasks, where each task j is assigned a specific kinematic viscosity ν from the set 

{0.005 , 0.02 , 0.035 , 0.05 }, which is linearly spaced in the range [0 .005 , 0.05] . Fig. S-5 provides a visualization of the solutions for the 2D Navier-Stokes equation under different parameter settings. 

6) 3D Advection Equation: The 3D Advection equation evaluates scalability by modeling transport in volumetric flow fields. The equation is formulated as: 

∂u 

∂t + βx

∂u 

∂x + βy

∂u 

∂y + βz

∂u 

∂z = 0 , (13) where βx, βy, βz denote the advection velocities in the x, y, and z directions, respectively. The system is subject to an initial condition u(x, y, z, t = 0) = u0(x, y, z ). Notably, this equation admits an analytical solution: u(t, x, y, z ) = u0(x − βxt, y − βyt, z − βz t).7

For our implementation, we construct the initial condition as a product of three sinusoidal waves: 

u0(x, y, z ) = A1 sin( k1x + ϕ1) · A2 sin( k2y + ϕ2) · A3 sin( k3z + ϕ3), (14) where ki = 2 πn i, with ni being random integers sampled from the interval [1 , n max ]. This corresponds to a computational domain size of Lx = Ly = Lz = 1 . Here, the amplitudes A1, A 2, A 3 are fixed at 1.0, and 

ϕi represents randomly selected phases from the interval [0 , 2π].In our experimental setup, we configure nmax = 8 . The dataset is generated directly from the analytical solution by sampling 1100 points in the spatial domain x, y, z ∈ [0 , 1] and temporal domain t ∈ [0 , 1] .We generate Ntasks = 4 distinct tasks. Each task j is assigned a specific advection velocity βx from the set {0.1, 0.4, 0.7, 1.0}, which is linearly spaced in the range [0 .1, 1.0] . Concurrently, each task is also assigned a unique, randomly sampled advection velocity βy and βz from the interval U(0 .1, 1.0) . Fig. S-6 provides a visualization of the solutions for the 3D Advection equation under different parameter settings. 

B. Baselines Details 

In this section, we provide detailed descriptions of the five baseline methods used for comparison in our experiments:  

> •

PhySO [2]: A physics-constrained symbolic regression framework combining neural network-guided search with physical priors and dimensional constraints, excelling in interpretable PDE discovery.  

> •

DSR [3]: A deep learning-based symbolic regression approach using recurrent neural networks and risk-seeking policy gradients to efficiently recover mathematical expressions from noisy or noiseless data.  

> •

SP-GPSR [4]: A genetic programming symbolic regression method employing simplification-pruning operators and multi-objective fitness evaluation, yielding compact analytical solutions for complex differential equations.  

> •

GNOT [5]: A transformer-based neural operator encoding PDEs via graph structures, supporting multi-input and irregular meshes for scalable multi-task PDE learning.  

> •

Geo-FNO [6]: A Fourier neural operator framework that incorporates learned domain deformations to map physical domains onto latent spaces, enabling the solution of PDEs on arbitrary geometries. 

C. Hyper-parameters Details 

For NMIPS, the experimental hyperparameters were set as follows: population size N = 50 , random mating probability rmp = 0 .2, mutation probability 0.002 , number of generations 100 , and maximum number of evaluations 5000 . Relevant parameters in baseline methods, such as population or iteration counts, were kept consistent to ensure fair comparisons. Regarding the symbolic library, for the 1D, 2D, and 3D advection equations, the candidate symbol library consists of {+, −, ×, sin }, excluding PDE parameters. For the other three equations, the library is extended to {+, −, ×, sin , exp , log } to accommodate the increased functional complexity. III. MORE EXPERIMENTAL RESULTS 

A. More Results on the 1D Advection Equation 

To provide a more granular assessment of model fidelity, Fig. S-7 visualizes the spatial-temporal error distributions for the specific case of β = 1 .000 . The “Exact” panel displays the characteristic traveling wave patterns of the advection equation. As shown in the error heatmaps, NMIPS yields a predominantly dark field, indicating minimal pointwise error throughout the simulation domain. In contrast, the error maps for baselines such as PhySO and SP-GPSR exhibit prominent, bright bands of high error aligned with the wave propagation. These artifacts suggest that the baseline methods struggle to perfectly capture the phase velocity or amplitude of the traveling waves, whereas NMIPS maintains high fidelity to the exact dynamics over time. 8                                                                 

> Fig. S-7. A spatial-temporal error analysis of different methods for the 1D Advection equation.
> Fig. S-8. A spatial-temporal error analysis of different methods for the 1D Advection-Diffusion equation. TABLE S-I MSE OF DIFFERENT METHODS ON 1D A DVECTION -D IFFUSION EQUATION
> Num αβNMIPS PhySO DSR SP-GPSR GNOT Geo-FNO
> 0123
> 00.100 0.001 /2.61E-01 2.78E-01 2.60E-01 2.52E-01 2.30E-01 2.70E-01 5.44E-01 3.15E-01 10.400 0.002 1.08E-02 /1.08E-02 1.08E-02 2.62E-02 1.91E-02 1.15E-02 3.11E-01 1.89E-02 20.700 0.001 1.47E-01 1.48E-01 /1.49E-01 1.71E-01 1.63E-01 1.47E-01 3.35E-01 1.56E-01 31.000 0.004 5.13E-02 5.13E-02 5.11E-02 /8.01E-02 9.11E-02 7.18E-02 3.52E-01 6.58E-02
> Avg 1.19E-01 1.32E-01 1.26E-01 1.25E-01 3.85E-01 1.39E-01

B. More Results on the 1D Advection-Diffusion Equation 

Qualitative results in Fig. S-8 further illustrate NMIPS’s robustness. The error heatmaps demonstrate that NMIPS maintains minimal pointwise error (indicated by the dark field) even in advection-dominated regions. In contrast, baseline methods like PhySO and SP-GPSR exhibit distinct bright error bands aligned with the direction of wave propagation, suggesting a limitation in capturing the precise phase and amplitude of the steep gradients. This confirms that NMIPS possesses strong generalization capabilities, effectively balancing the dynamics of advection and diffusion. 

C. More Results on the 2D Advection Equation 

Qualitative results in Fig. S-9 further highlight the precision of NMIPS on the 2D Advection equation. The error heatmaps reveal that NMIPS achieves a uniformly low error distribution (predominantly dark field) across the entire spatial domain over time. In contrast, baseline methods such as PhySO and SP-GPSR display distinct localized regions of high error (bright spots), particularly concentrated near the domain corners or peak value regions. This discrepancy suggests that while competing methods may capture the general wave pattern, they struggle to maintain accuracy at the extrema, whereas NMIPS accurately reconstructs the full spatial field with high fidelity. 

D. More Results on the 2D Navier-Stokes Equation 

The source of NMIPS’s accuracy is further elucidated by the symbolic expressions in Table S-III. While NMIPS discovers a structured, interpretable expression involving linear and trigonometric terms (u ∝ (x − c) sin( y)), baseline methods converge on highly convoluted forms involving nested logarithms 9                                                           

> Fig. S-9. A spatial-temporal error analysis of different methods for the 2D Advection equation. TABLE S-II MSE OF DIFFERENT METHODS ON 2D N AVIER -S TOKES EQUATION
> Num νNMIPS PhySO DSR SP-GPSR GNOT Geo-FNO
> 0123
> 00.005 /1.19E-01 1.18E-01 1.31E-01 9.86E-02 1.16E-01 1.30E-01 7.58E-02 1.74E-01 10.020 4.36E-02 /4.47E-02 5.04E-02 1.14E-01 4.50E-02 4.45E-02 1.07E-01 4.74E-02 20.035 2.36E-02 2.99E-02 /2.10E-02 2.06E-02 3.52E-02 2.19E-02 1.12E-01 4.12E-02 30.050 1.40E-02 2.09E-02 1.44E-02 /2.09E-02 1.52E-02 1.44E-02 1.10E-01 2.24E-02
> Avg 5.25E-02 6.36E-02 5.29E-02 5.26E-02 1.01E-01 7.13E-02

and exponentials (e.g., sin(log( x)) in DSR). This contrast indicates that NMIPS effectively balances numerical precision with physical parsimony, avoiding the overfitting artifacts commonly observed in competing approaches. This structural superiority directly translates into the robust predictive performance visualized in Fig. S-10. The error heatmaps provide compelling evidence of NMIPS’s capability in handling complex fluid dynamics. NMIPS maintains a consistently dark error profile, indicating minimal deviation from the exact solution even under non-linear conditions. In contrast, the incorrect symbolic forms of the baselines manifest as severe failure modes: PhySO exhibits a sharp diagonal artifact and a massive region of high error (bright triangular area) in the lower half of the domain, while DSR and SP-GPSR show visible distributed error patterns. This confirms that by successfully identifying the correct governing physical laws, NMIPS avoids the structural instabilities that lead to significant deviations in baseline models. 10                                

> TABLE S-III DISCOVERED EXPRESSIONS FOR THE 2D N AVIER -S TOKES EQUATION BY DIFFERENT SYMBOLIC REGRESSION METHODS
> Method Expression
> NMIPS u= ( x−0.493) sin( y) + 0 .037 t
> PhySO u=−sin(0 .332 (log( y−x)−y(y−0.589)) + 0 .195) −0.332
> DSR u= ( y−x) sin( y e y) sin( log( x))
> SP-GPSR u=−ey−xsin(0 .150 y)
> Fig. S-10. A spatial-temporal error analysis of different methods for the 2D Navier-Stokes equation.

E. More Results on the 3D Advection Equation 

Quantitative superiority is directly explained by the discovered expressions in Table S-IV. NMIPS identifies a clean, physically plausible polynomial form that correctly captures the linear advective re-lationship between spatial and temporal coordinates. In contrast, competing methods fail to uncover the true governing structure: PhySO and DSR converge on complex, non-intuitive trigonometric functions, while SP-GPSR yields incorrect interaction terms. The failure of baseline methods to identify the correct symbolic form leads to their higher prediction errors, whereas NMIPS effectively balances numerical stability with the recovery of simple, interpretable solutions in high-dimensional spaces. IV. S CIENTIFIC INSIGHT 

Moving beyond numerical accuracy, a critical capability of AI-driven PDE solvers is the autonomous dis-covery of the underlying “scientific skeleton” of physical systems. We analyzed the converged expression 11                                                                        

> TABLE S-IV DISCOVERED EXPRESSIONS FOR THE 3D A DVECTION EQUATION BY DIFFERENT SYMBOLIC REGRESSION METHODS
> Method Expression
> NMIPS u= 0 .0000352 ·yz 2(2 x−t)
> PhySO u= sin (z(6.244 x(z−0.558) 2−0.295 ))
> DSR u=z3sin( z(x+ 1) −xt −1)
> SP-GPSR u= 1 .217 z(t−x) ( y−z)
> Fig. S-11. Runtime comparison of different methods on different equations. TABLE S-V SUMMARY OF INVARIANT SYMBOLIC SKELETONS DISCOVERED BY NMIPS ACROSS VARYING PDE FAMILIES
> PDE Family Dimensions Dominant Physics Key Invariant Skeleton
> Advection 1D Linear Transport x−βt
> Advection 2D / 3D Multi-dim Transport xi−βit
> Burgers’ 1D Nonlinearity + Shock ln( . . . )or x−t
> Advection-Diffusion 1D Transport + Dissipation exp( f(x)−c·t)
> Navier-Stokes 2D Vorticity + Viscosity exp( −ct ) sin( x) sin( y)

trees discovered by NMIPS across six diverse PDE families. The results reveal that our method successfully captures invariant physical mechanisms, ranging from characteristic lines in hyperbolic systems to the separation of variables in complex fluid dynamics. For hyperbolic transport problems, the fundamental physical invariant is the concept of characteristic lines. Specifically, in the 1D Advection tasks with varying velocities ( β), NMIPS consistently converged on linear substructures of the form (x − βt ). This indicates the algorithm successfully “rediscovered” that the solution is a traveling wave that propagates without deformation. Crucially, this capability scales to higher dimensions. In the 2D and 3D Advection tasks, the solver demonstrated dimensional decoupling. For instance, in 3D flows, it isolated specific transport axes, recovering terms like (z − t) or (y − βt ). This confirms that NMIPS reconstructs the characteristic hyperplanes governing volumetric transport rather than merely memorizing spatial patterns. In systems where multiple physical mechanisms compete, NMIPS demonstrated the ability to disentan-gle these effects symbolically. For the 1D Advection-Diffusion equation, the solver identified a composite structure: a traveling wave argument (x−ct ) modulated by an exponential decay envelope exp( −kt ). This signifies a physical understanding that diffusion acts as an irreversible energy dissipation mechanism on top of the convective transport. In the nonlinear Burgers’ equation, the solver captured distinct regimes. In 12 

low-viscosity tasks, it recovered the inviscid limit approximation (x − t), while in shock-forming regimes, it introduced logarithmic or nested exponential terms to approximate the steep gradients. Finally, in the 2D Navier-Stokes equation, NMIPS achieved its most significant discovery. The analytical solution is characterized by the separation of spatial and temporal dynamics: ω(x, y, t ) = sin( x) sin( y)e−2νt .Remarkably, our experimental results show that the solver converged on a symbolic skeleton of the exact form: 

u ≈ exp( c1 · t)

︸ ︷︷ ︸

> Temporal Decay

· sin( x) · sin( c2 · y)

︸ ︷︷ ︸

> Spatial Mode

(15) This finding confirms that NMIPS can autonomously rediscover the method of separation of variables, correctly modeling the viscous decay of vorticity as a temporal multiplier independent of the spatial flow structure. Table S-V summarizes the dominant symbolic skeletons identified across these PDE families, unequivocally demonstrating that NMIPS acts as a transparent tool for scientific discovery. V. D ISCUSSION 

A. Negative Transfer and Task Interference 

A critical concern in multitask learning is whether knowledge sharing across tasks may inadvertently degrade performance due to task interference. In NMIPS, such adverse effects are alleviated by performing knowledge transfer at the symbolic structure level rather than through direct parameter sharing. Specifically, candidate solutions are represented as expression trees whose coefficients and fitness evaluations remain task-specific. This design enables the framework to exploit shared mathematical structures among PDE instances while preserving sufficient flexibility to accommodate task-dependent characteristics. Moreover, the MFO mechanism evaluates each individual strictly with respect to its associated task, and selection is guided by task-aware fitness measures. As a result, solution candidates that are beneficial for one PDE instance but detrimental to others are naturally suppressed during evolution, preventing the accumulation of harmful cross-task biases. This selective and structure-oriented sharing strategy fundamentally differs from conventional neural multitask models that rely on hard or soft parameter sharing. However, it should be noted that when PDE instances within the same family exhibit substantial differences in initial conditions, boundary conditions, or solution regularity, structure-level transfer may become less effective. In such scenarios, adaptive task grouping or similarity-aware transfer strategies could further enhance robustness, which remains an important direction for future research. 

B. Scalability to Higher-Dimensional PDEs 

The experimental results demonstrate that NMIPS remains effective for PDEs up to three spatial dimen-sions, which already cover the vast majority of physical systems encountered in real-world applications, such as fluid dynamics, transport phenomena, and wave propagation. While PDEs formulated in dimensions higher than three do exist, they typically arise from abstract mathematical constructions or augmented state spaces rather than direct physical modeling. From a methodological perspective, extending symbolic regression-based PDE solvers to substantially higher dimensions introduces significant challenges. The symbolic search space grows combinatorially with dimensionality, and the evaluation of PDE residuals becomes increasingly expensive, making exhaustive structure exploration impractical. Moreover, higher-dimensional PDEs rarely admit concise analytical solutions, which further reduces the practical utility of symbolic discovery in such settings. Consequently, NMIPS is primarily targeted at low to moderate dimensional PDEs where analytical structure discovery is both feasible and scientifically meaningful. Addressing scalability beyond this regime may require hybrid approaches that combine symbolic structure learning with neural surrogates or dimensionality reduction techniques. 13 

REFERENCES 

[1] G. Delic, “Numerical solution of ordinary differential equations,” in Guide to Numerical Algorithm Design and Development: Including Legacy Examples from Fortran and MathCAD in High Precision . Springer Nature Switzerland, 2026, pp. 197–232. [2] W. Tenachi, R. Ibata, and F. I. Diakogiannis, “Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws,” The Astrophysical Journal , vol. 959, no. 2, p. 99, dec 2023. [3] B. K. Petersen, M. Landajuela, T. N. Mundhenk, C. P. Santiago, S. K. Kim, and J. T. Kim, “Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients,” in International Conference on Learning Representations , 2021. [4] L. Cao, Z. Zheng, C. Ding, J. Cai, and M. Jiang, “Genetic programming symbolic regression with simplification-pruning operator for solving differential equations,” in Neural Information Processing . Springer Nature Singapore, 2024, pp. 287–298. [5] Z. Hao, Z. Wang, H. Su, C. Ying, Y. Dong, S. Liu, Z. Cheng, J. Song, and J. Zhu, “GNOT: A general neural operator transformer for operator learning,” in Proceedings of the 40th International Conference on Machine Learning , ser. Proceedings of Machine Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., vol. 202. PMLR, 23–29 Jul 2023, pp. 12 556–12 569. [6] Z. Li, D. Z. Huang, B. Liu, and A. Anandkumar, “Fourier neural operator with learned deformations for pdes on general geometries,” 

Journal of Machine Learning Research , vol. 24, no. 388, pp. 1–26, 2023.