Title: Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery

URL Source: https://arxiv.org/pdf/2602.13021v2

Published Time: Tue, 17 Feb 2026 02:57:08 GMT

Number of Pages: 29

Markdown Content:
# Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery 

Jing Xiao 1 Xinhai Chen 1 Jiaming Peng 1 Qinglin Wang 1 Menghan Jia 1 Zhiquan Lai 1 Guangping Yu 1

Dongsheng Li 1 Tiejun Li 1 Jie Liu 1

# Abstract 

Symbolic Regression (SR) aims to discover inter-pretable equations from observational data, with the potential to reveal underlying principles be-hind natural phenomena. However, existing ap-proaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evo-lution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Anneal-ing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and estab-lishing a guarantee against pseudo-equations. Ex-perimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining ro-bustness to varying prior quality, noisy data, and data scarcity. 

# 1. Introduction 

Symbolic Regression (SR) aims to extract interpretable mathematical equations from observational data (Schmidt & Lipson, 2009), revealing underlying principles behind natu-ral phenomena and facilitating scientific discovery (Wang et al., 2019; Udrescu & Tegmark, 2020; Makke & Chawla, 

> 1

College of Computer Science and Technology, National Uni-versity of Defense Technology, Changsha 410073, China. Corre-spondence to: Xinhai Chen <chenxinhai16@nudt.edu.cn >.

Preprint. February 17, 2026. OOD  OOD ID   

> Training Region
> Criterion: MSE
> Pseudo -Equation Trap
> MSE :5.3e-2(Higher)
> Incorrectly Rejected
> by fitting noisy data poorly
> MSE: 1. 8e-7(Lower)
> Incorrectly Accepted
> by fitting noisy data perfectly

Figure 1. The Pseudo-Equation Trap. The pseudo-equation (or-ange) is incorrectly accepted as it fits noisy training data perfectly, despite deviating from the true underlying equation. In contrast, the consistent candidate (blue) is incorrectly rejected because it fits noise poorly, even though it aligns with the true equation. 

2024). Unlike black-box models driven solely by predictive accuracy (De Florio et al., 2024), SR prioritizes interpretabil-ity and consistency with fundamental scientific principles, enabling reliable extrapolation beyond training distribution rather than merely fitting data (Cranmer et al., 2020). However, existing SR approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. This problem is further worsened in real-world scenarios, where observations are often noisy and scarce, making pseudo-equations more likely to be produced and leading to catastrophic divergence under out-of-distribution (OOD) conditions (Karniadakis et al., 2021). As illustrated in Figure 1, scientifically consistent candidates can be incor-rectly rejected due to higher errors caused by noise, while structurally incorrect pseudo-equations are accepted. Current approaches struggle to escape this trap because their optimization objectives are dominated by empirical risk min-imization (Arjovsky et al., 2019), lacking explicit constraints to ensure scientific consistency and thereby degenerating equation discovery into empirical fitting rather than prin-1

> arXiv:2602.13021v2 [cs.LG] 16 Feb 2026 Prior-Guided Symbolic Regression

cipled scientific modeling. Specifically, search-based ap-proaches (McCormick, 2019; Burlacu et al., 2020; Virgolin et al., 2021; Cranmer, 2023; Petersen et al., 2019; Landa-juela et al., 2022) impose only implicit constraints through operator design and complexity heuristics, and therefore re-main dominated by empirical risk minimization during the search. Transformer-based approaches (Biggio et al., 2021; Kamienny et al., 2022; Shojaee et al., 2023; Ying et al., 2025) rely on pre-training on synthetic data to induce in-ductive biases toward scientific consistency, which function as constraints; however, these constraints collapse under distributional shift, degenerating the discovery process into merely empirical fitting. LLM-based approaches (Grayeli et al., 2024; Shojaee et al., 2024; Wang et al., 2025) use LLMs as surrogates to impose scientific consistency con-straints, but this guidance is prompt-based and thus remains implicit, making it vulnerable to hallucinations (Ji et al., 2023), leading to pseudo-equations. Therefore, when empir-ical risk minimization governs the discovery, hallucinated pseudo-equations can emerge in early stages and be reused as context in subsequent iterations, leading to their progres-sive amplification across generations. To address these challenges, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline con-sisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable con-straint programs, and employs a Prior-Annealing Con-strained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. This approach mitigates the dominance of empirical risk minimization and safeguards against the Pseudo-Equation Trap. Our main contributions are summarized as follows: â€¢ Problem Formulation: We formalize the Pseudo-Equation Trap, which reveals the fundamental diver-gence that arises between empirical risk minimization and scientific consistency in equation discovery. â€¢ Method: We propose PG-SR, a prior-guided SR frame-work that explicitly incorporates constraints to ensure scientific consistency, mitigating the dominance of em-pirical risk minimization. â€¢ Theory: We show that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a theoretical guarantee against pseudo-equations. â€¢ Experiments: Extensive experiments across different domains demonstrate that PG-SR outperforms existing approaches, maintaining robustness to varying prior quality, noisy data, and data scarcity. 

# 2. Related Work 

2.1. Search-Based Symbolic Regression 

Search-based approaches formulate symbolic regression as a combinatorial optimization problem. Genetic programming approaches (Koza, 1994), such as GPlearn (McCormick, 2019), Operon (Burlacu et al., 2020), GP-GOMEA (Virgolin et al., 2021), and PySR (Cranmer, 2023), perform heuristic searches by evolving expression trees, while approaches like DSR (Petersen et al., 2019) and uDSR (Landajuela et al., 2022) employ reinforcement learning for sequential gener-ation (Uc-Cetina et al., 2023). Despite their effectiveness, these approaches impose only implicit constraints through operator design and complexity heuristics, allowing em-pirical risk minimization to dominate search process and leading to pseudo-equations. 

2.2. Transformer-Based Symbolic Regression 

Transformer-based approaches, including NeSymReS (Big-gio et al., 2021), E2E (Kamienny et al., 2022), and Sym-former (Vastl et al., 2024), utilize large-scale pretraining on synthetic datasets to enable end-to-end equation genera-tion. TPSR (Shojaee et al., 2023) further integrates Monte Carlo Tree Searchâ€“based planning (Kocsis & Szepesv Â´ari, 2006) into the decoding process. Building on this paradigm, PhyE2E (Ying et al., 2025) incorporates second-order deriva-tive decomposition and dimensional constraints to improve physical consistency. These pretraining strategies induce in-ductive biases toward scientific consistency from synthetic data, which function as constraints; however, these con-straints are tied to the training distribution and collapse un-der distributional shift, allowing empirical risk minimization to dominate discovery and resulting in pseudo-equations. 

2.3. LLM-Based Symbolic Regression 

Recently, leveraging LLMs (Brown et al., 2020; Achiam et al., 2023; Team et al., 2023) for symbolic regression has emerged as a new direction. LASR (Grayeli et al., 2024) employs an LLM-extracted semantic concept library to guide discovery; LLM-SR (Shojaee et al., 2024) and Fun-Search (Romera-Paredes et al., 2024) exploit the code gener-ation abilities of LLMs (Chen, 2021) to construct equations; DrSR (Wang et al., 2025) proposes a dual-reasoning mech-anism that combines data-driven insights with reflective feedback to guide LLM generation. Although these meth-ods use LLMs as surrogates to impose scientific consistency constraints, such guidance is prompt-based and remains implicit, rendering it vulnerable to hallucinations (Ji et al., 2023) and leading to pseudo-equations. When empirical risk minimization dominates discovery, pseudo-equations can emerge in early stages, be reused as context, and propagate across generations, amplifying these hallucinations. 2Prior-Guided Symbolic Regression 1. Warm -up  

> Context
> Data Analysis
> Data
> Data Statistics
> Prior Constraint Checker
> L-BFGS -B
> Output: Equation Skeletons
> Skeleton Program Skeleton Program

3. Refinement   

> Context
> Residual Analysis
> Residual Statistics
> Prior Constraint Checker
> L-BFGS -B
> Repair
> Passed Failed
> Output: Equation Skeletons
> Skeleton Program Skeleton Program
> Skeleton
> Refinement Trigger

2. Evolution  

> Experience Pool

ð‘¬   

> Warmup
> Refine
> Periodically
> L-BFGS -B
> Prior Constraint Checker
> (Initially Empty)
> Refinement Trigger
> Equation
> Pool
> Fetch Skeletons as
> In -Context Examples
> Output: Equation Skeletons
> Skeleton Program Skeleton Program
> Optimize Params
> Optimize Params
> Optimize Params
> Insight
> Insight
> Pool
> Fetch the best and
> similar skeletons
> and insight from ð‘¬
> Prompt
> LLM
> Prompt
> LLM
> Prompt
> LLM
> Prior -Annealed
> Constrained Evaluation
> Failed
> Repair
> Passed

Prior Constraint Checker  

> Data Problem
> Description
> Data Analysis
> Prior Constraint Checker
> LLM
> LLM
> Visualization
> Domain
> Knowledge
> Human -Verified
> Prior Constraints
> Data
> Statistics
> Prompt
> Checking Program Checking Program
> Reflection
> LLM
> Fetch Insight
> as Guidance
> Why & How

Figure 2. Overview of the PG-SR framework with a three-stage pipeline comprising Warm-up, Evolution, and Refinement. The left panel illustrates the construction of the prior constraint checker. 

# 3. Methodology 

3.1. Problem Formulation 

Let D = {(xi, y i)}Ni=1 denote the dataset. The objective is to identify a function f âˆ— âˆˆ H , where H denotes the hypothesis space, that maximizes a score based on the Mean Squared Error (MSE), denoted by SMSE (f, D), defined as 

SMSE (f, D) = âˆ’MSE (f, D) = âˆ’ 1

N

> N

X

> i=1

 yi âˆ’ f (xi)2, (1) 

f âˆ— = arg max 

f âˆˆH SMSE (f, D). (2) To incorporate prior knowledge, let C denote a set of sci-entific prior constraints. We then define a boolean prior constraint checker V as 

V (f, C) = 

(

1, if f satisfies all constraints in C,

0, otherwise . (3) To jointly account for numerical accuracy and prior con-straints, we define a total score for each candidate function: 

S(f, D, C) = SMSE (f, D) Â· w(V (f, C)) , (4) where w(V (f, C)) assigns greater emphasis to candidates satisfying the constraints while retaining contributions from those that violate some constraints to preserve diversity. The symbolic regression problem is thus reformulated as 

f âˆ— = arg max 

f âˆˆH S(f, D, C). (5) 

3.2. Theoretical Analysis 

We formalize the PG-SR within the framework of statistical learning theory (Mohri et al., 2018), define the Pseudo-Equation Trap, and demonstrate how incorporating explicit prior constraints can effectively mitigate it. 

Definition 3.1 (Symbolic Hypothesis Space) . Let X âŠ† Rd

and Y âŠ† R be the input and output spaces. The hypothesis space H is defined as the set of all valid symbolic expression trees bounded by a maximum depth L.Standard symbolic regression seeks a function f âˆ— âˆˆ H 

that minimizes the empirical risk Ë†RN (f ). However, as established in statistical learning theory (Mohri et al., 2018), minimizing empirical risk does not guarantee minimizing the expected risk R(f ).

Lemma 3.2 (Generalization Bound) . Given the hypothe-sis space H, assume the loss function is Î»-Lipschitz and bounded by M . For any Î´ > 0, with probability at least 

1 âˆ’ Î´, the expected risk R(f ) satisfies: 

R(f ) â‰¤ Ë†RN (f ) + 2 Î»RN (H) + M

r log(1 /Î´ )2N (6) 

where RN (H) denotes the Rademacher complexity of H.(See Appendix A.2 for the detailed formulation.) 

Based on this standard bound, we can formally derive the origin of pseudo-equations. 

Corollary 3.3 (The Pseudo-Equation Trap) . In symbolic regression, the hypothesis space H grows exponentially with 

3Prior-Guided Symbolic Regression 

depth L, causing RN (H) to be excessively large. This loose-ness in the generalization bound allows for the existence of functions fpseudo âˆˆ H such that Ë†RN (fpseudo ) â‰ˆ 0 (fitting data perfectly) while R(fpseudo ) â‰« 0 (violating scientific principles). We define such functions as pseudo-equations. 

To address this, PG-SR restricts the discovery process to a scientifically consistent subspace, as defined below. 

Definition 3.4 (Prior-Constrained Subspace) . Given a set of prior constraints C verified by domain experts, the con-strained subspace is HC = {f âˆˆ H | V (f, C) = 1 }.

Proposition 3.5 (Consistency-Guaranteed Generalization) .

Assuming ftrue âˆˆ H C , enforcing constraints C reduces the Rademacher complexity: 

RN (HC ) â‰¤ R N (H). (7) 

Consequently, searching in HC yields a tighter general-ization bound than searching in H. The formal proof is provided in Appendix A.3. 

Proposition 3.5 reveals that the effectiveness of PG-SR de-pends not on reducing the size of the search space, but on aligning HC with the underlying truth. Unlike approaches that incorporate constraints only implicitly and may exclude the ground truth, PG-SR employs explicit, human-verified constraints to prune pseudo-equation regions of H while preserving the ground truth. 

3.3. Framework Overview 

PG-SR is a prior-guided SR framework with a three-stage pipeline: warm-up, evolution, and refinement (Figure 2). The framework manages candidate equations in a global experience pool E, which includes an equation pool of candidate equations maintained using logically partitioned islands (Whitley et al., 1999) to preserve diversity, and an insight pool for storing insights from success and failure experiences. In each iteration, LLMs generate new hypothe-ses based on high-scoring historical candidates and insights from previous iterations; the resulting candidates are subse-quently optimized, evaluated, and used to update the pools. This framework ensures continuous evolution in both scien-tific consistency and numerical accuracy. Detailed prompt templates are provided in Appendix G. 3.3.1. P RIOR -CONSTRAINT CHECKER AND 

PRIOR -A NNEALED CONSTRAINED EVALUATION 

To ensure scientific consistency, we introduce a prior con-straint checker V that validates candidate equations against a set of predefined constraints C. Since directly constructing these prior constraints is challenging, we adopt an LLM-assisted workflow to generate them (Figure 2). First, an LLM extracts domain knowledge from the problem descrip-tion. Then, human experts combine the LLM outputs with statistical patterns derived from data analysis to filter candi-date priors. Finally, these validated priors are used by the LLM to generate the final usable prior constraints, which are represented in executable program form. For example, in modeling E. coli growth dynamics, the LLM extracts relevant biological knowledge from the problem descrip-tion, such as the inhibition of growth under extreme tem-perature or pH conditions; human experts integrate this knowledge with statistical patterns to verify candidate pri-ors, after which the LLM translates the validated priors into executable prior constraint programs. These constraints con-form to established biological principles rather than relying on ground-truth equations. Further details on the priors are provided in Appendix D. While explicit prior constraints can mitigate pseudo-equations, applying them too rigidly early in the discovery process can restrict exploration. Therefore, we propose the Prior-Annealed Constrained Evaluation (PACE) mechanism, which allows temporary violations initially and gradually tightens enforcement as evolution proceeds. PACE instanti-ates the prior weighting term w(Â·) in Eq. (4) via a shrink-and-shift transformation of SMSE . Specifically, SMSE is min-max normalized within each island to make candidates more distinguishable: ËœSMSE (f, D) = SMSE âˆ’min SMSE    

> max SMSE âˆ’min SMSE

, where 

min SMSE and max SMSE are the minimum and maximum scores in the island. The total score with PACE is then defined as               

> S(f, D,C) =
> (
> Ïƒ(t)((1 âˆ’Î²) + 2 Î²ËœSMSE )âˆ’Î´(t),V= 0 ,
> (1 âˆ’Î²) + 2 Î²ËœSMSE ,V= 1 ,
> (8)

where Î² is a fixed scaling parameter. For constraint-violating candidates, the score is transformed as           

> Ï•(t) = Btâˆ’1
> Bâˆ’1,Ïƒ(t) = [1 âˆ’Î·Â·Ï•(t)] +,Î´(t) = Î±Â·Ï•(t).(9)

with t = Ncurr /N max , where Ncurr and Nmax are the cur-rent and total sampled candidates. Further details on the PACE mechanism can be found in the appendix B. 3.3.2. S TAGE I: D ATA -DRIVEN WARM -UP 

The Warm-up stage initializes the experience pool E with a data-driven set of preliminary equations, providing a solid foundation for subsequent evolution. Statistical and struc-tural analyses of the dataset D, including sampled points, variable ranges, correlations, and nonlinear contributions, extract key data features to guide the LLM in generating equation skeletons aligned with the underlying system be-havior. Skeletons are then optimized with L-BFGS-B (Byrd et al., 1995) to minimize MSE, and all candidates are re-tained regardless of prior constraints. If none of the gener-ated skeletons satisfy the prior constraints, a repair proce-dure is triggered to modify them. Finally, candidates are assigned cyclically to islands to ensure even distribution. 4Prior-Guided Symbolic Regression                                                                                                                                                                                                              

> Table 1. Quantitative results of different approaches. â€œ-â€ indicates that the method failed to generate equations.
> Method E. coli Growth Stressâ€“Strain CRK Oscillator 1 Oscillator 2
> ID â†“OOD â†“ID â†“OOD â†“ID â†“OOD â†“ID â†“OOD â†“ID â†“OOD â†“
> Search-based approaches
> GPLearn 1.07e+00 1.03e+00 3.94e-01 1.00e+00 1.09e+00 1.01e+00 9.75e-03 5.51e-01 1.87e-01 2.71e-01 PySR 1.51e-01 7.10e-01 1.87e-02 7.72e-02 1.33e-09 2.21e-08 6.12e-12 2.55e-05 4.40e-10 2.05e-06 DSR 1.82e-01 3.28e-01 3.42e-01 7.94e-01 1.78e-01 3.80e+00 1.04e-02 3.80e-01 1.87e-01 2.82e-01 uDSR 5.10e-01 2.02e+00 8.81e-02 4.93e-01 2.54e-10 2.11e-07 1.41e-05 1.65e-02 1.89e-03 2.93e-02 Operon 4.52e-01 9.98e-01 3.46e-02 1.15e-01 1.75e-07 2.33e-07 3.78e-04 3.80e-03 1.74e-01 1.02e-01 GP-GOMEA 3.68e-01 4.51e+01 7.09e-02 2.15e-01 1.13e-08 1.36e-07 1.41e-03 1.29e+00 1.50e-01 1.63e-01
> Transformer-based approaches
> TPSR â€“â€“6.02e-01 1.95e+00 8.48e-07 2.44e-04 4.81e-03 5.13e-01 3.07e-01 9.01e-01 E2E 8.91e-01 8.32e-01 1.16e-01 5.12e-01 2.51e-03 3.21e-01 4.01e-03 4.77e-02 1.88e-01 7.18e-01 NeSymReS â€“â€“8.58e-01 7.46e-01 9.70e-01 3.64e+02 4.01e-03 5.42e-01 9.85e-01 9.99e-01 PhyE2E 2.14e-01 5.33e-01 3.21e-02 1.47e-01 1.26e-05 1.79e-03 5.93e-03 1.65e-02 9.93e-02 6.36e-01
> LLM-based approaches (Backbone: Llama-3.3-70B)
> LLM-SR 4.61e-03 2.19e-02 1.40e-02 6.77e-02 1.32e-08 4.55e-07 1.13e-05 1.11e-02 8.99e-08 1.60e-05 LaSR 1.81e-02 2.24e-02 1.64e-02 6.18e-02 9.55e-11 1.72e-07 7.15e-06 5.44e-03 6.63e-06 8.80e-04 DrSR 1.92e-02 2.77e-02 2.11e-02 7.30e-02 1.64e-10 8.30e-08 7.73e-13 1.87e-06 9.51e-05 1.92e-03
> PG-SR(Ours) 1.15e-03 3.37e-03 6.14e-03 3.21e-02 6.02e-11 3.21e-08 3.84e-12 5.64e-07 3.13e-09 7.17e-07
> LLM-based approaches (Backbone: GPT-4o-mini)
> LLM-SR 9.34e-03 1.22e-02 2.10e-02 8.60e-02 1.14e-07 1.61e-04 4.24e-09 1.63e-05 2.32e-11 5.08e-07 LaSR 8.46e-03 1.00e-01 2.04e-02 8.77e-02 3.01e-09 7.25e-08 1.13e-05 9.66e-03 2.68e-08 3.51e-05 DrSR 9.49e-02 2.23e-01 2.42e-02 1.41e-01 1.42e-10 3.44e-08 1.45e-12 3.78e-06 1.43e-04 1.04e-02
> PG-SR(Ours) 1.32e-03 1.59e-03 1.49e-02 4.63e-02 1.74e-11 1.22e-08 6.62e-14 2.95e-08 1.79e-11 8.32e-11

3.3.3. S TAGE II: P RIOR -GUIDED EVOLUTION 

In this stage, equations are iteratively evolved using parallel islands to prevent premature convergence. A prompt is con-structed for the LLM by randomly selecting an island and applying a two-level sampling strategy to choose skeletons. Within each island, candidate skeletons with the same SMSE 

score are grouped into a cluster. Clusters are then sampled according to their PACE scores using a softmax distribution: 

P (cluster i) = exp( Si/Ï„ )

PKj=1 exp( Sj /Ï„ ) , (10) Where Si is the PACE score of cluster i, K is the number of clusters, and Ï„ is the temperature. Within each cluster, skele-tons are sampled based on length, with shorter programs favored. If a cluster contains valid skeletons (V = 1) , sam-pling is done only from them; otherwise, all skeletons are sampled using the same strategy. The prompt combines the problem description, exemplar skeletons from the equation pool, and insights retrieved from the insight pool, guiding the LLM to generate better candidates. After parsing the generated equations, parameters are optimized via L-BFGS-B to minimize MSE. Candidates are validated against prior constraints with a stochastic retry mechanism: if a candidate fails after optimization, its parameters are re-initialized and re-optimized up to 10 times. Then, they are scored using 

SMSE and added to E to guide evolution. The evolution continues until the maximum iterations are reached. 3.3.4. S TAGE III: R ESIDUAL -E NHANCED REFINEMENT WITH REFLECTION MECHANISM 

To further refine equation skeletons, we implement a residual-enhanced refinement with a reflection mechanism, periodically applied to the best candidates from each is-land. For each island optimum, the process begins by ana-lyzing its residual vector using statistical metrics (NMSE, bias, skewness, and kurtosis), SHAP values, and decision-tree-based high-error region analysis to identify defect variables and feature subspaces responsible for major fit-ting errors. For structural guidance, we retrieve reference candidates from E whose error profiles ri are aligned with the current residual r, measured by cosine similarity cosine sim (r, ri) = râŠ¤ri/(âˆ¥râˆ¥âˆ¥ riâˆ¥). For each top-ranked reference candidate, the system retrieves relevant informa-tion from the Insight Pool to construct an LLM prompt. If prior refinement histories exist, the prompt includes (i) the original equation, (ii) the improved equation, and (iii) a natural language explanation of the structural modification; otherwise, only the raw equation is provided. After each re-finement, the system triggers a reflection mechanism: when a performance improvement is achieved, successful experi-ences are summarized; otherwise, failure cases are analyzed. The resulting insights are added to the Insight Pool to guide skeleton evolution within the same island, while successful experiences can be retrieved by other islands in subsequent refinements based on residual vector similarity. 5Prior-Guided Symbolic Regression  

> Figure 3. Qualitative comparison of predictive trajectories for PG-SR and baselines on ID (gray) and OOD (colored) regions.

# 4. Experimental Results 

4.1. Dataset 

We evaluate our method on datasets from biology, materials, chemistry, and physics drawn from LLM-SRBench (Shojaee et al., 2025), to avoid LLM memorization. 

E. coli Growth: E. coli growth dynamics under varying substrate, temperature, and pH conditions. 

Material Stressâ€“Strain: Experimental stressâ€“strain mea-surements of Aluminum 6061-T651 collected across differ-ent temperature settings. 

Chemical Reaction Kinetics: A chemical reaction system describing the temporal evolution of species concentrations. 

Nonlinear Oscillator 1: A time-invariant nonlinear oscilla-tor whose dynamics depend solely on the system states. 

Nonlinear Oscillator 2: A time-dependent nonlinear oscil-lator driven by explicit temporal forcing. Further details on the datasets are provided in Appendix C. 

4.2. Baselines and PG-SR Configuration 

We compare PG-SR with state-of-the-art symbolic re-gression approaches across three categories. Search-based approaches include GPLearn (McCormick, 2019), PySR (Cranmer, 2023), DSR (Petersen et al., 2019), uDSR (Landajuela et al., 2022), Operon (Burlacu et al., 2020), and GP-GOMEA (Virgolin et al., 2021). Transformer-based approaches include TPSR (Shojaee et al., 2023), E2E (Kamienny et al., 2022), NeSymReS (Big-gio et al., 2021), and PhyE2E (Ying et al., 2025). LLM-based approaches include LLM-SR (Shojaee et al., 2024), LaSR (Grayeli et al., 2024), and DrSR (Wang et al., 2025). All experiments involving LLM-based approaches are con-ducted using GPT-4o-mini (OpenAI, 2024). Implementation details are provided in Appendix E and Appendix F. For all approaches, the reported experimental results correspond to the best performance across multiple runs. 6Prior-Guided Symbolic Regression  

> Figure 4. OOD generalization of PySR and LLM-SR with and without prior augmentation, compared to PG-SR.

4.3. Quantitative Results 

We evaluate PG-SR using both ID and OOD data. OOD per-formance reflects whether the discovered equations adhere to underlying scientific principles, making it a key metric for scientific discovery. We use the normalized mean squared error (NMSE) as the evaluation metric, defined as 

NMSE = 

> 1
> N

PNi=1 (yi âˆ’ Ë†yi)21

> N

PNi=1 (yi âˆ’ Â¯y)2 . (11) As shown in Table 1, PG-SR achieves the lowest NMSE on both ID and OOD data across all datasets, demonstrating strong predictive accuracy and scientific consistency. Given that PG-SR relies on LLMs as a generation engine, it is important to assess whether this performance is sensitive to the backbone model. Therefore, we evaluate PG-SR with another LLM backbone, Llama-3.3-70B (Dubey et al., 2024). Results across most datasets indicate comparable performance across backbones, suggesting that PG-SR is relatively robust to the choice of LLM backbone. Besides, Appendix H provides the equations discovered by PG-SR across all datasets, with the Oscillator 2 case showing an almost exact recovery of the ground-truth equation. 

4.4. Qualitative Results 

For a more intuitive comparison, Figure 3 visualizes the predictive trajectories, illustrating the behavior of each method. As shown in Figure 3, within the ID regime (gray dots), PG-SR and all baseline approaches closely match the ground truth trajectories, indicating that most approaches can achieve accurate interpolation. However, in the OOD re-gions (colored dots), PG-SR exhibits better alignment with the ground truth dynamics across diverse systems, while baseline approaches often deviate or diverge. Overall, PG-SR maintains scientific consistency with the underlying equations, following ground truth trajectories in both ID and OOD regions.  

> Figure 5. PG-SR performance under different prior quality settings: No Prior, Weak Priors, Wrong Priors, and Full Priors.

4.5. Impact of Explicit Prior Constraints 

To investigate whether explicit prior constraints can miti-gate pseudo-equations, we evaluated other representative baseline approaches, including PySR (search-based) and LLM-SR (LLM-based), with explicit prior constraints in-corporated. For a controlled comparison, both PySR and LLM-SR additionally leverage the prior constraint checker and the PACE mechanism from PG-SR for candidate evalua-tion, while preserving their original search and optimization procedures; specifically, PySR incorporates these compo-nents via a customized loss function, whereas LLM-SR adopts the same strategy as PG-SR. In contrast, transformer-based approaches require retraining on large-scale synthetic data to incorporate priors and are unsuitable for comparison. As shown in Figure 4, prior-augmented PySR and LLM-SR achieve better OOD generalization than their vanilla versions, demonstrating that explicit prior constraints can ef-fectively improve scientific consistency. Nevertheless, their performance remains inferior to PG-SR, demonstrating the superiority of the PG-SR framework. 

4.6. Sensitivity Analysis on Prior Quality 

Since prior knowledge may be incomplete or even mislead-ing, we further assessed the sensitivity of PG-SR to prior quality under four settings on the E. coli Growth dataset: No Prior, Weak Priors (specifying only optimal temperature 37 â—¦C and pH 7), Wrong Priors (incorrect values, e.g., opti-mal temperature 15 â—¦C and pH 4), and Full Priors (details in Appendix D). As shown in Figure 5, Full Priors yield the best performance, while Weak Priors provide partial im-provements. Although wrong priors degrade performance, the results remain comparable to the no-prior setting, indi-cating that PG-SR does not collapse under misleading prior information. This robustness is enabled by PACE, which moderates the influence of priors by allowing observed data to retain influence during evolution. 7Prior-Guided Symbolic Regression              

> Table 2. Ablation study.
> Variant ID NMSE â†“OOD NMSE â†“
> Full Variant 1.32e-3 1.59e-3
> w/o Warmup 5.70e-3 5.09e-2 w/o PACE 1.28e-3 2.98e-1 w/o Refine 8.75e-2 1.91e-1

4.7. Ablation Study 

To evaluate the contribution of each component in PG-SR, we conduct ablation studies on the E. coli Growth dataset, with results summarized in Table 2. Removing the warm-up stage (w/o Warmup) results in low-quality equation skele-tons in the experience pool during the initial stage, leading to poor initialization and adversely affecting subsequent generation, thereby degrading overall performance within the same number of iterations. Removing the PACE mecha-nism (w/o PACE) leads to the direct discarding of equation skeletons that violate prior constraints. This introduces dis-continuities into the search landscape, which restricts search diversity and hinders the optimization process in the early stages, ultimately resulting in performance degradation. Re-moving the refinement stage (w/o Refine) makes PG-SR more prone to local optima that satisfy the constraints but exhibit poor data fitting, highlighting the role of the refine-ment mechanism in guiding the search out of such regions. Overall, these ablation results indicate that each component effectively contributes to the performance of PG-SR. 

4.8. Robustness to Noisy Data 

In real-world scenarios, data are often corrupted by noise, posing a challenge to existing approaches grounded in em-pirical risk minimization. To investigate whether PG-SR remains robust under such conditions, we add Gaussian perturbations to the Oscillator 1 dataset: 

Ëœx = x+Ïµ, Ïµ âˆ¼ N (0 , Ïƒ 2), Ïƒ âˆˆ { 0.01 , 0.05 , 0.1}. (12)   

> Figure 6. Noise robustness analysis. OOD NMSE under different noise levels ( Ïƒ) for PG-SR and baseline models.
> Figure 7. Data Scarcity Analysis. OOD NMSE under different data availability settings for PG-SR and baseline models.

Since the training data are corrupted by noise, prior checking in this setting is performed in a statistical manner rather than through pointwise value matching. As shown in Figure 6, as the noise level increases, baseline approaches tend to overfit noisy observations, leading to poor OOD generalization. While PG-SR is also slightly affected by noise, it maintains substantially better robustness than baselines due to the guidance provided by explicit prior constraints. 

4.9. Robustness to Data Scarcity 

In real-world settings, data acquisition is often costly or con-strained, making discovery under limited data essential. To evaluate PG-SR in data-scarce regimes and validate Propo-sition 3.5, we analyze performance from 100% down to 5% data availability. As shown in Figure 7, PG-SR exhibits a slight NMSE increase when the data availability is reduced from 100% to 50%. In this regime, the reduced data make it harder to distinguish between different skeletons that satisfy the priors, leading to a temporary performance degradation. When the data are further reduced from 50% to 5%, the discovery process becomes dominated by the priors, which strongly restrict the hypothesis space, leaving the remaining candidate solutions constrained in structure and thus making performance insensitive to further changes in data scale. 

# 5. Conclusion 

This paper proposes PG-SR to address the Pseudo-Equation Trap in symbolic regression, taking a step toward scientific consistency in equation discovery. Experiments show that PG-SR outperforms SOTA baselines across diverse domains. Despite its effectiveness, PG-SR still relies on manual in-tervention when constructing executable prior constraints. Future work will explore automating prior constraint gen-eration by fine-tuning LLMs to synthesize executable con-straint programs from problem descriptions and data statis-tics, moving PG-SR toward a fully autonomous framework. 8Prior-Guided Symbolic Regression 

# Broader Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 

# References 

Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. Invariant risk minimization. arXiv preprint arXiv:1907.02893 , 2019. Bartlett, P. L. and Mendelson, S. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of machine learning research , 3(Nov):463â€“482, 2002. Biggio, L., Bendinelli, T., Neitz, A., Lucchi, A., and Paras-candolo, G. Neural symbolic regression that scales. In 

International Conference on Machine Learning , pp. 936â€“ 945. Pmlr, 2021. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. 

Advances in neural information processing systems , 33: 1877â€“1901, 2020. Burlacu, B., Kronberger, G., and Kommenda, M. Operon c++ an efficient genetic programming framework for sym-bolic regression. In Proceedings of the 2020 genetic and evolutionary computation conference companion , pp. 1562â€“1570, 2020. Byrd, R. H., Lu, P., Nocedal, J., and Zhu, C. A limited memory algorithm for bound constrained optimization. 

SIAM Journal on scientific computing , 16(5):1190â€“1208, 1995. Chen, M. Evaluating large language models trained on code. 

arXiv preprint arXiv:2107.03374 , 2021. Cranmer, M. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582 , 2023. Cranmer, M., Sanchez Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., and Ho, S. Discovering sym-bolic models from deep learning with inductive biases. 

Advances in neural information processing systems , 33: 17429â€“17442, 2020. De Florio, M., Kevrekidis, I. G., and Karniadakis, G. E. Ai-lorenz: A physics-data-driven framework for black-box and gray-box identification of chaotic systems with symbolic regression. Chaos, Solitons & Fractals , 188: 115538, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024. Grayeli, A., Sehgal, A., Costilla Reyes, O., Cranmer, M., and Chaudhuri, S. Symbolic regression with a learned concept library. Advances in Neural Information Process-ing Systems , 37:44678â€“44709, 2024. Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. Survey of halluci-nation in natural language generation. ACM computing surveys , 55(12):1â€“38, 2023. Kamienny, P.-A., dâ€™Ascoli, S., Lample, G., and Charton, F. End-to-end symbolic regression with transformers. 

Advances in Neural Information Processing Systems , 35: 10269â€“10281, 2022. Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., and Yang, L. Physics-informed machine learning. Nature Reviews Physics , 3(6):422â€“440, 2021. Kocsis, L. and Szepesv Â´ari, C. Bandit based monte-carlo planning. In European conference on machine learning ,pp. 282â€“293. Springer, 2006. Koza, J. R. Genetic programming as a means for program-ming computers by natural selection. Statistics and com-puting , 4(2):87â€“112, 1994. Landajuela, M., Lee, C. S., Yang, J., Glatt, R., Santiago, C. P., Aravena, I., Mundhenk, T., Mulcahy, G., and Pe-tersen, B. K. A unified framework for deep symbolic regression. Advances in Neural Information Processing Systems , 35:33985â€“33998, 2022. Makke, N. and Chawla, S. Interpretable scientific discovery with symbolic regression: a review. Artificial Intelligence Review , 57(1):2, 2024. McCormick, T. gplearn: Genetic programming in python. 

https://github.com/trevorstephens/gp learn , 2019. GitHub repository. McDiarmid, C. et al. On the method of bounded differences. 

Surveys in combinatorics , 141(1):148â€“188, 1989. Mohri, M., Rostamizadeh, A., and Talwalkar, A. Founda-tions of machine learning . MIT press, 2018. 9Prior-Guided Symbolic Regression 

OpenAI. Gpt -4o mini: advancing cost - efficient intelligence. 

https://openai.com/is-IS/index/gpt-4 o-mini-advancing-cost-efficient-intel ligence/ , 2024. Petersen, B. K., Landajuela, M., Mundhenk, T. N., Santi-ago, C. P., Kim, S. K., and Kim, J. T. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. arXiv preprint arXiv:1912.04871 , 2019. Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S., Wang, P., Fawzi, O., et al. Mathematical discoveries from program search with large language models. Nature , 625 (7995):468â€“475, 2024. Schmidt, M. and Lipson, H. Distilling free-form natural laws from experimental data. science , 324(5923):81â€“85, 2009. Shojaee, P., Meidani, K., Barati Farimani, A., and Reddy, C. Transformer-based planning for symbolic regression. 

Advances in Neural Information Processing Systems , 36: 45907â€“45919, 2023. Shojaee, P., Meidani, K., Gupta, S., Farimani, A. B., and Reddy, C. K. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400 , 2024. Shojaee, P., Nguyen, N.-H., Meidani, K., Farimani, A. B., Doan, K. D., and Reddy, C. K. Llm-srbench: A new benchmark for scientific equation discovery with large language models. arXiv preprint arXiv:2504.10415 ,2025. Talagrand, M. New concentration inequalities in prod-uct spaces. Inventiones mathematicae , 126(3):505â€“563, 1996. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Sori-cut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. Uc-Cetina, V., Navarro-Guerrero, N., Martin-Gonzalez, A., Weber, C., and Wermter, S. Survey on reinforcement learning for language processing. Artificial Intelligence Review , 56(2):1543â€“1575, 2023. Udrescu, S.-M. and Tegmark, M. Ai feynman: A physics-inspired method for symbolic regression. Science ad-vances , 6(16):eaay2631, 2020. Vastl, M., Kulh Â´anek, J., Kubal Â´Ä±k, J., Derner, E., and Babu Ë‡ska, R. Symformer: End-to-end symbolic regression using transformer-based architecture. IEEE Access , 12:37840â€“ 37849, 2024. Virgolin, M., Alderliesten, T., Witteveen, C., and Bosman, P. A. Improving model-based genetic programming for symbolic regression of small expressions. Evolutionary computation , 29(2):211â€“237, 2021. Wang, R., Wang, B., Li, K., Zhang, Y., and Cheng, J. Drsr: Llm based scientific equation discovery with dual reasoning from data and experience. arXiv preprint arXiv:2506.04282 , 2025. Wang, Y., Wagner, N., and Rondinelli, J. M. Symbolic regression in materials science. MRS communications , 9 (3):793â€“805, 2019. Whitley, D., Rana, S., and Heckendorn, R. B. The island model genetic algorithm: On separability, population size and convergence. Journal of computing and information technology , 7(1):33â€“47, 1999. Ying, J., Lin, H., Yue, C., Chen, Y., Xiao, C., Shi, Q., Liang, Y., Yau, S.-T., Zhou, Y., and Ma, J. A neural symbolic model for space physics. Nature Machine Intelligence ,pp. 1â€“16, 2025. 10 Prior-Guided Symbolic Regression 

# A. Theoretical Analysis 

In this appendix, we provide detailed proofs for the theoretical results presented in Section 3.2, based on the statistical learning theory framework (Mohri et al., 2018). Let P denote an unknown joint distribution over X Ã— Y . The training dataset D = {(xi, y i)}Ni=1 consists of N i.i.d. samples drawn from P.

A.1. Preliminaries and Assumptions 

To ensure the theoretical validity of the generalization bounds in the context of symbolic regression, we adopt the following standard regularity assumption regarding the hypothesis space and loss function. 

Assumption A.1 (Boundedness) . Let X âŠ‚ Rd be the compact input domain and Y âŠ‚ R be the bounded output space. We assume that the hypothesis space H consists of functions that are bounded within the domain. Specifically, there exists a constant B > 0 such that for any candidate function f âˆˆ H and any x âˆˆ X , |f (x)| â‰¤ B. Consequently, assuming the target 

y is also bounded, the loss function â„“(f (x), y ) is bounded by a constant M < âˆž and is Lipschitz continuous with respect to the prediction. 

Remark: In practice, this assumption is enforced by detecting and rejecting candidate equations that exhibit singularities or numerical overflow within the domain of interest, or by applying a truncation operator to the function outputs. 

A.2. Proof of Lemma 3.2 (Generalization Bound) Lemma 3.2 (Generalization Bound). Let H be the hypothesis space. Assume the loss function â„“ is Î»-Lipschitz with respect to its first argument and bounded by M . For any Î´ > 0, with probability at least 1 âˆ’ Î´, the generalization error of any 

f âˆˆ H is bounded by: 

R(f ) â‰¤ Ë†RN (f ) + 2 Î»RN (H) + M

r log(1 /Î´ )2N . (13) 

Proof. The proof follows the standard symmetrization argument in statistical learning theory (Mohri et al., 2018). To ensure the hypothesis space H is well-defined and has finite Rademacher complexity, we consider the set of symbolic expression trees with a maximum depth L.Let Î¦( D) = sup f âˆˆH (R(f ) âˆ’ Ë†RN (f )) . We aim to bound Î¦( D). Since the loss function is bounded by M , changing one example (xi, y i) to (xâ€²

> i

, y â€²

> i

) changes Î¦( D) by at most M/N . By McDiarmidâ€™s inequality (McDiarmid et al., 1989), for any 

Î´ > 0, with probability at least 1 âˆ’ Î´:

Î¦( D) â‰¤ ED [Î¦( D)] + M

r log(1 /Î´ )2N . (14) Next, we bound the expectation ED [Î¦( D)] . We introduce a ghost dataset Dâ€² = {(xâ€²

> i

, y â€²

> i

)}Ni=1 drawn from the same distribution P:

ED [Î¦( D)] = ED

"

sup  

> fâˆˆH



EDâ€² [ Ë†Râ€² 

> N

(f )] âˆ’ Ë†RN (f )

#

(15) 

â‰¤ ED,Dâ€²

"

sup  

> fâˆˆH

( Ë†Râ€² 

> N

(f ) âˆ’ Ë†RN (f )) 

#

(by Jensenâ€™s inequality ) (16) 

= ED,Dâ€²

"

sup  

> fâˆˆH

1

N

> N

X

> i=1

(â„“(f (xâ€²

> i

), y â€²

> i

) âˆ’ â„“(f (xi), y i)) 

#

. (17) Since D and Dâ€² are i.i.d., introducing Rademacher variables Ïƒ = ( Ïƒ1, . . . , Ïƒ N ) where Ïƒi âˆˆ {âˆ’ 1, +1 } uniformly does not 11 Prior-Guided Symbolic Regression 

change the distribution. Thus: 

ED [Î¦( D)] â‰¤ ED,Dâ€²,Ïƒ

"

sup  

> fâˆˆH

1

N

> N

X

> i=1

Ïƒi(â„“(f (xi), y i) âˆ’ â„“(f (xâ€²

> i

, y â€²

> i

))) 

#

(18) 

â‰¤ 2ED,Ïƒ

"

sup  

> fâˆˆH

1

N

> N

X

> i=1

Ïƒiâ„“(f (xi), y i)

#

(19) 

= 2 RN (â„“ â—¦ H ). (20) Assuming the loss function â„“ is Î»-Lipschitz (as implied by Assumption A.1), by Talagrandâ€™s contraction lemma (Talagrand, 1996; Bartlett & Mendelson, 2002), RN (â„“ â—¦ H ) â‰¤ Î»RN (H). Substituting this back completes the proof. 

A.3. Proof of Proposition 3.5 (Complexity Reduction) Proposition 3.5. Let C be a set of restrictive prior constraints satisfying ftrue âˆˆ H C . The Rademacher complexity satisfies 

RN (HC ) â‰¤ R N (H).Proof. By Definition 3.4, the constrained subspace is HC = {f âˆˆ H | V (f, C) = 1 }, implying HC âŠ† H .The empirical Rademacher complexity is derived as follows: 

RN (HC ) = EÏƒ

"

sup  

> fâˆˆH C

1

N

> N

X

> i=1

Ïƒif (xi)

#

(21) 

â‰¤ EÏƒ

"

sup  

> fâˆˆH

1

N

> N

X

> i=1

Ïƒif (xi)

#

(22) 

= RN (H).

Remark. While the theoretical inequality is non-strict ( â‰¤) without stronger assumptions on C, a strict reduction ( RN (HC ) <

RN (H)) is expected in practice. Consider the set of pseudo-equations Q = H \ H C . Functions in Q typically possess high capacity and can flip signs to match the random noise Ïƒ. Since C explicitly prunes these candidates, the effective search space complexity is strictly reduced. 

# B. Mechanism Analysis for PACE 

To understand the effectiveness of the PACE strategy, we analyze its impact on the search landscape continuity and the dynamic trade-off between exploration and constraint satisfaction. 

B.1. Gradient Preservation in Search Landscape 

Standard constrained optimization often employs a hard-filtering baseline, where invalid candidates ( f / âˆˆ H C ) are assigned a worst-case score (e.g., âˆ’âˆž ) or discarded. This creates a discontinuous fitness landscape with zero gradients in the prior-violating regions, causing the search to stall if the evolutionary path requires traversing scientifically inconsistent intermediate states. Here, the term gradient is used only to describe directional signals in the score-based search, rather than gradients of a differentiable objective. In contrast, PACE maintains informative selection gradients during the exploration phase ( t < 1). Recall the total score for invalid candidates: S(f, t ) = Ïƒ(t)Sbase (f ) âˆ’ Î´(t). Since Ïƒ(t) > 0 and Î´(t) are time-dependent scalars constant across the population at any generation t, the scoring function is an affine transformation of the baseline reward. Consequently, for any two invalid candidates f1, f 2, the ranking order is preserved: 

S(f1, t ) > S (f2, t ) â‡â‡’ Sbase (f1) > S base (f2). (23) This linearity ensures that the optimizer receives informative feedback driven by numerical accuracy even within the invalid subspace. By eliminating the zero-gradient plateaus typical of hard-filtering, PACE allows the algorithm to utilize high-performing but inconsistent structures as evolutionary stepping stones toward valid solutions. 12 Prior-Guided Symbolic Regression  

> Figure 8. Dynamic score boundaries induced by the PACE (Prior-Annealing Constraint Evaluation) mechanism.

B.2. Dynamic Exploration-Constraint Trade-off 

To visually illustrate how scientific priors are progressively enforced, Figure 8 depicts the dynamic reward boundaries induced by the PACE mechanism. Mathematically, the time-varying parameters Ïƒ(t) and Î´(t) implement a smooth homotopy from unconstrained exploration to strict constraint enforcement. This creates a mutable failure region for invalid candidates while maintaining a fixed success region for valid ones, effectively managing two distinct risks: â€¢ Exploration Phase (Low t): With Ï•(t) â‰ˆ 0, the penalty Î´(t) is negligible and the reward width Ïƒ(t) â‰ˆ 1. In this stage, the failure region is broad, mimicking the unconstrained baseline. This configuration minimizes the Search Failure Risk by preventing the premature rejection of promising but scientifically inconsistent candidates, allowing the search beam to encompass diverse structural prototypes. â€¢ Enforcement Phase (High t): As t â†’ 1, the curriculum schedule (controlled by exp base = 60 ) triggers a sharp increase in penalty Î´(t) and a collapse in reward variance Ïƒ(t). This effectively tightens the failure boundaries, creating a steep potential barrier against invalid candidates. This regime minimizes the Generalization Risk by compelling the population to converge into the scientifically consistent subspace HC .This dynamic annealing mechanism does not impose strict constraints at the early stage of the search. Instead, it allows the model to explore the hypothesis space freely and gradually strengthens the prior constraints in later stages. This design effectively avoids the pseudo-equation trap, where models achieve good numerical fit but violate scientific principles, thereby ensuring that the final discovered equations strike a principled balance between numerical accuracy and scientific consistency. 13 Prior-Guided Symbolic Regression 

# C. Details on Datasets 

To simulate the real-world discovery process and prevent the risk of LLM recitation, our benchmarks are drawn from LLMSR-Bench (Shojaee et al., 2025). â€¢ E. coli Growth : This dataset describes the growth dynamics of Escherichia coli as a function of substrate concentration (S), temperature ( T ), and pH. The model adopts a multiplicative structure, 

dB dt = fB (B) Â· fS (S) Â· fT (T ) Â· fpH (pH) ,

which captures the combined influence of biological capacity and environmental factors. To avoid trivial recapitulation of standard biological growth models, we introduce novel nonlinear formulations for environmental dependencies. The explicit growth equation is given by 

dB dt = Î¼max B

 SKs + S

 tanh  k(T âˆ’ x0)

1 + c(T âˆ’ xdecay )4 exp  âˆ’| pH âˆ’ pH opt | sin 2

 (pH âˆ’ pH min )Ï€

pH max âˆ’ pH min 



.

â€¢ Material Stress-Strain (Real-World Experimental Data) : Unlike synthetic benchmarks, this dataset comprises actual experimental data from tensile tests of Aluminum 6061-T651 across a range of temperatures from 20 â—¦C to 300 â—¦C.This problem challenges the model to recover empirical relationships from noisy, real-world observations where no universally accepted theoretical closed-form expression exists. The curves exhibit complex non-linearity, including distinct elastic, plastic, and failure regions that vary significantly with temperature. To rigorously test the out-of-domain generalization capabilities on real-world data, we allocate the data corresponding to T = 200 â—¦C specifically for use as the out-of-domain validation set, withholding it entirely from the training process. â€¢ Chemical Reaction Kinetics (CRK) : This benchmark describes how the concentration of a chemical species evolves over time. It incorporates standard exponential decay behavior alongside additional nonlinear saturation effects to evaluate model robustness in capturing governing equations for chemical decay and reaction rates. 

dA dt = âˆ’0.1899 Â· A(t)2 + 0.4598 Â· A(t)2

0.7498 Â· A(t)4 + 1 (24) where A(t) denotes concentration at time t. This equation incorporates a second-order decay term alongside a synthetic nonlinear saturation term, evaluating symbolic reasoning in data-driven kinetic modeling. â€¢ Nonlinear Oscillators : These systems follow the general differential form Ë™x + f (t, x, Ë™x) = 0 , describing the complex interplay between an oscillatorâ€™s position, velocity, and forces. Both datasets share the same time range (0 , 50) and initial values {x = 0 .5, v = 0 .5}. To effectively evaluate the generalization capability of the discovered equations, we employ a strategic data partitioning scheme where the simulation data is divided into training, in-domain validation, and out-of-domain validation sets based on the trajectory time. Specifically, the time interval T = [0 , 20) is utilized to evaluate out-of-domain generalization. 

â€“ Oscillator 1 : A time-independent system governed by the specific equation: 

Ë™v = 0 .8 sin( x) âˆ’ 0.5v3 âˆ’ 0.2x3 âˆ’ 0.5xv âˆ’ x cos( x)

It combines trigonometric, polynomial, and mixed terms to test the recovery of complex nonlinear interactions without temporal forcing. 

â€“ Oscillator 2 : A time-dependent system governed by the specific equation: 

Ë™v = 0 .3 sin( t) âˆ’ 0.5v3 âˆ’ xv âˆ’ 5.0x exp(0 .5x)

This introduces explicit temporal forcing and exponential nonlinearities to challenge the modelâ€™s ability to handle time-varying dynamics. 14 Prior-Guided Symbolic Regression 

# D. Summary of Prior Constraints 

Table 3 lists the explicit prior constraints used in our method. As shown in Figure 9, these constraints are constructed using an LLM-assisted workflow: general scientific principles suggested by LLMs are checked and adjusted based on analyses of the training data. This process converts domain knowledge into executable constraint programs. For example: â€¢ E. coli Growth: Fundamental biological constraints, including near-zero growth at vanishing population density and near-zero growth under lethal environmental conditions (e.g., extreme temperature or pH), are observed in the training data. Additionally, unimodal responses to environmental factors coincide with the density truncation seen in the dataset. â€¢ Stressâ€“Strain: Due to noise, constraints are not imposed pointwise but derived from statistical trends in the data combined with established mechanical knowledge of Al 6061-T651. Specifically, these constraints include monotonic stress growth in the elastic and early plastic regimes, retention of load-carrying capacity at large deformation, observable thermal softening at elevated temperatures, and a bounded near-zero stress level at small strain. â€¢ CRK: Reaction dynamics, including vanishing reaction rates at the equilibrium state, are consistently observed in the data. Moreover, the equilibrium point itself is identifiable from the concentration trajectories, which monotonically converge toward a steady state, providing strong empirical support for equilibrium consistency and global stability. â€¢ Oscillator-1: Restoring forces toward equilibrium and velocity-opposing damping follow classical physical intuition and are supported by symmetric trajectories and monotonic amplitude decay. Phase portraits and joint state statistics further reveal clear departures from linear dynamics, while bounded trajectories are consistent with the finite range of the observed state distribution. â€¢ Oscillator-2: Asymmetric restoring behavior is evident from the response statistics, which distinguish between positive and negative displacements. Furthermore, stable periodic structures in both time series and phase portraits confirm explicit periodic driving. Nonlinearity is substantiated by curvature and state-dependent distortions in phase space, alongside higher-order statistical dependencies in the data.                                         

> Table 3. Summary of prior constraints for each problem.
> Problem Prior Constraints E. coli Growth
> â€¢ Multivariate dynamics: db =f(b, s, T, pH )
> â€¢ Biological causality: db = 0 when b= 0
> â€¢ Viability boundaries: db â‰¤0at lethal conditions (eg. extreme Temperature or pH) â€¢ Unimodal response to temperature and pH â€¢ Asymmetric temperature response (sharper decay at high T)
> Stressâ€“Strain
> â€¢ Thermo-mechanical coupling: Ïƒ=f(Îµ, T )
> â€¢ Near-zero stress behavior in the small-strain regime â€¢ Monotonic stress growth across elastic and early plastic regimes with work hardening â€¢ Statistically observable thermal softening at elevated temperatures â€¢ Bounded and numerically stable model outputs (no NaN/Inf)
> CRK
> â€¢ Reaction dynamics system: dA/dt =f(A, t )
> â€¢ Equilibrium consistency at Aeq
> â€¢ Global stability toward equilibrium â€¢ Non-negativity constraint: dA/dt > = 0 at A= 0
> â€¢ Mandatory nonlinearity
> Oscillator-1
> â€¢ State-dependent dynamics: a=f(x, v ), no explicit time dependence â€¢ Restoring mechanism toward equilibrium â€¢ Dissipative damping that opposes velocity â€¢ Nonlinearity requirement â€¢ Bounded trajectories under typical initial conditions
> Oscillator-2
> â€¢ Non-autonomous dynamics: a=f(t, x, v )
> â€¢ Restoring mechanism toward equilibrium and asymmetric restoring force â€¢ Dissipative damping opposing velocity â€¢ Bounded time-dependent driving term â€¢ Nonlinearity requirement â€¢ Bounded trajectories under typical initial conditions

15 Prior-Guided Symbolic Regression I have analyzed the data . The distributions confirm a clear 

Unimodal Response where growth peaks at 37 Â°C and pH 7, and 

distinct Viability Boundaries where the growth rate strictly drops 

to zero at extreme temperature or extreme pH levels .

â€¦â€¦ 

Now, please convert these principles into an executable Python 

constraint checker .                             

> def Check_Constraints (b, s, temp, pH, db ):
> # 1. Biological Causality (No spontaneous generation)
> if b == 0 and db > 1e -6:
> return False
> # 2. Viability Boundaries (Death at extremes)
> if (temp >100 or pH <1 or pH >13 )and db >= 1e -6:
> return False
> # 3. Unimodal Response
> if not unimodal (temp )or not unimodal(pH):
> return False
> # 4. Other Constraintsâ€¦â€¦
> return True # Passed all checks

Python 

Here is the checker I generated by the principles we discussed .

Certainly . Based on general biological domain knowledge, E. coli 

growth typically adheres to the following principles :

â€¢ Biological Causality : Growth rate must be 0 when biomass is 0

(no spontaneous generation) .

â€¢ Viability Boundaries : Growth rate must be negative (death) 

under extreme conditions (e .g., extreme Temperature or 

extreme pH) .

â€¢ Unimodal Response : The growth response to environmental 

factors (like Temperature and pH) is typically unimodal (having a

single optimum), rather than monotonic .

â€¦â€¦ 

I am currently investigating the growth characteristics of E.

coli . Please provide some relevant properties that can be 

encoded as mathematical constraints .

Figure 9. Illustration of the prior constraint checker in PG-SR, using the E. coli Growth task as an example to encode domain priors as executable constraint programs. 

16 Prior-Guided Symbolic Regression 

# E. Details on Baselines 

We compare PG-SR against three distinct categories of state-of-the-art symbolic regression approaches. The underlying principles and specific implementation settings for each baseline are detailed as follows: 

E.1. Search-based approaches 

These approaches formulate symbolic regression as a combinatorial optimization problem, searching for the optimal expression structure through evolutionary algorithms or reinforcement learning. â€¢ GPLearn(McCormick, 2019) : A standard implementation of Genetic Programming (GP) that evolves a population of expression trees using genetic operators such as crossover, mutation, and reproduction. 

â€“ Settings : Population size is set to 1000, with 2000 generations and a tournament selection size of 20. 

â€“ Function set : The function set consists of {add , sub , mul , div , sqrt , log , exp , sin , cos , abs , neg ,

inv }.

â€“ Other settings : All other hyperparameters follow the default settings of the original implementation. â€¢ PySR (Cranmer, 2023) : An advanced SR method that employs asynchronous multi-island GP-based evolutions. 

â€“ Settings : Population size is set to 50, with 2000 generations and a tournament selection size of 10. 

â€“ Function set : The function set consists of {add , sub , mul , div , sqrt , log , exp , sin , cos , abs , neg ,

inv }.

â€“ Other settings : All other hyperparameters follow the default settings of the original implementation. â€¢ DSR (Deep Symbolic Regression) (Petersen et al., 2019) : Employs an RNN-based policy to sample mathematical expressions in the form of pre-order traversal sequences, optimized via a risk-seeking policy gradient. 

â€“ Settings : The number of samples is set to 200,000, with a batch size of 100 and an exploration parameter Ïµ = 0 .05 .

â€“ Function set : The function set consists of {add , sub , mul , div , sqrt , log , exp , sin , cos , abs , neg ,

inv }.

â€“ Other settings : All other hyperparameters follow the default settings of the original implementation. â€¢ uDSR (Landajuela et al., 2022) : A hybrid framework that unifies deep reinforcement learning with genetic program-ming, combining the global exploration capability of RL with the local search efficiency of GP. 

â€“ Settings : The number of samples is set to 200,000, with a batch size of 100 and an exploration parameter Ïµ = 0 .05 .

â€“ Prior : A length prior is enabled ( on = true ), with the maximum expression length set to 20. 

â€“ Function set : The function set consists of {add , sub , mul , div , sqrt , log , exp , sin , cos , abs , neg ,

inv }.

â€“ Polynomial optimizer : The polynomial optimizer parameters are set to a maximum degree of 2 and a coefficient tolerance of 1 Ã— 10 âˆ’5.

â€“ Other settings : All other hyperparameters follow the default settings of the original implementation. â€¢ Operon (Burlacu et al., 2020) : A highly optimized C++ framework that uses a diverse set of selection and mutation schemes. 

â€“ Settings : All hyperparameters follow the default settings of the original implementation. â€¢ GP-GOMEA (Virgolin et al., 2021) : A model-based GP approach that identifies linkage groups between variables to effectively preserve and combine useful sub-expressions. 

â€“ Settings : All hyperparameters follow the default settings of the original implementation. 17 Prior-Guided Symbolic Regression 

E.2. Transformer-based approaches 

These approaches enable end-to-end equation generation by leveraging large-scale pretraining on massive synthetic datasets. â€¢ TPSR (Transformer-based Planning for Symbolic Regression) (Shojaee et al., 2023) : Integrates Monte Carlo Tree Search into the decoding process of a pretrained Transformer, allowing the model to iteratively plan and refine the structure of symbolic expressions. 

â€“ Backbone : A pretrained E2E Transformer model is used as the backbone network. 

â€“ Settings : All other hyperparameters follow the default configuration of the original implementation. â€¢ E2E (End-to-End Symbolic Regression) (Kamienny et al., 2022) : A sequence-to-sequence pre-trained Transformer architecture for symbolic regression. 

â€“ Settings : All other hyperparameters follow the default settings of the original implementation. â€¢ NeSymReS (Biggio et al., 2021) : A pioneering pre-trained Transformer-based symbolic regression model. 

â€“ Model configuration : Experiments are conducted using the 100M-parameter pretrained checkpoint. 

â€“ Other settings : All other hyperparameters follow the default settings of the original implementation. â€¢ PhyE2E (Ying et al., 2025) : A recent sequence-to-sequence pre-trained Transformer model for symbolic regression. 

â€“ Settings : All other hyperparameters follow the default settings of the original implementation. 

E.3. LLM-based approaches 

These approaches leverage LLMs as program generators or evolutionary operators to guide the search for symbolic expressions. â€¢ LaSR (Grayeli et al., 2024) : A Julia-based framework that integrates genetic algorithms with LLM. 

â€“ Settings : All hyperparameters follow the default configuration of the original implementation. â€¢ LLM-SR (Shojaee et al., 2024) : An LLM-driven symbolic regression framework. 

â€“ Settings : All remaining hyperparameters follow the default configuration of the original implementation. â€¢ DrSR (Wang et al., 2025) : An LLM-driven symbolic regression framework with dual reasoning mechanisms. 

â€“ Settings : All hyperparameters follow the default settings of the released codebase. 18 Prior-Guided Symbolic Regression 

# F. PG-SR Configuration 

Table 4 presents the specific configurations. 

Table 4. Complete Configuration Overview of PG-SR. 

Parameter / Flag Default Description 

Core Experiment Configuration 

num samplers 1 Number of parallel samplers 

num evaluators 1 Number of parallel evaluators 

samples per prompt 4 Number of candidate hypotheses generated per prompt 

evaluate timeout seconds 30 Timeout for evaluating a single hypothesis (seconds) 

api model gpt-4o-mini LLM used for generation 

keep docstrings True Whether to retain docstrings in extracted code 

max sample num 10,000 Global maximum number of samples 

Experience Buffer Configuration 

functions per prompt 2 Number of historical hypotheses included in each prompt 

num islands 10 Number of islands (sub-populations) maintained in parallel 

reset period 4 hours Period for resetting underperforming islands 

cluster sampling temp init 0.1 Initial temperature for cluster-based sampling 

cluster sampling temp period 30,000 Temperature decay period (in sampling steps) 

Prior Constraint & PACE Configuration 

beta 0.6 Success reward range [1 âˆ’ Î², 1 + Î²]

pace alpha 1.2 Maximum downward shift for failure region 

pace eta 1.0 Maximum shrinkage ratio for failure region 

pace exp base 60.0 Exponential base controlling annealing speed 

n l-bfgs-b retries 10 L-BFGS-B retries when physics check fails 

max repair rounds 3 Maximum repair rounds per equation 

Warmup Stage Configuration 

warmup num skeletons 100 Total number of skeletons requested during warmup 

warmup skeletons per call 5 Number of skeletons per LLM call 

min physics passed warmup 10 Minimum physics-passed skeletons required 

max warmup repair iterations 40 Maximum repair iterations in warmup 

Refinement Stage Configuration 

refine start ratio 0.01 Ratio of budget after which refinement starts 

refine interval 100 Trigger refinement every N samples 

refine num skeletons 10 Total number of skeletons requested per refinement 

refine skeletons per call 2 Number of skeletons per LLM call 

max refine repair iterations 6 Maximum repair iterations in refinement 

repair good examples num 3 Max good examples shown in repair prompt 

repair history num 3 Max repair history shown in repair prompt 

19 Prior-Guided Symbolic Regression 

# G. Details on Prompt Designs 

We provide the concrete prompt templates used in our experiments below. 

G.1. Data-Driven Warm-up Prompt Warmup Prompt 

[Task: Data-Driven Scientific Hypothesis Generation] Role : You are a helpful assistant tasked with discovering mathematical function structures for {domain }.

Goal : Based on the Analysis Report, Variable Definitions and the Hard Rules below, produce EXACTLY {num funcs } candidate {equation desc } ({equation form }). 

[Variable Definitions] 

{variable definitions }

[Analysis Report] 

{analysis report }

[Hard Rules] 

{hard rules block }

You must reason step-by-step. For each equation, provide a brief physical justification in the code comments (# Reasoning: ...) BEFORE writing the formula. 

[Output Format] import numpy as np MAX_NPARAMS = 10 PARAMS_INIT = [1.0]*MAX_NPARAMS 

def equation_v1({signature_vars}, params): â€™â€™â€™ Args: {signature_desc} params: Array of parameters [params[0], params[1], ...] to be optimized. Returns: {return_var}: {return_desc} â€™â€™â€™ 

# Reasoning: ... 

{return_var} = {example_formula1} 

return {return_var} 

20 Prior-Guided Symbolic Regression 

G.2. Evolution Prompt Evolution Prompt 

[System Message] 

You are a helpful assistant tasked with discovering mathematical function structures for scientific systems. Complete the â€™equationâ€™ function below, considering the physical meaning and relationships of inputs. All parameters are constrained to be NON-NEGATIVE (â‰¥0). Write minus signs explicitly in the equation structure if needed. Explain your reasoning briefly before completing the function. Letâ€™s think step by step. 

[User Prompt - From Specification Template] 

""" {problem_description_from_specification} [Learnings from Recent Attempt] The following are ideas summarized based on past experiences: {reflection} Use the above ideas and following equations to guide your equation improvement. Feel free to explore entirely different structures if they lead to better performance. """ 

import numpy as np MAX_NPARAMS = 10 PARAMS_INIT = [1.0]*MAX_NPARAMS {specification_code} 

def equation_v0({signature_vars}, params): 

# Lower score version 

{return_var} = {example_formula_0} 

return {return_var} 

def equation_v1({signature_vars}, params): 

# Medium score version 

{return_var} = {example_formula_1} 

return {return_var} 

def equation_v2({signature_vars}, params): â€™â€™â€™Improved version of â€˜equation_v1â€˜.â€™â€™â€™ 

# (Implementation to be generated) 

21 Prior-Guided Symbolic Regression 

G.3. Residual Analysis Prompt Residual Analysis Prompt 

[Task: Analyze Residual Patterns] Role : You are an expert data analyst specializing in symbolic regression diagnostics for {domain }.

Goal : Analyze the residual statistics and identify key patterns that explain why the current equation fails. 

[Variable Definitions] 

{variable definitions }

[Current Equation] 

{code_str} 

[Raw Residual Statistics] 

{raw statistics }

[Analysis Instructions] 

Based on the statistics above, provide a concise analysis that: 

â€¢ Identifies the PRIMARY deficiency of the current equation (what mathematical structure is missing or incorrect?) 

â€¢ Explains which input regions or variable ranges are most problematic 

â€¢ Analyze and summarize how changes of each independent variable influence the dependent variable, and the possible intrinsic relationships among independent variables. 

â€¢ Suggests what type of mathematical modifications would address these issues (e.g., nonlinear terms, interactions, saturation effects) Be specific and actionable. Focus on structural insights rather than parameter tuning. 

[Output Format] 

Provide your analysis briefly in the following JSON format: {"primary_deficiency": "Description of the main structural problem with the current equation", "problematic_regions": "Description of which input regions have the highest errors and why", "variable_relationships": "Analysis of how each independent variable ({input_vars}) influences the dependent variable ({output_var}) and their intrinsic relationships. Hint: analyze the functional relationship between each input variable and the output in different intervals", "suggested_modifications": "Specific mathematical structures or terms that should be added/modified" }

22 Prior-Guided Symbolic Regression 

G.4. Refinement Prompt Refine User Prompt 

[Task: Residual-Guided Equation Refinement] Role : You are a helpful assistant tasked with refining mathematical function structures for {domain }.

Goal : Based on the Residual Diagnostic Report, produce EXACTLY {refine n } refined versions of the current best equation. 

[Variable Definitions] 

{variable definitions }

[Current Best Equation] 

{code_str} 

[Experience Hints] 

{experience hints }

(Optional: Retrieved from experience buffer based on similar equations.) 

[Residual Diagnostic Report] 

{analysis report }

[Refinement Instructions] 

Based on the analysis above: 

â€¢ Focus on addressing the PRIMARY DEFICIENCY identified in the report 

â€¢ Target the PROBLEMATIC REGIONS where the current equation fails most 

â€¢ Consider the VARIABLE RELATIONSHIPS to understand how each input affects the output in different regimes 

â€¢ Implement the SUGGESTED MODIFICATIONS to fix structural issues 

â€¢ Pay attention to {defect var }, which contributes most to prediction error 

â€¢ If the analysis suggests fundamental structural limitations, do not hesitate to explore completely different formulations 

[Hard Rules] 

{hard rules block }

[Output Format] import numpy as np 

def equation_v1({signature_vars}, params): 

# Reasoning: ... 

{return_var} = ... 

return {return_var} 

23 Prior-Guided Symbolic Regression 

G.5. Improvement Analysis Prompt Improvement Analysis Prompt 

[Task: Analyze Equation Improvement] Role : You are an expert scientist analyzing mathematical equation improvements in 

{domain }.

Goal : Understand why a refined equation outperformed the original and extract actionable insights. 

[Score Explanation] {score explanation }

[Variable Definitions] {variable definitions }

[Original Equation] (score: {original score }){original_equation} 

[Analysis of Residual Errors in the Original Equation] {residual summary }

(Note: These are the residual patterns/errors of the Original Equation that need fixing.) 

[Improved Equation] (score: {improved score }){improved_equation} 

[Score Improvement] : {score gain } ({score gain percent }%) 

[Analysis Instructions] 

1. What structural changes were made? (new terms, different function forms, interactions, etc.) 2. Explain how the structural changes corrected the specific patterns observed in the original equationâ€™s residuals. 3. Provide an actionable insight that could help improve similar equations. 4. Start your response with a <thinking> block to analyze the mathematical structural changes step-by-step. 

[Output Format] 

Your response must strictly follow this structure: 

PART 1: Thinking Process 

Wrap your step-by-step analysis inside a <thinking> tag. Example: <thinking> The original equation lacked... The new term interaction(x, y) captures... </thinking> 

PART 2: Structured JSON 

Provide the final result in a standard JSON block: {"insight": "Briefly describe the mathematical changes, explain why they are effective in the domain context, and conclude with one actionable takeaway for future optimization." }

24 Prior-Guided Symbolic Regression 

G.6. Island Reflection Prompt Island Reflection Prompt 

[Task: Reflection Analysis] Role : You are an expert in scientific equation discovery and mathematical modeling. 

Domain : {domain }

[Variable Definitions] {variable definitions }

You are analyzing {total equations } equation candidates generated during a refinement process for the {problem name } problem. 

[Original Equation (to be refined)] Score: {baseline score } (higher is better, score = -MSE) {original_equation} Below are the refined candidates. Each is labeled with a status showing whether it improved the score and whether it passed constraint checks: 

â€¢ Improved/NotImproved: compared to baseline score 

â€¢ Valid/Invalid: constraint check results 

[Refined Candidates Results] 

{detailed_results} 

[Task] : Analyze these results and provide structured guidance for future evolution. Compare the refined candidates against the original equation above. You need to make your answer as concise as possible. Please provide your analysis in the following format: 

What Worked Well 

[List 1-3 structural patterns or features that led to success. Please summarize useful experience.] 

â€¢ Pattern 1: [describe what worked and why] 

â€¢ Pattern 2: [describe what worked and why] 

â€¢ Pattern 3: [describe what worked and why] 

What Didnâ€™t Work 

[Analyze failures in two categories:] 

Constraint Violations (Invalid) 

[List patterns that violated domain constraints. Please summarize the lessons you can draw from it.] 

â€¢ Pattern 1: [describe what structural issue caused constraint violation and why] 

Score Not Improved (NotImproved but Valid) 

[List patterns that passed constraints but didnâ€™t improve score. Please summarize what lessons can you draw from it.] 

â€¢ Pattern 1: [describe what structural limitation prevented score improvement and why] 

â€¢ Pattern 2: [describe what structural limitation prevented score improvement and why] 

Recommendations for Future Evolution 

[Provide 1-3 specific, actionable recommendations] 

â€¢ Recommendation 1: [what to try or avoid] 

â€¢ Recommendation 2: [what to try or avoid] 

â€¢ Recommendation 3: [what to try or avoid] Focus on general structural patterns, not specific parameter values. Be specific and actionable. 

25 Prior-Guided Symbolic Regression 

G.7. Repair Prompt Repair User Prompt 

[Task: Repair Refined Equation Based on Prior Constraint Violation] Role : You are a repair-oriented assistant tasked with fixing a failed mathematical equation structure that violates known constraints in the {domain }.

Goal : Using the Prior Constraints as the primary guide and the Residual Diagnostic Report as supporting evidence, produce EXACTLY {refine n } repaired variant(s) of the current best equation by modifying its structure to directly fix the identified failure. 

[Variable Definitions] {variable definitions }

[Current Best Equation] 

{code_str} 

[Residual Diagnostic Report] {analysis report }

[Prior Constraints] {physics block }

Use the Prior Constraints as the primary reference to understand why the refined equation failed. Use the Residual Diagnostic Report only as supporting evidence to identify which terms or variable dependencies may have caused the violation. Focus your analysis on residual patterns most directly related to the failure reason, especially those involving {defect var }.

[Good Examples] {good examples block }

[Original Best Equation] 

{original_equation} 

[Failed Refined Equation] 

{failed_equation} 

[Failure Reason] {failure reason }

[Suggested Fix] {fix hint block }

[Previous Failed Attempts] {repair history block }

[Repair Instructions] 

1. Analyze WHY the equation failed based on the failure reason above and the prior constraints. 2. {history instruction }

3. Use no more than 10 parameters. 

[Output Format] def equation({signature_vars}, params): 

# Reasoning: [explain how you fixed the prior violation] 

{return_var} = [your repaired formula] 

return {return_var} 

26 Prior-Guided Symbolic Regression 

# H. Equations Discovered by PG-SR 

H.1. E. coli Growth: Equations Discovered by Our Method 

For the E. coli Growth dataset, the ground-truth equation discovered by PG-SR is: 

dB dt = Î¼max B

 SKs + S

 tanh  k(T âˆ’ x0)

1 + c(T âˆ’ xdecay )4 exp  âˆ’| pH âˆ’ pH opt | sin 2

 (pH âˆ’ pH min )Ï€

pH max âˆ’ pH min 



, (25) where B is the bacterial population, S the substrate concentration, T the temperature, and pH the acidity level. Using our method, PG-SR initially discovered: 

dB dt = Î¸Â·B SS + K Â·exp 



âˆ’ (37 âˆ’ T )2

2Ïƒ21



Â·exp 



âˆ’ T âˆ’ 37 

Ïƒ2



Â·



1 + max( T âˆ’ 35 , 0) 

Î·



Â·



1 âˆ’ |pH âˆ’ 7|

Ïƒp

Î±

Â·

 B2

(B + Îº)2



,

(26) with parameters optimized as: 

K = 0 .99 , Î· = 4 .06 , Ïƒ1 = 2 .69 , Ïƒ2 = 2 .72 ,Ïƒp = 6 .42 , Î± = 6 .50 , Îº = 7 .15 Ã— 10 âˆ’3, Î¸ = 0 .31 . (27) Structurally, the equation discovered by PG-SR follows a multiplicative growth formulation, integrating biomass-dependent growth, substrate limitation, temperature modulation, and pH-dependent inhibition, which is consistent with the ground-truth equation. The temperature effect is modeled through an asymmetric response around a reference temperature, with an additional temperature-dependent amplification term introduced at elevated temperatures, again in agreement with the formulation of the ground-truth model. The pH-dependent factor introduces a nonlinear inhibitory effect centered around neutral conditions, which is likewise consistent with the ground-truth equation. In addition, the biomass-related term incorporates a nonlinear saturation effect to modulate growth at higher population densities. Overall, the discovered equation is highly consistent with the ground-truth model in terms of the underlying growth mechanisms and key functional components, while differing in the specific functional forms employed. 

H.2. Stressstrain: Equations Discovered by Our Method 

For the Stressstrain dataset, no ground-truth equation is available. PG-SR discovered: 

Ïƒ(Îµ, T ) = I



Îµ â‰¤ Ïƒy (Îµ, T )

E0



(E0 Îµ) + I



Îµ > Ïƒy (Îµ, T )

E0

 h

Ïƒy (Îµ, T )



1 + h (Îµ âˆ’ Ïƒy (Îµ,T ) 

> E0

)



âˆ’ Î· (Îµ âˆ’ Ïƒy (Îµ,T ) 

> E0

)2i

, (28) 

Ïƒy (Îµ, T ) = max 



Ïƒy0



1 âˆ’ (a1T + a2T 2 + a3T 3 + a4 ÎµT )



, Ïƒ sat 



. (29) with parameters optimized as: 

E0 = 36 .468489 , Ïƒy0 = 0 .833684 , a1 = 0 .000000 , a2 = 0 .415737 ,a3 = 0 .387977 , a4 = 0 .101623 , Ïƒsat = 0 .255751 ,Î· = 0 .443807 , h = 0 .397738 .

(30) The discovered constitutive relation exhibits a clear elasticâ€“plastic structure with temperature-dependent yielding. The elastic regime follows a linear Hookean response, while the plastic regime is governed by a competition between linear hardening and quadratic softening. The yield stress decreases nonlinearly with temperature and is bounded from below by a saturation stress, ensuring physical consistency at high temperatures. Notably, the model identifies a coupling term between strain and temperature in the yield function, indicating that thermal softening effects intensify at larger strains. Overall, the discovered equation aligns well with established physical understanding of thermomechanical material behavior and is broadly consistent with the known constitutive characteristics of Al 6061-T651, despite the absence of ground-truth governing equations. 27 Prior-Guided Symbolic Regression 

H.3. CRK: Equations Discovered by Our Method 

For the concentration dataset, the ground-truth governing equation is 

dA dt = âˆ’0.1899 A2 + 0.4598 A2

0.7498 A4 + 1 , (31) where A denotes the substance concentration. PG-SR initially discovered the following highly flexible expression: 

dA dt = A

> p0

 (AA)Aâˆ’p1

 (Aâˆ’p2 + p3)(A+p4)



(âˆ’p5 exp( A) + p6) . (32) The optimized parameters (all positive) are: 

p0 = 0 .9216 , p1 = 0 .1439 , p2 = 0 .8598 , p3 = 0 .1417 , p4 = 0 .0136 , p5 = 0 .1417 , p6 = 0 .4579 .

Although the equation skeleton discovered by PG-SR differs substantially from the ground-truth equation in its structure, it nevertheless represents a highly expressive functional form that not only fits the observed data effectively, but also reproduces the characteristic dynamics in the vicinity of equilibrium points, thereby satisfying the key steady-state properties and local dynamical consistency constraints observed in the system. 

H.4. Oscillator1: Equations Discovered by Our Method 

For the oscillator1 dataset, the ground-truth governing equation of the damped nonlinear oscillator is 

Ë™v = 0 .8 sin( x) âˆ’ 0.5 v3 âˆ’ 0.2 x3 âˆ’ 0.5 xv âˆ’ x cos( x), (33) where x denotes the position and v denotes the velocity. PG-SR initially discovered the following candidate equation: 

Â¨x = âˆ’ c1v âˆ’ c2 sin( x) + c3x3 + c4v âˆ’ c5v2 âˆ’ c6xv 

âˆ’ c7x5 âˆ’ c8v3 + c9 âˆ’ c10 sin(2 x). (34) The optimized parameters (all positive) are: 

c1 = 0 .5503 , c2 = 0 .0954 , c3 = 0 .0810 , c4 = 0 .5503 ,c5 = 1 .68 Ã— 10 âˆ’7, c6 = 0 .5000 , c7 = 0 .0203 ,c8 = 0 .5000 , c9 = 5 .49 Ã— 10 âˆ’9, c10 = 0 .0523 .

(35) Negligible terms were removed, including the canceling linear damping terms âˆ’c1v + c4v, as well as small-magnitude terms c5, c7, c9, and c10 . This yields the simplified PG-SR equation: 

Ë™v â‰ˆ âˆ’ 0.0954 sin( x) + 0 .0810 x3 âˆ’ 0.5000 xv âˆ’ 0.5000 v3. (36) The simplified PG-SR equation successfully reproduces the dominant nonlinear oscillator dynamics by capturing cubic nonlinearities in both position and velocity, the interaction between position and velocity, and sinusoidal driving. Although the recovered coefficients differ slightly from the ground truth, particularly for the sin( x) and x3 terms, the discovered equation correctly identifies all key functional structures present in the true system. 28 Prior-Guided Symbolic Regression 

H.5. Oscillator2: Equations Discovered by Our Method 

For the oscillator2 dataset, the ground-truth governing equation is given by 

Ë™v = 0 .3 sin( t) âˆ’ 0.5 v3 âˆ’ xv âˆ’ 5.0 x exp(0 .5x). (37) PG-SR initially discovered the following candidate equation: 

Ë™v = A1 sin( t) + A2 cos( t) âˆ’ k1xe 0.5x âˆ’ b1v âˆ’ b2v3

âˆ’ Î±xv âˆ’ Î²v 2x âˆ’ Î´|x|v âˆ’ Î³vx 2 âˆ’ Î³x 2. (38) The optimized parameters are: 

k1 = 5 .00001 , b1 = 0 .0, b2 = 0 .500037 , A1 = 0 .299999 , A2 = 0 .0,Î± = 1 .000045 , Î² = 0 .0, Î´ = 0 .0, Î³ = 0 .0.

After removing negligible terms ( b1, A2, Î², Î´, and Î³), the simplified PG-SR equation becomes 

Ë™v â‰ˆ 0.3 sin( t) âˆ’ 0.5 v3 âˆ’ xv âˆ’ 5.0 x exp(0 .5x). (39) A direct comparison with the ground-truth equation shows that the discovered equation exactly matches the true governing equation. All principal components, including the driving term sin( t), the restoring force xe 0.5x, the cubic damping term v3,and the interaction term xv , are recovered with identical coefficients. These results demonstrate that PG-SR fully achieves scientific discovery by identifying the underlying functional structure governing the system dynamics. 

# I. Computational Resources and Inference Cost 

According to the experimental setup, we use gpt-4o-mini as the backbone model. Each PG-SR experiment is configured with a global budget of 10,000 samples. All experiments were conducted on a local workstation equipped with an Intel Core i9-14900HX CPU and an NVIDIA GeForce RTX 4090 GPU. Under this configuration, a complete run on a standard benchmark (e.g., Oscillator1) takes approximately 10 hours. The total token usage per run is approximately 13.36 million input tokens and 4.49 million output tokens, totaling roughly 17.85 million tokens. Based on standard commercial API pricing, the estimated cost per run is approximately $4.70 USD. Although this approach incurs higher time and monetary costs compared to traditional search-based approaches, it remains highly accessible for academic research without requiring expensive high-performance computing clusters. More importantly, given the substantial gains in scientific consistency and OOD generalization (as shown in Table 1), this cost is negligible. 29