---
title: "Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery"
authors: "Jing Xiao, Xinhai Chen, Jiaming Peng, Qinglin Wang, Menghan Jia, Zhiquan Lai, Guangping Yu, Dongsheng Li, Tiejun Li, Jie Liu"
date: 2026-02-13
pdf: "https://arxiv.org/pdf/2602.13021v2"
tags: ["query:sr"]
score: 10.0
evidence: 先验引导的符号回归用于科学一致性与方程发现
tldr: 符号回归（SR）旨在从数据中发现可解释方程，但现有方法常陷入“伪方程陷阱”，即拟合度高但违背科学原理。本文提出PG-SR框架，通过预热、演化和精炼三阶段流程，引入先验约束检查器和先验退火约束评估（PACE）机制，将领域先验转化为可执行约束。理论证明该方法能降低假设空间复杂度并提高泛化能力，实验表明其在多领域优于现有基准，且对噪声和数据稀缺具有鲁棒性。
motivation: 现有符号回归方法过度依赖经验风险最小化，导致发现的方程虽然拟合数据但往往违背基本的科学原理。
method: 提出PG-SR框架，通过先验约束检查器和先验退火约束评估机制，在三阶段演化过程中显式引入领域先验约束。
result: PG-SR在多个领域均优于最先进的基准方法，并在面对低质量先验、噪声数据和样本稀缺时表现出极强的鲁棒性。
conclusion: 通过将科学先验融入符号回归过程，PG-SR有效解决了伪方程问题，为发现具有科学一致性的物理定律提供了可靠途径。
---

## Abstract
Symbolic Regression (SR) aims to discover interpretable equations from observational data, with the potential to reveal underlying principles behind natural phenomena. However, existing approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Annealing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a guarantee against pseudo-equations. Experimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining robustness to varying prior quality, noisy data, and data scarcity.