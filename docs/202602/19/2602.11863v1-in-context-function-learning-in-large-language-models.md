---
title: In-Context Function Learning in Large Language Models
authors: "Elif Akata, Konstantinos Voudouris, Vincent Fortuin, Eric Schulz"
date: 2026-02-12
pdf: "https://arxiv.org/pdf/2602.11863v1"
tags: ["query:sr"]
score: 6.0
evidence: 使用大语言模型进行上下文函数学习和回归
tldr: 本研究通过高斯过程（GP）的视角探讨了大语言模型（LLM）的上下文学习能力。通过在已知GP先验生成的多元标量函数上进行受控实验，研究发现LLM的学习曲线受核函数影响，并能随着示例增加接近GP回归的最优误差下界。此外，分析揭示了LLM偏向于非平滑核的归纳偏置，并证明了通过强化学习或监督微调可有效调整这些偏置，从而提升模型在特定函数任务上的学习效率。
motivation: 旨在通过高斯过程框架量化分析大语言模型在上下文学习中处理连续函数任务的能力及其内在的归纳偏置。
method: 在已知高斯过程先验生成的多元函数序列上测试模型，并将其预测误差与经验GP回归下界及1-NN上界进行对比分析。
result: LLM的学习曲线随示例数量增加而接近GP最优下界，且模型在预测时表现出对非平滑核函数的天然归纳偏置。
conclusion: 该框架量化了LLM作为GP学习者的表现，并证明了通过后训练手段可以有效引导和优化模型的归纳偏置。
---

## Abstract
Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn from known GP priors. We evaluate prediction error in relation to the number of demonstrations and compare against two principled references: (i) an empirical GP-regression learner that gives a lower bound on achievable error, and (ii) the expected error of a 1-nearest-neighbor (1-NN) rule, which gives a data-driven upper bound. Across model sizes, we find that LLM learning curves are strongly influenced by the function-generating kernels and approach the GP lower bound as the number of demonstrations increases. We then study the inductive biases of these models using a likelihood-based analysis. We find that LLM predictions are most likely under less smooth GP kernels. Finally, we explore whether post-training can shift these inductive biases and improve sample-efficiency on functions sampled from GPs with smoother kernels. We find that both reinforcement learning and supervised fine-tuning can effectively shift inductive biases in the direction of the training data. Together, our framework quantifies the extent to which LLMs behave like GP learners and provides tools for steering their inductive biases for continuous function learning tasks.