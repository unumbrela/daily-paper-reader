---
title: "Chimera: Neuro-Symbolic Attention Primitives for Trustworthy Dataplane Intelligence"
authors: "Rong Fu, Wenxin Zhang, Xiaowen Ma, Kun Liu, Wangyu Wu, Ziyu Kong, Jia Yee Tan, Tailong Luo, Xianda Li, Zeli Su, Youjin Wang, Yongtai Liu, Simon Fong"
date: 2026-02-13
pdf: "https://arxiv.org/pdf/2602.12851v1"
tags: ["query:sr"]
score: 7.0
evidence: 硬件受限推理的神经符号框架
tldr: Chimera是一个旨在可编程数据平面上部署神经符号注意力原语的框架。针对硬件资源受限和行为需可审计的挑战，它结合了核化线性注意力、分层键选择和级联融合机制，在保持神经网络表达能力的同时引入符号化约束。该框架通过硬件感知映射和双时间尺度更新，实现了在商用交换机资源限制下的线速、高保真推理，为构建可信的数据平面智能提供了新方案。
motivation: 在可编程数据平面部署复杂学习模型面临硬件资源严苛限制以及对行为可预测性、可审计性的迫切需求。
method: 提出一种结合核化线性注意力近似、两层键选择层次结构和级联融合机制的神经符号框架，并配合硬件感知映射协议。
result: 实验证明该架构能在商用可编程交换机的资源预算内实现高保真推理，并支持稳定的线速运行。
conclusion: Chimera成功将神经计算与符号约束融合，为在受限网络硬件上实现可信、高效的流量分析奠定了基础。
---

## Abstract
Deploying expressive learning models directly on programmable dataplanes promises line-rate, low-latency traffic analysis but remains hindered by strict hardware constraints and the need for predictable, auditable behavior. Chimera introduces a principled framework that maps attention-oriented neural computations and symbolic constraints onto dataplane primitives, enabling trustworthy inference within the match-action pipeline. Chimera combines a kernelized, linearized attention approximation with a two-layer key-selection hierarchy and a cascade fusion mechanism that enforces hard symbolic guarantees while preserving neural expressivity. The design includes a hardware-aware mapping protocol and a two-timescale update scheme that together permit stable, line-rate operation under realistic dataplane budgets. The paper presents the Chimera architecture, a hardware mapping strategy, and empirical evidence showing that neuro-symbolic attention primitives can achieve high-fidelity inference within the resource envelope of commodity programmable switches.