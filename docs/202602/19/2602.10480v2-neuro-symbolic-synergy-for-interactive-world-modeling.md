---
title: Neuro-Symbolic Synergy for Interactive World Modeling
authors: "Hongyu Zhao, Siyu Zhou, Haolin Yang, Zengyi Qin, Tianyi Zhou"
date: 2026-02-11
pdf: "https://arxiv.org/pdf/2602.10480v2"
tags: ["query:sr"]
score: 6.0
evidence: 用于世界建模和规则一致性的神经符号集成
tldr: "本研究针对大语言模型在世界模型中易产生幻觉及符号模型缺乏语义表达力的问题，提出了神经-符号协同框架（NeSyS）。该框架将LLM的概率语义先验与可执行符号规则相结合，通过符号规则直接约束LLM的输出分布，并在两者间交替训练。实验表明，NeSyS在ScienceWorld等环境中的预测准确率显著提升，且在保持精度的前提下减少了50%的训练数据需求。"
motivation: 旨在解决大语言模型在处理确定性规则时易产生幻觉，以及符号模型在复杂语义环境下表达力不足的局限性。
method: 提出NeSyS框架，利用符号规则动态约束LLM的输出概率分布，并仅在符号规则无法解释的轨迹上对神经模型进行微调。
result: "在ScienceWorld、Webshop和Plancraft三个环境中的实验证明，NeSyS在提升预测准确率的同时，减少了50%的训练数据量。"
conclusion: 神经与符号的协同作用能有效结合逻辑一致性与语义表达力，显著提升交互式世界模型的鲁棒性与数据效率。
---

## Abstract
Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.