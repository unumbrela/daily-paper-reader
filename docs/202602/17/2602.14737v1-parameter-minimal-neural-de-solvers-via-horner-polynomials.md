---
title: Parameter-Minimal Neural DE Solvers via Horner Polynomials
title_zh: 基于霍纳多项式的参数极小化神经微分方程求解器
authors: "T. Matulić, D. Seršić"
date: 2026-02-16
pdf: "https://arxiv.org/pdf/2602.14737v1"
tags: ["keyword:SR", "query:SR"]
score: 8.0
evidence: 通过Horner多项式进行方程发现，以极小参数求解微分方程
tldr: 本研究提出了一种基于霍纳（Horner）多项式的极简参数神经微分方程求解器。该方法通过将假设空间限制在霍纳分解多项式，构建了仅含极少量可学习系数的隐式可微试解，并利用固定低阶项精确满足初始条件。为进一步提升精度，引入了分段样条扩展。实验证明，该模型在仅需数十个参数的情况下，在常微分方程和热传导方程基准测试中优于同规模的MLP和正弦表示模型，实现了极高的资源效率。
motivation: 旨在开发一种参数量极小且能高效求解微分方程的神经架构，以满足资源受限环境下的科学建模需求。
method: 利用霍纳分解多项式构建试解，通过固定低阶系数强制执行初始条件，并引入分段样条技术增强逼近能力。
result: 在多个ODE和热传导方程实验中，仅含数十个参数的霍纳网络在精度上显著超过了同等规模的MLP和正弦基准模型。
conclusion: 霍纳网络在参数效率与求解精度之间取得了优异的平衡，为高效科学计算建模提供了一种实用的方案。
---

## 摘要
我们提出了一种用于求解微分方程的参数极小化神经架构，通过将假设类限制为霍纳分解多项式，产生了一个仅包含少量可学习系数的隐式、可微试解。通过固定低阶多项式的自由度，在构造上精确地强制执行初始条件，因此训练仅专注于匹配配置点处的微分方程残差。为了在不放弃低参数方案的情况下减少逼近误差，我们引入了一种分段（“类样条”）扩展，在子区间上训练多个小型霍纳模型，同时在段边界处强制执行连续性（以及一阶导数连续性）。在说明性的常微分方程（ODE）基准测试和热传导方程示例中，仅具有数十个（或更少）参数的霍纳网络能够准确匹配解及其导数，并在相同的训练设置下优于小型多层感知机（MLP）和正弦表示基准，展示了资源高效型科学建模中实用的准确度-参数权衡。

## Abstract
We propose a parameter-minimal neural architecture for solving differential equations by restricting the hypothesis class to Horner-factorized polynomials, yielding an implicit, differentiable trial solution with only a small set of learnable coefficients. Initial conditions are enforced exactly by construction by fixing the low-order polynomial degrees of freedom, so training focuses solely on matching the differential-equation residual at collocation points. To reduce approximation error without abandoning the low-parameter regime, we introduce a piecewise ("spline-like") extension that trains multiple small Horner models on subintervals while enforcing continuity (and first-derivative continuity) at segment boundaries. On illustrative ODE benchmarks and a heat-equation example, Horner networks with tens (or fewer) parameters accurately match the solution and its derivatives and outperform small MLP and sinusoidal-representation baselines under the same training settings, demonstrating a practical accuracy-parameter trade-off for resource-efficient scientific modeling.

---

## 论文详细总结（自动生成）

这是一份关于论文《Parameter-Minimal Neural DE Solvers via Horner Polynomials》的结构化深入分析总结：

### 1. 核心问题与整体含义（研究动机和背景）
论文的核心问题是如何在**极低参数量**的情况下，实现对微分方程（DE）的高精度数值求解。
*   **背景**：物理信息神经网络（PINNs）等传统方法通常使用多层感知机（MLP）作为函数逼近器。这些模型参数量大（通常数万至数百万），且初始条件（IC）往往通过损失函数中的“软惩罚”项来强制执行，导致训练不稳定且需要复杂的超参数调优。
*   **动机**：受生物系统（如 C. elegans 神经系统）能以极少神经元实现复杂行为的启发，作者提出通过增强结构先验和硬约束来替代参数堆叠，从而开发出一种资源高效、解释性强且易于部署的求解器。

### 2. 方法论：核心思想与关键技术
论文提出了一种基于**霍纳方案（Horner Scheme）**的神经架构，称为 **Horner Network (HN)**。
*   **核心思想**：将解的假设空间限制在霍纳分解形式的多项式中。霍纳形式 $P(t) = a_0 + t(a_1 + t(a_2 + \dots))$ 在数值计算上比标准幂级数更稳定且高效。
*   **硬约束初始条件**：通过数学构造，直接固定多项式的低阶系数。例如，对于 $n$ 阶 ODE，令 $a_i = x^{(i)}(0)$（$i=0 \dots n-1$）。这样模型在初始点处永远精确满足条件，训练过程只需优化高阶系数以最小化微分方程残差。
*   **分段样条扩展（Spline-like Extension）**：为了处理长区间或复杂波动，将定义域划分为多个子区间，每个区间由一个独立的微型 Horner 网络负责，并通过损失函数强制执行段间的一阶或高阶导数连续性。
*   **多维扩展**：通过嵌套 Horner 结构（即系数本身也是另一个变量的 Horner 网络）来处理偏微分方程（PDE）。

### 3. 实验设计
*   **基准问题**：
    *   **Type A**：一阶线性 ODE（指数衰减）。
    *   **Type B**：一阶非线性 ODE。
    *   **Type C**：二阶线性 ODE（阻尼振荡）。
    *   **PDE 案例**：一维热传导方程。
*   **对比方法（Baselines）**：
    1.  **宽 MLP**：5 层 256 宽，使用 Leaky ReLU 激活（约 26 万参数）。
    2.  **窄 MLP**：4 层 5 宽，使用 Sigmoid 激活（106 参数）。
    3.  **SIREN**：4 层 5 宽，使用正弦激活函数（106 参数）。
*   **评估指标**：在 100,000 个密集评估点上计算解及其一阶、二阶导数的均方根误差（RMSE）。

### 4. 资源与算力
*   **训练设置**：使用 Adam 优化器，初始学习率 $10^{-3}$，训练 10,000 个 epoch。
*   **配置点**：ODE 使用 200-400 个配置点，PDE 使用约 5000 个点。
*   **算力说明**：论文**未明确提及**具体的 GPU 型号或训练时长。但由于其参数量极小（仅 10-50 个参数），可以推断其计算开销远低于传统的深度学习模型，适合在嵌入式或低功耗设备上运行。

### 5. 实验数量与充分性
*   **实验规模**：针对 3 种不同性质的 ODE 和 1 种 PDE 进行了详细对比。
*   **充分性**：实验不仅对比了函数值的拟合，还重点对比了**高阶导数**的准确性。这在 DE 求解中至关重要，因为残差涉及导数。
*   **客观性**：作者在相同的训练时长和配置点下对比了不同架构，并展示了参数量与精度的极端对比（10 vs 260,000），证明了结构先验的威力。

### 6. 主要结论与发现
*   **参数效率极高**：Horner 网络仅需 **10 个左右**的可学习参数，其精度却比拥有 **26 万参数**的 Leaky ReLU MLP 高出 2-3 个数量级。
*   **导数拟合更优**：由于多项式的平滑性，Horner 网络在预测解的导数（速度、加速度等）时表现远超基于分段线性激活（ReLU）的网络。
*   **硬约束优势**：通过架构嵌入初始条件，消除了损失函数中不同项之间的权重平衡问题，使收敛更可靠。
*   **分段法的有效性**：样条化扩展在仅微增参数的情况下，显著提升了对复杂解的逼近能力。

### 7. 优点（亮点）
*   **极简主义**：挑战了“深度学习必须深”的固有印象，证明了在科学计算中，正确的数学归纳偏置（Inductive Bias）胜过模型规模。
*   **硬约束设计**：解决了 PINNs 中最头疼的初始/边界条件权重调优问题。
*   **高解释性**：模型的参数直接对应多项式系数，比黑盒 MLP 更易于分析和验证。

### 8. 不足与局限
*   **高维扩展性挑战**：虽然展示了 2D 热传导方程，但对于更高维（如 10D 以上）的 PDE，嵌套 Horner 结构的参数量可能会随维度呈指数增长（维度灾难）。
*   **Runge 现象风险**：尽管有分段扩展，但如果单段多项式阶数过高，仍可能在边界处出现震荡。
*   **复杂几何体适应性**：目前实验主要在规则区间（线段、矩形）上进行，对于复杂几何边界的 PDE 适应性尚未验证。
*   **非线性项处理**：对于极度非线性或具有奇异性的方程，纯多项式基底可能需要极多的分段才能拟合。

（完）
