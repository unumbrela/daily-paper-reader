---
title: "BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations"
title_zh: BEACONS：用于偏微分方程的有界误差、代数可组合神经网络求解器
authors: "Jonathan Gorard, Ammar Hakim, James Juno"
date: 2026-02-16
pdf: "https://arxiv.org/pdf/2602.14853v1"
tags: ["keyword:SR", "query:SR"]
score: 6.0
evidence: 具有形式化验证和物理定律提取能力的偏微分方程神经求解器
tldr: 针对神经网络在偏微分方程（PDE）求解中难以在训练数据外可靠泛化的局限性，本文提出了 BEACONS 框架。该框架结合特征线法预测 PDE 解的解析性质，为浅层神经网络构建严格的 L^inf 误差界限，并通过代数组合构建深层架构以抑制误差。BEACONS 包含自动代码生成和定理证明系统，能为线性及非线性 PDE 提供具有收敛性、稳定性和守恒性保证的可验证解，实现了在推断区域的可靠外推。
motivation: 解决传统神经网络在计算物理中因缺乏形式化验证而无法在训练数据范围外提供可靠 PDE 求解结果的问题。
method: 利用特征线法建立浅层网络的误差界限，并通过代数组合构建深层架构，同时配套自动定理证明系统生成正确性证书。
result: "在平流、Burgers' 及 Euler 方程等 1D/2D 算例中，BEACONS 展现了远超训练数据的可靠外推能力和受控的误差。"
conclusion: BEACONS 证明了通过形式化验证和代数组合，可以构建出比传统 PINN 更具严谨理论保证和外推稳定性的神经 PDE 求解器。
---

## 摘要
神经网络在训练数据凸包之外进行可靠泛化的传统局限性，为计算物理学带来了重大挑战。在计算物理中，人们通常希望在远超实验或解析验证范围的状态下求解偏微分方程（PDE）。在本文中，我们展示了如何通过构建经过形式化验证的 PDE 神经网络求解器来规避这些局限性。这些求解器具有严格的收敛性、稳定性和守恒特性，因此即使在外推状态下，其正确性也能得到保证。通过利用特征线法（method of characteristics）预验预测 PDE 解的解析性质（即使在远离训练域的区域），我们展示了如何为浅层神经网络逼近的最坏情况 L^inf 误差构建严格的外推界限。随后，通过将 PDE 解分解为更简单函数的复合，我们展示了如何基于组合深度学习的思想，将这些浅层神经网络组合成深层架构，从而抑制逼近过程中的巨大 L^inf 误差。由此产生的框架称为 BEACONS（有界误差、代数可组合神经网络求解器），它既包含用于神经网络求解器本身的自动代码生成器，也包含一个用于生成机器可检查正确性证书的定制自动化定理证明系统。我们将该框架应用于各种线性和非线性 PDE，包括线性平流方程、无粘性 Burgers 方程以及完整的一维和二维可压缩 Euler 方程，并阐明了 BEACONS 架构如何能够以可靠且有界的方式将解外推到远超训练数据的范围。文中还讨论了该方法相对于经典物理信息神经网络（PINN）方法的各种优势。

## Abstract
The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.