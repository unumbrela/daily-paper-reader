---
title: Pseudo-differential-enhanced physics-informed neural networks
title_zh: 伪微分增强的物理信息神经网络
authors: Andrew Gracyk
date: 2026-02-16
pdf: "https://arxiv.org/pdf/2602.14663v1"
tags: ["keyword:SR", "query:SR"]
score: 6.0
evidence: 用于科学发现的物理信息神经网络
tldr: 本研究提出了一种伪微分增强的物理信息神经网络（PINN），将传统的梯度增强技术扩展至傅里叶空间。通过在傅里叶空间进行微分运算（即乘以波数），该方法显著提高了训练效率和学习精度。研究证明该方法能改善神经切向核（NTK）的特征值衰减，有效缓解频率偏置问题，且在处理分数阶导数及非欧几里得域的复杂问题时表现出极强的灵活性和鲁棒性。
motivation: 旨在通过在傅里叶空间引入高阶微分增强项，解决传统PINN在训练效率、高频特征捕获及复杂几何域适应性方面的不足。
method: 利用傅里叶变换将PDE残差转化为傅里叶空间中的代数乘法实现伪微分增强，并将其作为增强项加入优化目标函数中。
result: 实验表明该方法在更少的迭代次数下达到了更低的数值误差，并能有效处理稀疏采样点及分数阶微分方程。
conclusion: 伪微分增强为提升PINN的收敛性能和泛化能力提供了一种高效且数学完备的框架，特别适用于高频物理现象的建模。
---

## 摘要
我们提出了伪微分增强的物理信息神经网络 (PINNs)，这是梯度增强在傅里叶空间中的一种扩展。PINNs 的梯度增强要求将 PDE 残差取至比 PDE 规定更高的微分阶数，并将其作为增强项添加到目标函数中，以提高训练效果和整体学习保真度。我们提出在应用傅里叶变换后执行相同的过程，因为在适当的衰减条件下，傅里叶空间中的微分等同于与傅里叶波数相乘。我们的方法快速且高效。我们的方法通常能在更少的训练迭代中实现更优的 PINN 与数值误差对比，可能与少样本配置点良好配合，并能在低配置点设置下偶尔打破训练平台期。此外，我们的方法适用于分数阶导数。我们证明了我们的方法改善了神经切线核 (NTK) 的谱特征值衰减，因此有助于在训练早期学习高频成分，从而减轻多项式阶数甚至在平滑激活函数下更高阶数的频率偏置影响。我们的方法兼容 PINNs 中的先进技术，如傅里叶特征嵌入。针对通过快速傅里叶变换 (FFT) 进行离散傅里叶变换时存在的网格依赖性问题，我们通过蒙特卡洛方法等手段，展示了我们的方法在替代的欧几里得和非欧几里得域上具有更强的网格灵活性和不变性。

## Abstract
We present pseudo-differential enhanced physics-informed neural networks (PINNs), an extension of gradient enhancement but in Fourier space. Gradient enhancement of PINNs dictates that the PDE residual is taken to a higher differential order than prescribed by the PDE, added to the objective as an augmented term in order to improve training and overall learning fidelity. We propose the same procedure after application via Fourier transforms, since differentiating in Fourier space is multiplication with the Fourier wavenumber under suitable decay. Our methods are fast and efficient. Our methods oftentimes achieve superior PINN versus numerical error in fewer training iterations, potentially pair well with few samples in collocation, and can on occasion break plateaus in low collocation settings. Moreover, our methods are suitable for fractional derivatives. We establish that our methods improve spectral eigenvalue decay of the neural tangent kernel (NTK), and so our methods contribute towards the learning of high frequencies in early training, mitigating the effects of frequency bias up to the polynomial order and possibly greater with smooth activations. Our methods accommodate advanced techniques in PINNs, such as Fourier feature embeddings. A pitfall of discrete Fourier transforms via the Fast Fourier Transform (FFT) is mesh subjugation, and so we demonstrate compatibility of our methods for greater mesh flexibility and invariance on alternative Euclidean and non-Euclidean domains via Monte Carlo methods and otherwise.