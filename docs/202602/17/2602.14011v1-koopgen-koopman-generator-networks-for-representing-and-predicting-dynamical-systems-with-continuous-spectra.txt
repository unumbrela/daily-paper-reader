Title: KoopGen: Koopman Generator Networks for Representing and Predicting Dynamical Systems with Continuous Spectra

URL Source: https://arxiv.org/pdf/2602.14011v1

Published Time: Tue, 17 Feb 2026 02:09:20 GMT

Number of Pages: 25

Markdown Content:
# KoopGen: Koopman generator networks for representing and predicting dynamical systems with continuous spectra 

# Liangyu Su 1, Jun Shu 1, Rui Liu 2, Deyu Meng 1, Zongben Xu 1

1School of Mathematics and Statistics and Ministry of Education Key Lab of Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, 710049, Shaanxi, China. 

2School of Mathematics, South China University of Technology, Guangzhou, 510640, Guangdong, China. Contributing authors: suliangyu0917@stu.xjtu.edu.cn; junshu@mail.xjtu.edu.cn; scliurui@scut.edu.cn; dymeng@xjtu.edu.cn; zbxu@xjtu.edu.cn; 

Abstract 

Representing and predicting high-dimensional and spatiotemporally chaotic dynamical systems remains a fundamental challenge in dynamical systems and machine learning. Although data-driven models can achieve accurate short-term forecasts, they often lack stability, interpretability, and scalability in regimes dominated by broadband or continuous spectra. Koopman-based approaches pro-vide a principled linear perspective on nonlinear dynamics, but existing methods rely on restrictive finite-dimensional assumptions or explicit spectral parame-terizations that degrade in high-dimensional settings. Against these issues, we introduce KoopGen, a generator-based neural Koopman framework that mod-els dynamics through a structured, state-dependent representation of Koopman generators. By exploiting the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components, KoopGen separates conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning. Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability, while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations. 

Keywords: Dynamic system, Koopman operator, spatiotemporal chaos, operator learning, spectral theory 

1

> arXiv:2602.14011v1 [cs.LG] 15 Feb 2026

# 1 Introduction 

The rapid growth of sensing and data acquisition technologies has led to an unprece-dented availability of high-resolution dynamical data, spanning turbulent flows and climate systems to neural recordings and engineered infrastructures [1, 2]. While such data encode the intrinsic laws and multiscale interactions governing complex sys-tems, extracting interpretable and predictive dynamical structure from raw time series remains a fundamental challenge, central to scientific discovery, system control, and forecasting across a wide range of disciplines [3, 4]. Many data-driven approaches have been developed to model nonlinear dynamics, each emphasizing different trade-offs between interpretability, scalability, and accu-racy. Traditional numerical methods, including sparse regression (e.g., SINDy [5, 6]) and geometric techniques such as diffusion maps and time-delay embeddings [7, 8], recover explicit equations or low-dimensional structures when certain assumption exist, but struggle for high-dimensional and highly nonlinear systems. In contrast, modern neural network–based models, including reservoir computing and Neural ODEs [9–12], achieve strong short-term prediction but typically lack interpretability and exhibit limited long-term stability. Consequently, few existing approaches jointly provide interpretability, scalability, and robust long-horizon prediction. The Koopman operator framework offers a principled alternative by lifting nonlin-ear dynamics to a linear evolution in an infinite-dimensional function space [13, 14]. This perspective enables spectral analysis of nonlinear systems, yielding interpretable modal structures and improved long-term stability without restrictive sparsity or low-dimensional assumptions [3]. However, the Koopman operator is inherently infinite dimensional. Finite-mode representations are only possible for systems with discrete spectra, whereas generic nonlinear systems are characterized by continuous spectra, implying a continuum of interacting modes. Classical data-driven methods, such as dynamic mode decomposition (DMD), approximate the Koopman operator by projection onto a finite-dimensional subspace [15–17]. Although asymptotically justified, for systems dominated by continuous spec-tra there exist no finite-dimensional invariant subspaces, and in practice only finite and noisy data are available. Consequently, such approximations yield truncated operators that inadequately capture the global spectral structure, thereby limiting accuracy and generalization in complex and high-dimensional systems. To address above limitation, deep learning approaches offer a promising avenue, which have demonstrated the ability to approximate smooth mappings and effectively represent continuously varying spectral structures derived from data [18]. However, most deep Koopman and neural dynamical models [19–25] continue to rely on fixed or explicitly parameterized finite-dimensional operators in a learned observation space. For systems dominated by continuous spectra, this assumption is generically vio-lated, leading to spectral truncation, instability, and performance degradation in chaotic and high-dimensional regimes. The DeepKoopman model proposed by Lusch et al. [26] demonstrated the feasibility of state-dependent operator parameterization, but challenges in scalability, stability, and generalization remain. Here, we introduce Koopman Generator model (KoopGen), a mathematically grounded, physically interpretable, and data-adaptive neural architecture (as shown 2Fig. 1 : Overview of the Koopman Generator model (KoopGen) for repre-senting and predicting dynamical systems with the continuous spectrum. (a) In the data-driven Koopman learning framework, system is first lifted to an obser-vation space, z(t) = Φ( x(t)), where the dynamics are modeled linearly. The classical 

state-independent model is unable to capture the complex behavior of continuous-spectrum systems. KoopGen instead employ a state-dependent formulation, allowing the operator spectrum to vary continuously along trajectories and enabling accurate representation of complex dynamics. (b) The KoopGen model consists of a gating net-work and two operator sets, { bGn}Nn=1 and { eGm}Mm=1 , corresponding to skew-adjoint (conservative) and self-adjoint (dissipative) components, respectively. The gating net-work adaptively weights these generators based on the current latent state z(t), yielding a state-dependent Koopman operator K(z(t)) that advances the dynamics forward in time. (c) Each generator is parameterized in a structure-preserving real block form, ensuring exact enforcement of the skew-adjoint and self-adjoint constraints and encoding physically interpretable dynamical properties. in Fig. 1) for learning dynamical systems with continuous spectra. KoopGen embeds the Cartesian decomposition of the Koopman generator directly into the model design, enabling a principled separation of conservative and dissipative dynamics within a state-dependent operator framework. This structure enhances interpretability and 3long-term stability, while scaling to high-dimensional and spatiotemporally chaotic systems in regimes that are beyond the reach of existing approaches. Most notably, our work introduce the first unified framework capable of accurately representing and predicting systems across a broad spectrum of complexity, from simple nonlinear oscillators to strongly chaotic attractors, and from low-dimensional dynamics to high-dimensional spatiotemporal chaos. Through extensive experiments on four representative continuous-spectrum systems, including a nonlinear pendulum, the low-dimensional chaotic Lorenz-63, the high-dimensional chaotic Lorenz-96 and the spatiotemporally chaotic Kuramoto–Sivashinsky equation, we demonstrate that our approach not only surpasses existing baselines in long-term predictive accuracy and representation quality, but also achieves a substantial gains in scalability, interpretabil-ity, and physical fidelity. These results establish KoopGen as a new operator-theoretic framework for learning complex system, resolving a longstanding limitation that has hindered progress in Koopman theory and application. 

# 2 Results 

# 2.1 Koopman operator, generator and spectral decomposition 

Koopman operator theory provides a linear perspective on nonlinear dynamical sys-tems. For a system ˙x = f (x) with flow map F t and a complex Hilbert space of observables F, the Koopman operator Kt : F → F is defined by 

Ktg = g ◦ F t, g ∈ F . (1) Although the state dynamics F t may be nonlinear, Kt is linear on F. The associated generator G of the Koopman semigroup {K t}t≥0 is defined as 

Gg = lim 

> t→0+

Ktg − gt . (2) By the spectral theorem [27], under some mild conditions, Koopman operator Kt can be decomposed as 

Kt = X

> λ∈σp

etλ Pλ +

Z

> σc

etλ dEc(λ), (3) where Pλ denotes the spectral projection onto the eigenspace for an isolated eigenvalue 

λ ∈ σp and Ec(·) is the projection-valued measure of the continuous spectrum σc.When the Koopman spectrum is purely discrete, each projection Pλ reduces to an expansion in Koopman eigenfunction ϕλ,

Pλg = ⟨g, ϕ λ⟩ϕλ. (4) In this case, observables in the span of eigenfunctions admit the representation 

g = X

> λ∈σp

aλϕλ, Ktg = X

> λ∈σp

aλetλ ϕλ, (5) 4providing a complete and explicit linear characterization of their evolution. However, many physically relevant systems possess a continuous spectrum. In such settings, the Koopman eigenfunction may not exist or may fail to span any informative subspace, making spectral representations substantially more difficult and highlighting the need for alternative learning-based formulations. 

# 2.2 Data-driven Koopman learning 

In practice, any approximation of the Koopman operator implicitly relies on two fundamental components: a observation space Span {φ1, φ 2, · · · , φ d}, where φi is a scalar-valued function, and a finite dimensional representation K of the Koopman operator restricted to this space. Abstractly, this can be expressed as Φ ( F (x(t))) = Kz(t), z(t) = Φ ( x(t)) ∈ Cd, (6) where Φ = ( φ1, φ 2, · · · , φ d) is a vector-valued observation function. Classical methods such as DMD [15] and extended DMD (EDMD) [16] rely on a prescribed dictionary of observables. More recent neural Koopman models, such as LRAN and its variants [20, 21, 23–25], replace this fixed dictionary with learned observation functions. Despite their algorithmic differences, all these methods share the same underlying assumption: the existence of a finite-dimensional subspace that is invariant under the Koopman operator. Such an assumption is consistent with systems whose Koopman spectrum is discrete, for which eigenfunctions provide a nat-ural basis. However, for systems with a significant continuous spectrum, nontrivial finite-dimensional invariant subspaces generally do not exist. This limitation motivates relaxing the fixed Koopman operator K in Eq. 6to a state-dependent operator K(z(t)), allowing the operator spectrum to vary continuously along the trajectories, Φ ( F (x(t))) = K(z(t)) z(t), z(t) = Φ ( x(t)) ∈ Cd. (7) Early efforts of this idea is the DeepKoopman model proposed by Lusch et al. [26], which could interpreted as learning both the Φ and the operator K(z(t)) whose struc-ture is constrained to be diagonal with spectra {λi}ki=1 predicted by an auxiliary neural network. The empirical success of DeepKoopman on typical low-dimensional continuous-spectrum systems, such as the nonlinear pendulum and the Lorenz–63 system (Fig. 2a and Fig. 2b), marks an important breakthrough for Koopman-based learning under continuous spectra. Nevertheless, this success does not extend to high-dimensional dynamics. As shown in Fig. 2c and Fig. 2d, DeepKoopman exhibits pronounced perfor-mance degradation on the Lorenz–96 and Kuramoto–Sivashinsky systems, highlighting fundamental challenges in scaling to high-dimensional chaotic regimes. These limita-tions stem from the explicit parameterization of the spectrum. First, the model does not explicitly encode physical or dynamical structure, which hampers long-term gen-eralization when complex, multiscale interactions dominate the dynamics. Second, the number of predicted spectra grows with system dimensionality, leading to poor 5(a) (b)  

> (c) (d)

Fig. 2 : Prediction results comparison for KoopGen, DeepKoopman and LRAN across different system. The horizontal axis denotes the prediction horizon, and the vertical axis reports the prediction error. LRAN [20] represents Koopman models with a state-independent transfer operator (Eq. 6), whereas DeepKoopman [26] corresponds to the state-of-the-art state-dependent Koopman approach (Eq. 7). The first row shows results for two low-dimensional continuous-spectrum systems, while the second row presents two high-dimensional continuous-spectrum systems. DeepKoopman improves substantially over state-independent methods in low-dimensional settings but degrades markedly in high dimensions, indicating limited scalability. In contrast, KoopGen consistently achieves the lowest prediction errors across all systems, demonstrating superior accuracy, stability, and scalability for continuous-spectrum dynamics. scalability and increased computational burden. Finally, direct spectral modeling is prone to numerical instabilities during training and long-horizon prediction, reducing robustness in strongly chaotic or spatiotemporally complex systems. 62.3 A Koopman generator structure in continuous-spectrum dynamics 

The main goal of this work is to design an interpretable, generalizable and scalable model capable of representing and predicting various dynamical systems with contin-uous spectra. To this end, we introduce the Koopman Generator model (KoopGen), a Koopman operator learning framework that parameterizes infinitesimal generators of dynamics, which encode differential structure of the flow and admit well-defined operator-theoretic decompositions. Modeling the generator, rather than the Koop-man operator itself, enables an interpretable separation of conservative and dissipative mechanisms and provides a structured pathway to construct stable discrete-time oper-ators via exponentiation. We begin by reviewing the Cartesian decomposition of Koopman generators, which underpins the design of our model. 

2.3.1 Cartesian decomposition of Koopman generators 

Since the Koopman operator and its generator are linear operators acting on complex-valued Hilbert spaces, their structure admits a Cartesian decomposition, analogous to separating a complex number into its real and imaginary parts. Specifically, for a bounded Koopman generator G, a unique Cartesian decomposition exists [27] 

G = ˆG + ˜G, ˆG∗ = − ˆG, ˜G∗ = ˜G, (8) where bG is skew-adjoint and eG is self-adjoint. This decomposition is not only mathematically fundamental, but also carries clear dynamical significance. Based on the Stone’s theorem [28], the skew-adjoint generator bG generates a uni-tary Koopman semigroup { bKt}t> 0. Unitary evolution preserves inner products and norms, implying that bG captures the reversible, oscillatory, and conservative aspects of the system dynamics. In particular, such skew-adjoint dynamics do not create or dissipate energy, but rather redistribute it across modes, which is a hallmark of con-servative transport processes. This behavior typically arises from rotational flows and Hamiltonian subsystems, where the total energy is preserved while being transferred among interacting components, and no contraction or expansion occurs. In contrast, the self-adjoint part eG generates a positive semigroup { eKt}t> 0, whose spectral properties are determined by the real spectrum of eG. Negative spectral com-ponents correspond to contraction and dissipation, leading to attractor formation and energy decay, while positive components encode stretching and amplification, as commonly observed in chaotic and hyperbolic dynamics. This decomposition suggests a natural design principle for data-driven Koopman models. Rather than learning a generic generator, explicitly parameterizing its skew-adjoint and self-adjoint components preserve their distinct structural and dynamical roles. Such a structure-preserving parameterization ensures that conservative trans-port and irreversible processes are modeled in a controlled and interpretable manner, while remaining compatible with the underlying operator theory. 72.3.2 KoopGen model 

Guided by the Cartesian decomposition, we introduce the KoopGen model, which pro-vides an interpretable and scalable parameterization of the Koopman generator while explicitly preserving its skew-adjoint and self-adjoint structure. Specifically, we model the generator as a state-dependent convex combination of finite learnable operators. Specifically, the skew-adjoint and self-adjoint parts are parameterized independently as 

bG(z(t)) = 

> N

X

> n=1

bwn(z(t)) bGn,

> N

X

> n=1

bwn = 1 , bwn ≥ 0, (9) 

eG(z(t)) = 

> M

X

> m=1

ewm(z(t)) eGm,

> M

X

> m=1

ewm = 1 , ewm ≥ 0, (10) where { bGn}Nn=1 and { eGm}Mm=1 are sets of learnable skew-adjoint and self-adjoint oper-ators, respectively, and the gating networks bw(·) and ew(·) adaptively weight their contributions based on the state. The above construction is mathematically well grounded. Both the self-adjoint and skew-adjoint operator classes are closed under convex combination, ensuring that the Cartesian structure remains exact. In addition, the formulation effectively decouples the state dimension from the network parameter, thereby guaranteeing its scalability to high-dimensional systems that would be intractable for previous methods. Finally, the learnable Koopman generator is then given by 

G(z(t)) = bG(z(t)) + eG(z(t)) , (11) enabling the model to adaptively blend conservative transport and irreversible dis-sipation. Together, these features unify dimension-independent scalability, structural stability, and interpretable dynamics within a single framework for learning complex systems. Based on the classical Hille–Yosida generation theorem [29], a well-defined gen-erator induces the Koopman operators through the exponential map, providing a principled link between continuous and discrete time dynamics. Accordingly, the discrete-time learnable Koopman operator for time step ∆ t is computed by 

K∆t(z(t)) = exp  ∆t G (z(t)) . (12) In this work, we focus on datasets with uniformly sampled time points. Consequently, the time step ∆ t is a fixed constant, and for notational simplicity, we omit the super-script in what follows, writing K(z(t)) directly. Finally, the learned Koopman operator is used to advance the state forward, 

z(t + 1) = K (z(t)) z(t). (13) 

2.3.3 Structure-preserving real parameterizations of generators 

To ensure exact adherence to skew-adjoint and self-adjoint conditions, each learnable generator is parameterized using its canonical real block representation. In details, a 8learnable skew-adjoint operator is represented in complex form as 

bG = bA + i bB, bA⊤ = − bA, bB⊤ = bB, (14) and realized in real coordinates through 

bG =

" bA − bB

bB bA

#

, bA = 12 (P − P ⊤), bB = 12 (Q + Q⊤), (15) with learnable P, Q ∈ RD×D.Similarly, each learnable self-adjoint takes the complex form 

eG = eA + i eB, eA⊤ = eA, eB⊤ = − eB, (16) with real block representation 

eG =

" eA − eB

eB eA

#

, eA = 12 (U + U ⊤), eB = 12 (V − V ⊤), (17) with learnable U, V ∈ RD×D.This parameterization guarantees exact structural preservation while remaining fully differentiable and efficient, enabling stable training of model that respect both mathematical constraints and physical consistency. 

# 2.4 Numerical Examples 

We evaluate the proposed KoopGen framework on four representative dynamical systems spanning increasing nonlinearity, chaos, and dimensionality. This hierarchy enables a systematic assessment of representation accuracy, scalability, and long-term predictive stability in continuous-spectrum regimes. KoopGen is compared with two representative Koopman-based neural mod-els: LRAN [20], which assumes a state-independent Koopman operator (Eq. 6), and DeepKoopman [26], which allows state-dependent operators via explicit spec-tral parameterization (Eq. 7). As summarized in Fig. 2, DeepKoopman consistently improves upon LRAN in low-dimensional systems, confirming the benefit of relax-ing the fixed-operator assumption when finite-dimensional invariant subspaces do not exist. However, this advantage degrades rapidly with increasing dimensionality: in high-dimensional chaotic systems, DeepKoopman exhibits accelerated error growth and reduced robustness. In contrast, KoopGen achieves uniformly lower prediction errors across all systems, with the performance gap widening at longer horizons. These results demonstrate that KoopGen provides both improved short-term accu-racy and markedly enhanced long-term stability, particularly in high-dimensional continuous-spectrum systems. 9(a) (b) (c) (d) 

Fig. 3 : Learned Koopman eigenfunction and operator structure for the nonlinear pendulum. The eigenfunction exhibits a magnitude aligned with Hamiltonian energy level sets and a smoothly varying phase that parameterizes the periodic motion. The learned Koopman spectra reveal two operators with eigenvalues on the unit circle at angular frequencies ±0.15 π and ±0.06 π, corresponding to counterclockwise and clockwise rotational dynamics. The spatial distribution of operator weights shows clear physical organization: the counterclockwise mode dominates near turning points, while the clockwise mode is activated near equilibrium crossings. Together, these operators provide a continuous, bidirectional decomposition of the pendulum flow, with the gating network adapting smoothly along the trajectory. 

2.4.1 Nonlinear pendulum 

We first consider a nonlinear pendulum, a simple yet canonical continuous-spectrum system, ¨x = − sin( x), ⇒

( ˙x1 = x2,

˙x2 = − sin( x1). (18) Despite its simplicity, the system exhibits energy-dependent frequencies that challenge most neural methods. KoopGen captures this behavior through a compact eigenfunc-tion whose magnitude aligns with Hamiltonian energy level sets and whose phase varies smoothly along each orbit, effectively parameterizing the periodic motion and providing an intrinsic phase coordinate that tracks the dynamical progression along each orbit (Fig. 3a and 3b). Fig. 3d shows the spectrum of learned unitary koopman operators, two distinct operators emerge with spectra located on the unit circle at angular frequencies of approximately ±0.15 π and ±0.06 π. These correspond to rotational dynamics with opposite directions of phase evolution, one counterclockwise and one clockwise, reflect-ing the bidirectional nature of the pendulum’s motion in phase space. The spatial distribution of operator weights in Fig. 3c reveals clear physical meaning. The counterclockwise operator dominates near the turning points, where the pendulum momentarily stops and local phase evolution slows, while the clockwise operator 10 becomes active near the equilibrium crossing, where the angular velocity is highest and the phase advances rapidly. Together, the two operators form a smooth, comple-mentary decomposition: as the pendulum swings back and forth, the gating network continuously shifts attention between them, capturing alternating rotational modes without discontinuity. 

2.4.2 Lorenz-63 system 

We next examine the Lorenz–63 system, a prototypical low-dimensional chaotic flow, 



˙x = σ(y − x),

˙y = x(ρ − z) − y, 

˙z = xy − βz, 

(19) with σ = 10 , β = 8 /3, and ρ = 28, corresponding to the original system stud-ied by Lorenz. The system arises from a truncated Galerkin approximation of the Navier–Stokes equations for thermal fluid motion, where the variables x, y, z can be interpreted as the convective intensity, horizontal temperature variation, and vertical temperature variation, respectively. As shown in Fig. 2b and Fig B2 (depicted in Appendix B), KoopGen achieves substantially improved prediction accuracy compared with existing methods, particu-larly at longer prediction horizons. This stems from KoopGen’s ability to capture the intrinsic structure of the Lorenz dynamics. The three learned eigenfunctions provide a compact and interpretable decomposi-tion of the Lorenz attractor. The first eigenfunction shows a smooth phase progression from −π to π together with a monotonic amplitude increase along each spiral arm. This structure reflects the cyclic motion around the two wings and the gradual growth of stretching energy as trajectories move away from the core of each wing. Large ampli-tudes appear near the inter-wing transition channel, consistent with the higher energy required for wing switching. The second eigenfunction exhibits a symmetric ampli-tude pattern across the attractor and only weak phase variation concentrated around zero. Rather than encoding rotational behavior, this mode primarily distinguishes the two wings through a sign change, reflecting the intrinsic left–right symmetry of the attractor. Its behavior is consistent with the global partitioning of the state space into two metastable regions. The third eigenfunction shows the opposite pattern, mini-mal amplitude modulation but a clear and continuous 2 π phase winding on attractor. Crucially, its phase remains continuous even across the wing-switching region, where trajectories undergo the rapid divergence that drives chaotic behavior. This mode therefore provides a coherent phase coordinate that tracks the rotational progres-sion around each equilibrium and seamlessly bridges transitions between the lobes. Such continuous phase evolution across switching is a hallmark of the Lorenz flow’s stretching-and-folding mechanism, and its accurate reconstruction demonstrates that the learned Koopman representation captures the core dynamical structure underlying Lorenz chaos. Together, the three modes separate energy growth, lobe identification, and global rotational progression, showing that the model reconstructs not only the geometry but 11 Fig. 4 : Illustration of magnitude and phase of the KoopGen eigenfunctions for the Lorenz-63 system. Learned Koopman eigenfunctions of the Lorenz attractor. The three eigenfunctions yield a compact and interpretable decomposition of the Lorenz dynam-ics. The first eigenfunction encodes the cyclic motion around each wing through a smooth phase progression and amplitude growth along the spiral arms, with large amplitudes concentrated near the inter-wing transition channel. The second eigen-function captures the left–right symmetry of the attractor, exhibiting weak phase variation but a sign change that separates the two metastable lobes. The third eigen-function provides a coherent phase coordinate with a continuous 2 π winding across the attractor, remaining smooth even through wing-switching events. Together, the learned eigenfunctions recover the core stretching–folding mechanism of the Lorenz flow and provide an interpretable spectral representation of its chaotic dynamics. 12 (a) (b) 

Fig. 5 : Comparison of prediction errors for KoopGen, DeepKoopman and LRAN on the Lorenz–96 system. These results illustrate that KoopGen maintains low and spatially coherent errors, whereas others exhibit faster error accumulation and loss of dynamical structure at longer times. also the essential switching and instability structures that define the chaotic dynamics of the Lorenz system. By capturing these global intrinsic dynamical structures, our approach achieves a faithful linearization of the flow in an appropriate observable space. This structural fidelity is precisely what enables stable long-term prediction in a system as shown in Fig. 2. 

2.4.3 Lorenz-96 system 

To assess scalability, we consider the Lorenz–96 system with K = 36 variables, a standard high-dimensional chaotic benchmark. 

dx i

dt = ( xi+1 − xi−2) xi−1 − xi + F, i = 0 , . . . , K − 1, (20) with cyclic indexing imposed through xi+K = xi. We take K = 36 variables and the classical forcing parameter F = 8, a regime known to produce sustained high-dimensional chaotic dynamics. Each variable xi represents a scalar quantity (such as temperature or vorticity) distributed along a periodic latitude circle, and the coupling structure models advection, damping, and external forcing. Representative examples of ground-truth trajectories and corresponding prediction errors are shown in Fig. 5, with additional examples provided in Appendix B. These 13 (a) (b) 

Fig. 6 : Comparison of prediction errors for KoopGen, DeepKoopman and LRAN on the Kuramoto–Sivashinsky equation. While all models achieve accurate short-term predictions, KoopGen significantly suppresses long-term error growth and preserves the underlying spatiotemporal patterns, in contrast to the rapid degradation observed for DeepKoopman and LRAN. examples show that both the KoopGen and the DeepKoopman make quantitatively accurate short term predictions. While the predictions after 20 steps, KoopGen out-performs others significantly as shown in Fig. 2c, demonstrating that KoopGen has a stronger representation ability and a more stable long-term prediction capability for complex high dimensional systems. 

2.4.4 Kuramoto-Sivashinsky system 

Finally, we test KoopGen on the Kuramoto–Sivashinsky equation, a canonical model of spatiotemporal chaos, 

ut + uxx + uxxxx + uu x = 0 , x ∈ [0 , L ], (21) on a domain L = 8 π, which corresponds to a regime just beyond the onset of chaos. This system presents a particularly stringent test due to the coupling of temporal chaos with broadband spatial interactions. Some example of ground truth and corresponding predicted error on the test data sequences are shown in Fig. 6 with additional examples provided in the Appendix B. All methods achieve quantitatively accurate predictions over short horizons, indicating 14 comparable expressive power in capturing the local dynamics. However, as the pre-diction horizon increases, their behaviors diverge markedly. Beyond approximately 30 time steps, errors in the DeepKoopman and LRAN model grow rapidly and accumulate across space, leading to visible distortion of the spatiotemporal patterns. In contrast, KoopGen maintains coherent structures and substantially lower error growth, yielding predictions that remain faithful to the underlying dynamics over extended horizons. This pronounced gap highlights the improved generalization and long-term stability of KoopGen for spatiotemporal systems. 

# 3 Conclusion 

We have introduced a mathematically grounded and physically interpretable neural architecture that embeds the Cartesian decomposition directly into the structure of Koopman generators, enabling a scalable and adaptive representation of dynamical systems with continuous spectra. The proposed framework extends Koopman-based learning beyond fixed or finite-dimensional operator assumptions and naturally scales to high-dimensional and spatiotemporally chaotic regimes that have traditionally been inaccessible to spectral operator methods. Across all evaluated systems, the model achieves stable long-horizon predic-tions while preserving operator-theoretic consistency. Beyond predictive accuracy, the framework recovers intrinsic dynamical structure in an interpretable manner. In the Lorenz system, for instance, learned eigenfunctions faithfully reconstruct the stretching-folding geometry and wing switching dynamic of the attractor underpin chaotic behavior. This level of structural fidelity distinguishes the proposed approach from prior data-driven models and helps explain its robustness in long-term forecast-ing: it captures the mechanisms that generate chaos, rather than merely tracking short-term trajectories. Despite these advances, several challenges remain. From a modeling perspective, further improvements in computational efficiency and parameter optimization are required to extend the framework to even higher-dimensional systems and longer pre-diction horizons. From a theoretical standpoint, a more comprehensive analysis of approximation error, stability guarantees, and generalization in the presence of finite data and noise remains an open problem. In addition, while the present study demon-strates strong performance on canonical benchmark systems, the effectiveness of the proposed approach on real-world data will be left for future study. Overall, this work establishes a principled foundation for data-driven Koopman operator learning in continuous-spectrum systems and opens new avenues for combin-ing operator-theoretic structure with modern neural architectures. We anticipate that continued theoretical developments and real-world validations will further broaden its applicability and deepen its impact across scientific and engineering domains. 15 4 Methods 

# 4.1 Generating the datasets 

We generated datasets by numerically integrating the governing differential equations in Python. For each dynamical system, 20,000 trajectories were obtained from ran-domly sampled initial conditions. The data were partitioned into training (70%), validation (10%), and testing (20%) sets. The pendulum dataset is created from random initial conditions x, where x1 ∈

[−3.1, 3.1] (just under [ −π, π ]), x2 ∈ [−2, 2], and the potential function is under 0.99 as same as the DeepKoopman [26]. The potential function for the pendulum is 12 x2 −

cos( x1). These ranges are chosen to sample the pendulum in the full phase space where the pendulum approaches having an infinite period. The Lorenz-63 dataset is created from random initial conditions x = ( x1, x 2, x 3), where x1 ∈ [−18 , 18], x2 ∈ [−20 , 20], and x3 ∈ [0 , 50]. To ensure trajectories lie on the attractor, each trajectory is initialized from a uniformly sampled state within the above ranges and integrated forward for a burn-in time of 10.0 units (with 1000 intermediate steps) whose states are discarded. The final state of the burn-in is then used as the effective initial condition. From each such state, trajectories are simulated over t ∈ [0 , 4] with step size ∆ t = 0 .01, giving 401 snapshots per trajectory. The Lorenz-96 dataset is created from random initial conditions independently and uniformly from the interval [ −5, 5] K and we take K = 36 variables and the classical forcing parameter F = 8, a regime known to produce sustained high-dimensional chaotic dynamics. Each initial condition undergoes a burn-in integration of 10 time units to ensure that trajectories settle onto the attractor. We then simulate the system over the time interval t ∈ [0 , 4] with a sampling step of ∆ t = 0 .01, producing 401 snapshots per trajectory. The KS dataset is created by simulating the PDE on a periodic spatial domain 

x ∈ [0 , L ] with length L = 8 π. The solution is discretized with N = 128 equispaced grid points, and advanced in time using an ETDRK4 [30] scheme with step size ∆ t = 1 .0. In spectral space, the linear operator is given by ˆL(k) = k2 − k4, and nonlinear terms are computed via FFTs with 2/3 dealiasing to avoid aliasing errors. Initial conditions are constructed in Fourier space by exciting a small number of low-frequency modes (e.g. m = 1 , 2, 3) with random complex amplitudes and their conjugates, ensuring real-valued solutions. Each trajectory undergoes a spin-up phase of 20 steps, during which the transient dynamics are discarded to ensure the system reaches its statistically steady regime. 

# 4.2 Loss function 

Our training objective consists of two terms that jointly enforce accurate state-space prediction and consistency of the latent linear dynamics. Let x(t) denote the ground-truth trajectory and ˆ x(t) the corresponding model prediction. The encoded Koopman eigenfunction is given by z(t) = Φ  x(t), and the latent state evolves according to the learned linear update ˆz(t) = K(ˆ z(t − 1)) ˆ z(t − 1) , ˆz(0) = z(0) . (22) 16 To robustly capture dynamical structures, we measure errors using Sobolev norms [31] rather than standard ℓp norms. Unlike pointwise losses, Sobolev norms penalize dis-crepancies not only in amplitude but also in temporal derivatives, thereby aligning the learning objective with the smoothness and spectral characteristics of dynamical trajectories. The overall loss function is defined as 

L = x(t) − ˆx(t) k,p + α z(t) − ˆz(t) k,p , (23) where α > 0 balances the reconstruction and linear latent dynamics consistency terms that listed in Table 2, and ∥ · ∥ k,p denotes the Sobolev norm 

∥u(t)∥k,p =

> k

X

> i=0

u(i)(t) pp

!1/p 

, (24) with u(i)(t) denoting the i-th temporal derivative of u(t). In this work, we adopt the case p = 2, for which the Sobolev norm [31] admits a convenient Fourier-domain representation, 

∥u(t)∥2 

> k, 2

=

> ∞

X

> ξ=−∞

 1 + ξ2 + · · · + ξ2k ˆu(ξ) 2, (25) where ˆ u(ξ) is the Fourier transform of u(t). 

# 4.3 Training and testing 

All models were implemented in PyTorch [32] and trained on an NVIDIA RTX 3090 GPU. Network architecture parameters are shown in Table 1, the hyperparameter of the autoencoders, training epochs and batch size for all models are the same. During training, each trajectory x(t) is uniformly sampled at 30 equally spaced time instances and during the test, start from a randomly selected initial point set and predict 50 steps forward. Network parameters were initialized using the default PyTorch initialization scheme and optimized with the AdamW optimizer [33]. We adopted a partitioned opti-mization strategy in which the main network, gating network, and generator modules were assigned separate learning rates and training hyperparameters, as summarized in Table 2. Learning rates were scheduled using a OneCycle policy [34] with a warm-up ratio of 0.1. 

# References 

[1] Vinuesa, R., Brunton, S.L.: Enhancing computational fluid dynamics with machine learning. Nature Computational Science 2(6), 358–366 (2022) [2] Durstewitz, D., Koppe, G., Thurm, M.I.: Reconstructing computational sys-tem dynamics from neural data with recurrent neural networks. Nature Reviews Neuroscience 24 (11), 693–710 (2023) 17 Table 1 : Network architecture for different dynamical systems                                       

> Component Pendulum Lorenz-63 Lorenz-96 KS equation
> Main network depth 3444Main network width 32 128 256 1024 Latent dimension 2664 256 Gating network depth 2222Gating network width 32 128 256 1024 Number of bG2664 32 Number of eG02816 Step ∆ t0.1 0.1 0.1 0.1

Table 2 : Optimization hyperparameters                              

> Hyperparameter Pendulum Lorenz–63 Lorenz–96 KS
> LR (Main Net) 0.005 0.005 0.005 0.001 LR (Gate Net) 0.001 0.001 0.001 0.001 LR (Generators) 0.005 0.005 0.005 0.001 Loss weight α0.1 1.0 0.1 1.0 Training epochs 100 200 500 500 Batch Size 128 128 128 128

[3] Brunton, S.L., Budiˇ si´ c, M., Kaiser, E., Kutz, J.N.: Modern koopman theory for dynamical systems. SIAM Review (2022) [4] Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S., Van Katwyk, P., Deac, A., et al. : Scientific discovery in the age of artificial intelligence. Nature 620 (7972), 47–60 (2023) [5] Brunton, S.L., Proctor, J.L., Kutz, J.N.: Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences 113 (15), 3932–3937 (2016) [6] Champion, K., Lusch, B., Kutz, J.N., Brunton, S.L.: Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences 116 (45), 22445–22451 (2019) [7] Giannakis, D., Majda, A.J.: Nonlinear laplacian spectral analysis for time series with intermittency and low-frequency variability. Proceedings of the National Academy of Sciences 109 (7), 2222–2227 (2012) [8] Berry, T., Cressman, J.R., Greguri´ c-Ferenˇ cek, Z., Sauer, T.: Time-scale separation from diffusion-mapped delay coordinates. SIAM Journal on Applied Dynamical Systems 12 (2), 618–649 (2013) [9] Pathak, J., Hunt, B., Girvan, M., Lu, Z., Ott, E.: Model-free prediction of large 18 spatiotemporally chaotic systems from data: A reservoir computing approach. Physical review letters 120 (2), 024102 (2018) [10] Lukoˇ seviˇ cius, M., Jaeger, H.: Reservoir computing approaches to recurrent neural network training. Computer science review 3(3), 127–149 (2009) [11] Chen, R.T., Rubanova, Y., Bettencourt, J., Duvenaud, D.K.: Neural ordinary differential equations. Advances in neural information processing systems 31 

(2018) [12] Rubanova, Y., Chen, R.T., Duvenaud, D.K.: Latent ordinary differential equations for irregularly-sampled time series. Advances in neural information processing systems 32 (2019) [13] Koopman, B.O.: Hamiltonian systems and transformation in hilbert space. Proceedings of the National Academy of Sciences 17 (5), 315–318 (1931) [14] Koopman, B.O., Neumann, J.v.: Dynamical systems of continuous spectra. Proceedings of the National Academy of Sciences 18 (3), 255–263 (1932) [15] Schmid, P.J.: Dynamic mode decomposition of numerical and experimental data. Journal of fluid mechanics 656 , 5–28 (2010) [16] Williams, M.O., Kevrekidis, I.G., Rowley, C.W.: A data–driven approximation of the koopman operator: Extending dynamic mode decomposition. Journal of Nonlinear Science 25 (6), 1307–1346 (2015) [17] Rosenfeld, J.A., Kamalapurkar, R., Gruss, L.F., Johnson, T.T.: Dynamic mode decomposition for continuous time systems with the liouville operator. Journal of Nonlinear Science 32 (1), 5 (2022) [18] Brunton, S.L., Kutz, J.N.: Promising directions of machine learning for partial differential equations. Nature Computational Science 4(7), 483–494 (2024) [19] Takeishi, N., Kawahara, Y., Yairi, T.: Learning koopman invariant subspaces for dynamic mode decomposition. Advances in neural information processing systems 

30 (2017) [20] Otto, S.E., Rowley, C.W.: Linearly recurrent autoencoder networks for learning dynamics. SIAM Journal on Applied Dynamical Systems 18 (1), 558–593 (2019) [21] Yeung, E., Kundu, S., Hodas, N.: Learning deep neural network representa-tions for koopman operators of nonlinear dynamical systems. In: 2019 American Control Conference (ACC), pp. 4832–4839 (2019). IEEE [22] Eivazi, H., Guastoni, L., Schlatter, P., Azizpour, H., Vinuesa, R.: Recurrent neural networks and koopman-based frameworks for temporal predictions in a low-order model of turbulence. International Journal of Heat and Fluid Flow 90 , 108816 19 (2021) [23] Azencot, O., Erichson, N.B., Lin, V., Mahoney, M.: Forecasting sequential data using consistent koopman autoencoders. In: International Conference on Machine Learning, pp. 475–485 (2020). PMLR [24] Alford-Lago, D.J., Curtis, C.W., Ihler, A.T., Issan, O.: Deep learning enhanced dynamic mode decomposition. Chaos: An Interdisciplinary Journal of Nonlinear Science 32 (3) (2022) [25] Nayak, I., Chakrabarti, A., Kumar, M., Teixeira, F.L., Goswami, D.: Temporally-consistent koopman autoencoders for forecasting dynamical systems. Scientific Reports 15 (1), 22127 (2025) [26] Lusch, B., Kutz, J.N., Brunton, S.L.: Deep learning for universal linear embed-dings of nonlinear dynamics. Nature communications 9(1), 4950 (2018) [27] Van Neerven, J.: Functional Analysis vol. 201. Cambridge University Press, Cambridge (2022) [28] Stone, M.H.: On one-parameter unitary groups in hilbert space. Annals of Mathematics 33 (3), 643–648 (1932) [29] Engel, K.-J., Nagel, R.: One-parameter Semigroups for Linear Evolution Equations vol. 194. Springer, New York (2000) [30] Kassam, A.-K., Trefethen, L.N.: Fourth-order time-stepping for stiff pdes. SIAM Journal on Scientific Computing 26 (4), 1214–1233 (2005) [31] Adams, R.A., Fournier, J.J.: Sobolev Spaces vol. 140. Elsevier, Oxford (2003) [32] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019) [33] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: Interna-tional Conference on Learning Representations (2019) [34] Smith, L.N., Topin, N.: Super-convergence: Very fast training of neural networks using large learning rates. In: Artificial Intelligence and Machine Learning for Multi-domain Operations Applications, vol. 11006, pp. 369–386 (2019). SPIE 20 Table A1 : Hyperparameter settings for DeepKoopman                      

> Hyperparameter Pendulum Lorenz-63 Lorenz-96 KS
> Learning rate 0.001 0.001 0.001 0.0001 Num complex eigenvalues 1332 512 Num real eigenvalues 0000Loss weight α0.1 1.0 0.1 1.0

Table A2 : Hyperparameter settings for LRAN              

> Hyperparameter Pendulum Lorenz-63 Lorenz-96 KS
> Learning rate 0.001 0.001 0.001 0.001 Loss weight α0.1 1.0 0.1 0.1

# Appendix A Experiment settings 

The hyperparameters used for DeepKoopman and LRAN are summarized in Tables A1 and A2, respectively. Unless otherwise specified, all remaining architectural and opti-mization settings are identical to those used for KoopGen and are reported in Tables 1 and 2. 

# Appendix B More results 

This section provides additional prediction results for all methods and systems con-sidered in the main text, serving as a supplementary and more detailed illustration of their behaviors under a wider range of initial conditions. Overall, the trends observed here are consistent with those reported in the main text. All error plots report the absolute prediction error, |ˆx − x|.Fig. B1 and Fig. B2 present the prediction error for nonlinear Pendulum and Lorenz-63 system, respectively. For each system, we evaluate 50-step forward pre-dictions from 4,000 randomly sampled initial conditions. For these low-dimensional continuous-spectrum systems, state-dependent Koopman formulations yield markedly improved predictions compared with state-independent operators, highlighting the importance of adaptive transfer operators. In particular, KoopGen produces error distributions that remain relatively uniform across the attractor, indicating stable pre-dictive behavior over a broad range of states. As shown in Fig. B2, DeepKoopman produces pronounced error amplification near the outer regions of the attractor’s two lobes, where trajectories exhibit strong sensitivity to initial conditions. These local-ized error bursts indicate limited robustness in capturing the global geometry and instability structure of chaotic dynamics. Fig. B3 and Fig. B4 further present prediction error distributions for the high-dimensional Lorenz–96 system and the Kuramoto–Sivashinsky equation. These 21 Fig. B1 : Comparison of prediction errors for KoopGen, DeepKoopman and LRAN on Pendulum systems. systems pose substantially greater challenges due to their high dimensionality, broad-band spectra, and strong spatiotemporal chaos. KoopGen maintains comparatively low prediction errors and exhibits more coherent long-term evolution over multiple prediction steps. This behavior suggests that the generator-based parameterization enables the model to better accommodate the complex interplay between Existing deep Koopman and neural and dissipation that governs high-dimensional chaotic dynamics. Taken together, these supplementary results provide a more detailed view of the predictive characteristics of KoopGen across a diverse set of dynamical regimes. They further illustrate how incorporating structural information at the network architecture can support stable and scalable learning of continuous-spectrum systems. 22 Fig. B2 : Comparison of prediction errors for KoopGen, DeepKoopman and LRAN on Lorenz-63 systems. 23 (a) (b)  

> (c) (d)

Fig. B3 : Comparison of prediction errors for KoopGen, DeepKoopman and LRAN on Lorenz-96 system. 24 (a) (b)  

> (c) (d)

Fig. B4 : Comparison of prediction errors for KoopGen, DeepKoopman and LRAN on KS equation. 25