Title: BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations

URL Source: https://arxiv.org/pdf/2602.14853v1

Published Time: Tue, 17 Feb 2026 03:20:50 GMT

Number of Pages: 31

Markdown Content:
# BEACONS: B OUNDED -E RROR , A LGEBRAICALLY -C OMPOSABLE 

# NEURAL SOLVERS FOR PARTIAL DIFFERENTIAL EQUATIONS 

Jonathan Gorard 

Princeton University Princeton, NJ 08544, USA 

gorard@princeton.edu 

Ammar Hakim, James Juno 

Princeton Plasma Physics Laboratory Princeton, NJ 08540, USA 

{ahakim, jjuno}@pppl.gov 

# ABSTRACT 

The traditional limitations of neural network architectures in reliably generalizing substantively beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve partial differential equations (PDEs) in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we demonstrate how it is possible to circumvent these limitations by constructing formally-verified neural network architectures for the solution of PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics 

to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L∞ errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural network approximations together to form deep architectures, building on ideas from 

compositional deep learning , in which the large L∞ errors in the approximations of discontinuous functions have been suppressed by composing them with smoother ones (thus generalizing the theory of flux limiters in traditional numerical solvers). The resulting framework, called BEACONS 

(Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness . We apply the BEACONS framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers’ equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way, as compared to conventional neural network architectures of equivalent size. Various advantages of the BEACONS approach over the classical PINN (Physics-Informed Neural Network) approach are discussed. 

# 1 Introduction 

Conventional machine learning wisdom suggests that neural networks (NNs) are highly effective tools for interpolating 

between training data within a given domain or statistical distribution, but frequently struggle at extrapolating to Out-of-Domain or Out-of-Distribution (OOD) cases[ 10 ][ 2 ]. This is often formulated as the statement that neural networks perform well on cases that lie near or within the convex hull of their training sets, but fail when tested on cases that lie far outside it. For example, most analyses of physics-informed neural network (PINN) architectures have focused on evaluating their ability to interpolate partial differential equation solutions within the temporal ranges on which they have been trained[ 37 ], and have shown that the predictions of PINN architectures tend to deviate significantly from the ground truth solution when made to extrapolate beyond such ranges[ 24 ][ 45 ]. Modern foundation model 

(FM) architectures effectively circumvent this conventional extrapolatory limitation by pretraining on a sufficiently large and diverse corpus of data that the convex hull of the pretraining set becomes arbitrarily expansive and high-dimensional[ 3 ], in particular such that a very wide range of seemingly unrelated tasks lie strictly within (or otherwise near) it, and therefore the model is able to exhibit an impressive capacity for generalization even by “mere interpolation” 

> arXiv:2602.14853v1 [cs.LG] 16 Feb 2026

of the pretraining set. In some trivial sense, accurate extrapolation far outside the convex hull of the training set is a fundamental mathematical impossibility: for any given subdomain, there exist infinitely many functions whose values all agree on that subdomain (but which may differ arbitrarily outside of it), and there is no guarantee that a neural network which has been trained only on that subdomain will fit the “correct” (or even “approximately correct”) one. In fact, this is deeply related to a variety of instabilities that occur in classical numerical interpolation, such as the Gibbs and Runge phenomena in polynomial and Fourier interpolation, and more broadly to the idea that an interpolation which is accurate over a fixed interval may diverge arbitrarily rapidly outside of it. We will return to this theme later in the paper. It has been proposed that deep neural network architectures exhibit an implicit bias towards fitting functions with low algorithmic/Kolmogorov complexity (i.e. the Occam’s razor hypothesis [33 ]), which may help to restrict the search space of possible function extrapolations somewhat. However, even if the Occam’s razor hypothesis were a strict mathematical theorem or provable guarantee, there still exist many possible and relevant criteria for “correctness” of a function extrapolation beyond minimization of algorithmic complexity. In traditional computational physics, the situation is rather different. Large, complex simulation codes are frequently pushed into regimes well beyond anything that can be validated against either experiments or analytical theory, and in many cases the results are nevertheless deemed trustworthy. In certain cases, this degree of trustworthiness can be even explicitly quantified. This is in large part due to a range of foundational results in classical numerical analysis, which make it possible to construct finite volume, finite element, finite difference, discontinuous Galerkin, etc. schemes with rigorous mathematical and physical correctness properties, such as enforcement of conservation laws to machine precision[ 46 ], or guaranteed L2 stability[ 7 ]. Indeed, in previous work[ 20 ], the authors demonstrated that it was possible to construct a bespoke automated theorem-proving framework which, when coupled with an automatic code-generator, was capable of synthesizing fully formally-verified numerical solvers for hyperbolic partial differential equations, with machine-checkable certificates of correctness (covering properties such as thermodynamic consistency, strict hyperbolicity preservation, and flux continuity) up to machine precision. One of the principal contentions of the present paper is that neural networks, properly conceived, simply represent another class of fundamental numerical method, and can therefore be rigorously analyzed using essentially the same mathematical techniques. More accurately, neural networks represent a vast generalization of all classical numerical methods: all numerical methods ultimately work by approximating an infinite-dimensional function space by a finite-dimensional subspace, yet only in neural networks are the bases for these finite-dimensional subspaces treated as fully dynamic, flexible, and adaptive to the specific problem. By this token, if we regard neural networks as a generalization of classical numerical techniques, we intend to address the following question: could we imagine designing formally-verified neural network architectures with rigorous robustness, convergence, stability, and correctness properties, just as we have demonstrated previously in the classical case? To this end, this paper introduces the framework of BEACONS : Bounded-Error, Algebraically-COmposable Neural Solvers for partial differential equations (PDEs). The “bounded-error” aspect has its origins in some of the classical theorems of Mhaskar[ 31 ][ 29 ], Pinkus[ 36 ], and others on the approximation of Cn-smooth functions by simple, feed-forward neural networks (especially multi-layer perceptrons with a single hidden layer) with smooth activation functions, in which rigorous bounds on the L∞ errors of such approximations can be proven, dependent only upon the dimensionality of the function, the number of continuous derivatives, and the width of the neural network. Here, we focus specifically on the case of neural network approximations for solutions to hyperbolic PDEs. Unlike in the case of arbitrary function interpolation, in this case the number of continuous derivatives of the function can be known apriori , at every point in the domain (even at points arbitrarily far removed from the subdomain on which the neural network has been trained), by virtue of the method of characteristics [ 40 ]. Crucially, when combined with an appropriate choice of PDE residual-based loss function, this technique enables one to prove fully extrapolatory error bounds on the neural network approximation, rather than merely the conventional interpolatory ones. For shallow neural network architectures (with only a single hidden layer), such bounds may nevertheless be infeasibly large for any reasonable number of neurons, which in turn motivates the “algebraically-composable” aspect of the BEACONS framework. Building upon intuitions from applied category theory[ 15 ][ 19 ] and compositional deep learning[ 16 ][ 17 ], the idea behind algebraic composability is to assemble deeper, more expressive neural network architectures by composing together shallower, more tractable ones[ 18 ], in such a way that the error bounds still remain tightly controlled. In particular, we demonstrate how it is possible to improve an otherwise unfavorable L∞ bound on the neural network approximation of a highly discontinuous function (e.g. a shock wave, in the context of hyperbolic PDEs) by composing it with several favorable neural network approximations of smooth, slowly-varying functions. Such an approach effectively generalizes the use of flux limiters [27 ] and total variation diminishing schemes[ 22 ] in conventional finite volume numerical methods. 2At this point, it is worth stressing several crucial distinctions between the BEACONS approach and the traditional PINN approach 1. In a traditional PINN architecture, the latent space of the neural network is heavily constrained (for instance through specialized choices of loss functions, penalty functions, and so on) to prevent the neural network from entering an “invalid” region of the latent space that violates certain physical correctness properties, such as conservation laws, thermodynamic principles, etc. This approach works well for simple toy problems (especially ones involving linear or almost-linear differential operators), where the latent space structure is convex and/or geometrically simple. However, for more sophisticated problems involving complex, tightly coupled, highly non-linear systems of PDEs, the latent space structure becomes highly non-convex and geometrically complicated, and one quickly reaches the situation where the shortest paths between two “valid” regions of latent space (and therefore the paths preferentially traversed by a gradient descent algorithm) may be forced to intersect an otherwise “invalid” region. For example, the shortest path between two regions of latent space corresponding to thermodynamically-consistent solutions (with convex entropy functionals) may traverse through a “ravine” consisting of solutions in which the entropy functional is non-convex, and therefore in which the laws of thermodynamics are violated. In such a scenario, a traditional PINN architecture will simply fail to converge, or will converge to a physically incorrect but “valid” result on the wrong side of the “ravine”. On the other hand, in the BEACONS approach, the latent space is entirely unconstrained.: the provable error bounds on the approximations of PDE solutions arise from the compositional structure of the architecture itself, rather than through any a priori restriction of the training process. During the gradient descent process for a BEACONS architecture, the neural network is fully permitted to traverse physically “invalid” regions of the latent space (e.g. corresponding to entropy-violating solutions, non-energy-conserving solutions, etc.) as necessary, without penalty, since there is nevertheless a provable guarantee by virtue of its mathematical structure that the architecture will 

eventually converge to within some bounded distance of a “valid” region, in the limit of infinite epochs (and infinite training data). Formal certifications for a posteriori error bounds on PINN inferences have previously been studied by Ernst et al.[12], Eiras et al.[11], and others. The BEACONS software framework itself comprises: a Racket-based[ 13 ] domain-specific language (DSL) for specify-ing both hyperbolic PDE systems and compositional neural network architectures for approximating them; an automated theorem-proving framework that is able to prove both rigorous analytical bounds on the L∞ errors of individual layers in the neural network, as well as robust algebraic bounds on how those errors interact when the individual layers are composed into a deeper architecture; and an automatic code-generator that is able to synthesize optimized C code for generating the training data, constructing the neural network architecture, performing the training, validating the results, and inferring new solutions. This framework is designed to integrate seamlessly into the existing formal verification pipeline that the authors previously developed for hyperbolic PDE solvers, enabling the training data itself to be generated by fully formally-verified numerical solvers, thereby facilitating the construction of fully end-to-end 

formally-verified neural network solvers with machine-checkable certificates of correctness, across every stage of the training, validation, and inference process. We begin in Section 2 by introducing the overarching mathematical theory of the BEACONS framework. In Section 2.1, we start by formulating both classical numerical methods (e.g. finite element and finite volume methods) and neural network-based methods (e.g. PINN-style methods using PDE residual-based loss functions) for solving PDEs within a common mathematical framework, namely the construction of finite-dimensional approximations to infinite-dimensional function spaces. In Section 2.2, we proceed to introduce the classical results of Mhaskar[ 31 ], as summarized by Mhaskar and Poggio[ 32 ], on the approximation of Cn-smooth functions by multi-layer perceptrons with a single hidden layer, and combine these with the method of characteristics for hyperbolic PDEs[ 40 ] (which can be used to predict the smoothness of PDE solutions a priori, purely as a function of the smoothness of their initial data, as well as the analytical properties of their flux functions and/or the eigenstructure of their flux Jacobians ). Using this combination, we prove one of the two central theorems underlying the BEACONS framework: a rigorous extrapolatory bound on the worst-case L∞ error of a shallow neural network approximation to a hyperbolic PDE solution. Finally, in Section 2.3, we prove the second central theorem of the BEACONS framework, illustrating that the worst-case L∞ error for a shallow neural network approximation to a discontinuous PDE solution (e.g. a shock wave) may be improved by decomposing the solution into a composition of a smooth and slowly-varying function (with a small Lipschitz constant 

L) and a discontinuous one. Each function is approximated by its own shallow neural network, such that the arbitrarily large L∞ error in the neural network approximation of the discontinuous function is suppressed in the composition by the arbitrarily small L∞ error in the neural network approximation of the smooth one. This theorem furnishes us with an algebraic composability rule for shallow neural network architectures, and therefore a straightforward algorithm for assembling deep BEACONS architectures for approximating complex PDE solutions with favorable L∞ bounds by composing together shallower neural network architectures for approximating simpler, smoother functions. 

> 1For further details on these critiques of PINN architectures, we refer the reader to the systematic investigations of McGreivy and Hakim[28].

3We move on in Section 3 to the details of the software implementation of the BEACONS framework itself. We begin by summarizing the design of the Racket-based DSL and its constituent data structures for representing hyperbolic PDE systems, numerical simulations, and their associated neural solvers. Then, we move on to introducing the design of the automatic code-generator for synthesizing optimized C code for generating numerical training data, setting up and training the BEACONS architectures themselves, and validating the resulting neural solvers. Finally, we conclude by discussing the design of the automated theorem-proving algorithm, including explanations for how fully executable (i.e. machine-checkable) proofs of worst-case L∞ error bounds on the BEACONS architectures are generated, by means of a Racket-based symbolic rewriting system that implements fully correctness-preserving algebraic transformations of symbolic expressions, always respecting the IEEE-754 standard for floating-point arithmetic. Section 4 presents various numerical results obtained by actually running the BEACONS framework end-to-end, for three illustrative equations (or equation systems), in both one and two dimensions. Section 4.1 shows results for the 1D and 2D (scalar) linear advection equation, while Section 4.2 shows results for the 1D and 2D (scalar) inviscid Burgers’ equation, with all training data generated by fully formally-verified numerical solvers. Section 4.3 shows results for the 1D and 2D (coupled, vector) compressible Euler equations, with training data now generated by an unverified but otherwise high-resolution numerical solver (and therefore with all BEACONS error bounds proved conditionally , subject to the assumption that the solver is mathematically correct). In each case, we compare BEACONS architectures with a variety of sizes against ordinary feed-forward neural network architectures of equivalent width and depth, and perform L2 and 

L∞ error analysis, as well as a rigorous conservation analysis, on the results. These error and conservation analyses are performed both pointwise, i.e. for the final simulation frame only, as well as integrated across time. In all cases, we find that the BEACONS architectures exhibit significantly lower L2 and L∞ errors, significantly lower conservation errors, more favorable convergence and stability properties, and greater capability for capturing, preserving, and extrapolating qualitative features (especially in higher dimensions), than their ordinary neural network counterparts of equivalent size. We also find, as expected, that the true L∞ errors exhibited by the BEACONS architectures fall well within the worst-case L∞ bounds proven by the formal verification pipeline. We end in Section 5 with some concluding remarks and possible directions for future research. The entire BEACONS framework is open source, and has been seamlessly integrated into the existing formal verification pipeline for hyperbolic PDE solvers described in [ 20 ], which is itself part of the gkylcas project 2. Although the resulting numerical solvers can be executed as purely standalone pieces of C code, they can also be integrated and run as part of the larger G KEYLL computational multi-physics simulation framework 3, which then provides the requisite (non-formally-verified) infrastructure for running and training on larger-scale simulations, such as parallelism, grid generation, data input/output, etc. 

# 2 Mathematical Theory 

2.1 Neural Network Approximations for Partial Differential Equations 

Let Ω ⊂ Rd denote a d-dimensional computational domain , and let: 

u : Ω → R, (1) represent an unknown scalar field defined over this domain. This scalar field is assumed to represent a solution to the scalar partial differential equation (PDE): 

∀x ∈ Ω, L [u] ( x) = s (x) , (2) obeying the boundary condition: 

∀x ∈ ∂Ω, B [u] ( x) = g (x) , (3) where L is an arbitrary (potentially non-linear) differential operator, B is a boundary operator, s : Ω → R is an arbitrary source term (or forcing function), and g : ∂Ω → R is a boundary function. For instance, for a first-order hyperbolic PDE in conservation law form, evolving in a single spatial dimension, one has d = 2 (since our computational domain here spans both space and time, so x = ( t, x ) ∈ Ω ⊂ R2), with differential operator L given by: 

L [u] = ∂u ∂t + ∂f (u)

∂x , (4) 

> 2https://github.com/ammarhakim/gkylcas
> 3https://github.com/ammarhakim/gkeyll

4where f : R → R is a scalar flux function and u : Ω → R now represents a conserved quantity. Since the space and time coordinates are treated here on the same footing, no distinction is made between initial conditions and (spatial) boundary conditions: both are treated as special cases of space-time boundary conditions. For example, to represent Dirichlet initial or boundary conditions, one simply has B [u] = u, while to represent Neumann initial or boundary conditions, one has: 

B [u] = ∂u ∂t , or B [u] = ∂u ∂x , (5) respectively. In a classical numerical method, one is typically trying to approximate an infinite-dimensional function space defined over the computational domain Ω (such as the Sobolev space H1 (Ω) ) by a finite-dimensional one: 

Vh = span {φ1, φ 2, . . . , φ N } , (6) where each φi : Ω → R is a scalar-valued basis function , such that our PDE solution u : Ω → R is approximated by the linear combination: 

uh (x) = 

> N

X

> i=1

Uiφi (x) , (7) where (uh : Ω → R) ∈ V h, N is the total number of degrees of freedom , and the coefficients Ui ∈ R are arbitrary. For instance, in a finite element or discontinuous Galerkin method[ 5][ 6], the φi : Ω → R represent nodal or modal basis functions, while in a finite volume method[ 44 ], the φi : Ω → R represent piecewise functions of compact support (where the region of support in each case is localized to a single cell), etc. One may regard neural networks as being a grand generalization of this same idea, wherein the basis functions themselves are permitted to vary. Consider the case of a multi-layer perceptron (MLP) consisting of a single hidden layer, itself comprising N hidden neurons. Each hidden neuron effectively defines a “basis function” ψi:

∀i ∈ { 1, . . . , N } , ψi



x; W (1) , b(1) 

= σ



> d

X

> j=1

W (1)  

> ij

xj + b(1) 

> i

 , (8) where x ∈ Ω is an input vector, W (1) ∈ RN ×d is a matrix of hidden layer weights, b(1) ∈ RN is a vector of hidden layer biases, and σ : R → R is a non-linear activation function. Then, the output layer assembles an approximate solution uθ : Ω → R as a linear combination of these “basis functions” (plus an optional scalar bias): 

uθ (x) = b(2) +

> N

X

> i=1

W (2) 1i ψi



x; W (1) , b(1) 

, (9) where W(2) ∈ R1×N is a vector of output layer weights, b(2) ∈ R is an output layer bias, and θ ∈ R1+( d+2) N is a vector representing all trainable parameters, i.e. the concatenation of all entries of W (1) , b(1) , W(2) , and b(2) . If we hold the hidden layer weights W (1) and biases b(1) fixed, and set the output layer bias b(2) = 0 , then we recover exactly the same form of approximation as in a classical numerical method (with the output layer weights W(2) playing the role of the coefficients Ui ∈ R in the basis expansion). For this reason, the finite-dimensional function space spanned by possible neural network approximations: 

VNN = uθ | θ ∈ RP , (10) where P is the total number of trainable parameters (i.e. P = 1 + ( d + 2) N for a multi-layer perceptron with a single hidden layer), is strictly higher-dimensional than the function space Vh spanned by classical numerical approximations, with the former being N -dimensional and the latter being (at least) (1 + ( d + 2) N )-dimensional. In order to construct this neural network approximation (uθ : Ω → R) ∈ V NN to the PDE solution 

(u : Ω → R) ∈ H1 (Ω) , we begin by sampling Nint points x(i) 

> int

from the interior of the computational domain Ω \ ∂Ω,along with Nbound points x(i) 

> bound

from its boundary ∂Ω:5n

x(i)

> int

oNint 

> i=1

⊂ Ω \ ∂Ω, and 

n

x(i)

> bound

oNbound 

> i=1

⊂ ∂Ω. (11) Next, we compute the scalar PDE residual rPDE 



x(i) 

> int

; θ



for each sampled point on the interior: 

∀i ∈ { 1, . . . , N int } , rPDE 



x(i) 

> int

; θ



= L [uθ ]



x(i)

> int



− s



x(i)

> int



, (12) along with the scalar boundary residual rBC 



x(i)

> bound

; θ



for each sampled point on the boundary: 

∀i ∈ { 1, . . . , N bound } , rBC 



x(i)

> bound

; θ



= B [uθ ]



x(i)

> bound



− g



x(i)

> bound



, (13) from which we can calculate the PDE and boundary scalar loss functions JPDE (θ) and JBC (θ), by means of a least squares approach, namely: 

JPDE (θ) = 1

Nint 

> Nint

X

> i=1



rPDE 



x(i) 

> int

; θ

 2

, and JBC (θ) = 1

Nbound 

> Nbound

X

> i=1



rBC 



x(i)

> bound

; θ

 2

, (14) respectively[4]. The overall scalar loss function J (θ) is then assembled as a linear combination of the two: 

J (θ) = λPDE JPDE (θ) + λBC JBC (θ) , (15) where the coefficients λPDE , λ BC ∈ R are arbitrary (with λPDE effectively characterizing how much to penalize violations of the PDE itself, and λBC effectively characterizing how much to penalize violations of the initial and boundary conditions). The purpose of the neural network training process is now to compute a set of parameters θ∗ that (at least approximately) minimizes this combined loss function: 

θ∗ = arg min 

> θ∈RP

{J (θ)} , (16) typically by means of a gradient descent algorithm, for example by setting the initial parameter vector θ0 = 0 and then iterating: 

θn+1 = θn − ηn∇θ J (θn) , (17) until the desired convergence is achieved (i.e. until ∥θn+1 − θn∥∞ < ε for some ε > 0), where ηn ∈ R represents a scalar learning rate (analogous to a time-step ∆t in a classical explicit numerical method). By linearity, we can write out the P components of the gradient vector: 

∇θ J (θ) = 

h ∂J (θ) 

> ∂θ 1
> ∂J (θ)
> ∂θ 2

· · · ∂J (θ)

> ∂θ P

i⊺

∈ RP , (18) explicitly as: 

∀p ∈ { 1, . . . , P } , ∂J (θ)

∂θ p

= λPDE 

 ∂J PDE (θ)

∂θ p



+ λBC 

 ∂J BC (θ)

∂θ p



, (19) which we can then expand via the chain rule in terms of the PDE and boundary residuals rPDE (xint ; θ) and 

rBC (xbound ; θ), along with their respective derivatives, as: 

∂J (θ)

∂θ p

= λPDE 

 2

Nint 

> Nint

X

> i=1

rPDE 



x(i) 

> int

; θ

  ∂r PDE 



x(i) 

> int

; θ



∂θ p



+ λBC 

 2

Nbound 

> Nbound

X

> i=1

rBC 



x(i)

> bound

; θ

  ∂r BC 



x(i)

> bound

; θ



∂θ p

 . (20) 6Generalizing the above analysis to the case of coupled systems of PDEs is relatively straightforward. Specifically, suppose that our unknown quantity is now a vector field: 

U : Ω → Rm, (21) where m ≥ 2 represents the total number of unknown variables, and that this vector field is assumed to represent a solution to the coupled PDE system: 

∀x ∈ Ω, L [U] ( x) = S (x) , (22) obeying the boundary condition: 

∀x ∈ ∂Ω, B [U] ( x) = G (x) , (23) where L is now an arbitrary and potentially non-linear vector of differential operators (such that L [U] ( x) ∈ Rm), B

is now a vector of boundary operators (such that B [U] ( x) ∈ Rm), S : Ω → Rm is now an arbitrary source (or forcing) vector field, and G : ∂Ω → Rm is now a boundary vector field. For instance, for a first-order system of hyperbolic PDEs in a single spatial dimension, in conservation law form, the vector of differential operators L is now given by: 

L [U] = ∂U

∂t + ∂F (U)

∂x , (24) with flux vector field F : Rm → Rm, and with U : Ω → Rm now representing a vector field of conserved quantities. Our neural network approximation Uθ : Ω → Rm to the conserved vector field U : Ω → Rm can now be assembled componentwise by taking linear combinations of the same “basis functions” ψi : Ω → R as in the scalar case: 

∀i ∈ { 1, . . . , m } , Uθ,i (x) = b(2)  

> i

+

> N

X

> j=1

W (2)  

> ij

ψj



x; W (1) , b(1) 

, (25) where now W (2) ∈ Rm×N has been promoted to a matrix of output layer weights, and b(2) ∈ Rm has been promoted to a vector of output layer biases. The structure of the hidden layer remains unchanged from before, while the total number of trainable parameters is now P = m + ( m + d + 1) N . Now the PDE residual rPDE 



x(i) 

> int

; θ



is a vector for each sampled point on the interior of the domain: 

∀i ∈ { 1, . . . , N int } , rPDE 



x(i) 

> int

; θ



= L [Uθ ]



x(i)

> int



− S



x(i)

> int



, (26) and likewise for the boundary residual rBC 



x(i)

> bound

; θ



for each sampled point on the boundary of the domain: 

∀i ∈ { 1, . . . , N bound } , rBC 



x(i)

> bound

; θ



= B [Uθ ]



x(i)

> bound



− G



x(i)

> bound



, (27) To convert these vector-valued PDE and boundary residuals into forms that are compatible with a scalar loss function, as needed for gradient descent, we compute the weighted Euclidean norms, with weight matrix W ∈ Rm×m (assumed to be symmetric and positive semi-definite), yielding: 

rPDE 



x(i) 

> int

; θ



> W

=

vuut

> m

X

> α=1
> m

X

> β=1

rPDE ,α 



x(i) 

> int

; θ



Wαβ rPDE ,β 



x(i) 

> int

; θ



, (28) and: 

rBC 



x(i)

> bound

; θ



> W

=

vuut

> m

X

> α=1
> m

X

> β=1

rBC ,α 



x(i)

> bound

; θ



Wαβ rBC ,β 



x(i)

> bound

; θ



, (29) respectively, with the diagonal components of W effectively characterizing how much to penalize conservation errors in each of the conserved quantities separately, and the off-diagonal components of W effectively characterizing how much to penalize conservation errors in particular combinations of the conserved quantities. Therefore, the PDE and boundary scalar loss functions JPDE (θ) and JBC (θ) now become: 7JPDE (θ) = 1

Nint 

> Nint

X

> i=1
> m

X

> α=1
> m

X

> β=1

rPDE ,α 



x(i) 

> int

; θ



Wαβ rPDE ,β 



x(i) 

> int

; θ



, (30) and: 

JBC (θ) = 1

Nbound 

> Nbound

X

> i=1
> m

X

> α=1
> m

X

> β=1

rBC ,α 



x(i)

> bound

; θ



Wαβ rBC ,β 



x(i)

> bound

; θ



, (31) respectively, and so each of the P components of the gradient vector ∇θ J (θ) become: 

∂J (θ)

∂θ p

= λPDE 

 2

Nint 

> Nint

X

> i=1
> m

X

> α=1
> m

X

> β=1

rPDE ,α 



x(i) 

> int

; θ



Wαβ 

 ∂r PDE ,β 



x(i) 

> int

; θ



∂θ p



+ λBC 

 2

Nbound 

> Nbound

X

> i=1
> m

X

> α=1
> m

X

> β=1

rBC ,α 



x(i)

> bound

; θ



Wαβ 

 ∂r BC ,β 



x(i)

> bound

; θ



∂θ p

 . (32) All other aspects of the gradient descent algorithm remain unchanged from the scalar case. 

2.2 Bounding Errors on Neural Network Approximations for Hyperbolic PDEs 

Here, we follow the notational conventions of Mhaskar and Poggio[ 32 ], and consider our computational domain Ω to be the compact d-dimensional region Ω = Id = [ −1, 1] d, whose space X = C (Ω) = C  Id of continuous functions 

f : Id → R is equipped with the standard L∞-norm: 

∀f ∈ X, ∥f ∥∞ = max 

> x∈Id

{| f (x)|} . (33) If we fix the activation function σ : R → R to be smooth (i.e. C∞, infinitely differentiable), then the set VNN of possible neural network approximations uθ : Rd → R forms, with appropriate normalization of the coefficients, a subspace of 

X. Let Wr,d denote the Sobolev space of functions f : Rd → R for which all partial derivatives up to order r ≥ 1 exist and are continuous, in which all functions have been normalized such that: 

∀f ∈ Wr,d , ∥f ∥∞ + X

> 1≤∥ k∥1≤r

Dkf ∞ ≤ 1, (34) where ∥k∥1 = dP

> i=1

ki is the standard L1-norm on Rd, and ki ∈ Z, such that the sum is taken over the set of all partitions of integers between 1 and r, with the operator Dk denoting the composition of k1 partial derivatives with respect to x1,

k2 partial derivatives with respect to x2, etc. Then, we have (by Theorem 2.1 of Mhaskar and Poggio[32]): 

Theorem 1 Suppose that: 

VNN = uθ | θ ∈ RP , (35) 

represents the space of possible functions uθ : Ω → R that can be computed by a multi-layer perceptron with P

trainable parameters, comprising a single hidden layer consisting of N hidden neurons. Suppose, moreover, that the activation function σ : R → R for such a perceptron is C∞-smooth, and not given by a polynomial on any subinterval of R. Then, one has: 

∀f ∈ Wr,d , inf  

> uθ∈V NN

{∥ f − uθ ∥∞} = O  N − rd

 . (36) Thus, with appropriate normalization, a d-dimensional Cn-smooth function u : Ω → R can be approximated by a neural network uθ : Rd → R with a single hidden layer (containing N hidden neurons), with a worst-case L∞ error on 8the order of N − nd . Henceforth, we will therefore move from using r to using n to denote the number of continuous derivatives of a function f (as in “ f is a Cn-smooth function”). Remarkably, in the case where u : Ω → R corresponds to a solution to a hyperbolic PDE (especially a first-order hyperbolic PDE in conservation law form), the method of characteristics enables one to predict a priori how many continuous partial derivatives of u : Ω → R must exist, at any point within the space-time domain of the PDE, based purely on the smoothness of the initial data and the analytical properties of the flux function f : R → R[ 8]. Since these predictions on the smoothness of u : Ω → R can still be made in regimes that may be arbitrarily far from the neural network’s training data, this enables one to prove extrapolatory error bounds on the correctness of the neural network solution uθ : Rd → R, as we outline below. Suppose that we have a first-order hyperbolic PDE in conservation law form, evolving in one spatial dimension, with no source terms: 

∂u ∂t + ∂f (u)

∂x = 0 , (37) obeying the initial condition (at t = 0 ): 

u (0 , x ) = u0 (x) , (38) where u0 : R → R is (for now) a function of indeterminate smoothness. Assuming that the flux function f : R → R is at least C1-smooth, this is equivalent (via the chain rule) to: 

∂u ∂t + f ′ (u)

 ∂u ∂x 



= 0 , (39) which, by expanding out the ordinary derivatives in terms of partial derivatives, we see is equivalent to the ordinary differential equation: 

du dt = 0 , along curves satisfying dx dt = f ′ (u) . (40) Since u is constant in time (i.e. u is a conserved quantity), f ′ (u) is also constant, and therefore these “curves” are really straight lines in the x − t plane (though in the presence of non-vanishing source terms, du dt ̸ = 0 and so this simplification no longer applies). By integrating the equation dx dt = f ′ (u) in time, we obtain: 

x − x0 = f ′ (u) ( t − t0) (41) 

= f ′ (u0 (x0)) t, (42) since t0 = 0 , and u (t, x 0) = u0 (x0) for all t ≥ 0 since u is constant in time along any such curve. Thus, 

x = x0 + f ′ (u0 (x0)) t is the equation for a straight characteristic line emanating from point x0, and we have: 

∂x ∂x 0

= 1 + f ′′ (u0 (x0)) u′ 

> 0

(x0) t, (43) by the chain rule, assuming now that the flux function f : R → R is at least C2-smooth, and that the initial data 

u0 : R → R is at least C1-smooth. When two or more of these characteristic lines intersect, the function u : Ω → R is forced to be many-valued (and hence the solution becomes discontinuous)[ 9], which is equivalent to the derivative ∂u ∂x becoming singular. Again using the fact that u (t, x ) = u0 (x0) for all t ≥ 0 along characteristic lines, we can evaluate this derivative via the chain rule as: 

∂u ∂x = ∂u 0 (x0)

∂x = u′ 

> 0

(x0)

 ∂x 0

∂x 



= u′ 

> 0

(x0)

 ∂x ∂x 0

−1

(44) 

= u′ 

> 0

(x0)1 + f ′′ (u0 (x0)) u′ 

> 0

(x0) t , (45) which becomes singular whenever: 9t = −1

f ′′ (u0 (x0)) u′ 

> 0

(x0) . (46) Therefore, the earliest time t∞ at which blowup occurs (and hence the solution u : Ω → R fails to be even C1-smooth) is given by: 

t∞ =



sup 

> x0∈R

{− f ′′ (u0 (x0)) u′ 

> 0

(x0)}

−1

, (47) subject to the convention that 10 = + ∞. Once a finite-time blowup has occurred, the physically relevant solution (i.e. the entropy solution [35]) is, at best, only piecewise Cn. In summary, we have: 

Lemma 1 Suppose that u : Ω → R is a solution to a (homogeneous) first-order hyperbolic PDE in conservation law form: 

∂u ∂t + ∂f (u)

∂x = 0 , (48) 

where the initial data: 

u (0 , x ) = u0 (x) , (49) 

is given by a Cn-smooth function u0 : R → R, and the flux is given by a Cn+1 -smooth function f : R → R, where 

n ≥ 1. Then: 1. If the flux is linear (i.e. f ′′ = 0 ), or the flux is convex and the initial data is monotonically increasing (i.e. 

f ′′ ≥ 0 and u′ 

> 0

≥ 0), or the flux is concave and the initial data is monotonically decreasing (i.e. f ′′ ≤ 0 and 

u′ 

> 0

≤ 0), then the solution u : Ω → R remains Cn-smooth for all t ≥ 0.2. Otherwise, the solution u : Ω → R remains Cn-smooth for all 0 ≤ t < t ∞, where: 

t∞ =



sup 

> x0∈R

{− f ′′ (u0 (x0)) u′ 

> 0

(x0)}

−1

, (50) 

and is discontinuous (though still piecewise Cn) for all t ≥ t∞.

Note that, although we have presented these results in terms of scalar PDEs, it is relatively straightforward to extend all of the above analysis to the case of coupled (vector) systems of hyperbolic PDEs too. In particular, if U : Ω → Rm is now a solution to a (homogeneous) system of first-order hyperbolic PDEs in conservation law form: 

∂U

∂t + ∂F (U)

∂x = 0, (51) where the initial data: 

U (0 , x ) = U0 (x) , (52) is given by a Cn-smooth vector field U0 : R → Rm, and the flux is given by a Cn+1 -smooth vector field F : Rm → Rm,where n ≥ 1, then we can apply a higher-dimensional generalization of the method of characteristics in much the same way. Assuming that system is strictly hyperbolic , i.e. that the flux Jacobian JF, defined by: 

JF = ∇UF (U) , (53) is diagonalizable, with all of its eigenvalues λi being both real and distinct, we can construct the higher-dimensional analog of the characteristic equation, namely the following (fully decoupled) system of Riccati-type equations: 

∀i ∈ { 1, . . . , m } , dω i (U)

dt + αi (U) ω2 

> i

(U) + X 

> j̸=i

βij (U) ωi (U) ωj (U) = 0 , (54) with each ωi representing a different wave or mode of the system. In the above, we have introduced: 10 ωi (U) = li (U) ·

 ∂U

∂x 



, and αi (U) = ( ∇Uλi (U)) · ri (U) , (55) where li (U) and ri (U) denote the left and right eigenvectors of the flux Jacobian JF, respectively (the elements of the matrix βij (U) are smooth coefficients built from the eigenstructure of JF, the details of which are not relevant here). The salient point is that, when αi = 0 , this is the analog of the linear flux (i.e. f ′′ = 0 ) case for scalar PDEs: namely, the corresponding wave ωi does not steepen into a shock, and the corresponding part of the solution remains Cn-smooth. When αi̸ = 0 , then the corresponding wave ωi does indeed steepen to induce a finite-time blowup of ∂U  

> ∂x ∞

at: 

t∞,i =



inf  

> x0∈R

{− αi (U0 (x0)) ωi (U0 (x0)) }

−1

, (56) again subject to the convention that 10 = + ∞, whereupon the corresponding part of the solution fails to be even 

C1-smooth, and the physically relevant (entropy) solution from that point onwards is, at best, only piecewise Cn.Combining Lemma 1 with Theorem 2.1 of Mhaskar and Poggio[ 32 ], we obtain the main result of this section regarding error bounds for shallow neural network approximations for hyperbolic PDEs. 

Theorem 2 Suppose that u : Ω → R is a solution to a (homogeneous) first-order hyperbolic PDE in conservation law form: 

∂u ∂t + ∂f (u)

∂x = 0 , (57) 

where the initial data: 

u (0 , x ) = u0 (x) , (58) 

is given by a Cn-smooth function u0 : R → R, and the flux is given by a Cn+1 -smooth function f : R → R, where 

n ≥ 1. Suppose, moreover, that: 

VNN = uθ | θ ∈ RP , (59) 

represents the space of possible functions uθ : Ω → R that can be computed by a multi-layer perceptron with P

trainable parameters, comprising a single hidden layer consisting of N hidden neurons, with the activation function given by a C∞-smooth function σ : R → R that is not a polynomial on any subinterval of R. Then: 1. If the flux is linear (i.e. f ′′ = 0 ), or the flux is convex and the initial data is monotonically increasing (i.e. 

f ′′ ≥ 0 and u′ 

> 0

≥ 0), or the flux is concave and the initial data is monotonically decreasing (i.e. f ′′ ≤ 0 and 

u′ 

> 0

≤ 0), then there exists a neural network uθ ∈ V NN such that: 

∀t ≥ 0, ∥u − uθ ∥∞ = O  N − nd

 . (60) 

2. Otherwise, there exists a neural network uθ ∈ V NN such that: 

∀0 ≤ t < t ∞, ∥u − uθ ∥∞ = O  N − nd

 , (61) 

where: 

t∞ =



sup 

> x0∈R

{− f ′′ (u0 (x0)) u′ 

> 0

(x0)}

−1

, (62) 

and: 

∀t ≥ t∞, ∥u − uθ ∥∞ = O (1) . (63) 

In this case, there nevertheless exists a function: 

11 eu (t, x ) = 



u(1)  

> θ1

, x ≤ x1 (t) ,u(2)  

> θ2

, x1 (t) < x ≤ x2 (t) ,

· · · ,

(64) 

where each u(i) 

> θi

∈ V NN is a neural network, and each xi : R → R is a function tracking the position of a single discontinuity, such that: 

∀t ≥ 0, ∥u − eu∥∞ = O  N − nd

 . (65) 

2.3 Algebraic Composability: Assembling Deep Neural Networks with Bounded Errors 

The results of the preceding section indicate that shallow neural network architectures (i.e. multi-layer perceptrons consisting of a single hidden layer) can approximate Cn-functions with vanishingly small L∞ errors (i.e. O (0) ) in the smooth limit as n → ∞ , but that these approximations become arbitrarily poor (i.e. O (1) ) in the discontinuous limit as n → 0. In particular, to obtain any reasonable bound on the L∞ error for a neural network approximation to a hyperbolic PDE permitting shocks and other weak/discontinuous solutions, even when considering piecewise approximations, the number of neurons N in the hidden layer must be made impractically large, and the accuracy is still fundamentally restricted by the smoothness of the initial data. Our objective now is to ascertain whether these error bounds can be improved by considering deeper neural network architectures instead[ 30 ], obtained as compositions of multiple shallower ones. For instance, suppose that, instead of attempting to approximate a PDE solution u (t, x )

directly using a shallow neural network, we instead assemble the solution as a composition: 

u (t, x ) = f (g (t, x ) , h (t, x )) , (66) of three other functions f , g, and h, each of which can be approximated by shallow neural networks sharing the same number of inputs and outputs. On the surface, this does not appear to represent an improvement, since if the functions f ,

g, and h are Cα-, Cβ -, and Cγ -smooth, respectively, then their composition f (g (t, x ) , h (t, x )) is Cmin {α,β,γ }-smooth by the chain rule. Thus, at least one of the three functions in the composition must be at least as discontinuous as the function u (t, x ) itself, and therefore the bound on the L∞ error of its neural network approximation must be at least as large. However, by making a judicious choice of functions f , g, and h to appear in the composition, it is possible to suppress this relatively large L∞ error in such a way that the bound on the L∞ error of the composition u (t, x ) is nevertheless smaller overall, as we shall show below. The argument generalizes inductively to an arbitrary number of functions with arbitrary arities, composed in an arbitrary manner, and therefore can be applied to construct arbitrarily deep neural network architectures with favorable L∞ bounds. We refer to this property as algebraic composability ,taking inspiration from prior work on compositional approaches to deep learning[16][17]. Let f : R → R and g : R → R be arbitrary functions, without any assumptions of differentiability, and let ef : R → R

and eg : R → R be (shallow) neural network approximations to them with L∞ errors of ef and eg , respectively, i.e: 

f − ef ∞ = ef , ∥g − eg∥∞ = eg . (67) We now wish to determine a bound on the L∞ error of the (deep) neural network approximation ef ◦ eg to the composition 

f ◦ g. By applying the triangle inequality, we obtain: 

f ◦ g − ef ◦ eg ∞

= f ◦ g − f ◦ eg + f ◦ eg − ef ◦ eg ∞

(68) 

≤ ∥ f ◦ g − f ◦ eg∥∞ + f ◦ eg − ef ◦ eg ∞

. (69) The first term ∥f ◦ g − f ◦ eg∥∞ is effectively quantifying how much the value of f varies in response to a change in its input from g to eg (a change which is bounded by ∥g − eg∥∞), while the second term f ◦ eg − ef ◦ eg ∞

is effectively quantifying how different the values of f and ef can be for a given fixed input of eg (a difference which is bounded by 

f − ef ∞

). Introducing the modulus of continuity ωf : R → R of f as: 

ωf (δ) = sup 

> |u−v|≤ δ

{| f (u) − f (v)|} , (70) 12 we therefore have the bound: 

∥f ◦ g − f ◦ eg∥∞ + f ◦ eg − ef ◦ eg ∞ ≤ ωf (∥g − eg∥∞) + f − ef ∞

(71) 

= ef + ωf (eg ) . (72) If f : R → R is L-Lipschitz on the interval between g and eg, then we can further bound: 

ωf (eg ) ≤ Le g . (73) Assembling this all together, we obtain the main result of this section regarding the behavior of L∞ errors of neural network approximations under composition: 

Proposition 1 Suppose that f : R → R and g : R → R are arbitrary functions, subject to the hypothesis that f is 

L-Lipschitz everywhere. Suppose that ef : R → R and eg : R → R are (shallow) neural network approximations to f

and g with L∞ errors of ef and eg respectively: 

f − ef ∞

= ef , ∥g − eg∥∞ = eg . (74) 

Then the composition of (shallow) neural network approximations ef ◦ eg is a (deep) neural network approximation to the composite function f ◦ g with L∞ error bounded by: 

f ◦ g − ef ◦ eg ∞

≤ ef + Le g . (75) In other words, if we can decompose a discontinuous function u : R → R into a composition (f ◦ g) : R → R of a smooth, slowly-varying function f : R → R, and a discontinuous function g : R → R, then we expect the error ef in the approximation of f to be small, and the error eg in the approximation of g to be large. However, if f is L-Lipschitz everywhere, then we can suppress the arbitrarily large error in the approximation of g by making the Lipschitz constant 

L of f as small as possible (in practice, this is restricted only by the range of u4), such that the product Le g remains as small as possible, and the overall error in the approximation of u = f ◦ g becomes dominated, if possible, by the relatively small error ef in the approximation of f . This effectively gives us a prescription for how to construct deep neural network architectures for the approximation of discontinuous functions (e.g. solutions to hyperbolic PDEs permitting shocks and other discontinuous solutions) with favorable L∞ bounds: decompose the discontinuous function into a composition of a large number of smooth, slowly-varying functions, with all of the discontinuities “packaged” into a single, highly discontinuous function appearing right at the start of the chain of compositions. Then, suppress the large errors in the approximation of this first, discontinuous function to the greatest extent possible, by forcing the Lipschitz constants of the slowly-varying functions to be as small as possible. This procedure is precisely the prescription that BEACONS uses for assembling deep neural network approximations to PDE solutions with sharply bounded L∞ errors. Note that this basic idea, namely that one can improve the accuracy and stability of a numerical approximation in the vicinity of a shock (or other discontinuity or region of sharp gradient) by composing the approximate solution with a function that artificially reduces its gradient, is also the key insight behind the theory of flux limiters [27 ] and high-order 

total variation diminishing (TVD) schemes[ 22 ] in the classical numerical analysis of hyperbolic PDEs. In this respect, our results on the algebraic compositionality of deep BEACONS networks may be regarded as a generalized (and iterated) version of the classical circumvention of Godunov’s theorem by means of a non-linear flux limiter. As a minimal example of how such a decomposition u (x) = f (g (x)) , of a discontinuous function u : R → R into a smooth, slowly-varying ( L-Lipschitz) function f : R → R and a discontinuous function g : R → R, could be con-structed, consider the trivial case: 

f (x) = xC , and g (x) = Cu (x) , (76) where C > 0 is some positive constant, such that f is C∞-smooth, and the Lipschitz constant of f is given by L = 1 

> C

.Clearly this trivial case does not confer any actual advantage, since although we can make the Lipschitz constant L of f

arbitrarily small by increasing the value of C, this also has the effect of increasing the error eg in the approximation of 

g by exactly the same amount. However, within (the failure of) this minimal example lies the key insight to the general        

> 4Since the range of fmust be a non-strict superset of the range of u, and a lower bound on the Lipschitz constant Lof fis determined by its range.

13 algorithm: select a C∞-smooth function f whose Lipschitz constant L can be precisely controlled, whose range is a non-strict superset of the range of u:

range ( u) ⊆ range ( f ) , (77) and which is either strictly-increasing or strictly-decreasing, such that its inverse f −1 : R → R is unique, and therefore: 

g (x) = f −1 (u (x)) , (78) can be constructed unambiguously, such that the error eg in the approximation of g grows as less than 1 

> L

as L becomes small. For example, a good general candidate is: 

f (x) = arcsinh ( x)

C , and g (x) = sinh ( Cu (x)) , (79) where again C > 0 is some positive constant such that the Lipschitz constant of f is given by L = 1 

> C

. The operative distinction between this case and the previous (trivial) case lies in the non-linearity of arcsinh ( x); again, this is analogous to the theory of high-order TVD schemes for hyperbolic PDEs, wherein the flux limiter must necessarily be non-linear in order to circumvent the axioms of Godunov’s theorem successfully. The BEACONS framework automatically trials several such candidate functions (primarily involving simple trigonometric and hyperbolic functions, and their inverses) until it finds one which minimizes the growth rate in the error eg in the approximation of g, as a function of 1 

> L

. Each layer of the BEACONS network is then trained separately, with its own specialized loss function, via the standard backpropagation algorithm for multi-layer perceptrons. 

# 3 Software Implementation 

In the authors’ previous work[ 20 ] on formally-verified numerical solvers for hyperbolic PDEs, a domain-specific language (DSL) for representing hyperbolic PDE systems in Racket[ 13 ] was introduced, along with an automatic code-generator that was capable of converting these high-level Racket representations into optimized C code for a variety of different numerical algorithms, and an automated theorem-prover that was able to prove fully executable (and therefore fully machine-checkable) correctness theorems regarding the resulting solvers. The Racket-based DSL was designed to be sufficiently general as to encode both individual scalar PDEs and coupled (vector) systems of PDEs, in any number of dimensions. The code-generator was then able to generate standalone finite volume numerical solvers for these equations using either Lax-Friedrichs (low-resolution) fluxes[ 25 ] or Roe (high-resolution) fluxes[ 39 ], and using either a fully unsplit method (in 1D), or a second-order Strang split method (in higher dimensions)[ 42 ]. Although naively these solvers are all first-order accurate in space, the code-generator was also able to extend the spatial accuracy to second-order in all smooth regions by replacing the piecewise-constant representation of the solution with a piecewise-linear reconstruction obtained using a given choice of flux limiter (e.g. minmod, monotonized-centered, van Leer, or super-bee). Finally, the theorem-proving framework was then able to prove various correctness properties of the resulting numerical schemes, such as (strict) hyperbolicity preservation, CFL stability, convexity/local Lipschitz continuity, and flux continuity, in the case of the Lax-Friedrichs and Roe solvers, as well as symmetry and second-order total variation diminishing (TVD), in the case of the flux limiters for second-order flux extrapolation. The generated proofs were all, themselves, standalone pieces of symbolic Racket code which could be executed (with this execution constituting an automatic check of the validity of the proof), and which could therefore be regarded as executable certificates of correctness . One of the key design principles underlying the theorem-prover was that only symbolic transformations respecting the underlying algebraic structure of the IEEE-754 standard for floating-point arithmetic should be permitted, since only these transformations would guarantee preservation of correctness of the corresponding generated C code (we thus referred to such transformations as strictly correctness-preserving algebraic transformations). To this end, we constructed bespoke algorithms for automatic symbolic simplification ( symbolic-simp ), automatic symbolic differentiation ( symbolic-diff ), and automatic symbolic evaluation of limits ( evaluate-limit ) of arbitrary Racket expressions. In the present work, we have extended the Racket-based DSL to support the specification of certain hyperparameters for BEACONS architectures (namely width, depth, and maximum number of training steps). We have also extended the code-generator to support the automatic synthesis of optimized C code for both training and validating BEACONS networks for each equation system, with the training data being entirely supplied by formally-verified numerical solvers. Although the solvers themselves are fully dependency-free, the neural network training and validation is performed using the minimalist kann 5 neural network library. Finally, we have also extended the theorem-proving framework to 

> 5https://github.com/attractivechaos/kann

14 facilitate the generation of automatic proofs of worst-case L∞ error bounds for the BEACONS approximations, based on the analytical properties of the flux function (or the eigensystem of its Jacobian), the analytical properties of the initial data, and the BEACONS hyperparameters. As described in the previous section, this theorem-proving algorithm begins by using the smoothness of the initial data and the analytical properties of the flux to prove a worst-case L∞

bound on the approximation of the solution by a shallow neural network architecture (i.e. a multi-layer perceptron with a single hidden layer), and then proceeds to trial possible decompositions of the solution into compositions of various smooth candidate functions combined with a single discontinuous function. It continues this process until it finds a decomposition which minimizes the worst-case L∞ error in the approximation of the overall composition by means of a deep neural network architecture (subject to the constraint that this architecture is consistent with the desired BEACONS hyperparameters). In Figure 1, we show an example of the 1D compressible Euler equations being represented using a data structure within our Racket-based DSL, plus additional data structures for representing the initial conditions of a 1D Sod-type shock tube problem, along with the hyperparameters for a shallow BEACONS network consisting of 6 layers, with 64 neurons per layer, and 10,000 maximum training steps. We then also show part of the output from the automatic code-generator for this particular example, illustrating how a Lax-Friedrichs solver with second-order flux extrapolation can be used to generate arbitrary amounts of training data, which is then automatically used to train (and subsequently validate) the desired BEACONS architecture. In Figure 2, we show two examples of output from the automated theorem-proving framework, corresponding to fully executable Racket proofs of flux continuity for a 1D Roe solver for the inviscid Burgers’ equation (i.e. a correctness property of the underlying numerical solver), and of a bound on the worst-case L∞ error for the non-smooth parts of a shallow BEACONS architecture for approximating the output of this solver (i.e. a correctness property of the BEACONS architecture trained on the output of the solver). 

Figure 1: On the left, an example of the extended Racket data structure for representing a system of hyperbolic PDEs (in this case, the 1D compressible Euler equations), a collection of simulation parameters (in this case, representing a 1D Sod-type shock tube problem), and a collection of hyperparameters for a shallow BEACONS architecture (6 layers, 64 neurons per layer, 10,000 maximum training steps). On the right, the resulting optimized C code output by the automatic code-generator for running the specified simulation (using a Lax-Friedrichs solver with second-order flux extrapolation), generating the necessary training data, and training the BEACONS architecture accordingly. As previously, the theorem-proving algorithm is implemented in a purely equational fashion, by constructing a globally confluent and strongly normalizing symbolic rewriting system[ 1][ 38 ], and then proceeding to apply these rewriting rules to an arbitrary symbolic Racket expression until the rewriting sequence terminates at a normal form; if two Racket expressions terminate at the same normal form, then this rewriting sequence is interpreted as a proof that the original expressions were symbolically equivalent. It is worth emphasizing that the proofs of correctness for the underlying numerical solvers and the proofs of correctness for the BEACONS architectures trained on the output of these solvers are conceptually distinct, and logically independent of one another. The former class of proofs effectively guarantee that the function represented by the training data is mathematically correct (i.e. agrees with the true analytical solution to the underlying PDE system in the appropriate limit), even if this function cannot itself be accurately approximated by a given BEACONS architecture. On the other hand, the latter class of proofs effectively guarantee that the BEACONS architecture is correctly computing an approximation (with bounded error) to the function represented by the training data, even if this function is not itself mathematically correct with respect to the underlying PDE system. Of course, it is preferable to have both guarantees wherever possible, but there may be occasions where it is not possible to produce a fully formally-verified underlying numerical solver, yet it may nevertheless be desirable to synthesize a formally-verified BEACONS architecture using that solver as a source of training data subject to the unproven assumption that the solver is correct. We will encounter precisely this scenario later on in the paper, in the context of the compressible Euler 15 Figure 2: On the left, the final few steps of an executable Racket proof of flux continuity for a Roe solver for the 1D inviscid Burgers’ equation. On the right, the final few steps of an executable Racket proof of a bound on the worst-case 

L∞ error (for the non-smooth parts of the solution) for a shallow BEACONS architecture for solving the 1D inviscid Burgers’ equation. equations; in this case, no formally-verified solver currently exists, yet a BEACONS architecture with bounded error can nevertheless be synthesized subject to the assumption that the generated solver is correct. 

# 4 Numerical Results 

Note that, for all examples presented within this section, both the BEACONS architectures and the non-BEACONS (fully-connected) neural network architectures use the hyperbolic tangent loss function σ (x) = tanh ( x), since it satisfies the requisite C∞-smoothness hypothesis for Theorem 2.1 of Mhaskar and Poggio[ 32 ]. The learning rate in all cases is set to 10 −4, the maximum number of epochs to 50, and the minimum number of epochs to 10. Fully-connected neural networks are trained in a single shot, on the entire PDE solution u, using standard full-batch 6 gradient descent with backpropagation. BEACONS architectures are trained layer-by-layer, with each layer trained on a single constituent function f appearing in the composition of the overall solution u (with its own specialized loss function), again using standard full-batch gradient descent with backpropagation, as described in Section 3. 

4.1 Linear Advection Equation 

Our first numerical test case will be to solve the scalar linear advection equation: 

∂u ∂t + ∂ (au )

∂x = 0 , (80) i.e. a scalar conservation equation with linear flux f (u) = au , where a ∈ R is an arbitrary (constant) advection speed. Our automated theorem-proving framework is able to produce full proofs of correctness for both the Lax-Friedrichs and Roe-type finite volume solvers for this equation; hyperbolicity-preservation, CFL stability, and local Lipschitz continuity of the Lax-Friedrichs solver require a total of 38, 50, and 44 proof steps to establish, respectively, while hyperbolicity-preservation and flux conservation of the Roe-type solver require a total of 60 and 97 proof steps to establish, respectively. The first test problem will be a one-dimensional Riemann problem over the spatial domain 

[−1.0, 1.0] , with advection speed a = 1 .0, and initial data given by: 

> 6We opt to use full-batch gradient descent, as opposed to stochastic gradient descent with mini-batching, in order to make the results fully deterministic and hence more easily reproducible. The problems described here are sufficiently small that the computational cost of doing this is not prohibitive. Extension of these methods to stochastic gradient descent with mini-batching is straightforward.

16 u0 (x) = 

1.0, for x ≤ 0.0,

0.0, for x > 0.0. (81) We shall solve this problem numerically using the formally-verified Roe-type approximate Riemann solver (equivalent to an exact Riemann solver due to the linearity of the problem), with a CFL coefficient CCFL = 1 .0, and with a spatial resolution of 1024 cells. We evolve the problem numerically up until a final time of t = 1 .0, producing 100 frames of output in the process. We then train four different neural network architectures on a subset of this output, namely the first 33 frames (up until time t = 0 .33 ), and then ask each of them to predict the remainder of the simulation based on the learned solution function u (t, x ). The chosen neural network architectures are: a 6-layer BEACONS architecture with 64 neurons per layer, an 8-layer BEACONS architecture with 128 neurons per layer, and, for means of comparison, a 6-layer fully-connected (non-BEACONS) neural network with 64 neurons per layer, and an 8-layer fully-connected (non-BEACONS) neural network with 128 neurons per layer. For this problem, our automated theorem-proving framework proves a worst-case L∞ error of 0.903602 (across both the smooth and non-smooth parts of the solution) for the 6-layer BEACONS architecture, and 0.707106 (across both the smooth and non-smooth parts of the solution) for the 8-layer BEACONS architecture. In each case, the theorem-prover requires a total of 99 proof steps to establish the bound. The (inferred) solutions from the formally-verified numerical solver, the 6-layer neural network, the 8-layer neural network, the 6-layer BEACONS architecture, and the 8-layer BEACONS architecture, at times t = 0 .5 and 

t = 0 .8, are shown in Figure 3. 

Figure 3: Results for the 1D linear advection Riemann problem at times t = 0 .5 (left) and t = 0 .8 (right), obtained using a formally-verified numerical solver (blue), a 6-layer neural network (red), an 8-layer neural network (yellow), a 6-layer BEACONS architecture (purple), and an 8-layer BEACONS architecture (green). The 6-layer neural network slightly overestimates the advection speed, and the 8-layer neural network significantly underestimates it; both neural network solutions exhibit substantial conservation errors. Both BEACONS solutions track the numerical solution more-or-less perfectly. We see that the 6-layer neural network solution exhibits large overshoots and undershoots in the region surrounding the propagating discontinuity, and slightly overestimates the overall advection speed. These overshoots and undershoots are reduced in the 8-layer neural network solution, which is broadly more stable, but the advection speed is now significantly underestimated. Both neural network solutions visibly fail to conserve the advected quantity to any significant extent, with the conservation errors in the 8-layer case being especially severe. Both of the BEACONS solutions remain very close to the numerical solution, with the advection speed estimated more-or-less perfectly. The 6-layer BEACONS solution exhibits some small overshoots and undershoots in the region surrounding the propagating discontinuity (similar to the 6-layer neural network solution). These overshoots and undershoots are slightly reduced in the 8-layer BEACONS solution, which appears marginally more diffusive in some respects, and both BEACONS solutions appear to conserve the advected quantity at least approximately. Table 1 shows the normalized L2 and L∞ errors (as compared against the formally-verified numerical solution) across the four different neural network architectures, both for the final frame only, and across all frames (using a per-frame average for the L2 error, and normalizing across all frames for the 

L∞ error). We see overall that both the final-frame and all-frame L2 and L∞ errors are significantly lower for the two BEACONS architectures than for the non-BEACONS architectures. In both the BEACONS and non-BEACONS cases, we see that increasing the number of layers has the effect of decreasing the L∞ errors at the expense of increasing the L2 errors (in the final-frame case for the non-BEACONS architectures, this increase is particularly severe). Table 17 2 shows the normalized conservation errors (as obtained by integrating over the conserved quantity, and comparing against the integral of the formally-verified numerical solution) across the four different neural network architectures, again both for the final frame only, and integrated across all frames. We see overall that both the final-frame and all-frame conservation errors are significantly lower for the two BEACONS architectures than for the non-BEACONS architectures. In the non-BEACONS case, we see that increasing the number of layers has the effect of dramatically increasing both the final-frame and all-frame conservation errors. In the BEACONS case, we observe the opposite effect, with a dramatic decrease in all conservation errors for the 8-layer BEACONS architecture over the 6-layer BEACONS architecture. Note that the largest L∞ errors observed for the two BEACONS architectures (i.e. 0.782192 and 0.633319, respectively) remain comfortably below the worst-case bounds (i.e. 0.903602 and 0.707106, respectively). Architecture L∞ Error (Final) L2 Error (Final) L∞ Error (All) L2 Error (All) 6-layer NN 1.033641 2.536744 1.076130 3.002461 8-layer NN 0.976984 9.030859 1.002587 4.807408 6-layer BEACONS 0.612160 1.132093 0.782192 1.061877 8-layer BEACONS 0.605036 1.211529 0.633319 1.189765 Table 1: L2 and L∞ error analysis for the 1D linear advection Riemann problem, comparing the 6-layer neural network, 8-layer neural network, 6-layer BEACONS, and 8-layer BEACONS solutions against the formally-verified numerical solution, both for the final predicted frame, and across all predicted frames. Architecture Conservation Error (Final) Conservation Error (Total) 6-layer NN -21.169941 498.706193 8-layer NN -130.422471 -4391.640256 6-layer BEACONS -3.770073 -155.321387 8-layer BEACONS 1.881948 49.570335 Table 2: Conservation analysis for the 1D linear advection Riemann problem, comparing the 6-layer neural network, 8-layer neural network, 6-layer BEACONS, and 8-layer BEACONS solutions against the formally-verified numerical solution, both for the final predicted frame, and across all frames. The second test problem will consider an extension of the linear advection equation to two dimensions (with the same advection speed a ∈ R in both directions): 

∂u ∂t + ∂ (au )

∂x + ∂ (au )

∂y = 0 , (82) with a square domain [−1.0, 1.0] × [−1.0, 1.0] . Our automated theorem-proving framework is again able to produce full proofs of correctness for both the Lax-Friedrichs and Roe-type finite volume solvers for this equation (including its second-order dimension splitting); hyperbolicity-preservation, CFL stability, and local Lipschitz continuity of the Lax-Friedrichs solver require a total of 65, 89, and 77 proof steps to establish, respectively, while hyperbolicity-preservation and flux conservation of the Roe-type solver require a total of 109 and 183 proof steps to establish, respectively. We advect a disk described by initial data: 

u0 (x, y ) = 

(

1.0, for (x + 0 .5) 2 + ( y + 0 .5) 2 ≤ 0.33 ,

0.0 for (x + 0 .5) 2 + ( y + 0 .5) 2 > 0.33 , (83) from the bottom left to the top right of the domain, with advection speed a = 1 .0. Once again, we solve this problem using the formally-verified Roe-type solver, a CFL coefficient CCFL = 1 .0, and now a spatial resolution of 256 × 256 

cells. We evolve the problem numerically up until a final time of t = 1 .0, again producing 100 frames of output, and now train only two different neural network architectures on the first 33 frames (up until time t = 0 .33 ), and again ask both of them to predict the remainder of the simulation based on the learned solution function u (t, x, y ).The chosen neural network architectures are an 8-layer BEACONS architecture with 128 neurons per layer, and, for means of comparison, an 8-layer fully-connected (non-BEACONS) neural network with 128 neurons per layer. For this problem, our automated theorem-proving framework proves worst-case L∞ errors of 1.216729 (for the smooth parts of the solution) and 1.483672 (for the non-smooth parts of the solution) for a 6-layer BEACONS architecture, and 1.0 (for the smooth parts of the solution) and 1.259921 (for the non-smooth parts of the solution) for the 8-layer BEACONS architecture used here. In each case, the theorem-prover requires a total of 187 proof steps to establish the bound. The (inferred) solutions from the formally-verified numerical solver, the 8-layer neural network, and the 18 8-layer BEACONS architecture, at times t = 0 .33 , t = 0 .66 , and t = 0 .99 , are shown in Figure 4. We see that the 8-layer neural network solution fails to preserve the shape of the disk, which gets progressively more distorted and “egg-shaped” as it approaches the top right of the domain. Tracking the center of this distorted disk indicates that the 8-layer neural network solution also underestimates the advection speed on average. On the other hand, the 8-layer BEACONS solution fully preserves the shape of the disk, and the advection speed is again estimated correctly. Indeed, the 8-layer BEACONS solution is in almost perfect agreement with the numerical solution, albeit with some very slight numerical diffusion around the boundaries of the disk. Table 3 shows the normalized L2 and L∞ errors (as compared against the formally-verified numerical solution) across the two different neural network architectures, both for the final frame only, and across all frames (using a per-frame average for the L2 error, and normalizing across all frames for the L∞ error). We see overall that both the final-frame and all-frame L2 and L∞ errors are significantly lower for the BEACONS architecture than for the non-BEACONS architecture. Note again that the largest L∞ error observed for the BEACONS architecture (i.e. 0.938123) remains comfortably below the worst-case bound (i.e. 1.259921). 

Figure 4: Results for the 2D linear advection disk problem at times t = 0 .33 (left), t = 0 .66 (middle), and t = 0 .99 

(right), obtained using a formally-verified numerical solver (top), an 8-layer neural network (middle), and an 8-layer BEACONS architecture (bottom). The 8-layer neural network progressively distorts the shape of the disk as it approaches the top right of the domain, causing it to become increasingly “egg-shaped”, while also underestimating the advection speed overall. The BEACONS architecture successfully preserves the shape of the disk, and shows near-perfect agreement with the numerical solution. 19 Architecture L∞ Error (Final) L2 Error (Final) L∞ Error (All) L2 Error (All) 8-layer NN 1.031098 40.481536 1.031098 22.772318 8-layer BEACONS 0.864784 9.363477 0.938123 6.747000 Table 3: L2 and L∞ error analysis for the 2D linear advection disk problem, comparing the 8-layer neural network and 8-layer BEACONS solutions against the formally-verified numerical solution, both for the final predicted frame, and across all predicted frames. 

4.2 Inviscid Burgers’ Equation 

Our second numerical test case will be to solve the scalar inviscid Burgers’ equation: 

∂u ∂t + u

 ∂u ∂x 



= ∂u ∂t + ∂   12 u2

∂x = 0 , (84) i.e. a scalar conservation equation with non-linear flux f (u) = 12 u2. Due to the non-linear nature of the flux, Burgers’ equation is able to exhibit shocks (i.e. weak/distribution solutions), as well as rarefaction waves . Our automated theorem-proving framework is able to produce full proofs of correctness for both the Lax-Friedrichs and Roe-type finite volume solvers for this equation; hyperbolicity-preservation, CFL stability, and local Lipscitz continuity of the Lax-Friedrichs solver require a total of 95, 121, and 101 proof steeps to establish, respectively, while hyperbolicity-preservation and flux conservation of the Roe-type solver require a total of 125 and 193 proof steps to establish, respectively. The first test problem will be a one-dimensional “top-hat” initial value problem with two initial discontinuities, defined over the spatial domain [0 .0, 6.0] , with initial data given by: 

u0 (x) = 

3.0, for 2.0 ≤ x ≤ 4.0,

−1.0 for x < 2.0 or x > 4.0. (85) As previously, we shall solve this problem numerically using the formally-verified Roe-type approximate Riemann solver (no longer equivalent to an exact Riemann solver, due to the non-linearity of the problem), with a CFL coefficient 

CCFL = 1 .0, and with a spatial resolution of 1024 cells. We evolve the problem numerically up until a final time of 

t = 1 .0, producing 100 frames of output in the process. As before, we train four different neural network architectures on the first 33 frames of this output (up until time t = 0 .33 ), and then ask each of them to predict the remainder of the simulation based on the learned solution function u (t, x ). The chosen neural network architectures, as before, are: a 6-layer BEACONS architecture with 64 neurons per layer, an 8-layer BEACONS architecture with 128 neurons per layer, and, for means of comparison, a 6-layer fully-connected (non-BEACONS) neural network with 64 neurons per layer, and an 8-layer fully-connected (non-BEACONS) neural network with 128 neurons per layer. For this problem, our automated theorem-proving framework proves worst-case L∞ errors of 1.216728 (for the smooth parts of the solution) and 1.483672 (for the non-smooth parts of the solution) for the 6-layer BEACONS architecture, and 1.0 (for the smooth parts of the solution) and 1.259921 (for the non-smooth parts of the solution) for the 8-layer BEACONS architecture 7. In each case, the theorem-prover requires a total of 163 proof steps to establish the bound. The (inferred) solutions from the formally-verified numerical solver, the 6-layer neural network, the 8-layer neural network, the 6-layer BEACONS architecture, and the 8-layer BEACONS architecture, at times t = 0 .5 and t = 0 .8, are shown in Figure 5. We see that the 6-layer neural network significantly overestimates the speed of the right-moving shock, slightly underestimates the speed of the left-moving rarefaction, and exhibits large oscillations/instabilities around the rarefaction wave. The overestimation of the right-moving shock speed is slightly reduced in the 8-layer neural network solution, but only marginally. The 8-layer neural network solution is somewhat more stable overall, but the mis-prediction of the rarefaction wave-speed is still present, and both neural network solutions fail to conserve the advected quantity and exhibit severe diffusion of the qualitative structure of the solution. Both of the BEACONS solutions remain very close to the numerical solution, with both the right-moving shock speed and the left-moving rarefaction speed estimated more-or-less exactly. The 6-layer BEACONS solution exhibits some small overshoots and undershoots in the region surrounding the right-moving shock wave, which are noticeably reduced in the 8-layer BEACONS solution, which appears slightly more diffusive in some respects but also marginally more stable overall. Both BEACONS solutions appear to conserve the advected quantity at least approximately, and do not diffuse the qualitative structure of the solution to any significant extent. Table 4 shows the normalized L2 and L∞ errors (as compared against the formally-verified numerical solution) across the four different neural network architectures, both for the final frame 

> 7It is not a coincidence that these bounds are identical to the bounds proved previously for the 2D linear advection disk problem: it turns out that the relevant smoothness properties of the solution are identical between the two tests.

20 Figure 5: Results for the 1D inviscid Burgers’ “top-hat” initial value problem at times t = 0 .5 (left) and t = 0 .8 (right), obtained using a formally-verified numerical solver (blue), a 6-layer neural network (red), an 8-layer neural network (yellow), a 6-layer BEACONS architecture (purple), and an 8-layer BEACONS architecture (green). Both the 6-layer and 8-layer neural networks significantly overestimate the speed of the right-moving shock and underestimate the speed of the left-moving rarefaction; both neural network solutions fail to conserve the advected quantity, and exhibit significant diffusion of the overall qualitative structure of the solution. Both BEACONS solutions track the numerical solution more-or-less perfectly, without significant instability, wave-speed mis-prediction, or conservation errors. only, and across all frames (using a per-frame average for the L2 error, and normalizing across all frames for the L∞

error). We see overall that both the final-frame and all-frame L2 and L∞ errors are significantly lower for the two BEACONS architectures than for the non-BEACONS architectures. In the non-BEACONS case, we see that increasing the number of layers has the effect of marginally decreasing the L2 and L∞ errors, while in the BEACONS case we see the opposite effect. Table 5 shows the normalized conservation errors (as obtained by integrating over the conserved quantity, and comparing against the integral of the formally-verified numerical solution) across the four different neural network architectures, again both for the final frame only, and integrated across all frames. We see overall that both the final-frame and all-frame conservation errors are significantly lower for the two BEACONS architectures than for the non-BEACONS architectures. Again, in the non-BEACONS case, we see that increasing the number of layers has the effect of significantly decreasing both the final-frame and all-frame conservation errors, while in the BEACONS case, we observe the opposite phenomenon. These findings, wherein the performance of the 8-layer BEACONS architecture appears strictly worse than that of the 6-layer BEACONS architecture, stand in contrast to our previous findings for the linear advection equation, and suggest that the 6-layer BEACONS architecture may be in some sense optimal (or near-optimal) for solving this particular class of non-linear initial value problem. Note that the largest L∞ errors observed for the two BEACONS architectures (i.e. 1.028023 and 0.986979, respectively) still nevertheless remain comfortably below the worst-case bounds (i.e. 1.483672 and 1.259921, respectively). Architecture L∞ Error (Final) L2 Error (Final) L∞ Error (All) L2 Error (All) 6-layer NN 1.310069 15.058654 1.311499 8.144540 8-layer NN 1.208962 12.127816 1.307407 6.691768 6-layer BEACONS 0.986979 2.165441 0.973603 1.554421 8-layer BEACONS 1.028023 2.404486 1.014091 1.636904 Table 4: L2 and L∞ error analysis for the 1D inviscid Burgers’ “top-hat” initial value problem, comparing the 6-layer neural network, 8-layer neural network, 6-layer BEACONS, and 8-layer BEACONS solutions against the formally-verified numerical solution, both for the final predicted frame, and across all frames. The second test problem will consider an extension of the inviscid Burgers’ equation to two dimensions (with identical fluxes in both directions): 

∂u ∂t + u

 ∂u ∂x 



+ u

 ∂u ∂y 



= ∂u ∂t + ∂   12 u2

∂x + ∂   12 u2

∂y = 0 , (86) 21 Architecture Conservation Error (Final) Conservation Error (Total) 6-layer NN 69.348586 2254.019031 8-layer NN 30.707755 1442.490291 6-layer BEACONS 4.459920 -0.569301 8-layer BEACONS 12.788327 -35.540890 Table 5: Conservation analysis for the 1D inviscid Burgers’ “top-hat” initial value problem, comparing the 6-layer neural network, 8-layer neural network, 6-layer BEACONS, and 8-layer BEACONS solutions against the formally-verified numerical solution, both for the final predicted frame, and across all frames. with a square domain [−1.0, 1.0] × [−1.0, 1.0] . Our automated theorem-proving framework is again able to produce full proofs of correctness for both the Lax-Friedrichs and Roe-type finite volume solvers for this equation (including its second-order dimension splitting); hyperbolicity-preservation, CFL stability, and local Lipschitz continuity of the Lax-Friedrichs solver require a total of 179, 231, and 191 proof steps to establish, respectively, while hyperbolicity-preservation and flux conservation of the Roe-type solver require a total of 239 and 375 proof steps to establish, respectively. We evolve a disk described by initial data (equivalent to the previous 2D linear advection case): 

u0 (x, y ) = 

(

1.0, for (x + 0 .5) 2 + ( y + 0 .5) 2 ≤ 0.33 ,

0.0, for (x + 0 .5) 2 + ( y + 0 .5) 2 > 0.33 , (87) starting from the bottom left of the domain, and then smearing out in a “comet” pattern as it moves towards the top right. Once again, we solve this problem using the formally-verified Roe-type solver, a CFL coefficient CCFL = 1 .0,and a spatial resolution of 256 × 256 cells. We evolve the problem numerically up until a final time of t = 1 .0 again, producing 100 frames of output, and now train only two different neural network architectures on the first 33 frames (up until time t = 0 .33 ), and again ask both of them to predict the remainder of the simulation based on the learned solution function u (t, x, y ). The chosen neural network architectures are an 8-layer BEACONS architecture with 128 neurons per layer, and, for means of comparison, an 8-layer fully-connected (non-BEACONS) neural network with 128 neurons per layer. For this problem, our automated theorem-proving framework proves a worst-case L∞ error of 1.483672 for the smooth parts of the solution, but is unable to prove any error bound on the non-smooth parts of the solution 8, for a 6-layer BEACONS architecture. Likewise, the framework proves a worst-case L∞ error of 1.259921 for the smooth parts of the solution, but is unable to prove any error bound on the non-smooth parts of the solution, for the 8-layer BEACONS architecture used here. For both architectures, the theorem-prover requires a total of 315 proof steps to establish a bound in the smooth case, and completes 221 proof steps before concluding that no bound can be proven (using the available methods) in the non-smooth case. The (inferred) solutions from the formally-verified numerical solver, the 8-layer neural network, and the 8-layer BEACONS architecture, at times t = 0 .33 , t = 0 .66 , and t = 0 .99 ,are shown in Figure 6. We see that the 8-layer neural network solution fails to predict the correct “comet” shape of the solution, causing it to appear “pointy” rather than rounded. The 8-layer neural network solution also overestimates the propagation speed of the advected quantity substantially, causing the “comet” to hit the boundary and exit the domain prematurely, prior to the end of the simulation. On the other hand, the 8-layer BEACONS solution correctly predicts both the correct qualitative shape and the correct quantitative propagation speed of the “comet” structure, exhibiting only very slight numerical diffusion surrounding the propagation front as compared to the numerical solution. Table 6 shows the normalized L2 and L∞ errors (as compared against the formally-verified numerical solution) across the two different neural network architectures, both for the final frame only, and across all frames (using a per-frame average for the L2 error, and normalizing across all frames for the L∞ error). We see overall that both the final-frame and all-frame 

L2 and L∞ errors are significantly lower for the BEACONS architecture than for the non-BEACONS architecture. Note again that the largest L∞ error observed for the BEACONS architecture (i.e. 0.595719) remains comfortably below the provable worst-case bound (i.e. 1.259921). 

4.3 Compressible Euler Equations 

Our third and final numerical test case will be to solve the system of compressible Euler equations :

> 8The reason for this is that the theorem-prover concludes, correctly, that in the infinite-time limit of this 2D inviscid Burgers’ disk problem, the shock discontinuity eventually damps to zero, and the solution becomes asymptotically smooth everywhere. Therefore it is unable to prove an error bound for non-smooth parts of the solution, because no non-smooth parts exist asymptotically.

22 Figure 6: Results for the 2D inviscid Burgers’ disk problem at times t = 0 .33 (left), t = 0 .66 (middle), and t = 0 .99 

(right), obtained using a formally-verified numerical solver (top), an 8-layer neural network (middle), and an 8-layer BEACONS architecture (bottom). The 8-layer neural network mis-predicts the shape of the solution, causing it to appear much more “pointy” than it is, while also overestimating the propagation speed significantly, causing the advected quantity to hit the boundary of the domain before the end of the simulation. The BEACONS architecture successfully predicts both the correct shape and the correct propagation speed of the solution, matching the numerical solution more-or-less perfectly. 

∂∂t 

" ρρu ρE 

#

+ ∂∂x 

 ρu ρu 2 + Pu (ρE + P )

 = 0, (88) i.e. a system of conservation laws for mass, momentum, and total energy U = [ ρ, ρu, ρE ]⊺, with non-linear flux 

F (U) = ρu, ρu 2 + P, u (ρE + P )⊺, where we assume an ideal gas equation of state, relating the total energy E and pressure P by: 23 Architecture L∞ Error (Final) L2 Error (Final) L∞ Error (All) L2 Error (All) 8-layer NN 0.976685 37.415877 0.653738 17.396335 8-layer BEACONS 0.319521 2.688830 0.595719 1.888049 Table 6: L2 and L∞ error analysis for the 2D inviscid Burgers’ disk problem, comparing the 8-layer neural network and 8-layer BEACONS solutions against the formally-verified numerical solution, both for the final predicted frame, and across all predicted frames. 

E =

 Pγ − 1



+ 12 ρu 2, ⇔ P =



E − 12 ρu 2



(γ − 1) , (89) where γ ∈ R is an arbitrary (constant) adiabatic index . In contrast to the previous cases, we do not attempt any formal verification of the underlying numerical solvers for these equations; instead, we prove the bounds on the BEACONS architectures conditionally (i.e. subject to the hypothesis that the underlying numerical solvers are correct). The first test problem will be a one-dimensional Riemann problem known as the Sod shock tube , defined over the spatial domain 

[0 .0, 1.0] , with adiabatic index γ = 1 .4, and initial data given by: 

U0 (x) = 

[1 .0, 0.0, 2.5] ⊺ , for x ≤ 0.5,

[0 .125 , 0.0, 0.25] ⊺ , for x > 0.5. (90) As previously, we shall solve this problem numerically using a high-resolution (but now unverified) Roe-type approxi-mate Riemann solver with a CFL coefficient CCFL = 0 .95 , and with a spatial resolution of 2048 cells. We evolve the problem numerically up until a final time of t = 0 .2, producing 200 frames of output in the process. Similar to before, we train four different neural network architectures on the first 66 frames of this output (up until time t = 0 .067 ), and then ask each of them to predict the remainder of the simulation based on the learned solution function U (t, x ). The chosen neural network architectures, as before, are: a 6-layer BEACONS architecture with 64 neurons per layer, an 8-layer BEACONS architecture with 128 neurons per layer, and, for means of comparison, a 6-layer fully-connected (non-BEACONS) neural network with 64 neurons per layer, and an 8-layer fully-connected (non-BEACONS) neural network with 128 neurons per layer. For this problem, our automated theorem-proving framework proves a worst-case 

L∞ error of 0.903602 (for density ρ, across both the smooth and non-smooth parts of the solution) for the 6-layer BEACONS architecture, and 0.707106 (for density ρ, across both the smooth and non-smooth parts of the solution) for the 8-layer BEACONS architecture 9. In each case, the theorem-prover requires a total of 98,532 steps to establish the bound. The (inferred) solutions for ρ (density) from the formally-verified numerical solver, the 6-layer neural network, the 8-layer neural network, the 6-layer BEACONS architecture, and the 8-layer BEACONS architecture, at times t = 0 .1 and t = 0 .2, are shown in Figure 7. We see that the 6-layer neural network significantly underestimates the speed of the left-moving rarefaction, and completely diffuses out the qualitative structure of the contact discontinuity and the right-moving shock, which are not captured at all. The contact and shock waves also fail to be captured by the 8-layer neural network, although the mis-prediction of the left-moving rarefaction speed is now marginally less severe. Both neural networks visibly fail to conserve the mass density of the fluid. Both of the BEACONS solutions correctly estimate the left-moving rarefaction speed more-or-less perfectly, although the 6-layer BEACONS architecture still diffuses out the qualitative structure of the rest of the solution, and in particular fails to capture the contact discontinuity or the right-moving shock. However, the 8-layer BEACONS solution remains very close to the numerical solution, with all three waves (and their corresponding wave-speeds) being captured correctly, with only slight density overshoots and undershoots in the region surrounding the contact discontinuity and the right-moving shock. Both BEACONS solutions appear to conserve the mass density of the fluid at least approximately, with no significant diffusion of the qualitative structure of the solution in the 8-layer BEACONS case. Table 7 shows the normalized L2 and L∞ errors (as compared against the high-resolution numerical solution) across the four different neural network architectures, both for the final frame only, and across all frames (using a per-frame average for the L2 error, and normalizing across all frames for the L∞

error). We see that the final-frame L2 and L∞ errors, along with the the all-frame L2 errors, are significantly lower for the two BEACONS architectures than for the non-BEACONS architectures, although the all-frame L∞ errors remain broadly comparable between the BEACONS and non-BEACONS cases. In the non-BEACONS case, we see that increasing the number of layers has the effect of marginally decreasing the final-frame L2 and L∞ errors, along with the all-frame L2 errors, but has negligible effect on the all-frame L∞ errors. In the BEACONS case, we see that increasing the number of layers has the effect of significantly decreasing both the final-frame and all-frame L2 errors, but has a     

> 9Note that these bounds are identical to the 1D linear advection case described previously, which stems from the fact that the governing equation for density ρis equivalent to the linear advection equation when the fluid velocity uis held constant.

24 Figure 7: Results for the 1D compressible Euler Sod-type shock tube problem at times t = 0 .1 (left) and t = 0 .2

(right), obtained using a high-resolution numerical solver (blue), a 6-layer neural network (red), an 8-layer neural network (yellow), a 6-layer BEACONS architecture (purple), and an 8-layer BEACONS architecture (green). Both the 6-layer and 8-layer neural networks completely diffuse the qualitative structure of the contact discontinuity and the right-moving shock, and significantly underestimate the speed of the left-moving rarefaction. Both BEACONS solutions successfully predict the speed of the left-moving rarefaction; in the 6-layer BEACONS solution again the qualitative structure of the contact discontinuity and the right-moving shock is diffused, but the 8-layer BEACONS solution tracks to numerical solution more-or-less perfectly. negligible (or even marginally deleterious) effect on each of the final-frame or all-frame L∞ errors. Table 8 shows the normalized conservation errors (as obtained by integrating over the conserved quantity, and comparing against the integral of the high-resolution numerical solution) across the four different neural network architectures, again both for the final frame only, and integrated across all frames. We see overall that both the final-frame and all-frame conservation errors are significantly lower for the two BEACONS architectures than for the non-BEACONS architectures. In both the BEACONS and non-BEACONS cases, we see that increasing the number of layers has the effect of decreasing both the final-frame and all-frame conservation errors, although these decreases are altogether more substantial in the BEACONS case. Note that the largest L∞ errors observed for the two BEACONS architectures (i.e. 0.422681 and 0.523623, respectively) remain comfortably below the worst-case bounds (i.e. 0.903602 and 0.707106, respectively). Architecture L∞ Error (Final) L2 Error (Final) L∞ Error (All) L2 Error (All) 6-layer NN 0.230453 4.522916 0.444256 2.075029 8-layer NN 0.159231 3.002094 0.452337 1.546402 6-layer BEACONS 0.076894 1.004591 0.422681 0.758668 8-layer BEACONS 0.080181 0.411615 0.523623 0.328464 Table 7: L2 and L∞ error analysis for the 1D compressible Euler Sod-type shock tube problem, comparing the 6-layer neural network, 8-layer neural network, 6-layer BEACONS, and 8-layer BEACONS solutions against the high-resolution numerical solution, both for the final predicted frame, and across all frames. Architecture Conservation Error (Final) Conservation Error (Total) 6-layer NN 133.878340 6219.332405 8-layer NN 83.996340 5127.036345 6-layer BEACONS 3.343495 281.148077 8-layer BEACONS -0.991722 -211.287987 Table 8: Conservation analysis for the 1D compressible Euler Sod-type shock tube problem, comparing the 6-layer neural network, 8-layer neural network, 6-layer BEACONS, and 8-layer BEACONS solutions against the high-resolution numerical solution, both for the final predicted frame, and across all frames. The second test problem will consider an extension of the compressible Euler equations to two dimensions: 25 ∂∂t 



ρρu ρv ρE 

 + ∂∂x 



ρu ρu 2 + Pρuv u (ρE + P )

 + ∂∂y 



ρv ρvu ρv 2 + Pv (ρE + P )

 = 0, (91) i.e. a system of conservation laws for mass, x-momentum, y-momentum, and total energy 

U = [ ρ, ρu, ρv, ρE ]⊺, with non-linear directional fluxes Fx (U) = ρu, ρu 2 + P, ρuv, u (ρE + P )⊺ and 

Fy (U) = ρv, ρvu, ρv 2 + P, v (ρE + P )⊺, with the natural extension to the ideal gas equation of state (for adiabatic index γ ∈ R): 

E =

 Pγ − 1



+ 12 ρ  u2 + v2 , ⇔ P +



E − 12 ρ  u2 + v2

(γ − 1) . (92) As in the 1D case, we do not attempt any formal verification of the underlying numerical solvers for these equations. We initialize a two-dimensional quadrants problem[ 41 ][ 26 ] on a square domain [0 .0, 1.0] × [0 .0, 1.0] , with four initial discontinuities: 

U0 (x, y ) = 



[1 .5, 0.0, 0.0, 3.75] ⊺ , for x ≥ 0.8 and y ≥ 0.8[0 .5323 , 0.641954 , 0.0, 1.1371] ⊺ , for x < 0.8 and y ≥ 0.8,

[0 .5323 , 0.0, 0.641954 , 1.1371] ⊺ , for x ≥ 0.8 and y < 0.8[0 .138 , 0.166428 , 0.166428 , 0.273212] ⊺ , for x < 0.8 and y < 0.8,

(93) and with adiabatic index γ = 1 .4, yielding a highly complex and intricate solution structure consisting of multiple interacting waves. Once again, we solve this problem using a high-resolution (but unverified) Roe-type approximate Riemann solver, with a CFL coefficient CCFL = 0 .95 , and a spatial resolution of 256 × 256 cells. We evolve the problem numerically up until a final time of t = 0 .8, producing 100 frames of output, and now train only two different neural network architectures on the first 33 frames (up until time t = 0 .27 ), and again ask both of them to predict the remainder of the simulation based on the learned solution function U (t, x, y ). The chosen neural network architectures are an 8-layer BEACONS architecture with 128 neurons per layer, and, for means of comparison, an 8-layer fully-connected (non-BEACONS) neural network with 128 neurons per layer. For this problem, our automated theorem-proving framework proves worst-case L∞ errors of 1.216729 (for density ρ in the smooth parts of the solution) and 1.483672 (for density ρ in the non-smooth parts of the solution) for a 6-layer BEACONS architecture, and 1.0 (for density ρ in the smooth parts of the solution) and 1.259921 (for density ρ in the non-smooth parts of the solution) for the 8-layer BEACONS architecture used here 10 . In each case, the theorem-prover requires a total of 142,104 proof steps to establish the bound. The (inferred) solutions from the high-resolution numerical solver, the 8-layer neural network, and the 8-layer BEACONS architecture, at times t = 0 .27 , t = 0 .53 , and t = 0 .8, are shown in Figure 8. We see that the 8-layer neural network completely fails to capture the qualitative structure of the solution, which is diffused beyond reasonable recognition, and also fails to predict the correct speeds and interactions for many of the relevant waves. In particular, the protruding “ray” and surrounding shock front structure on the top right of the domain, along with most of the internal shock and rarefaction structure in the middle and bottom left of the domain, are completely absent from the 8-layer neural network solution, which also mis-predicts the propagating wave front at the bottom left of the domain as being concave rather than convex in shape. On the other hand, the 8-layer BEACONS solution successfully predicts both the correct qualitative shape and the correct quantitative propagation speeds for all of the relevant waves and their non-linear interactions, exhibiting only an absence of certain fine structures (such as the “finger-like” instabilities present in the numerical solution at late times) due to increased numerical diffusion around the wave fronts. Table 9 shows the normalized L2 and L∞ errors (as compared against the high-resolution numerical solution) across the two different neural network architectures, both for the final frame only, and across all frames (using a per-frame average for the L2, and normalizing across all frames for the L∞ error). We see overall that both the final-frame and all-frame L2

and L∞ errors are significantly lower for the BEACONS architecture than for the non-BEACONS architecture. Note again that the largest L∞ error observed for the BEACONS architecture (i.e. 0.310002) remains comfortably below the worst-case bound (i.e. 1.259921). 

# 5 Concluding Remarks 

One of the overarching philosophical themes of this paper has been that neural networks represent a large and highly general class of fundamental numerical methods, and that they can consequently be rigorously analyzed in much the     

> 10 Again, these bounds are identical to the 2D linear advection case, due to the nature of the governing equation for density ρwhen the fluid velocity uremains constant.

26 Figure 8: Results for the 2D compressible Euler quadrants problem at times t = 0 .27 (left), t = 0 .53 (middle), and 

t = 0 .8 (right), obtained using a high-resolution numerical solver (top), an 8-layer neural network (middle), and an 8-layer BEACONS architecture (bottom). The 8-layer neural network completely diffuses the qualitative structure of the solution and mis-predicts many of the relevant wave-speeds; the protruding “ray” and surrounding structure on the top right of the domain is completely absent, the propagating wave front on the bottom left of the domain is incorrectly predicted as being concave rather than convex, etc. The BEACONS architecture predicts both the correct overall shape and the correct propagation speeds of the constituent waves of the solution, matching the numerical solution more-or-less perfectly (albeit with an absence of certain fine structures and instabilities due to increased numerical diffusion). same way as classical numerical algorithms (such as finite element, finite difference, finite volume, etc. methods), in many cases by using appropriate generalizations of the same mathematical techniques. One reason why such a research program seems not to have been pursued systematically in the past may be a consequence of Sutton’s so-called “bitter lesson”[ 43 ]: incremental improvements in neural network-based methods due to developments in underlying mathematical theory or breakthroughs in architectural design are often dwarfed by much larger improvements resulting simply from increases in computational scale, for instance by training on more data, incorporating larger numbers of parameters, or using deeper architectures. This stands in stark contrast to the case of computational physics, where increases in computational scale allow one to run larger and higher-resolution simulations more quickly, but do not 27 Architecture L∞ Error (Final) L2 Error (Final) L∞ Error (All) L2 Error (All) 8-layer NN 0.546258 25.843028 0.536434 12.315916 8-layer BEACONS 0.198093 2.958570 0.310002 1.539846 Table 9: L2 and L∞ error analysis for the 2D compressible Euler quadrants problem, comparing the 8-layer neural network and 8-layer BEACONS solutions against the high-resolution numerical solution, both for the final predicted frame, and across all predicted frames. lead to intrinsic improvements in the quality of the numerical methods themselves. Thus, developments in classical numerical methods have historically been bottlenecked by the rates of progress in their mathematical foundations and algorithmic design, yet neural network-based methods have hitherto been remarkably successful in evading their analogous limitations through the application of brute computational power. However, if neural network-based methods are to be truly successful in accelerating, supplementing, extending, or perhaps even wholesale replacing conventional numerical simulation codes for the solution of PDEs, then this requirement for a rigorous underlying mathematical theory guaranteeing the correctness (or at least bounding the errors) of their results can no longer be ignored. The success of computational physics derives, to a very great extent, from the reliability of its numerical simulation codes, and the guarantees of convergence, stability, consistency, and correctness that they are able to afford. Until or unless neural network-based methods are able to reach an equivalent level of reliability, the need for classical numerical codes will persist. A neurosymbolic approach, with a neural network-based solver augmented by an automated theorem-prover that is able to verify its correctness properties formally for each individual problem, seems potentially optimal for bridging this gap. We believe that the BEACONS framework, with its machine-checkable proof certificates and its ability to scale to arbitrarily deep architectures via algebraic composability, represents an important first step along this exciting path, as well as the beginning of a large-scale research program in its own right. In the short term, we anticipate extending the BEACONS paradigm to systems of ordinary differential equations (ODEs), as well as to systems of elliptic and parabolic PDEs, for instance by exploiting elliptic regularity theorems[ 14 ] in place of the method of characteristics in order to make a priori predictions regarding the smoothness of solutions. Although in this paper we have focused exclusively on the forward problem of inferring PDE solutions from initial data, we also expect many future applications of BEACONS-based approaches to PDE-constrained (non-convex) optimization[ 23 ]and dimensionality reduction[ 21 ], thereby enabling the construction of formally-verified surrogates for inverse problems 

too. Over a longer time horizon, just as multiple numerical solvers encompassing distinct laws of physics may be coupled together to form more complex multi-physics solvers, one could equally imagine coupling together multiple BEACONS architectures modeling distinct laws of physics to form more complex BEACONS-based foundation models .In particular, one could envision pretraining a large collection of specialized BEACONS architectures for simulating individual laws of physics, and then coupling those architectures together to form a “supernetwork” by means of a mixture-of-experts (MoE) approach[ 34 ]. The finetuning process for such a BEACONS-based foundation model would then consist of training this “supernetwork” to assemble a solution to a complex, multi-physics problem as a non-linear combination of these individual BEACONS solutions, each solving for a separate component of the overall multi-physics system, and with the “supernetwork” then learning how to couple these components together correctly in a fully formally-verified way. Initial experimentation with such an approach has already yielded highly promising results. One of the core principles of the BEACONS philosophy is that the development of classical numerical solvers and neural network-based solvers must necessarily go hand in hand: the formally-verified numerical solver is what enables the bootstrapping of the neural network-based solver via the generation of arbitrary amounts of (certifiably-correct) training data, while the boostrapped neural network-based solver effectively extends the classical numerical solver by extrapolating its solutions into regimes which have not yet been explicitly simulated. Indeed, perhaps the most exciting possibility is the ability of the neural network-based solver to extrapolate into regimes which cannot , even in principle, be explicitly simulated. For example, there may be regions of parameter space which cannot be modeled using any explicit time integration method, because the characteristic time-scales become so short that it would force the stable time-step of any explicit numerical method to “crash” to zero, yet the solution still remains well-defined in a mathematical sense. A BEACONS architecture, having been trained on explicit numerical solutions obtained from the unproblematic regions of the parameter space (in which the stable time-step remains finite), would thereby give one a systematic means of answering the following counterfactual question: what would the simulation code have predicted the solution to be, if it had been able to run in this regime without crashing? Such an ability to answer counterfactual questions regarding otherwise unsimulable parameter regimes, in a mathematically principled way, would be of obvious and enduring importance for computational physics. 28 Since neural networks are, in essence, arbitrarily efficient compressors of algorithmic information, the aforementioned process of bootstrapping a compressed neural network-based solver from an existing numerical simulation code may be regarded as a highly aggressive and non-deterministic form of compiler optimization. Modest amounts of non-determinism and small floating-point inaccuracies are already tolerated as part of standard compiler optimization routines in scientific computing, such as GCC’s -ffast-math flag; conventional neural network architectures merely represent a particularly extreme limit case of the same idea. The BEACONS paradigm may ultimately be thought of as an effort to promote the neural network “compiler” from a lossy, heuristic, and unprincipled one into a lossless, verified, and predictable one. 

# Acknowledgements 

J.G. was partially funded by the Princeton University Research Computing group. J.G., A.H., and J.J. were partially funded by the U.S. Department of Energy under Contract No. DE-AC02-09CH1146 via an LDRD grant. The development of G KEYLL was partially funded, besides the grants mentioned above, by the NSF-CSSI program, Award Number 2209471. No generative artificial intelligence was used in the production of this manuscript, nor in any part of the research described therein. 

# References 

[1] BAADER , F., AND NIPKOW , T. Term rewriting and all that , 1st paperback edition ed. Cambridge University Press, Cambridge New York Melbourne Madrid Cape Town, 1999. [2] BISHOP , C. M. Pattern Recognition and Machine Learning , softcover reprint of the original 1st edition 2006 (corrected at 8th printing 2009) ed. Information science and statistics. Springer New York, New York, NY, 2016. [3] BOMMASANI , R., H UDSON , D. A., A DELI , E., A LTMAN , R., A RORA , S., VON ARX , S., B ERNSTEIN , M. S., BOHG , J., B OSSELUT , A., B RUNSKILL , E., B RYNJOLFSSON , E., B UCH , S., C ARD , D., C ASTELLON , R., CHATTERJI , N., C HEN , A., C REEL , K., D AVIS , J. Q., D EMSZKY , D., D ONAHUE , C., D OUMBOUYA , M., DURMUS , E., E RMON , S., E TCHEMENDY , J., E THAYARAJH , K., F EI -F EI , L., F INN , C., G ALE , T., G ILLESPIE ,L., G OEL , K., G OODMAN , N., G ROSSMAN , S., G UHA , N., H ASHIMOTO , T., H ENDERSON , P., H EWITT , J., HO, D. E., H ONG , J., H SU , K., H UANG , J., I CARD , T., J AIN , S., J URAFSKY , D., K ALLURI , P., K ARAMCHETI ,S., K EELING , G., K HANI , F., K HATTAB , O., K OH , P. W., K RASS , M., K RISHNA , R., K UDITIPUDI , R., KUMAR , A., L ADHAK , F., L EE , M., L EE , T., L ESKOVEC , J., L EVENT , I., L I, X. L., L I, X., M A, T., MALIK , A., M ANNING , C. D., M IRCHANDANI , S., M ITCHELL , E., M UNYIKWA , Z., N AIR , S., N ARAYAN , A., NARAYANAN , D., N EWMAN , B., N IE , A., N IEBLES , J. C., N ILFOROSHAN , H., N YARKO , J., O GUT , G., O RR ,L., P APADIMITRIOU , I., P ARK , J. S., P IECH , C., P ORTELANCE , E., P OTTS , C., R AGHUNATHAN , A., R EICH ,R., R EN , H., R ONG , F., R OOHANI , Y., R UIZ , C., R YAN , J., R É, C., S ADIGH , D., S AGAWA , S., S ANTHANAM ,K., S HIH , A., S RINIVASAN , K., T AMKIN , A., T AORI , R., T HOMAS , A. W., T RAMÈR , F., W ANG , R. E., WANG , W., W U, B., W U, J., W U, Y., X IE , S. M., Y ASUNAGA , M., Y OU , J., Z AHARIA , M., Z HANG , M., ZHANG , T., Z HANG , X., Z HANG , Y., Z HENG , L., Z HOU , K., AND LIANG , P. On the opportunities and risks of foundation models, 2022. [4] BREVIS , I., M UGA , I., AND VAN DER ZEE , K. G. A machine-learning minimal-residual (ML-MRes) framework for goal-oriented finite element discretizations. Computers & Mathematics with Applications 95 (Aug. 2021), 186–199. [5] COCKBURN , B., AND SHU , C.-W. The Local Discontinuous Galerkin Method for Time-Dependent Convection-Diffusion Systems. SIAM Journal on Numerical Analysis 35 , 6 (Dec. 1998), 2440–2463. [6] COCKBURN , B., AND SHU , C.-W. The Runge–Kutta Discontinuous Galerkin Method for Conservation Laws V. 

Journal of Computational Physics 141 , 2 (Apr. 1998), 199–224. [7] COURANT , R., F RIEDRICHS , K., AND LEWY , H. On the Partial Difference Equations of Mathematical Physics. 

IBM Journal of Research and Development 11 , 2 (Mar. 1967), 215–234. [8] COURANT , R., AND HILBERT , D. Methods of mathematical physics. 2: Partial differential equations / by R. Courant. 2. repr ed. Wiley-VCH, Weinheim, 2009. [9] DEBNATH , L. Nonlinear Partial Differential Equations for Scientists and Engineers . Birkhäuser Boston, Boston, 2012. [10] D EVORE , R. A. Nonlinear approximation. Acta Numerica 7 (Jan. 1998), 51–150. [11] EIRAS , F., B IBI , A., B UNEL , R., D VIJOTHAM , K. D., T ORR , P., AND KUMAR , M. P. Efficient error certification for physics-informed neural networks, 2024. 29 [12] ERNST , L., R EKATSINAS , N., AND URBAN , K. A posteriori certification for neural network approximations to pdes, 2025. [13] FELLEISEN , M., F INDLER , R. B., F LATT , M., K RISHNAMURTHI , S., B ARZILAY , E., M CCARTHY , J., AND 

TOBIN -H OCHSTADT , S. The Racket Manifesto. In 1st Summit on Advances in Programming Languages (SNAPL 2015) (Dagstuhl, Germany, 2015), T. Ball, R. Bodik, S. Krishnamurthi, B. S. Lerner, and G. Morriset, Eds., vol. 32 of Leibniz International Proceedings in Informatics (LIPIcs) , Schloss Dagstuhl – Leibniz-Zentrum für Informatik, pp. 113–128. [14] FERNÁNDEZ -R EAL , X., AND ROS -O TON , X. Regularity theory for elliptic PDE . Zurich lectures in advanced mathematics. European Mathematical Society [EMS] Publishing House GmbH, Berlin, Germany, 2022. [15] FONG , B., AND SPIVAK , D. I. An Invitation to Applied Category Theory: Seven Sketches in Compositionality ,1 ed. Cambridge University Press, July 2019. [16] G AVRANOVI ´ C, B. Compositional deep learning, 2019. [17] GAVRANOVI ´ C, B., L ESSARD , P., D UDZIK , A., VON GLEHN , T., A RAÚJO , J. G. M., AND VELI ˇ CKOVI ´ C, P. Position: Categorical deep learning is an algebraic theory of all architectures, 2024. [18] G ORARD , J. A functorial perspective on (multi)computational irreducibility, 2022. [19] GORARD , J. Applied category theory in the wolfram language using categorica i: Diagrams, functors and fibrations, 2024. [20] GORARD , J., AND HAKIM , A. Shock with confidence: Formal proofs of correctness for hyperbolic partial differential equation solvers, 2025. [21] GORARD , J., H AKIM , A., Q IN , H., P ARFREY , K., AND JHA , S. Improved dimensionality reduction for inverse problems in nuclear fusion and high-energy astrophysics, 2025. [22] HARTEN , A. High Resolution Schemes for Hyperbolic Conservation Laws. Journal of Computational Physics 135 , 2 (Aug. 1997), 260–278. [23] JAIN , P. Non-Convex Optimization for Machine Learning . No. v.32 in Foundations and Trends® in Machine Learning Ser. Now Publishers, Norwell, MA, 2017. [24] KIM , J., L EE , K., L EE , D., J HIN , S. Y., AND PARK , N. DPM: A Novel Training Method for Physics-Informed Neural Networks in Extrapolation. Proceedings of the AAAI Conference on Artificial Intelligence 35 , 9 (May 2021), 8146–8154. [25] LAX , P. D. Weak solutions of nonlinear hyperbolic equations and their numerical computation. Communications on Pure and Applied Mathematics 7 , 1 (Feb. 1954), 159–193. [26] LEVEQUE , R. J. Wave Propagation Algorithms for Multidimensional Hyperbolic Systems. Journal of Computa-tional Physics 131 , 2 (Mar. 1997), 327–353. [27] LEVEQUE , R. J. Finite volume methods for hyperbolic problems , 10. printing ed. Cambridge texts in applied mathematics. Cambridge Univ. Press, Cambridge, 2011. [28] MCGREIVY , N., AND HAKIM , A. Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations. Nature Machine Intelligence 6 , 10 (Sept. 2024), 1256–1269. [29] MHASKAR , H., AND KHACHIKYAN , L. Neural networks for function approximation. In Proceedings of 1995 IEEE Workshop on Neural Networks for Signal Processing (Cambridge, MA, USA, 1995), IEEE, pp. 21–29. [30] M HASKAR , H., L IAO , Q., AND POGGIO , T. Learning functions: When is deep better than shallow, 2016. [31] MHASKAR , H. N. Neural Networks for Optimal Approximation of Smooth and Analytic Functions. Neural Computation 8 , 1 (Jan. 1996), 164–177. [32] MHASKAR , H. N., AND POGGIO , T. Deep vs. shallow networks: An approximation theory perspective. Analysis and Applications 14 , 06 (Nov. 2016), 829–848. [33] MINGARD , C., R EES , H., V ALLE -P ÉREZ , G., AND LOUIS , A. A. Deep neural networks have an inbuilt Occam’s razor. Nature Communications 16 , 1 (Jan. 2025), 220. [34] MU, S., AND LIN , S. A comprehensive survey of mixture-of-experts: Algorithms, theory, and applications, 2026. [35] OSHER , S. Riemann Solvers, the Entropy Condition, and Difference. SIAM Journal on Numerical Analysis 21 , 2 (Apr. 1984), 217–235. [36] PINKUS , A. Approximation theory of the MLP model in neural networks. Acta Numerica 8 (Jan. 1999), 143–195. 30 [37] RAISSI , M., P ERDIKARIS , P., AND KARNIADAKIS , G. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics 378 (Feb. 2019), 686–707. [38] ROBINSON , J. A., AND VORONKOV , A. Handbook of automated reasoning . Elsevier MIT Press, Amsterdam New York Cambridge, Mass, 2001. [39] ROE , P. Approximate Riemann Solvers, Parameter Vectors, and Difference Schemes. Journal of Computational Physics 135 , 2 (Aug. 1997), 250–258. [40] ROE , P. L. Characteristic-Based Schemes for the Euler Equations. Annual Review of Fluid Mechanics 18 , 1 (Jan. 1986), 337–365. [41] SCHULZ -R INNE , C. W., C OLLINS , J. P., AND GLAZ , H. M. Numerical Solution of the Riemann Problem for Two-Dimensional Gas Dynamics. SIAM Journal on Scientific Computing 14 , 6 (Nov. 1993), 1394–1414. [42] STRANG , G. On the Construction and Comparison of Difference Schemes. SIAM Journal on Numerical Analysis 5, 3 (Sept. 1968), 506–517. [43] S UTTON , R. The bitter lesson, 2019. URL http://www. incompleteideas. net/IncIdeas/BitterLesson. html (2019). [44] TORO , E. F. Riemann solvers and numerical methods for fluid dynamics: a practical introduction , 3rd ed ed. Springer, Dordrecht New York, 2009. [45] WANG , S., AND PERDIKARIS , P. Long-time integration of parametric evolution equations with physics-informed DeepONets. Journal of Computational Physics 475 (Feb. 2023), 111855. [46] WESSELING , P. Principles of Computational Fluid Dynamics , vol. 29 of Springer Series in Computational Mathematics . Springer Berlin Heidelberg, Berlin, Heidelberg, 2001. 31