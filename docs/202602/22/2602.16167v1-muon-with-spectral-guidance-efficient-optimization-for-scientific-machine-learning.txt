Title: Muon with Spectral Guidance: Efficient Optimization for Scientific Machine Learning

URL Source: https://arxiv.org/pdf/2602.16167v1

Published Time: Thu, 19 Feb 2026 01:24:27 GMT

Number of Pages: 19

Markdown Content:
# MUON WITH SPECTRAL GUIDANCE : E FFICIENT OPTIMIZATION FOR SCIENTIFIC MACHINE LEARNING 

Binghang Lu ‚àó

Elmore Family School of Electrical and Computer Engineering Purdue University West Lafayette, IN 47907, USA 

Jiahao Zhang ‚àó

Department of Mathematics Purdue University West Lafayette, IN 47907, USA 

Guang Lin ‚Ä†

Department of Mathematics & School of Mechanical Engineering Purdue University West Lafayette, IN 47907, USA February 19, 2026 

# ABSTRACT 

Physics-informed neural networks and neural operators often suffer from severe optimization diffi-culties caused by ill-conditioned gradients, multi-scale spectral behavior, and stiffness induced by physical constraints. Recently, the Muon optimizer has shown promise by performing orthogonalized updates in the singular-vector basis of the gradient, thereby improving geometric conditioning. How-ever, its unit‚Äìsingular-value updates may lead to overly aggressive steps and lack explicit stability guarantees when applied to physics-informed learning. In this work, we propose SpecMuon , a spectral-aware optimizer that integrates Muon‚Äôs orthogonalized geometry with a mode-wise relaxed scalar auxiliary variable (RSAV) mechanism. By decomposing matrix-valued gradients into singu-lar modes and applying RSAV updates individually along dominant spectral directions, SpecMuon adaptively regulates step sizes according to the global loss energy while preserving Muon‚Äôs scale-balancing properties. This formulation interprets optimization as a multi-mode gradient flow and enables principled control of stiff spectral components. We establish rigorous theoretical properties of SpecMuon, including a modified energy dissipation law, positivity and boundedness of auxiliary variables, and global convergence with a linear rate under the Polyak‚Äì≈Åojasiewicz condition. Numeri-cal experiments on physics-informed neural networks, DeepONets, and fractional PINN‚ÄìDeepONets demonstrate that SpecMuon achieves faster convergence and improved stability compared with Adam, AdamW, and the original Muon optimizer on benchmark problems such as the one-dimensional Burgers‚Äô equation and fractional partial differential equations. 

K eywords Scientific Machine Learning ¬∑ Physics-Informed Neural Networks ¬∑ Neural Operators ¬∑ Spectral Optimization ¬∑ Scalar Auxiliary Variable ¬∑ Parametric PDEs 

# 1 Introduction 

The rapid development of deep learning has profoundly influenced scientific computing, offering new paradigms for modeling, simulation, and inverse problems governed by partial differential equations (PDEs). In particular, physics-informed neural networks (PINNs) [ 1, 2, 3] and neural operators, such as DeepONets [ 4 , 5 ] and Fourier Neural Operators [ 6 , 7], provide flexible frameworks for embedding physical laws into learning architectures or approximating solution operators between infinite-dimensional function spaces. These approaches have demonstrated considerable 

> ‚àó

Equal contribution. 

> ‚Ä†

Corresponding author: guanglin@purdue.edu 

> arXiv:2602.16167v1 [cs.LG] 18 Feb 2026

A PREPRINT - F EBRUARY 19, 2026 promise in accelerating simulations, enabling data-efficient learning, and handling high-dimensional problems that are challenging for traditional numerical solvers. Despite these successes, optimization remains a central bottleneck in physics-informed machine learning [ 8, 9]. Unlike standard supervised learning tasks in computer vision or natural language processing, the loss landscapes arising from physics constraints are often highly ill-conditioned, characterized by stiff gradients, strong anisotropy, and pronounced multi-scale spectral structure. The simultaneous presence of data-fitting terms, PDE residuals, and boundary or initial conditions can induce severe curvature imbalance across parameter directions, frequently leading to slow convergence, instability, or sensitivity to hyperparameter choices when using standard first-order optimizers such as stochastic gradient descent or Adam [ 10 ]. These difficulties become even more pronounced in operator learning settings, where matrix-valued parameters and coupled subnetworks further complicate the optimization geometry [11, 12, 13]. To mitigate these challenges, recent research has explored optimization methods that incorporate geometric or spectral information beyond simple coordinate-wise scaling [ 14 , 15 , 16 ]. Among these, the Muon optimizer [ 17 ] has attracted attention for its distinctive update strategy, which operates in the singular-vector basis of gradient matrices rather than the ambient parameter coordinates [ 17 ]. By orthogonalizing updates and equalizing singular values, Muon effectively reshapes the optimization geometry and reduces interference between directions with disparate curvature [ 18 ]. Although originally developed for large-scale language and vision models, Muon‚Äôs geometric perspective is particularly appealing for scientific machine learning, where matrix-valued parameters and spectral imbalance are ubiquitous. However, the standard Muon update deliberately discards singular value magnitudes, prioritizing geometric displacement over energy control. While this normalization stabilizes the spectrum of the update, it may also produce overly aggressive steps along dominant modes and lacks explicit mechanisms to ensure stability or monotonic decrease of the training loss. In contrast, the scalar auxiliary variable (SAV) methodology, originally developed for gradient flows in computational physics, offers a complementary perspective [ 19 , 20 , 21 ]. By augmenting the system with an auxiliary scalar tied to the energy, SAV-based schemes guarantee unconditional stability and modified energy dissipation. Recent adaptations of SAV ideas to neural network optimization [ 22 , 23 ] have demonstrated that energy-based mechanisms can improve robustness and convergence behavior [24, 25, 26], yet these approaches typically operate on aggregated gradients and do not explicitly address the spectral structure inherent in physics-informed models. This observation reveals a fundamental gap in current optimization strategies: geometric conditioning and energy stability are often treated separately, despite both being critical for efficient training in stiff, physics-constrained settings. In this work, we seek to bridge this gap by introducing a spectral-aware optimization framework that unifies orthogonalized geometry with principled energy control. Our central idea is to view optimization as a multi-mode gradient flow, where different spectral components of the gradient evolve under distinct stability constraints. Motivated by this perspective, we propose SpecMuon, a novel optimizer that integrates Muon‚Äôs singular-vector-based updates with a mode-wise relaxed scalar auxiliary variable (RSAV) mechanism [ 27 ]. By decomposing matrix-valued gradients into their dominant singular directions and applying RSAV updates individually to these modes, SpecMuon adaptively regulates step sizes according to the global loss energy while preserving Muon‚Äôs scale-balancing properties. This design enables selective damping of stiff spectral components without sacrificing rapid progress along well-conditioned directions. The main contributions of this paper are summarized as follows: ‚Ä¢ Algorithm Design: We introduce SpecMuon, a spectral-aware optimizer that combines orthogonalized gradient updates with a mode-wise RSAV mechanism, providing a unified treatment of geometric conditioning and energy stability. ‚Ä¢ Theoretical Analysis: We establish rigorous theoretical guarantees for the proposed method, including a modified energy dissipation law, positivity and boundedness of auxiliary variables, and global convergence with a linear rate under the Polyak‚Äì≈Åojasiewicz condition. ‚Ä¢ Numerical Validation: We demonstrate the effectiveness of SpecMuon through extensive numerical experi-ments on physics-informed neural networks, DeepONets, and fractional PINN‚ÄìDeepONets, showing improved convergence speed and stability compared with Adam, AdamW, and the original Muon optimizer. The remainder of this paper is organized as follows. Section 2 reviews the Muon optimizer and the scalar auxiliary variable framework. Section 3 presents the SpecMuon methodology and its theoretical analysis. Numerical experiments are reported in Section 4, and concluding remarks and future directions are given in Section 5. 2A PREPRINT - F EBRUARY 19, 2026 

# 2 Background and Related Work 

We consider the unconstrained optimization problem 

min  

> Œò

f (Œò) , Œò = {W (‚Ñì) ‚àà Rm‚Ñì√ón‚Ñì }L‚Ñì=1 ,

where f denotes the neural-network training loss. Although our numerical studies come from scientific computing (e.g., PDE-driven models), the method operates directly on any matrix-valued parameters optimization problem. 

2.1 Muon 

Let G(‚Ñì) := ‚àáW (‚Ñì) f (Œò) ‚àà Rm‚Ñì√ón‚Ñì be the block gradient. Its thin singular value decomposition (SVD) is 

G(‚Ñì) = U (‚Ñì)Œ£(‚Ñì) V (‚Ñì)‚ä§,

and we define the Frobenius-orthonormal rank-one directions 

Q(‚Ñì) 

> i

:= u(‚Ñì)

> i

 v(‚Ñì)

> i

‚ä§, ‚ü®Q(‚Ñì) 

> i

, Q (‚Ñì) 

> j

‚ü©F = Œ¥ij , ‚à•Q(‚Ñì) 

> i

‚à•F = 1 .

The original Muon step replaces the singular values by ones and moves in the orthogonalized direction spanned by the singular vectors. With stepsize Œ∑ > 0 and, in practice, a rank budget k‚Ñì ‚â§ rank( G(‚Ñì)), the update reads 

‚àÜW (‚Ñì) 

> Muon

= ‚àí Œ∑

> k‚Ñì

X

> i=1

Q(‚Ñì) 

> i

= ‚àí Œ∑ U (‚Ñì) V (‚Ñì)‚ä§.

This ‚Äúunit-SV‚Äù update preserves the singular vectors and discards the magnitudes, which stabilizes the spectrum of the step and reduces interference between directions with disparate curvature. In practice, Muon implements the unit-SV transformation without an explicit SVD. It approximates the polar factor of a normalized update matrix via a few iterations of the Newton‚ÄìSchulz method, e.g., 

Xt+1 = 32 Xt ‚àí 12 XtX‚ä§ 

> t

Xt,

which drives the singular values of Xt toward 1 while preserving the singular vectors. This yields an efficient orthogonalized direction on modern hardware. 

2.2 Scalar auxiliary variable (SAV) and the relaxed variant (RSAV) 

Consider the gradient flow for the loss 

ÀôŒò( t) = ‚àí ‚àá f  Œò( t),

and introduce a fixed shift Œ∫ > 0 so that f (Œò) + Œ∫ > 0. Define the scalar auxiliary variable: 

r(t) := 

q

f  Œò( t) + Œ∫ . 

The SAV idea couples Œò and r through the extended system 

ÀôŒò( t) + r(t)

p f (Œò( t)) + Œ∫ ‚àáf  Œò( t) = 0 , Àôr(t) = 12p f (Œò( t)) + Œ∫ ‚àáf  Œò( t)‚ä§ ÀôŒò( t). (2.1) The pair (2.1) is equivalent to the original gradient flow when r(t) = pf (Œò( t)) + Œ∫ holds exactly, yet it exposes a simple Lyapunov structure. 

Theorem 2.1 (Modified energy law) . Let (Œò( t), r (t)) satisfy (2.1) . Then the modified energy r(t)2 dissipates: 

ddt r(t)2 = ‚àí r(t)2

f (Œò( t)) + Œ∫ ‚àáf  Œò( t) 2 

> F

‚â§ 0.

Proof. Multiply the second equation in (2.1) by 2r(t) to get 

2r Àôr = r

‚àöf + Œ∫ ‚àáf ‚ä§ ÀôŒò.

3A PREPRINT - F EBRUARY 19, 2026 Combine it with the first equation, ÀôŒò = ‚àí r‚àöf +Œ∫ ‚àáf , to obtain: 

2r Àôr = ‚àí r2

f + Œ∫ ‚à•‚àá f ‚à•2 

> F

,ddt r2 ‚â§ 0.

Now let h > 0 be a time step and denote G(Œò) := ‚àáf (Œò) and E(Œò) := pf (Œò) + Œ∫. A first-order explicit SAV step is obtained by evaluating G at Œòk and by predicting r with a scalar decoupling: 

Àúr k+1 = rk

1 + h

2

‚à•G(Œò k)‚à•2

> F

 E(Œò k)2

, (2.2) 

Œòk+1 = Œò k ‚àí h Àúr k+1 

E(Œò k) G(Œò k). (2.3) Formula (2.2) comes from discretizing the second equation in (2.1) and substituting the discrete version of (2.3) to eliminate Œòk+1 , which yields a scalar correction for r. The pair (2.2) ‚Äì(2.3) decreases the discrete modified energy r2

for any h > 0.In [ 27 ], the SAV predictor Àúr k+1 can also be directly connected to the current loss value E(Œò k+1 ). RSAV enforces alignment by a convex relaxation: 

Àúr k+1 = rk

1 + h

2

‚à•G(Œò k)‚à•2

> F

 E(Œò k)2

, (2.4) 

Œòk+1 = Œò k ‚àí h Àúr k+1 

E(Œò k) G(Œò k), (2.5) 

rk+1 = Œæk Àúr k+1 +  1 ‚àí Œæk E(Œò k+1 ), Œæk ‚àà [0 , 1] . (2.6) The relaxation parameter Œæk is chosen to guarantee a discrete dissipation law. Let 

Dk+1 := 1

h Œòk+1 ‚àí Œòk 2 

> F

.

Fix a tolerance œà ‚àà (0 , 1) (e.g., œà = 0 .95 ). Select Œæ as the smallest root in [0 , 1] of the quadratic inequality 

 Œæ Àúr k+1 + (1 ‚àí Œæ)E(Œò k+1 )2 ‚àí  Àúr k+1 2 ‚àí  Àúr k+1 ‚àí r k2 ‚â§ œà D k+1 , (2.7) which has the explicit solution 

Œæk = max 

(

0, ‚àí b ‚àí ‚àö b2 ‚àí 4ac 

2a

)

,

with explicit coefficients a, b, c defined by 

a =  Àúr k+1 ‚àí E(Œò k+1 )2,b = 2 E(Œò k+1 ) Àúr k+1 ‚àí E(Œò k+1 ),c = f (Œò k+1 ) + Œ∫ ‚àí  Àúr k+1 2 ‚àí œà D k+1 .

Theorem 2.2 (Discrete energy dissipation for RSAV predictor and RSAV) . Fix h > 0 and Œ∫ > 0. Let 

Ek := 

q

f (Œò k) + Œ∫, Gk := ‚àáf (Œò k).

Assume the RSAV predictor updates following (2.4) and (2.5) . Define the discrete dissipation term 

Dk+1 := 1

h Œòk+1 ‚àí Œòk 2 

> F

.

4A PREPRINT - F EBRUARY 19, 2026 

Then the predictor satisfies  Àúr k+1 2 ‚àí  rk2 ‚â§ ‚àí Dk+1 ‚â§ 0. (2.8) 

Moreover, if the relaxation is chosen so that for some œà ‚àà [0 , 1) satisfying (2.6) and (2.7) , then 

 rk+1 2 ‚àí  rk2 ‚â§ ‚àí (1 ‚àí œà) Dk+1 ‚â§ 0. (2.9) 

Proof. From (2.5), 

Œòk+1 ‚àí Œòk = ‚àí h Àúr k+1 

Ek

Gk, =‚áí Dk+1 = 1

h ‚àí h Àúr k+1  

> Ek

Gk 

> 2
> F

= h  Àúr k+1 2 ‚à•Gk‚à•2

> F

E2

> k

.

On the other hand, the first predictor formula in (2.4) implies 

Àúr k+1 ‚àí rk = ‚àí h

2

‚à•Gk‚à•2

> F

E2

> k

Àúr k+1 .

Multiply both sides by ‚àí2Àú r k+1 to get the identity 

‚àí 2 Àúr k+1 ‚àí rk Àúr k+1 = h  Àúr k+1 2 ‚à•Gk‚à•2

> F

E2

> k

= Dk+1 . (2.10) Note that Àúr k+1 < r k because the denominator in (2.4) exceeds 1. Hence 

 Àúr k+1 2 ‚àí  rk2 =  Àúr k+1 ‚àí rk  Àúr k+1 + rk ‚â§  Àúr k+1 ‚àí rk   2Àú r k+1  = ‚àí Dk+1 ,

where the last equality uses (2.10). This proves (2.8). By assumption (2.7), we have 

 rk+1 2 ‚àí  Àúr k+1 2 ‚â§ œà D k+1 +  Àúr k+1 ‚àí rk2.

Add this inequality to the exact identity  Àúr k+1 2 ‚àí  rk2 = ‚àí Dk+1 ‚àí  Àúr k+1 ‚àí rk2 (from (2.10)) to obtain 

 rk+1 2 ‚àí  rk2 ‚â§ ‚àí (1 ‚àí œà) Dk+1 ,

which is (2.9). Because œà ‚àà [0 , 1) and Dk+1 ‚â• 0, this yields monotone decay of the modified energy r2.

# 3 Methodology 

The core idea of the proposed method is to decompose each matrix gradient into its singular directions and update mode by mode. Let 

G = U Œ£V ‚ä§ = X

> i

œÉi uiv‚ä§ 

> i

, Qi := uiv‚ä§ 

> i

.

A standard Muon step replaces the singular values by ones and moves in the orthogonalized direction: 

‚àí Œ∑ X

> i

Qi.

Equivalently, since G = P 

> i

œÉiQi, we can write 

‚àí Œ∑ X

> i

Qi = ‚àí X

> i

 Œ∑œÉi



œÉiQi,

which is a projected gradient step in the original spectral basis with per-mode learning rate 

hi = Œ∑œÉi

.

Thus a single Muon update can be viewed as a coordinated collection of independent micro-steps along the decoupled singular directions. 5A PREPRINT - F EBRUARY 19, 2026 

3.1 SpecMuon Algorithm 

We now present the proposed update in a form parallel to Section 2.2 and consistent with the notation of Section 2. Fix a matrix block W and an iteration index k. Define 

Ek := 

q

f (Œò k) + Œ∫ , Gk := ‚àáW f (Œò k).

Following Section 2.1, decompose the block gradient by a (truncated) SVD, 

Gk = U kŒ£k(V k)‚ä§ =

> kr

X

> i=1

œÉ ki Q ki , Q ki := u ki (v ki )‚ä§,

where the rank-one matrices {Q ki } are Frobenius-orthonormal: ‚ü®Q ki , Q kj ‚ü©F = Œ¥ij and ‚à•Q ki ‚à•F = 1 . This orthonormal-ity allows us to treat each singular direction as a scalar mode at the current iterate. A Muon step sets singular values to one and updates as ‚àíŒ∑ P 

> i

Q ki . A projected-gradient step in the original spectral basis reads ‚àí P 

> i

h ki œÉ ki Q ki . Matching coefficients mode by mode gives h ki œÉ ki = Œ∑, i.e., h ki = Œ∑/œÉ ki . Large singular values therefore receive smaller effective steps, while small singular values receive larger ones, reproducing Muon‚Äôs scale balancing in the spectral basis. We therefore set 

h ki := Œ∑œÉ ki

, Œ∑ > 0, g ki := œÉ ki

Ek .

To each singular direction Q ki we attach a scalar auxiliary variable r ki , initialized by r 0 

> i

= E0. This is the per-mode analogue of the single rk in Section 2.2, now aligned with the singular basis. Before moving the parameter, we form the RSAV predictor in each singular direction. Specializing the SAV decoupling of Section 2.2 to step h ki and direction Q ki

gives 

Àúr k+1  

> i

= r ki

1 + h ki

2 (g ki )2

= r ki

1 + Œ∑

2

œÉ ki

(Ek)2

.

Note that there is no matrix norm appears because ‚à•Q ki ‚à•F = 1 . The denominator depends only on œÉ ki and Ek.Once Àúr k+1  

> i

is computed, we advance the parameter along the singular directions: 

W k+1 = W k ‚àí Œ∑EkkrX

> i=1

Àúr k+1  

> i

Q ki .

Equivalently, in each mode the projected increment satisfies 

W k+1 ‚àí W k, Q ki F = ‚àí h ki Àúr k+1  

> i

g ki ,

and orthonormality ensures that contributions from different modes add without cross terms. After computing W k+1 , we evaluate the new energy root Ek+1 := p f (Œò k+1 ) + Œ∫ and align each auxiliary variable with the current loss level through the RSAV relaxation: 

r k+1  

> i

= Œæ ki Àúr k+1  

> i

+  1 ‚àí Œæ ki

 Ek+1 , Œæ ki ‚àà [0 , 1] .

The coefficient Œæ ki is chosen by the same short quadratic rule as in equation (2.7) , applied per mode with the quantities 

(Àú r k+1  

> i

, E k+1 , h ki , g ki ). The predictor provides a stable provisional magnitude and the relaxation uses information at the updated iterate to keep r k+1  

> i

tied to the current loss scale. At iteration k+1 the SVD is recomputed on the fresh gradient 

Gk+1 , yielding {Q k+1  

> i

, œÉ k+1  

> i

}; the relaxed values {r k+1  

> i

} serve as inputs to the new predictors, and the cycle repeats. The proposed algorithm is summarized in Algorithm 1 for implementation purpose. Note that the theoretical analysis in Sections 3.1‚Äì3.3 focuses on the core Muon-RSAV update without momentum, gradient normalization, or heuristic stabilizers. Algorithm 1 includes these additional components for practical performance and numerical robustness. As is standard in optimization analysis, the theoretical results describe the underlying update mechanism, while the full algorithm reflects implementation-level enhancements. 6A PREPRINT - F EBRUARY 19, 2026 

Algorithm 1 SpecMuon 

Require: Learning rate Œ∑, momentum Œº, top singular directions k, SAV smoothing factor Œæ, epsilon œµ 

> 1:

Initialize momentum buffer B0 ‚Üê 0 

> 2:

Initialize SAV state vector r0 ‚Üê ‚àöL0 ¬∑ 1 

> 3:

for t = 1 , . . . do  

> 4:

Compute gradient Gt ‚Üê ‚àá Œ∏ Lt(Œ∏t‚àí1) 

> 5:

Normalize gradient ÀÜGt ‚Üê Gt/(‚à•Gt‚à•F + œµ) 

> 6:

Compute SVD: U, S, V T ‚Üê SVD( ÀÜGt) 

> 7:

Initialize update direction Ot ‚Üê 0 

> 8:

// SAV update for top-k directions  

> 9:

for j = 1 , . . . , k do  

> 10:

uj , s j , v Tj ‚Üê j-th singular components from U, S, V T 

> 11:

Compute SAV learning rate Œ∑‚Ä≤ 

> j

‚Üê Œ∑/ (sj + œµ) 

> 12:

dg ‚Üê (sj ¬∑ uj vTj )/(‚àöLt + œµ) 

> 13:

rnew  

> j

‚Üê rt‚àí1,j /(1 + 0 .5 ¬∑ Œ∑‚Ä≤ 

> j

¬∑ ‚à• dg ‚à•F ) 

> 14:

Ot ‚Üê Ot + ( rnew  

> j

/(‚àöLt + œµ)) ¬∑ uj vTj 

> 15:

// Update r for next step  

> 16:

T ‚Üê (1 ‚àí Œæ)( rnew  

> j

)2 + Œæ(rt‚àí1,j )2 + (1 ‚àí Œæ)( rnew  

> j

‚àí rt‚àí1,j )2 

> 17:

œá ‚Üê (‚àöLt ‚àí ‚àöT )/(‚àöLt ‚àí rnew  

> j

+ œµ) 

> 18:

rt,j ‚Üê clamp( œá, 0, 1) ¬∑ rnew  

> j

+ (1 ‚àí clamp( œá, 0, 1)) ¬∑ ‚àöLt 

> 19:

end for  

> 20:

// Muon update for remaining directions  

> 21:

Uk:, S k:, V Tk: ‚Üê remaining singular components  

> 22:

Ot ‚Üê Ot + Uk: diag( Sk:) V Tk: 

> 23:

// Apply momentum and update parameters  

> 24:

Bt ‚Üê ŒºB t‚àí1 + Ot 

> 25:

Update parameters Œ∏t ‚Üê Œ∏t‚àí1 ‚àí Œ∑B t 

> 26:

end for  

> 27:

return Œ∏t

3.2 Computational efficiency 

The update in Section 3.1 is defined mode by mode in the singular basis. In practice, we keep only a small number of dominant modes. We therefore adopt a truncated spectral budget kr (‚Äútop-k‚Äù singular directions and implemented as 

rtop in Algorithem 1) and a matching truncation of auxiliary variables r ki . This preserves Muon‚Äôs geometric benefits while keeping the additional RSAV overhead negligible. For a block W ‚àà Rm√ón the dominant costs are: ‚Ä¢ Top-kr SVD / orthogonalization. A randomized range finder followed by a small SVD has complexity 

O(mn k r + ( m+n)k2 

> r

). When Muon uses a polar (Newton‚ÄìSchulz) iteration, we still require only a thin orthonormal basis. The cost is linear in kr once that basis is formed. ‚Ä¢ Per-mode RSAV algebra. The predictor Àúr k+1  

> i

and the relaxation for each retained mode are scalar operations, for a total of O(kr ) time and memory. ‚Ä¢ Aggregated parameter update. Forming Pkr 

> i=1

Àúr k+1  

> i

Q ki is a linear combination and is typically dominated by backprop to obtain Gk.Hence, relative to Muon, the incremental work of RSAV is O(kr ) scalars plus one evaluation of Ek+1 =pf (Œò k+1 ) + Œ∫. Note that two structural properties justify this truncation. Firstly, in many scientific-computing models the gradient spectrum is skewed, that is, a few singular directions carry most of the energy. Secondly, the RSAV equations are independent per mode. Each update uses only (œÉ ki , E k, r ki ) and the orthonormal direction Q ki . Dropping the tail therefore removes small-magnitude contributions without introducing cross-mode errors at the current iterate (orthonormality eliminates cross terms). If neglected directions later become important, they automatically re-enter when their singular values appear in the top-kr .7A PREPRINT - F EBRUARY 19, 2026 Truncating both the spectral basis (to kr modes) and the auxiliaries (to {ri}kr

> i=1

) yields an update whose cost is essentially that of Muon plus O(kr ) scalar work. The method captures the dominant geometry, adapts per-mode magnitudes via RSAV with negligible overhead, and scales gracefully with model size. 

3.3 Analysis of the Algorithm 

At each iterate k, the gradient of a matrix block admits the orthonormal singular basis Gk = Pkr 

> i=1

œÉ ki Q ki with 

‚ü®Q ki , Q kj ‚ü©F = Œ¥ij , and our update in Section 3.1 applies the RSAV predictor and relaxation per mode using the Muon-induced steps h ki = Œ∑/œÉ ki . Because these directions are Frobenius-orthonormal, all inner products that enter the discrete identities (e.g., the projected increments ‚ü®W k+1 ‚àí W k, Q ki ‚ü©F ) decouple across i. That is, there are no cross terms at the current iterate. Thus, every structural property introduced in Section 2, Section 3.1 and Section 3.2 can be proved for a single singular direction and then summed over i = 1 , . . . , k r to obtain the block-level statement. Although the SVD (and hence {Q ki }) is recomputed at each step, our estimates are established within iteration k and extend to the full update by simple summation. We therefore present the analysis in a single mode setting and the extension to the total Gk is immediate. 

Theorem 3.1 (Modified-energy dissipation (mode-wise and global)) . Suppose the Muon‚ÄìRSAV update of Section 3.1 with per-mode step sizes h ki = Œ∑/œÉ ki and predictors Àúr k+1  

> i

= r ki

  1 + h ki 

> 2

(g ki )2, where g ki = œÉ ki /E k and 

Ek = pf (Œò k) + Œ∫. After the parameter update W k+1 , let Ek+1 = pf (Œò k+1 ) + Œ∫ and define the per-mode relaxation 

r k+1  

> i

= Œæ ki Àúr k+1  

> i

+ (1 ‚àí Œæ ki ) Ek+1 , Œæ ki ‚àà [0 , 1] ,

with Œæ ki chosen by the quadratic rule of (2.7) using a fixed œà ‚àà (0 , 1) . Then, for each retained singular mode i,

 r k+1 

> i

2 ‚àí  r ki

2 ‚â§ ‚àí (1 ‚àí œà) D k+1  

> i

‚â§ 0, D k+1  

> i

:= 1

h ki

W k+1 ‚àí W k, Q ki 

> 2
> F

.

Summing over the kr retained modes yields the global dissipation of the modified energy M k := Pkr

> i=1

 r ki

2:

M k+1 ‚àí M k ‚â§ ‚àí (1 ‚àí œà)

> kr

X

> i=1

D k+1  

> i

‚â§ 0.

Proof. Fix an iteration k and a single singular direction Q ki . The update in one sigular direction in Section 3.1 is exactly the RSAV predictor with step h = h ki in Section 2.2: 

Àúr k+1  

> i

= r ki

1 + h ki 

> 2

(g ki )2

, W k+1 ‚àí W k, Q ki F = ‚àíh ki Àúr k+1  

> i

g ki .

Therefore the standard identity from Section 2.2 holds per mode: 

D k+1  

> i

= 1

h ki

W k+1 ‚àí W k, Q ki 

> 2
> F

= ‚àí 2 Àúr k+1  

> i

‚àí r ki

Àúr k+1  

> i

.

With the RSAV relaxation r k+1  

> i

= Œæ ki Àúr k+1  

> i

+ (1 ‚àí Œæ ki )Ek+1 and the same quadratic choice of Œæ ki as in Section 2.2, the single-mode inequality  r k+1 

> i

2 ‚àí  r ki

2 ‚â§ ‚àí (1 ‚àí œà) D k+1 

> i

follows verbatim from the proof in Section 2.2 by replacing h 7 ‚Üí h ki and g 7 ‚Üí g ki . Finally, the Frobenius-orthonormality 

‚ü®Q ki , Q kj ‚ü©F = Œ¥ij ensures that D k+1  

> i

is additive across i (no cross terms at iteration k), so summing the single-mode inequalities yields the final statement: 

M k+1 ‚àí M k ‚â§ ‚àí (1 ‚àí œà)

> kr

X

> i=1

D k+1  

> i

‚â§ 0.

Theorem 3.2 (Positivity and uniform lower bound for r ki ). Assume Œ∫ > 0 and that the objective is bounded below: 

f‚àó := inf  

> Œò

f (Œò) > ‚àí‚àû .

8A PREPRINT - F EBRUARY 19, 2026 

Initialize r 0 

> i

= E0 = pf (Œò 0) + Œ∫. Consider the per‚Äìmode Muon‚ÄìRSAV update 

Àúr k+1  

> i

= r ki

1 + h ki

2 (g ki )2

, r k+1  

> i

= Œæ ki Àúr k+1  

> i

+ (1 ‚àí Œæ ki ) Ek+1 ,

with g ki = œÉ ki /E k, Ek = pf (Œò k) + Œ∫, and Œæ ki ‚àà [0 , 1] . Then for all k ‚â• 0,

r ki > 0 and Àúr k+1  

> i

> 0

Proof. By assumption f‚àó > ‚àí‚àû , hence f (Œò k) + Œ∫ ‚â• Œ∫ + f‚àó > 0 for all k, so Ek = pf (Œò k) + Œ∫ ‚â• ‚àöŒ∫ + f‚àó > 0.

Induction: r 0 

> i

= E0 > 0. If r ki > 0, then Àúr k+1  

> i

= r ki

1 + h ki 

> 2

(g ki )2

‚àà (0 , r ki ], and the relaxation is a convex combination of positive terms: 

r k+1  

> i

= Œæ ki Àúr k+1  

> i

+ (1 ‚àí Œæ ki ) Ek+1 ‚â• (1 ‚àí Œæ ki ) Ek+1 ‚â• 0.

Hence r k+1  

> i

> 0.

Remark 3.3 . The assumption f‚àó > ‚àí‚àû , is mild and holds for standard losses (e.g., MSE, cross-entropy), where typically f‚àó = 0 . If only a known bound f (Œò) ‚â• ‚àí B is available, choosing Œ∫ > B guarantees the same uniform positivity via ‚àöŒ∫ ‚àí B.Now we move to the dissipation of the original energy for sufficiently small step size (one singular direction). We first list the standing assumptions used in this theorem, then prove a one‚Äìmode descent result. The extension to the block level follows by summation across modes, as explained at the beginning of Section 3.3. (A1) L‚Äìsmoothness: The loss function f : Rm√ón ‚Üí R has L‚ÄìLipschitz continuous gradient with respect to the Frobenius norm, i.e. 

‚à•‚àá f (X) ‚àí ‚àá f (Y )‚à•F ‚â§ L ‚à•X ‚àí Y ‚à•F for all X, Y. 

Equivalently, f satisfies the standard descent inequality 

f (Y ) ‚â§ f (X) + ‚ü®‚àá f (X), Y ‚àí X‚ü©F + L

2 ‚à•Y ‚àí X‚à•2 

> F

,

which will be used in the analysis below. (A2) Lower-bounded loss and shift. f‚àó := inf Œò f (Œò) > ‚àí‚àû and Œ∫ > 0, so that Ek = pf (Œò k) + Œ∫ ‚â•‚àöŒ∫ + f‚àó > 0.

Theorem 3.4 (One‚Äìmode dissipation of the original energy for a sufficiently small step) . Under assumptions (A1) and (A2), the one‚Äìstep decrease f (Œò k+1 ) ‚â§ f (Œò k) holds in the singular direction Q ki provided the stepsize parameter Œ∑

satisfies (3.2) , i.e. 

Œ∑ ‚â§ 2 œÉ ki Ek

L Àúr k+1 

> i

.

Proof. Fix an iteration k and a retained singular direction Q ki with ‚ü®Q ki , Q ki ‚ü©F = 1 and coefficient œÉ ki = ‚ü®Gk, Q ki ‚ü©F ,where Gk = ‚àáW f (Œò k). With h ki = Œ∑/œÉ ki and g ki = œÉ ki /E k, define 

Àúr k+1  

> i

= r ki

1 + h ki 

> 2

(g ki )2

, W k+1 = W k ‚àí Œ∑Ek Àúr k+1  

> i

Q ki .

As in Section 3.1, the relaxation that produces r k+1  

> i

is performed after updating W and is not needed here. By (A1) with Y := W k+1 ‚àí W k = ‚àí(Œ∑/E k)Àú r k+1  

> i

Q ki and ‚à•Q ki ‚à•F = 1 ,

f (Œò k+1 ) ‚â§ f (Œò k) + 

D

Gk, ‚àí Œ∑Ek Àúr k+1  

> i

Q ki

E

> F

+ L

2

Œ∑Ek Àúr k+1  

> i

Q ki

> 2
> F

= f (Œò k) ‚àí Œ∑Ek Àúr k+1  

> i

œÉ ki + L

2

Œ∑2

(Ek)2 (Àú r k+1  

> i

)2. (3.1) 9A PREPRINT - F EBRUARY 19, 2026 Thus, a sufficient local condition for f (Œò k+1 ) ‚â§ f (Œò k) in this mode is 

Œ∑Ek Àúr k+1  

> i

‚â§ 2 œÉ ki

L ‚áê‚áí Œ∑ ‚â§ 2 œÉ ki Ek

L Àúr k+1 

> i

. (3.2) Note that the local condition (3.2) is verifiable at runtime because it uses current quantities (œÉ ki , E k, Àúr k+1  

> i

). For a uniform (iterate‚Äìindependent) choice of Œ∑, one may use the truncation of the singular values. If we retain only modes with œÉ ki ‚â• œÉmin > 0 and enforce a predictor cap Àúr k+1  

> i

‚â§ Cr Ek with some Cr ‚â• 1 (e.g., by clipping the ratio 

Àúr k+1  

> i

/E k), then a single, iterate‚Äìindependent stepsize 

Œ∑ ‚â§ 2 œÉmin 

L C r

(3.3) guarantees f (Œò k+1 ) ‚â§ f (Œò k) in every retained mode and hence in total. 

Remark 3.5 . Summing (3.1) across the kr modes, Frobenius‚Äìorthonormal directions yields f (Œò k+1 ) ‚â§ f (Œò k) ‚àí

> Œ∑Ek

P 

> i

Àúr k+1  

> i

œÉ ki + L 

> 2
> Œ∑2
> (Ek)2

P

> i

(Àú r k+1  

> i

)2. Thus, if (3.2) holds for all retained i, the total energy decreases. This is the precise sense in which proving the one‚Äìmode inequality suffices at iteration k.We now analyze the convergence behavior of the proposed Muon‚ÄìRSAV algorithm under the same conditions (A1)‚Äì(A2) introduced previously, together with an additional regularity assumption stated below. (A3) ( the Polyak‚Äì≈Åojasiewicz (PL) condition :) There exists Œº > 0 such that  

> 12

‚à•‚àá f (Œò) ‚à•2 

> F

‚â• Œº f (Œò) ‚àí f‚àó

, ‚àÄ Œò.

where f‚àó = inf Œò f (Œò) > ‚àí‚àû . This condition generalizes strong convexity and is commonly satisfied by many overparameterized neural networks and PDE-constrained learning problems. 

Theorem 3.6 (Convergence and linear rate) . Let assumptions (A1)‚Äì(A3) hold. Then the sequence {f (Œò k)} generated by the proposed Muon‚ÄìRSAV method is nonincreasing and convergent. Moreover, 

lim  

> k‚Üí‚àû

‚à•‚àá f (Œò k)‚à•F = 0 .

If, in addition, the PL condition (A3) holds, the method converges linearly to the optimal value f‚àó, i.e., 

f (Œò k+1 ) ‚àí f‚àó ‚â§ (1 ‚àí œÅ)  f (Œò k) ‚àí f‚àó

, œÅ = 2œÑ (2 ‚àí œÑ )

L E c 0 Œº ‚àà (0 , 1] ,

where œÑ ‚àà (0 , 2] is the step‚Äìscaling parameter, E = ‚àöf‚àó + Œ∫ > 0, and c0 = cos 2‚à†(Àú r k+1 , œÉ k) ‚àà (0 , 1] quantifies the alignment between Àúr k+1 and the gradient singular spectrum. Proof. The one‚Äìmode descent inequality has been established in (3.1) . Summing it over all retained spectral directions yields 

f (Œò k+1 ) ‚â§ f (Œò k) ‚àí Œ∑Ek

X

> i

Àúrk+1  

> i

œÉki + L

2

 Œ∑Ek

2 X

> i

(Àú rk+1  

> i

)2. (3.4) Define Sk 

> 1

= P 

> i

Àúrk+1  

> i

œÉki and Sk 

> 2

= P

> i

(Àú rk+1  

> i

)2. For fixed Œòk, the right-hand side of (3.4) is a convex quadratic function of Œ∑. Its minimizer is 

Œ∑‚àó 

> k

= Ek

LSk

> 1

Sk

> 2

.

Setting Œ∑k = œÑ Œ∑ ‚àó 

> k

with œÑ ‚àà (0 , 2] yields 

f (Œò k+1 ) ‚àí f (Œò k) ‚â§ ‚àí œÑ (2 ‚àí œÑ )2

Ek

L

(Sk 

> 1

)2

Sk

> 2

. (3.5) Since Ek ‚â• E > 0 by (A2), the right-hand side of (3.5) is nonpositive, so {f (Œò k)} is monotonically decreasing and bounded below by f‚àó. Therefore f (Œò k) converges. 10 A PREPRINT - F EBRUARY 19, 2026 Next, by the Cauchy‚ÄìSchwarz inequality, 

Sk 

> 1

= X

> i

Àúrk+1  

> i

œÉki ‚â§

q

Sk 

> 2

‚à•‚àá W f (Œò k)‚à•F .

Summing (3.5) over k gives ‚àûX

> k=0

‚à•‚àá f (Œò k)‚à•2 

> F

< ‚àû,

which implies lim k‚Üí‚àû ‚à•‚àá f (Œò k)‚à•F = 0 . Hence, every limit point of {Œòk} is a stationary point. Finally, if (A3) holds, we apply the PL inequality: 

(Sk 

> 1

)2

Sk

> 2

=

D Àúr k+1 

‚à•Àúr k+1 ‚à•2

, œÉ kE2

‚â• c0 ‚à•œÉk‚à•22 = c0 ‚à•‚àá W f (Œò k)‚à•2 

> F

.

Substituting into (3.5) gives 

f (Œò k+1 ) ‚àí f (Œò k) ‚â§ ‚àí 2œÑ (2 ‚àí œÑ )

L E c 0 Œº  f (Œò k) ‚àí f‚àó

,

which leads to 

f (Œò k+1 ) ‚àí f‚àó ‚â§ (1 ‚àí œÅ) ( f (Œò k) ‚àí f‚àó),

where œÅ = 2œÑ (2 ‚àíœÑ ) 

> L

E c 0 Œº ‚àà (0 , 1] . Therefore, the sequence {f (Œò k)} converges to f‚àó at a linear rate. 

Remark 3.7 . The factor c0 = cos 2‚à†(Àú r k+1 , œÉ k) measures the correlation between the auxiliary predictors and the gradient spectrum. Empirically, both vectors are nonnegative and dominated by the top singular modes, so c0 remains positive in practice. Even if c0 becomes small, (3.5) still guarantees monotone convergence, while the PL condition ensures asymptotic linear convergence when c0 > 0.

# 4 Numerical Results 

4.1 Toy example: linear regression 

We begin with a linear regression problem to illustrate the behavior of the proposed SpecMuon optimizer in a controlled convex setting. Although linear regression is well understood, it provides a transparent testbed for examining optimization dynamics and spectral effects without the additional complexities introduced by nonlinear architectures. Specifically, we consider the least-squares problem 

min  

> W‚ààRm√ón

f (W ) := 12 ‚à•W X ‚àí Y ‚à•2 

> F

,

where X ‚àà Rn√óN denotes the input data matrix and Y ‚àà Rm√óN the corresponding targets. The gradient takes the closed form 

‚àáf (W ) = ( W X ‚àí Y )X‚ä§,

which is matrix-valued and whose singular spectrum reflects correlations present in the data. Consequently, the problem exhibits anisotropic curvature and serves as a minimal example in which spectral imbalance can influence optimization behavior. We compare standard gradient-based optimizers, including Adam, AdamW, and the original Muon, with the proposed SpecMuon method. All methods are initialized identically and tuned to their best-performing learning rates. For SpecMuon, we retain only the dominant two singular directions of the gradient and apply the mode-wise RSAV mechanism described in Section 3. Figure 1 reports the training loss trajectories. While all methods eventually converge to the global minimizer, clear differences emerge in the transient regime. Standard first-order methods exhibit slow initial progress due to curvature imbalance. SpecMuon improves convergence speed by adaptively regulating the magnitude of each spectral update through its energy-based auxiliary variables. This toy example highlights two key aspects of the proposed method. First, even in a convex setting, spectral geometry plays a decisive role in optimization efficiency. Second, incorporating an energy-aware mechanism into spectral updates provides additional robustness without sacrificing convergence speed. These observations motivate the application of SpecMuon to more challenging physics-informed learning problems, which we investigate next. 11 A PREPRINT - F EBRUARY 19, 2026 

Figure 1: Training loss (log scale) for the linear regression problem. SpecMuon (red) is compared with Muon, Adam, and AdamW. SpecMuon achieves faster convergence by combining spectral orthogonalization with mode-wise energy regulation. 

4.2 Physics-Informed Neural Network 

We next consider the one-dimensional viscous Burgers‚Äô equation, a nonlinear partial differential equation frequently used as a benchmark problem in scientific machine learning. The equation with Dirichlet boundary conditions is defined on the spatial domain Œ© = [ ‚àí1, 1] and temporal domain [0 , T ] as follows: 

‚àÇu ‚àÇt ‚àí ŒΩ ‚àÇ2u‚àÇx 2 + u ‚àÇu ‚àÇx = 0 , x ‚àà Œ©, t ‚àà [0 , T ], (4.1) 

u(x, t ) = 0 , ‚àÄx ‚àà ‚àÇŒ©, (4.2) 

u(x, 0) = ‚àí sin( œÄx ). (4.3) Here, u(x, t ) represents the solution over space and time, and ŒΩ is the viscosity chosen to be 0.01 /œÄ .To ensure a fair evaluation of optimizer performance, we conducted a systematic hyperparameter tuning procedure for all compared methods: Adam, AdamW, Muon, and the proposed SpecMuon. As summarized in Table 1, a grid search was performed over the learning rates to determine the optimal configuration for each optimizer, while other parameters were either set to established default values or tuned within a limited range. For the baseline optimizers, both Adam and AdamW achieved their best performance with a learning rate of 5 √ó 10 ‚àí3.The Muon optimizer performed best with a learning rate of 0.02 and momentum 0.02 . For the proposed SpecMuon method, the grid search identified a learning rate of 3 √ó 10 ‚àí3 and momentum 0.9 as optimal. In addition, SpecMuon introduces two hyperparameters related to spectral aggregation: rtop = 6 and Œ∑sav = 0 .2.Figure 2a presents the training loss trajectories for the PINN applied to Burgers‚Äô equation. The proposed SpecMuon optimizer (solid red curve) is compared with Adam, AdamW, and the original Muon optimizer. As shown in the figure, SpecMuon demonstrates improved convergence behavior relative to all baseline methods. In particular, it achieves faster initial descent and consistently maintains a lower loss throughout the full training duration of 10 , 000 epochs. Figure 2b reports an ablation study on the hyperparmeter rtop , which contros the number of dominant singular values retained in the spectral aggregation step. The results indicate that incorporating multiple leading spectral directions improves convergence performance, with rtop = 6 providing a favorable balance between stability and efficiency. The quantitative results are summarized in Table 2. The proposed SpecMuon optimizer achieves the lowest final training loss and the smallest mean squared error (MSE) among all compared methods. Although SpecMuon incurs slightly higher computational cost due to spectral decomposition and auxiliary-variable updates, it delivers improved solution accuracy and more stable convergence behavior. 12 A PREPRINT - F EBRUARY 19, 2026 Table 1: Hyperparameter configurations for the PINN experiment on Burgers‚Äô equation. Learning rates were selected via grid search. Other parameters were set to standard defaults unless otherwise specified. 

Optimizer Hyperparameter Values Searched Best Value Used 

Adam 

Learning Rate (lr) {0.01, 5e-3, 1e-3, 5e-4} 5e-3 Betas ( Œ≤1, Œ≤ 2) ‚Äì (0.9, 0.999) Weight Decay ( Œª) ‚Äì 0

AdamW 

Learning Rate (lr) {0.01, 5e-3, 1e-3, 5e-4} 5e-3 Betas ( Œ≤1, Œ≤ 2) ‚Äì (0.9, 0.999) Weight Decay ( Œª) {0.0, 0.01, 1e-3, 5e-4, 1e-4 } 5e-4 

Muon Learning Rate (lr) {0.1, 0.05, 0.02, 5e-3} 0.02 Momentum ( Œ≤) {0.0, 0.02, 1e-3, 1e-4, 5e-4} 0.02 

SpecMuon Learning Rate (lr) {0.1, 0.05, 0.01, 3e-3} 3e-3 Momentum ( Œ≤) {0.9, 0.01, 0.02, 1e-3, 1e-4, 5e-4} 0.9 rtop {2 - 8} 6SAV Eta ( Œ∑sav ) {0.2 - 0.8} 0.2    

> (a) Training loss (log scale) comparison among Adam, AdamW, Muon, and SpecMuon.
> (b) Ablation study on the spectral rank parameter rtop in Spec-Muon. Retaining multiple dominant singular directions im-proves convergence stability and speed.

Figure 2: PINN training on the one-dimensional Burgers‚Äô equation. 

Figure 3: Spatiotemporal solution u(x, t ) of the Burgers‚Äô equation obtained by PINNs trained with Adam(left), Muon(middle), and SpecMuon(right). SpecMuon produces a more accurate and smoother approximation of the reference solution. 13 A PREPRINT - F EBRUARY 19, 2026 Table 2: Quantitative comparison for the Burgers‚Äô PINN experiment. Reported metrics include final training loss, mean squared error (MSE), and total training time on an NVIDIA H100 80GB GPU. Best results are shown in bold. 

Optimizer Final Training Loss MSE Training Time (s) 

Adam (5 .88 ¬± 0.12) √ó 10 ‚àí4 1.42 √ó 10 ‚àí3 120 ¬± 12 

AdamW (7 .46 ¬± 0.15) √ó 10 ‚àí4 1.50 √ó 10 ‚àí3 117 ¬± 11 

Muon (3 .39 ¬± 0.05) √ó 10 ‚àí4 1.25 √ó 10 ‚àí3 149 ¬± 15 

SpecMuon (1 .55 ¬± 0.04) √ó 10 ‚àí4 5.06 √ó 10 ‚àí4 180 ¬± 18 

Figure 4: Spatial distribution of the L1 error for the Burgers‚Äô PINN trained with Adam(left), Muon(middle), and SpecMuon(right). SpecMuon exhibits reduced error magnitude. 

4.3 DeepONet 

To further evaluate the performance of the proposed optimizer in an operator-learning setting, we apply it to a DeepONet model trained on Burgers‚Äô equation 4.1. This experiment assesses whether the spectral‚Äìenergy coupling mechanism remains effective in architectures with coupled branch and trunk networks. The hyperparameter configurations obtained through grid search are summarized in Table 3. For Adam and AdamW, the optimal learning rate was 5 √ó 10 ‚àí3. For the proposed SpecMuon method, the grid search identified an optimal learning rate of 1 √ó 10 ‚àí3 and momentum 0.9. Notebly, the optimal spectral rank parameter was slightly higher for the DeepONet architecture, with rtop = 8 , reflecting the richer gradient structure induced by operator learning. Figure 5a presents the training loss trajectories for the DeepONet model applied to Burgers‚Äô equation. The proposed SpecMuon optimizer (solid red curve) is compared with Adam, AdamW, and the original Muon optimizer. As illustrated in the figure, SpecMuon exhibits faster convergence and achieves lower training loss throughout the training process. Figure 5b shows the ablation study with respect to the spectral rank parameter rtop . The results indicate that retaining a moderate number of dominant singular directions improves convergence behavior, with rtop = 8 yielding the best empirical performance in this setting. The quantitative comparison is reported in Table 4. The SpecMuon attains the lowest final training loss and mean squared error (MSE) among all compared optimizers. Although the additional spectral computations introduce moderate computational overhead, the improved accuracy and stability demonstrate the effectiveness of integrating mode-wise energy regulation into Muon‚Äôs orthogonalized update framework. 14 A PREPRINT - F EBRUARY 19, 2026 Table 3: Hyperparameter configurations for the DeepONet experiment on Burgers‚Äô equation. Learning rates were selected via grid search. The optimal spectral rank rtop increases compared with the PINN setting due to richer gradient structure. 

Optimizer Hyperparameter Values Searched Best Value Used 

Adam 

Learning Rate (lr) {0.01, 5e-3, 1e-3, 5e-4} 5e-3 Betas ( Œ≤1, Œ≤ 2) ‚Äì (0.9, 0.999) Weight Decay ( Œª) ‚Äì 0

AdamW 

Learning Rate (lr) {0.01, 5e-3, 1e-3, 5e-4} 5e-3 Betas ( Œ≤1, Œ≤ 2) ‚Äì (0.9, 0.999) Weight Decay ( Œª) {0.0, 0.01, 1e-3, 5e-4, 1e-4} 1e-2 

Muon Learning Rate (lr) {0.1, 0.05, 5e-3, 1e-3} 1e-3 Momentum ( Œ≤) {0.0, 0.02, 1e-3, 1e-4, 5e-4} 1e-4 

SpecMuon Learning Rate (lr) {0.1, 0.05, 0.01, 1e-3, 1e-4} 1e-3 Momentum ( Œ≤) {0.9, 0.01, 0.02, 1e-3, 1e-4, 5e-4} 0.9 rtop {2 - 8} 8SAV Eta ( Œ∑sav ) {0.2 - 0.8} 0.2  

> (a) Training loss(log scale) comparison among Adam, AdamW, Muon, and SpecMuon.
> (b) Ablation study on the spectral rank parameter rtop . Increas-ing the number of retained singular modes improves conver-gence up to an optimal range.

Figure 5: DeepONet training on the Burgers‚Äô equation. 

4.4 fPINN-DeepONet 

We next consider the classical two-dimensional heat equation on the unit square with homogeneous Dirichlet boundaries: 

‚àÇtu(x, y, t ) ‚àí Œ∫ ‚àÜu(x, y, t ) = f (x, y, t ), (x, y ) ‚àà Œ© ‚â° [0 , 1] 2, t ‚àà [0 , 1] , (4.4) 

u|‚àÇŒ© = 0 , (4.5) 

u(x, y, 0) = u0(x, y ). (4.6) To enable quantitative comparison, we consider the analytical solution 

u(x, y, t ) = sin( œÄx ) sin( œÄy ) e ‚àí2œÄ2Œ∫t , (4.7) which corresponds to the homogeneous case with zero forcing, i.e., f ‚â° 0 and initial condition u0(x, y ) =sin( œÄx ) sin( œÄy ).To ensure a fair comparison among optimizers, a grid search was conducted over the learning rate for each method. For the baseline optimizers (Adam and AdamW), standard values for the momentum coefficients and weight decay were considered. For Muon and the proposed SpecMuon, we additionally explored the influence of the spectral rank parameter rtop . The final hyperparameter configurations used in the comparative study are summarized in Table 5. 15 A PREPRINT - F EBRUARY 19, 2026 Table 4: Quantitative comparison for the Burgers‚Äô DeepONet experiment. Metrics include final training loss, MSE, and training time on an NVIDIA H100 80GB GPU. Best results are highlighted in bold. 

Optimizer Final Training Loss MSE Training Time (s) 

Adam (1 .89 ¬± 0.03) √ó 10 ‚àí3 0.0014 113 ¬± 12 

AdamW (4 .71 ¬± 0.05) √ó 10 ‚àí3 0.0013 114 ¬± 10 

Muon (6 .38 ¬± 0.12) √ó 10 ‚àí4 0.0011 204 ¬± 13 

SpecMuon (4 .82 ¬± 0.07) √ó 10 ‚àí4 0.0010 259 ¬± 10 

Table 5: Hyperparameter configurations for the fPINN‚ÄìDeepONet experiment on the two-dimensional heat equation. Learning rates were selected via grid search, and spectral rank rtop was tuned for stability and accuracy. 

Optimizer Hyperparameter Values Searched Best Value Used 

Adam 

Learning Rate (lr) {0.01, 5e-3, 1e-3, 5e-4} 5e-3 Betas ( Œ≤1, Œ≤ 2) ‚Äì (0.9, 0.999) Weight Decay ( Œª) ‚Äì 0

AdamW 

Learning Rate (lr) {0.01, 5e-3, 1e-3, 5e-4} 5e-3 Betas ( Œ≤1, Œ≤ 2) ‚Äì (0.9, 0.999) Weight Decay ( Œª) {0.0, 0.01, 1e-3, 5e-4, 1e-4} 1e-2 

Muon Learning Rate (lr) {0.1, 0.05, 5e-3, 1e-3} 1e-3 Momentum ( Œ≤) {0.0, 0.02, 1e-3, 1e-4, 5e-4} 1e-4 

SpecMuon Learning Rate (lr) {0.1, 0.05, 0.01, 1e-3, 1e-4} 1e-3 Momentum ( Œ≤) {0.9, 0.01, 0.02, 1e-3, 1e-4, 5e-4} 0.9 rtop {2 - 10} 8SAV Eta ( Œ∑sav ) {0.2 - 0.8} 0.2 The performance of each optimizer was evaluated using three metrics: the final training loss, the mean squared error (MSE) computed over the full space‚Äìtime grid, and the relative L2 error. The proposed SpecMuon optimizer achieves the lowest training loss and the smallest relative L2 error among the compared methods, demonstrating improved accuracy in approximating the analytical solution. The quantitative results are presented in Table 6. Although the additional spectral computations introduce moderate computational overhead, the results indicate that integrating spectral decomposition with energy-based regulation enhances convergence robustness and solution quality in the fractional PINN‚ÄìDeepONet framework.  

> (a) Training loss (log scale) comparison among Adam, AdamW, Muon, and SpecMuon.
> (b) Ablation study on the spectral rank parameter rtop . Moderate spectral truncation provides the best balance between stability and efficiency.

Figure 6: fPINN‚ÄìDeepONet training on the fractional PDE problem. 16 A PREPRINT - F EBRUARY 19, 2026 Table 6: Quantitative comparison for the fPINN‚ÄìDeepONet experiment. Training loss is reported at 500 epochs. Training time was measured on an NVIDIA H100 80GB GPU. Best results are highlighted in bold. 

Optimizer Training Loss MSE Training Time (s) 

Adam (3 .72 ¬± 0.04) √ó 10 ‚àí2 0.0268 19 .41 

AdamW (3 .64 ¬± 0.04) √ó 10 ‚àí2 0.0275 13 .37 

Muon (3 .52 ¬± 0.06) √ó 10 ‚àí2 0.0272 18 .24 

SpecMuon (2 .31 ¬± 0.05) √ó 10 ‚àí2 0.0262 30 .74 

# 5 Conclusion and Future Work 

In this work, we introduced SpecMuon, a spectral-aware optimization framework designed for physics-informed machine learning. By integrating orthogonalized gradient updates in the singular-vector basis with a mode-wise relaxed scalar auxiliary variable (RSAV) mechanism, SpecMuon unifies geometric conditioning and energy-based stabilization within a single optimizer. This formulation interprets optimization as a multi-mode gradient flow and enables adaptive control of stiff spectral components that commonly arise in physics-informed neural networks and neural operators. We established rigorous theoretical properties of the proposed method, including positivity of the auxiliary variables, a modified energy dissipation law, and global convergence guarantees. Under standard smoothness assumptions and the Polyak‚Äì≈Åojasiewicz condition, we further proved linear convergence to the global optimum. These results provide a solid mathematical foundation for the proposed algorithm and clarify the role of spectral decomposition and energy regulation in shaping stable optimization dynamics. Extensive numerical experiments demonstrated the effectiveness of SpecMuon across a hierarchy of problems. Starting from a simple linear regression model, we showed that spectral geometry and energy-aware updates can improve optimization behavior even in convex settings. We then validated the method on increasingly challenging physics-informed learning tasks, including PINNs, DeepONets, and fractional PINN‚ÄìDeepONets, where SpecMuon consistently achieved faster convergence and improved stability compared with Adam, AdamW, and the original Muon optimizer. These results confirm that incorporating spectral and energy information is particularly beneficial in stiff, multi-scale learning problems driven by physical constraints. Several directions for future research are worth exploring. From an algorithmic perspective, extending SpecMuon to stochastic or mini-batch settings and studying its interaction with variance reduction techniques would be valuable for large-scale applications. Incorporating adaptive strategies for selecting the number of retained spectral modes may further improve efficiency and robustness. From a theoretical standpoint, sharper convergence rates under weaker assumptions and a deeper analysis of spectral evolution across iterations remain open questions. Finally, applying the proposed framework to broader classes of operator learning problems, inverse problems, and multi-physics systems may reveal additional advantages of spectral energy‚Äìaware optimization in scientific machine learning. 

# Acknowledgment 

We would like to thank the support of National Science Foundation (DMS-2533878, DMS-2053746, DMS-2134209, ECCS-2328241, CBET-2347401 and OAC-2311848), and U.S. Department of Energy (DOE) Office of Science Advanced Scientific Computing Research program DE-SC0023161, the SciDAC LEADS Institute, and DOE‚ÄìFusion Energy Science, under grant number: DE-SC0024583. 

# References 

[1] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686‚Äì707, 2019. [2] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6), 05 2021. [3] B. Lu, C. Mou, and G. Lin. ipinner: An iterative physics-informed neural network with ensemble kalman filter. Journal of Computational Physics, page 114592, 2025. 17 A PREPRINT - F EBRUARY 19, 2026 [4] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3), 03 2021. [5] B. Lu, Z. Hao, C. Moya, and G. Lin. Fpinn-deeponet: A physics-informed operator learning framework for multi-term time-fractional mixed diffusion-wave equations. Journal of Computational Physics, 538:114184, 2025. [6] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations, 2021. [7] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Research, 24(89):1‚Äì97, 2023. [8] Tao Wang, Bo Zhao, Sicun Gao, and Rose Yu. Understanding the difficulty of solving cauchy problems with pinns, 2024. [9] Siddhartha Mishra and Roberto Molinaro. Estimates on the generalization error of physics-informed neural networks for approximating a class of inverse problems for pdes. IMA Journal of Numerical Analysis, 42(2):981‚Äì 1022, 06 2021. [10] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [11] Weijie Su, Stephen Boyd, and Emmanuel J. Cand√®s. A differential equation for modeling nesterov‚Äôs accelerated gradient method: Theory and insights. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. [12] Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 113(47), November 2016. [13] Guilherme Fran√ßa, Michael I Jordan, and Ren√© Vidal. On dissipative symplectic integration with applications to gradient-based optimization. Journal of Statistical Mechanics: Theory and Experiment, 2021(4):043402, April 2021. [14] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2408‚Äì2417, Lille, France, 07‚Äì09 Jul 2015. PMLR. [15] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks, 2019. [16] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [17] Keller Jordan et al. Muon: An optimizer for hidden layers in neural networks. Keller Jordan Blog, 2024. [18] Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology. arXiv preprint arXiv:2409.20325, 2024. [19] Jie Shen and Jie Xu. Convergence and error analysis for the scalar auxiliary variable (sav) schemes to gradient flows. SIAM Journal on Numerical Analysis, 56(5):2895‚Äì2912, 2018. [20] Jie Shen, Jie Xu, and Jiang Yang. The scalar auxiliary variable (sav) approach for gradient flows. Journal of Computational Physics, 353:407‚Äì416, 2018. [21] Jie Shen, Jie Xu, and Jiang Yang. A new class of efficient and robust energy stable schemes for gradient flows. SIAM Review, 61(3):474‚Äì506, 2019. [22] Shiheng Zhang, Jiahao Zhang, Jie Shen, and Guang Lin. A relaxed vector auxiliary variable algorithm for unconstrained optimization problems. SIAM Journal on Scientific Computing, 47(1):C126‚ÄìC150, 2025. [23] Jiahao Zhang, Christian Moya, and Guang Lin. A self-adaptive energy-based learning rate for stochastic gradient descent via vector auxiliary variable method. Engineering Applications of Artificial Intelligence, 160:111731, 2025. [24] Jiahao Zhang, Shiheng Zhang, Jie Shen, and Guang Lin. Energy-dissipative evolutionary deep operator neural networks. Journal of Computational Physics, 498:112638, 2024. 18 A PREPRINT - F EBRUARY 19, 2026 [25] Hailiang Liu and Xuping Tian. Aegd: Adaptive gradient descent with energy. Numerical Algebra, Control and Optimization, 2023. [26] Xinyu Liu, Jie Shen, and Xiangxiong Zhang. An efficient and robust scalar auxialiary variable based algorithm for discrete gradient systems arising from optimizations. SIAM Journal on Scientific Computing, 45(5):A2304‚Äì A2324, 2023. [27] Maosheng Jiang, Zengyan Zhang, and Jia Zhao. Improving the accuracy and consistency of the scalar auxiliary variable (sav) method with relaxation. Journal of Computational Physics, 456:110954, 2022. 19