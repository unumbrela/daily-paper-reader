---
title: "Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction"
title_zh: 实现张量逻辑：通过张量收缩统一 Datalog 与神经推理
authors: "Swapn Shah, Wlodek Zadrozny"
date: 2026-01-23
pdf: "https://arxiv.org/pdf/2601.17188v1"
tags: ["query:sr"]
score: 7.0
evidence: 统一符号推理与神经网络
tldr: 本研究旨在统一符号推理与神经网络，基于张量逻辑（Tensor Logic）框架，将逻辑规则等价于张量收缩（爱因斯坦求和）。通过三个实验验证了其有效性：首先证明了递归 Datalog 规则与迭代张量收缩在处理家谱图闭包时的等价性；其次在嵌入空间实现了零样本组合推理；最后在 FB15k-237 知识图谱上通过矩阵组合实现了高效的多步推理。该方法为构建兼具可靠性与学习能力的统一推理系统提供了理论与实证支持。
motivation: 旨在解决符号推理系统扩展性不足以及神经网络缺乏透明度的问题，寻求逻辑规则与神经计算在数学上的统一路径。
method: 利用张量收缩来实现逻辑规则，并通过可学习的变换矩阵在嵌入空间中构建关系矩阵，实现逻辑与神经推理的融合。
result: 实验成功实现了 Datalog 规则的等价转换、零样本组合推理，并在 FB15k-237 链接预测任务中通过矩阵组合达到了 0.3068 的 MRR。
conclusion: 验证了张量逻辑是统一 Datalog 与神经推理的有效框架，证明了矩阵组合能够在无需直接训练样本的情况下实现复杂的多步推理。
---

## 摘要
符号推理与神经网络的统一仍然是人工智能领域的一个核心挑战。符号系统具有可靠性和可解释性，但缺乏可扩展性；而神经网络提供了学习能力，却牺牲了透明度。Domingos 提出的张量逻辑（Tensor Logic）认为逻辑规则与爱因斯坦求和（Einstein summation）在数学上是等价的，为实现两者的统一提供了一条基于原则的路径。本文通过三个实验对该框架进行了实证验证。首先，我们通过计算一个包含 1,972 个个体和 1,727 条父子关系的圣经家谱图的传递闭包，证明了递归 Datalog 规则与迭代张量收缩之间的等价性，该计算在 74 次迭代中收敛，发现了 33,945 条祖先关系。其次，我们通过训练具有可学习变换矩阵的神经网络，在嵌入空间中实现了推理，并在留出查询（held-out queries）上展示了成功的零样本组合推理。第三，我们在 FB15k-237（一个包含 14,541 个实体和 237 种关系的大规模知识图谱）上验证了张量逻辑的叠加结构。利用 Domingos 的关系矩阵公式 $R_r = E^\top A_r E$，我们在标准链接预测上达到了 0.3068 的 MRR，在训练期间移除直接边的组合推理基准测试中达到了 0.3346 的 MRR，证明了矩阵组合能够在没有直接训练样本的情况下实现多跳推理。

## Abstract
The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework through three experiments. First, we demonstrate the equivalence between recursive Datalog rules and iterative tensor contractions by computing the transitive closure of a biblical genealogy graph containing 1,972 individuals and 1,727 parent-child relationships, converging in 74 iterations to discover 33,945 ancestor relationships. Second, we implement reasoning in embedding space by training a neural network with learnable transformation matrices, demonstrating successful zero-shot compositional inference on held-out queries. Third, we validate the Tensor Logic superposition construction on FB15k-237, a large-scale knowledge graph with 14,541 entities and 237 relations. Using Domingos's relation matrix formulation $R_r = E^\top A_r E$, we achieve MRR of 0.3068 on standard link prediction and MRR of 0.3346 on a compositional reasoning benchmark where direct edges are removed during training, demonstrating that matrix composition enables multi-hop inference without direct training examples.