Title: Traceable Latent Variable Discovery Based on Multi-Agent Collaboration

URL Source: https://arxiv.org/pdf/2602.14456v1

Published Time: Tue, 17 Feb 2026 02:56:33 GMT

Number of Pages: 12

Markdown Content:
# Traceable Latent Variable Discovery Based on Multi-Agent Collaboration 

## Huaming Du 

Southwestern University of Finance and Economics Chengdu, Sichuan, China 

## Tao Hu 

Southwestern University of Finance and Economics Chengdu, Sichuan, China 

## Yijie Huang 

Southwestern University of Finance and Economics Chengdu, Sichuan, China 

## Yu Zhao âˆ—

Southwestern University of Finance and Economics Chengdu, Sichuan, China 

## Guisong Liu 

Southwestern University of Finance and Economics Chengdu, Sichuan, China 

## Tao Gu 

Southwestern University of Finance and Economics Chengdu, Sichuan, China 

## Gang Kou 

Hunan University of Technology and Business Xiangjiang Laboratory Hunan, China 

## Carl Yang âˆ—

Emory University Atlanta, Georgia, United States j.carlyang@emory.edu 

Abstract 

Revealing the underlying causal mechanisms in the real world is crucial for scientific and technological progress. Despite notable advances in recent decades, the lack of high-quality data and the reliance of traditional causal discovery algorithms (TCDA) on the assumption of no latent confounders, as well as their tendency to overlook the precise semantics of latent variables, have long been major obstacles to the broader application of causal discovery. To address this issue, we propose a novel causal modeling framework, 

TLVD , which integrates the metadata-based reasoning capabilities of large language models (LLMs) with the data-driven modeling ca-pabilities of TCDA for inferring latent variables and their semantics. Specifically, we first employ a data-driven approach to construct a causal graph that incorporates latent variables. Then, we employ multi-LLM collaboration for latent variable inference, modeling this process as a game with incomplete information and seeking its Bayesian Nash Equilibrium (BNE) to infer the possible specific la-tent variables. Finally, to validate the inferred latent variables across multiple real-world web-based data sources, we leverage LLMs for evidence exploration to ensure traceability. We comprehensively evaluate TLVD on three de-identified real patient datasets provided by a hospital and two benchmark datasets. Extensive experimental results confirm the effectiveness and reliability of TLVD, with aver-age improvements of 32.67% in Acc, 62.21% in CAcc, and 26.72% in ECit across the five datasets.  

> âˆ—Corresponding author
> This work is licensed under a Creative Commons Attribution 4.0 International License.
> WWW â€™26, Dubai, United Arab Emirates
> Â©2026 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2307-0/2026/04 https://doi.org/10.1145/3774904.3792244

CCS Concepts 

â€¢ Mathematics of computing â†’ Causal networks ; â€¢ Informa-tion systems â†’ Web searching and information discovery .

Keywords 

Latent variable discovery, Information Retrieval, Large language model, Healthcare 

ACM Reference Format: 

Huaming Du, Tao Hu, Yijie Huang, Yu Zhao, Guisong Liu, Tao Gu, Gang Kou, and Carl Yang. 2026. Traceable Latent Variable Discovery Based on Multi-Agent Collaboration . In Proceedings of the ACM Web Conference 2026 (WWW â€™26), April 13â€“17, 2026, Dubai, United Arab Emirates. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3774904.3792244 

1 Introduction 

Causal discovery aims to identify causal relationships from ob-servational data and has been successfully applied in many fields [ 12 , 37 ]. However, traditional methodsâ€”such as the PC algorithm [45 ], GES [ 10 ], and LiNGAM [ 44 ]â€”typically assume that no latent confounders exist in the causal graph, an assumption that often does not hold in many real-world scenarios. Therefore, extensive re-search has been devoted to addressing this issue from two directions to improve causal structure learning. The first line of research focuses on inferring causal structures among observed variables despite the possible existence of la-tent confounders. Representative approaches include FCI and its variants based on conditional independence tests [ 3, 36 ], as well as over-complete ICA-based techniques that further exploit non-Gaussianity [ 43 ]. The second line of research focuses more on inferring causal structures among latent variables under the as-sumption that observed variables are not directly connected. This category includes Tetrad condition-based approaches [ 24 ], high-order moments-based methods [ 9, 49 ], matrix decomposition-based approaches [ 4 ], and mixture oracle-based methods[ 23 ]. Recently, Dong et al. [ 11 ] proposed a three-phase causal discovery algorithm 

> arXiv:2602.14456v1 [cs.LG] 16 Feb 2026

WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates Huaming Du et al. Input data                        

> Assets
> X-ray Smoke ... Cancer
> 10... 0
> 02... 0
> 04... 0
> 13... 1
> 15... 1
> (1) The waist-to-hip ratio is an
> important indicator of central obesity
> and is closely associated with the risk
> of metabolic syndrome.
> (2) Metabolic health is influenced by
> albumin and serum CA153 .
> There exists a bidirectional and
> biologically interpretable causal
> relationship between chronic systemic
> inflammation and tumor biomarkers
> associated with non-small cell lung
> cancer.
> (a) Existing methods
> (b) Our method
> Evidence
> Observed
> variables
> Latent
> variables
> Causal
> relationship
> Evidence :Based on factual evidence from literature, Wikipedia, and
> other sources
> Newly discovered
> variables
> Evidence used to
> support latent variables
> Causal
> graph
> Causal
> graph

Figure 1: A toy example of latent variable discovery using tabular data. 

based on rank constraints that can identify the complete causal structure involving both observed and latent variables. However, as shown in Figure 1(a), existing studies focus on recovering causal structures involving latent variables but rarely infer the specific latent variables and their semantics. In recent years, large language models (LLMs) [ 1, 16 ] have achieved remarkable breakthroughs in natural language understanding and generation, marking a critical milestone in the pursuit of artificial general intelligence. As the capabilities of LLMs continue to ad-vance, LLM-based agents [ 39 , 46 ] are increasingly becoming core components for integrating domain expertise and tools, effectively transforming these technological advances into practical applica-tions. Building upon this paradigm, multi-agent systems [ 26 , 41 ]incorporate multiple diverse agents to coordinate and leverage their respective strengths, offering high flexibility and adaptability to provide more comprehensive solutions to complex real-world problems. The rapid development of LLMs has also provided new solutions for causal discovery [54]. This paper focuses on a more challenging problem: traceable latent variable discovery , i.e., identifying specific latent variables and retrieving supporting evidence from multiple web data sources .This setting is more general and more practical for handling many real-world problems, such as uncovering unknown factors con-tributing to diseases. This challenge involves two fundamental questions: (i) How can multiple LLM-based agents be coordinated to efficiently propose and define possible latent variables? (ii) How can evidence be retrieved from diverse web data sources (e.g., aca-demic websites, Wikipedia, open databases, etc.) to consolidate the discovered latent variables? To address this challenging problem, we propose a multi-agent collaboration framework for latent variable discovery, which si-multaneously leverages real web data sources for evidence tracing in (causal) knowledge discovery. Specifically, we first employ la-tent variable causal discovery algorithms to recover causal graphs containing latent variables. Then, based on the recovered graph structure, we introduce game-theoretic principles, where multiple agents maintain their own belief networks to enable efficient col-laboration, and utilize Bayesian Nash Equilibrium to hypothesize latent variables. Finally, we utilize LLMs to iteratively retrieve and exploit causal evidence across multiple real-world web data sources. We conducted extensive experiments on our own real-world hospital dataset WCHSU as well as two benchmark datasets. Com-prehensive evaluation results demonstrate that TLVD exhibits clear advantages over existing SOTA methods. The experimental results and discussions, ablation studies, parameter analyses, case studies, and failure analyses, will be presented and analyzed in Section 4. The main contributions of this paper are as follows: 

â€¢ A unified latent variable discovery framework called TLVD, which integrates LLMs with TCDA and employs multi-agent collabora-tion to infer more plausible and semantically interpretable latent variables. 

â€¢ First, we incorporate the prior knowledge of LLMs, allowing them to serve as priors for latent variable explanation. Then, we model the multi-agent collaboration process as a game of incomplete information to efficiently reason about latent variables. Meanwhile, causal evidence is retrieved from multiple web data sources, and the latent variables are iteratively updated and validated. 

â€¢ Extensive experimental results on various real-world medical and generic benchmark datasets demonstrate the superiority of TLVD. 

2 Related work 2.1 Causal Discovery 

Traditional and LLM-based causal discovery algorithms [ 12 , 25 ] usu-ally assume that all task-relevant variables are observable [ 34 , 38 ]. However, latent variables are prevalent in practice and can lead these methods to produce spurious causal relations, which has mo-tivated extensive research on causal discovery with latent variables. Existing methods for handling latent variables in causal discovery can be broadly categorized into nine classes: Conditional inde-pendence constraints, Tetrad condition, Over-complete indepen-dent component analysis, Generalized independent noise, Mixture oracles-based, Rank deficiency, Heterogeneous data, and Score-based methods. Many methods fall within the constraint-based framework, combining conditional independence tests with alge-braic constraints to infer causal relations, with representative ap-proaches based on rank or tetrad constraints [ 11 , 21 ]. While most current methods still follow the constraint-based paradigm, some recent studies have started to formalize score-based approaches for latent causal discovery [ 35 , 55 ]. Although these methods have advanced the field of causal discovery, none of them attempt to identify the specific latent variables and their semantics. 

2.2 LLM-based Multi-Agent Systems 

LLMs have exhibited remarkable proficiency in tackling various sophisticated tasks. However, they still suffer from several inherent limitations , such as hallucination [ 22 ], their autoregressive nature (e.g., inability to engage in slow thinking [ 17 ]), and constraints imposed by scaling laws. Recent studies have extensively explored multi-agent collaboration frameworks based on LLMs, aiming to tackle complex cognitive and decision-making tasks [ 51 , 53 ]. One prominent paradigm involves explicit role-playing mechanisms to simulate human collaborative dynamics, assigning different LLM agents to specialized roles within an organization [ 20 ]. Other frame-works further enhance multi-agent collaboration through voting and consensus mechanisms [ 28 ], collective reasoning or discussion-based methods [ 7 ], and structured (e.g., graph [ 32 , 33 ]-based) agen-tic debate approaches [ 13 , 52 ], aiming to improve factual accuracy and logical consistency. However, current popular multi-LLM col-laboration frameworks lack solid theoretical foundations, offer no Traceable Latent Variable Discovery Based on Multi-Agent Collaboration WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates 

guarantees of convergence or cooperation, and have yet to be ex-plored in the field of causal discovery. 

3 Traceable Latent Variable Discovery Framework 

In this section, building upon game theory and reinforcement learn-ing, we propose an evidence-traceable multi-agent collaboration framework for exploring latent variables and their semantics. As illustrated in Figure 2, the framework primarily consists of three stages: (i) Stage I: Identification of latent causal graph structures; (ii) Stage II: Utilization of multi-agent collaboration to identify latent variables; (iii) Stage III: Validation of latent variables. It is impor-tant to note that this paper treats LLMs as large-scale background knowledge providers, while the core identifiability guarantees still rely on classical latent variable discovery algorithms. 

3.1 Identifying Latent Causal Graph Structures 

In this paper, we aim to address a more general scenario for latent variable causal discovery, where observed variables can be directly adjacent, and latent variables can flexibly be related to all other variables. That is, hidden variables can serve as confounders, me-diators, or effects of latent or observed variables, and even form a hierarchical structure (see Figure 1). This setup is quite general and practically meaningful for addressing many real-world prob-lems. RLCD [ 11 ] is currently the most advanced algorithm with theoretical guarantees, and thus, we use it to identify latent causal structures. In addition, we also explore the impact of other latent causal structure identification methods on the performance of our framework (see Appendix D). The specific formulation is as follows: 

Gâ€²

= ð‘…ð¿ð¶ð·  XG

 , (1) where XG denotes the data samples containing ð‘› observed variables, and Gâ€²

represents the Markov equivalence class. 

3.2 Identifying Latent Variables 

Previous research [ 11 , 21 ]has primarily focused on revealing the positional and structural information of latent variables, while over-looking the discovery of the latent variables themselves and their semantics. However, identifying latent variables is crucial for causal inference and its practical applications. Although LLMs are capable of performing complex tasks such as creative writing, reasoning, and decision-making, with abilities that in some aspects even sur-pass human level, they are still constrained by hallucinations [ 22 ], their autoregressive nature (e.g., inability to perform slow thinking [ 17 ]), and scaling laws [ 42 ]. Inspired by the theory of mind [ 14 ], multiple LLM-based agents systems have been developed, enabling teamwork and specialization by integrating the strengths and per-spectives of individual agents to achieve shared goals. Therefore, in this paper, we adopt LLM-based multi-agent collaboration to identify latent variables and their semantics. 

Xð‘– = ð‘€ð´ð¿ð¿ð‘€ (ð‘¥ ð‘– , ð¿ ð‘– ) , (2) where ð‘€ð´ð¿ð¿ð‘€ is the multi-agent collaboration system designed in this work, ð¿ ð‘– denotes the ð‘– -th latent variable, ð‘¥ ð‘– represents the Markov blanket corresponding to the ð¿ ð‘– in Gâ€²

, and Xð‘– denotes the concrete variable with explicit semantics corresponding to ð¿ ð‘– .However, achieving efficient collaboration among agents in MALLM is not straightforward and faces three key challenges . First, extensive inter-agent communication consumes a large number of tokens, increasing computational overhead [ 13 ]. Second, the volume of in-formation exchanged over multiple rounds can exceed the context-window capacity of LLMs, limiting system scalability [ 31 ]. Third, without well-defined coordination protocols, these systems may underperform compared to simple ensembling or self-consistency methods [ 29 ]. To address these challenges, we innovatively propose a novel approach for efficient coordination via Bayesian Nash Equi-librium (BNE), modeling multi-LLM interactions as an incomplete-information game to identify latent variables and their semantics. Next, we first define the process, then provide a detailed intro-duction to the MALLM, and finally conduct the theoretical analysis. 

3.2.1 Process Definition. We consider a system composed of ð‘ âˆ’ 1

execution LLMs and a coordinator LLM, where agents coordinate and interact solely based on their beliefs. Consistent with prior studies [ 30 ], we formally model this as a decentralized partially observable Markov decision process (DEC-POMDP), defined as a Markov game âŸ¨S , A, ð‘‚, P, Î©, R, ð›¾ âŸ©. Here, S represents the state space, including user queries and dialogue context; A = A1 Ã— Â· Â· Â· Ã— Að‘ is the joint action space, with each Að‘– defining agent ð‘– â€™s action as a prompt embedding ð‘Ž ð‘– = [ð‘‡ ð‘– , ð‘ ð‘– ], which controls the LLMâ€™s output behavior via temperature and repetition penalty; ð‘ is the number of agents; ð‘‚ is the joint observation space; P and Î© define the state transition and observation functions; R is the reward, and 

ð›¾ is the discount factor. Our objective is to identify a policy profile ðœ‹ = (ðœ‹ 1, . . . , ðœ‹ ð‘ ) that forms a BNE through belief coordination, such that no individual agent can improve the quality of its latent-variable generation and semantic inference by unilaterally changing its policy. 

3.2.2 BNE Implementation with MALLM. In this section, we in-troduce a MALLM framework compliant with DEC-POMDP. The MLLM adopts a hierarchical architecture where multiple executor LLMs operate locally under the guidance of a coordinator LLM. The framework consists of two stages: Inference (see Figure 4) and Op-timization (see Figure 3). Below, we provide a detailed introduction to the core modules. 

(1) Belief Update. Each execution LLM ð‘– maintains a belief network 

ðµ ð‘– (ðœ ð‘¡ ð‘– , ð‘œ ð‘¡ ð‘– ; ðœƒ ðµ ð‘– ), which implements its policy ðœ‹ ð‘– by mapping the local trajectory ðœ ð‘¡ ð‘– (composed of its previous actions and observations) and current observation ð‘œ ð‘¡ ð‘– âˆˆ ð‘‚ ð‘– into a belief state ð‘ ð‘– âˆˆ Rð‘‘ . The belief state characterizes the agentâ€™s understanding of the environ-ment and the behaviors of other agents under partial observability, enabling strategic decision-making without direct access to othersâ€™ outputs. The belief state ð‘ ð‘– is further used to generate the prompt embedding ð‘’ ð‘– = [ð‘‡ ð‘– , ð‘ ð‘– ], which controls the LLMâ€™s output regarding the latent variables and their semantics. The specific formulation is as follows: 

ð‘‡ ð‘– = ðœŽ (ð‘Š ð‘‡ ð‘ ð‘– + ð‘ ð‘‡ )

ð‘ ð‘– = ðœŽ  ð‘Š ð‘ ð‘ ð‘– + ð‘ ð‘ 

 , (3) where ðœŽ (Â·) is the sigmoid function. ð‘Š ð‘‡ and ð‘ ð‘‡ are learnable param-eters. The belief network ðµ ð‘– produces two outputs: (1) The prompt WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates Huaming Du et al. X-ray  Smoke  ...  Cancer 

0

0

1

0

0 ...  0

1 ...  0

0 ...  0

1 ...  0

X-ray  Smoke  ...  Cancer 

0

0

1

0

0 ...  0

1 ...  0

0 ...  0

1 ...  0

Numerical data 

Latent Causal 

Discovery 

Algorithm 

Latent 

variable 

Variable 

description 

L1 

L2 

L3 

xxxx 

xxxx 

xxxx 

Latent 

variable 

Variable 

description 

L1 

L2 

L3 

xxxx 

xxxx 

xxxx 

Knowledge Graph  Literature Data 

Causal structure with 

latent variables 

Judge A 

WIKIPEDIA 

Entities 

Relations 

A cause B? 

Input for Execution 

LLM  Answer 1  Answer 2  Answer 3 

Belief 

Network1 

Belief 

Network2 

Belief 

Network3 

Mixing 

Network 

MLP 

GRU 

MLP n

> t

# b 1âˆ’nt

# b( )

> n
> t
> n
> t

o

ï´

,1

> t

# b2

> t

# b3

> t

# b1

> t

# b3

> t

# b

Attention 3

> t

# e1

> t

# et

# E1

> t

# Q3

> t

# Q

Attention 3

> t

# F1

> t

# F

Final Output  Reward total t

# Q TD Loss 

Belief Encoder Belief Network2 

Belief Network1 

Belief Network2 

Agent1 

Agent2 

Agent3 

Query 

Coordinator LLM 

Belief 

Network 

X_1  Observed variables 

L1  Latent variables 

Causal relationship 

# ... 

MALLM 

Validation and Interpretation 

LLM 

MALLM:  multiple LLM -

based agents systems 

Figure 2: The overview of TLVD framework. 

embedding ð‘’ ð‘– , which serves as the action in the DEC-POMDP frame-work; (2) The local Q-value ð‘„ ð‘¡ ð‘– (ðœ ð‘¡ ð‘– , ð‘’ ð‘¡ ð‘– ; ðœ‘ ð‘– ), which estimates the ex-pected return from the current belief state. The belief state ð‘ ð‘– is also passed to a belief encoder for group-level processing. To optimize the belief network, we apply a Temporal Difference (TD) loss to update the parameters ðœƒ ðµ ð‘– = ðœ‘ ð‘– ,ð‘Š ð‘‡ , ð‘ ð‘‡ ,ð‘Š ð‘ , ð‘ ð‘ , which include the Q-value function parameters ðœ‘ ð‘– and the prompt embed-ding parameters, as illustrated in Figure 3. The TD loss is defined as follows: 

Lð‘– ð‘‡ ð· (ðœƒ ðµ ð‘– ) = ED

ï£®ï£¯ï£¯ï£¯ï£¯ï£°

ð‘Ÿ ð‘¡ ð‘– + ð›¾ max  

> ð‘’ ð‘¡ +1
> ð‘–

ð‘„ ð‘¡ +1 

> ð‘–

(ðœ ð‘¡ +1 

> ð‘–

, ð‘’ ð‘¡ +1 

> ð‘–

; ðœ‘ â€² 

> ð‘–

) âˆ’ ð‘„ ð‘¡ ð‘– (ðœ ð‘¡ ð‘– , ð‘’ ð‘¡ ð‘– ; ðœ‘ ð‘– )

!2ï£¹ï£ºï£ºï£ºï£ºï£»

, (4) where ð‘Ÿ ð‘¡ ð‘– = R ( ð‘  ð‘¡ , ð‘Ž ð‘¡ )ð‘– denotes the local reward signal, and ðœ‘ â€² 

> ð‘–

rep-resents the target network parameters updated via soft update mechanism. By minimizing Lð‘– ð‘‡ ð· , execution LLM ð‘– refines its belief state to improve local decision-making. To enable agents to implic-itly exchange beliefs and thereby facilitate system convergence to BNE, we employ an attention mechanism within the belief encoder to capture inter-agent dependencies in the belief states: 

B = Attention (ð‘Š ð‘„ b,ð‘Š ð¾ b,ð‘Š ð‘‰ b; ðœƒ ð‘’ ) , (5) where b = [ð‘ 1; . . . ; ð‘ ð‘ ] âˆˆ Rð‘ð‘‘ is the concatenated vector of the in-dividual belief states {ð‘ ð‘– }ð‘ ð‘– =1. The final output is: ð¸ = Bð‘Š ð‘‚ , where 

{ð‘Š ð‘„ ,ð‘Š ð¾ ,ð‘Š ð‘‰ } are learnable parameters and ð‘Š ð‘‚ is the output pro-jection. The belief encoder captures high-level interactions among execution LLMs, ensuring coherence in group behavior. 

(2) Mixing Network. Inspired by Qmax [ 40 ], the mixing network coordinates the integrated belief information from all execution LLMs, thereby driving the overall system toward optimization with respect to the BNE. Specifically, each agentâ€™s prompt embedding 

{ð‘’ ð‘– ð‘¡ }ð‘ ð‘– =1 is first processed through a self-attention mechanism to cap-ture inter-agent dependencies, producing intermediate embeddings 

{ðœ ð‘– ð‘¡ }ð‘ ð‘– =1. These intermediate embeddings {ðœ ð‘– ð‘¡ }ð‘ ð‘– =1 are then combined with the group-level belief representation ð¸ ð‘¡ to produce feature transformations {ð¹ ð‘– ð‘¡ }ð‘ ð‘– =1. This combination enables the network Reward TD Loss 

> Belief
> Network 1
> Belief
> Network 2
> Belief
> Network n

Belief Encoder  

> MLP
> GRU
> MLP
> Attention
> Attention
> Mixing Network
> Concatenate
> Optimization Phase
> for Belief Network
> Belief
> Network
> Final
> Output

...   

> Agent 1 Agent 2 Agent n
> ...
> ...
> ...
> ...

Figure 3: The train process of MALLM. 

to jointly integrate individual belief information (captured in ð‘’ ð‘– ð‘¡ )and collective belief dynamics (captured in ð¸ ð‘¡ ), thereby facilitat-ing coordinated optimization. Next, the local Q-values {ð‘„ ð‘– ð‘¡ }ð‘ ð‘– =1 and the transformed features {ð¹ ð‘– ð‘¡ }ð‘ ð‘– =1 are fed together into multi-head attention layers to compute the global Q-value ð‘„ ð‘¡ð‘œð‘¡ ð‘¡ . This global Q-value function accounts for localâ€“global interactions, ensuring that improvements in individual behaviors also contribute to over-all performance enhancement. To train the mixing network, the following loss function is minimized: 

Lmix (ðœ™ ) =ED

h ð‘Ÿ ð‘¡ð‘œð‘¡ + ð›¾ max   

> {ð‘’ ð‘¡ +1
> ð‘– }ð‘ ð‘– =1

ð‘„ ð‘¡ +1 

> ð‘¡ð‘œð‘¡

(ðœ ð‘¡ +1, {ð‘’ ð‘¡ +1 

> ð‘–

}ð‘ ð‘– =1; ðœ™ â€²)âˆ’ 

ð‘„ ð‘¡ ð‘¡ð‘œð‘¡ (ðœ ð‘¡ , {ð‘’ ð‘¡ ð‘– }ð‘ ð‘– =1; ðœ™ )2 i

+ ðœ† ð‘š ð‘ âˆ‘ï¸ 

> ð‘– =1

âˆ¥ð‘„ ð‘¡ ð‘– âˆ’ ð‘„ ð‘¡ ð‘¡ð‘œð‘¡ âˆ¥2 ,

(6) the term âˆ¥ð‘„ ð‘¡ ð‘– âˆ’ð‘„ ð‘¡ ð‘¡ð‘œð‘¡ âˆ¥2 ensures that local Q-values remain consistent with the global estimate. In addition, the target network parameters 

ðœ™ â€² are updated using a soft update rule: ðœ™ â€² â† ðœðœ™ + ( 1 âˆ’ ðœ )ðœ™ â€².Traceable Latent Variable Discovery Based on Multi-Agent Collaboration WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates Li cause X1?

> Query
> Input for agent
> Final Output Answer 1
> Answer 2
> Answer n
> Belief Network 1
> Belief Network 2
> Belief Network n
> Agent 1
> Agent 2
> Agent n
> Coordinator LLM
> Infer ence Phase
> Verification

... ... 

Figure 4: The reasoning process of MALLM. 

Through this mechanism, the mixing network can optimize local policies to improve the global objective, thereby promoting stable convergence during training. 

(3) Reward Design. The reward function Rdesign consists of the following four components . (a) Action Likelihood Reward ð‘Ÿ ð´ð¿ ð‘– =

min (ð‘… ð‘šð‘Žð‘¥ , sim (ð‘¢ ð‘– , ð¶ )) measures the consistency of the final output with the target via cosine similarity sim (ð‘¢ ð‘– , ð¶ ) = ð‘¢ ð‘– Â·ð¶    

> âˆ¥ð‘¢ ð‘– âˆ¥ âˆ¥ ð¶ âˆ¥

, where ð¶ 

denotes the output of the coordinator and ð‘¢ ð‘– represents the output of an individual executor. (b) Uncertainty Reduction Reward ð‘Ÿ ð‘ˆ ð‘… ð‘– =

Cð‘œð‘› ð‘– Â· sim (ð‘¢ ð‘– , ð¶ ) assesses the uncertainty of LLMs by considering both the modelâ€™s direct confidence and the similarity to the overall answer, where Cð‘œð‘› ð‘– represents the confidence of agent ð‘– . (c) Collab-orative Contribution Reward ð‘Ÿ ð¶ð¶ ð‘– = min (ð‘… ð‘šð‘Žð‘¥ , quality (ð‘¢ ð‘– , {ð‘¢ ð‘— }ð‘— â‰ ð‘– )) 

assesses each agentâ€™s contribution to the collective solution [ 50 ]. 

(d) Evidence Reliability Reward ð‘Ÿ ð¸ð‘… ð‘– = ð¶ð´ð‘ð‘ ð‘– measures the reliability of the evidence corresponding to each query result across multiple web data sources, where ð¶ð´ð‘ð‘ represents the evidence ratio. For further details, see Appendix A. The total reward is computed as: 

ð‘Ÿ ð‘– = ð›¼ 1ð‘Ÿ ð´ð¿ ð‘– + ð›¼ 2ð‘Ÿ ð‘ˆ ð‘… ð‘– + ð›¼ 3ð‘Ÿ ð¶ð¶ ð‘– + ð›¼ 4ð‘Ÿ ð¸ð‘… ð‘– , where ð›¼ 1 + ð›¼ 2 + ð›¼ 3 + ð›¼ 4 = 1.

(4) Inference Phase. During the inference phase, the coordinator LLM distributes query information containing latent variables to multiple execution LLMs, which independently generate their own answers. By incorporating the retrieved evidence from Section 3.3, the coordinator LLM aggregates both the answers and the evidence to produce the final output, as shown in Figure 4. 

3.2.3 Theoretical Analysis. In this subsection, we mainly analyze the existence and convergence of BNE. 

(1) Existence of BNE. To bridge the DEC-POMDP formulation with BNE analysis, we treat each agent ð‘– as a player, where each player forms and updates beliefs about other agentsâ€™ types based on a common prior and its own observations. Each playerâ€™s type 

ðœƒ ð‘– is determined by their internal beliefs and private observation history. Formally, if there exists a strategy profile {ðœ‹ âˆ— 

> ð‘–

}ð‘ ð‘– =1 such that for any player ð‘– :

Eðœƒ âˆ’ð‘– 

ð‘ˆ ð‘– (ðœ‹ âˆ— 

> ð‘–

(ðœƒ ð‘– ), ðœ‹ âˆ—âˆ’ð‘– (ðœƒ âˆ’ð‘– ))  â‰¥ Eðœƒ âˆ’ð‘– 

ð‘ˆ ð‘– (ðœ‹ â€² 

> ð‘–

(ðœƒ ð‘– ), ðœ‹ âˆ—âˆ’ð‘– (ðœƒ âˆ’ð‘– )) , âˆ€ðœ‹ â€² 

> ð‘–

, (7) where ðœƒ âˆ’ð‘– = (ðœƒ 1, . . . , ðœƒ ð‘– âˆ’1, ðœƒ ð‘– +1, . . . , ðœƒ ð‘ ) denotes the types of all agents except ð‘– . The utility function is defined as: 

ð‘ˆ ð‘– (ðœ‹ âˆ— 

> ð‘–

, ðœ‹ âˆ—âˆ’ð‘– , ðœƒ ð‘– , ðœƒ âˆ’ð‘– ) = E

" âˆžâˆ‘ï¸ 

> ð‘¡ =0

ð›¾ ð‘¡ ð‘Ÿ ð‘¡ ð‘– |ðœ‹ âˆ— 

> ð‘–

, ðœ‹ âˆ—âˆ’ð‘– , ðœƒ ð‘– , ðœƒ âˆ’ð‘– 

#

, (8) where ð‘Ÿ ð‘¡ ð‘– = ð‘… (ð‘  ð‘¡ , ð‘Ž ð‘¡ )ð‘– denotes the reward obtained by player ð‘– at time ð‘¡ , ð‘ denotes the number of players. 

Theorem 3.1. (Existence of BNE) In our MALLM framework, as-suming certain conditions [ 51 ] hold, then by Glicksbergâ€™s Fixed Point Theorem [ 2], there exists a BNE strategy profile ðœ‹ âˆ— = (ðœ‹ âˆ— 

> 1

, . . . , ðœ‹ âˆ— 

> ð‘

). A complete proof is provided in Appendix B.1. 

(2) Convergence of BNE. We analyze the convergence of the TLVD framework through Bayesian regret. First, to connect the theoretical analysis with the DEC-POMDP formulation, we define the Bayesian regret of each agent ð‘– over ð‘‡ steps as: 

ð‘… ð‘– (ð‘‡ ) = Eð‘  ð‘¡ ,ðœ‹ ð‘¡ 

" ð‘‡ âˆ‘ï¸  

> ð‘¡ =1

 ð‘‰ âˆ— 

> ð‘–

(ð‘  ð‘¡ ) âˆ’ ð‘‰ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ )#

, (9) where the optimal value function under BNE is: 

ð‘‰ âˆ— 

> ð‘–

(ð‘  ) = max 

> ðœ‹ ð‘–

Eðœ‹ âˆ—âˆ’ð‘– 

" âˆžâˆ‘ï¸ 

> ð‘¡ =0

ð›¾ ð‘¡ R ( ð‘  ð‘¡ , ð‘Ž ð‘¡ )ð‘– | ð‘  0 = ð‘ , ðœ‹ ð‘– , ðœ‹ âˆ—âˆ’ð‘– 

#

, (10) and ð‘‰ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ) is the value function under the current strategy profile 

ðœ‹ ð‘¡ = (ðœ‹ ð‘¡ 

> 1

, . . . , ðœ‹ ð‘¡ ð‘ ) at time ð‘¡ . The expectation accounts for random-ness in both state transitions (governed by ð‘ƒ ) and policy selections. The total Bayesian regret is therefore defined as: ð‘… (ð‘‡ ) = Ãð‘ ð‘– =1 ð‘… ð‘– (ð‘‡ ).Second, we introduce standard assumptions [ 51 ] and propose Lemma 3.1, which provides an upper bound on the Bayesian re-gret. A proof sketch is given below, with more details provided in Appendices B.2, C.1, and C.2. 

Lemma 3.1. (Performance Difference) For joint policies ðœ‹ = (ðœ‹ ð‘– , ðœ‹ âˆ’ð‘– )

and ðœ‹ â€² = (ðœ‹ â€² 

> ð‘–

, ðœ‹ â€²âˆ’ð‘– ), the difference in their value functions satisfies: 

ð‘‰ ðœ‹ â€² 

> ð‘–

(ð‘  ) âˆ’ ð‘‰ ðœ‹ ð‘– (ð‘  ) = 11âˆ’ð›¾ Eð‘  âˆ¼ð‘‘ ðœ‹ â€²

h

Eð‘Ž âˆ¼ðœ‹ â€² ð‘„ ðœ‹ ð‘– (ð‘ , ð‘Ž ) âˆ’ Eð‘Ž âˆ¼ðœ‹ ð‘„ ðœ‹ ð‘– (ð‘ , ð‘Ž )

i

, (11) 

where ð‘‘ ðœ‹ â€² is the state distribution under policy ðœ‹ â€², and ð‘Ž = (ð‘Ž ð‘– , ð‘Ž âˆ’ð‘– )

denotes the joint action from the action space A.

Note that the function ð‘„ ðœ‹ ð‘– (ð‘ , ð‘Ž ) in this lemma will be approxi-mated by neural networks in our implementation (see Section 3.2.2 for details). Consistent with existing studies [ 15 , 51 ], we apply this lemma to our regret analysis and obtain:                  

> ð‘… (ð‘‡ )=Ãð‘ ð‘– =111âˆ’ð›¾ Eð‘  ð‘¡ ,ðœ‹ ð‘¡
> "Ãð‘‡ ð‘¡ =1
> 
> Eð‘Ž âˆ—
> ð‘¡ âˆ¼ðœ‹ âˆ—ð‘„ ðœ‹ ð‘¡
> ð‘– (ð‘  ð‘¡ , ð‘Ž âˆ—
> ð‘¡ ) âˆ’ Eð‘Ž ð‘¡ âˆ¼ðœ‹ ð‘¡ ð‘„ ðœ‹ ð‘¡
> ð‘– (ð‘  ð‘¡ , ð‘Ž ð‘¡ )
> #

, (12) where ðœ‹ âˆ— denotes the BNE policies. By bounding the suboptimality 

ð›¿ ð‘¡ , we obtain: 

Eð‘Ž âˆ—  

> ð‘¡ âˆ¼ðœ‹ âˆ—

ð‘„ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž âˆ— 

> ð‘¡

) âˆ’ Eð‘Ž ð‘¡ âˆ¼ðœ‹ ð‘¡ ð‘„ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž ð‘¡ ) â‰¤ ð›¿ ð‘¡ , (13) where ð›¿ ð‘¡ = ð‘‚ (1/âˆšð‘¡ ) bounds policy suboptimality, ð‘‚ (Â·) is the as-ymptotic upper bound. Under standard regularity conditions, these errors can be bounded by constants ð¶ ð›¿ , respectively, which yields: 

ð‘… (ð‘‡ ) â‰¤ 

> ð‘

âˆ‘ï¸ 

> ð‘– =1

11 âˆ’ ð›¾ ð¶ ð›¿ ð‘‡ âˆ‘ï¸ 

> ð‘¡ =1

1

âˆšð‘¡ = ð‘‚ 

 ð‘ âˆšð‘‡ 

1 âˆ’ ð›¾ 



. (14) 

3.3 Verification of Latent Variables 

Despite the ability of multiple agents in TLVD to infer potential la-tent variables, further validation is necessary to ensure their plausi-bility and to achieve the traceability of the proposed method. There-fore, we leverage data sources (e.g., arXiv, Wikipedia, databases, and patientsâ€™ anonymized personal text reports) for verification, WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates Huaming Du et al. 

and feed the retrieved evidence back to the execution LLMs and the coordinator LLM to update their strategies and beliefs. 

3.4 Complexity Analysis 

The space complexity of our proposed TLVD mainly depends on the belief networks, belief encoder, and mixing network. For the belief networks, the complexity is O( ð‘ð‘‘ð‘‘ â€²), where ð‘‘ denotes the input dimension, and ð‘‘ â€² denotes the hidden layer dimension; for the belief encoder, the complexity is O( ð‘ð‘‘ â€²2); and for the mixing network, the complexity is O( ð‘‘ â€²2). Therefore, the space complexity of TLVD can be summarized as O( ð‘ð‘‘ â€²2).

4 Experiments 

Our study primarily focuses on the following research questions: 

RQ1 : How does the performance of TLVD compare with existing methods? RQ2 : What is the impact of model configurations on overall performance? RQ3 : How does each component of TLVD affect the overall performance? RQ4 : How do hyperparameter settings affect the performance of TLVD? RQ5 : How does TLVD operate in real-world examples? RQ6 : What is the impact of web data sources on TLVD, and what are the main causes of its failures? 

4.1 Experimental Setup 

4.1.1 Datasets & evaluations. In this section, we evaluate our method on five real-world datasets. As shown in Table 1, we utilized the de-identified WCHSU-Cancer and WCHSU-Pain datasets from real hospitals scenarios. Additionally, we also use two generic domain benchmark datasets (Multitasking Behaviour Study [ 19 ] and Teacherâ€™s Burnout Study [ 5]). Please note that the WCHSU dataset is newly collected and has not been made publicly available online, making it unlikely to have been used in training any existing LLMs, and there-fore posing no risk of data leakage . The evaluation metrics are ACC, CAcc, and ECit. Detailed descriptions can be found in Appendix A. For each dataset, we conduct five experiments with different random seeds and report the average performance. 

4.1.2 Baselines. We compare four types of baselines: single LLMs, deep research agents, multi-agent platforms, and multi-LLM reason-ing frameworks. The single LLMs mainly include GPT-5. The deep research agents include WideSearch [ 47 ], Gemini-deepresearch, Openai-deepresearch, Qwen-deepresearch, and Doubao-deepresearch. The multi-agent platforms include Autogen [ 48 ] and MiniMax, whereas the multi-LLM reasoning frameworks include CAMEL [27], Multiagent (Majority) [28], and Multiagent (Debate) [13]. 

4.1.3 Implementation details. In this section, the TLVD framework consists of one coordinator and two executor LLMs. During training, the episodes per task is set to 100, buffer size |D| to 32, optimizer to Adam, learning rate to 0.001, discount factor to 0.99, entity di-mension to 256, belief state dimension to 128, MLP hidden size to 256, and ðœ† ð‘š to 0.1. ð‘… ð‘šð‘Žð‘¥ is 1. For the early stopping mechanism, we utilize the same termination threshold settings as in previous studies [ 51 ]. To ensure a fair comparison with baseline methods, we employed three identical models across these LLMs. For hetero-geneous results, we also evaluated TLVD using different models, as shown in Table 4. All evaluations were conducted under a zero-shot setting on a server equipped with an NVIDIA GeForce A6000 GPU 

Table 1: Statistics of datasets. Dataset #Domain #Sample #Number of observed variables #Number of latent variables                 

> WCHSU-Cancer Medical 200,000 12/22 6/4
> WCHSU-Pain Medical 1,568 16 4
> Multitasking Behaviour Study Social Science 202 94
> Teacherâ€™s Burnout Study Social Science 599 32 11

with 48GB of memory. For the LLMs used to validate latent variables in web data sources, we employed the same LLMs as those used for the executors. For each causal validation query, we retrieved the top five most relevant texts from arXiv and Wikipedia each time. 

4.2 RQ1: Comparisons and Analysis 

Table 2 shows a performance comparison of different methods on the three WCHSU datasets. The results indicate that TLVD out-performs all baseline methods across all evaluation metrics. On average, in terms of ACC, TLVD achieves a 131.68% improvement over MiniMax, a 124.30% improvement over Autogen, and a 58.07% improvement over OpenAI-deepresearch. Moreover, when we con-structed new datasets by randomly sampling the original data (re-sults in Table 3), TLVD still maintained the top performance: It outperformed the runner-up, OpenAI-deepresearch, with average improvement of 32.96% in ACC, 239.72% in CAcc, and 52.86% in ECit. Notably, TLVD achieves performance improvements with fewer communication tokens compared to CAMEL, Multi-Agent Debate, and Multi-Agent Majority, which can be attributed to the design of our belief network (see Figure 10 in Appendix D). 

4.3 RQ2: Model Configuration Analysis 

To evaluate the impact of the coordinator LLM and execution LLM performance on the TLVD, and to investigate whether hetero-geneous execution LLMs can also achieve a BNE, we conducted two types of experiments : one pairing a strong coordinator LLM with weaker execution LLMs, and another pairing a weaker coor-dinator LLM with stronger execution LLMs. These experiments were further divided into homogeneous and heterogeneous exe-cution groups for detailed analysis. To ensure a fair comparison, the coordinator LLM was consistently set to Kimi k2 32B across all experiments. For the heterogeneous execution group, we used the following configurations: LLaMA3.1 8B and Qwen2.5 7B; as well as another configuration consisting of Qwen 2.5 72B and LLaMA3.1 70B. For the homogeneous execution group, two configurations were tested: one with two weak models (LLaMA 3.1 8B) and an-other with two strong models (LLaMA 3.1 70B). As shown in Table 4, stronger execution LLMs achieve BNE more efficiently by pro-viding higher-quality answers. Additionally, heterogeneous models perform worse than homogeneous models due to the increased difficulty in reaching BNE. 

4.4 RQ3: Ablation Studies 

To evaluate the effectiveness of different components in TLVD, we design the following variants: TLVD-v : Removes the multi-agent LLM collaboration module and uses only a voting strategy. TLVD-d :Traceable Latent Variable Discovery Based on Multi-Agent Collaboration WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates 

Table 2: Performance evaluation of different models. WCHSU-Cancer ( ð‘› = 12 ) WCHSU-Cancer ( ð‘› = 22 ) WCHSU-Pain Methods ACC CAcc ECit ACC CAcc ECit ACC CAcc ECit GPT5 0.134 Â±0.067 0.100 Â±0.094 0.031 Â±0.029 0.150 Â±0.122 0.116 Â±0.039 0.079 Â±0.027 0.150 Â±0.122 0.095 Â±0.061 0.095 Â±0.061 

Gemini-deepresearch 0.267 Â±0.081 0.286 Â±0.046 0.154 Â±0.024 0.300 Â±0.100 0.347 Â±0.042 0.236 Â±0.029 0.250 Â±0.000 0.338 Â±0.031 0.284 Â±0.026 

OpenAI-deepresearch 0.433 Â±0.082 0.560 Â±0.049 0.215 Â±0.019 0.550 Â±0.100 0.214 Â±0.165 0.214 Â±0.089 0.550 Â±0.100 0.253 Â±0.050 0.200 Â±0.039 

Qwen-deepresearch 0.200 Â±0.066 0.500 Â±0.000 0.077 Â±0.000 0.250 Â±0.000 0.271 Â±0.029 0.271 Â±0.029 0.300 Â±0.100 0.500 Â±0.158 0.105 Â±0.033 

Doubao-deepresearch 0.000 Â±0.000 0.150 Â±0.050 0.038 Â±0.070 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.150 Â±0.122 0.000 Â±0.000 0.000 Â±0.000 

Autogen 0.300 Â±0.125 0.000 Â±0.000 0.000 Â±0.000 0.450 Â±0.100 0.255 Â±0.036 0.200 Â±0.029 0.350 Â±0.122 0.089 Â±0.044 0.084 Â±0.042 

MiniMax 0.467 Â±0.067 0.600 Â±0.000 0.231 Â±0.000 0.300 Â±0.100 0.192 Â±0.030 0.171 Â±0.027 0.300 Â±0.100 0.071 Â±0.014 0.053 Â±0.008 

CAMEL 0.267 Â±0.081 0.000 Â±0.000 0.000 Â±0.000 0.400 Â±0.122 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 

Multi-Agent-Majority 0.134 Â±0.067 0.000 Â±0.000 0.000 Â±0.000 0.200 Â±0.100 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 

Multi-Agent Debate 0.234 Â±0.081 0.000 Â±0.000 0.000 Â±0.000 0.350 Â±0.122 0.000 Â±0.000 0.000 Â±0.000 0.200 Â±0.100 0.000 Â±0.000 0.000 Â±0.000 

TLVD 0.833 Â±0.000 0.900 Â±0.033 0.415 Â±0.015 0.750 Â±0.000 0.920 Â±0.040 0.329 Â±0.014 0.800 Â±0.100 0.860 Â±0.049 0.453 Â±0.026 

Table 3: Performance evaluation of different models, where we randomly sample half of the original dataset. WCHSU ( ð‘› = 12 ) WCHSU ( ð‘› = 22 ) WCHSU-Pain Methods ACC CAcc ECit ACC CAcc ECit ACC CAcc ECit GPT5 0.120 Â±0.098 0.074 Â±0.042 0.074 Â±0.042 0.000 Â±0.000 0.111 Â±0.070 0.033 Â±0.021 0.133 Â±0.163 0.51 Â±0.0037 0.36 Â±0.0056 

Gemini-deepresearch 0.240 Â±0.013 0.084 Â±0.042 0.084 Â±0.042 0.200 Â±0.163 0.160 Â±0.013 0.160 Â±0.013 0.266 Â±0.133 0.000 Â±0.000 0.000 Â±0.000 

OpenAI-deepresearch 0.600 Â±0.126 0.388 Â±0.029 0.347 Â±0.026 0.667 Â±0.000 0.180 Â±0.016 0.180 Â±0.016 0.600 Â±0.134 0.318 Â±0.047 0.318 Â±0.047 

Qwen-deepresearch 0.440 Â±0.080 0.225 Â±0.094 0.095 Â±0.039 0.200 Â±0.163 0.289 Â±0.089 0.087 Â±0.027 0.266 Â±0.133 0.000 Â±0.000 0.000 Â±0.000 

Doubao-deepresearch 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.133 Â±0.163 0.000 Â±0.000 0.000 Â±0.000 0.067 Â±0.133 0.000 Â±0.000 0.000 Â±0.000 

Autogen 0.480 Â±0.098 0.000 Â±0.000 0.000 Â±0.000 0.333 Â±0.000 0.186 Â±0.042 0.173 Â±0.039 0.400 Â±0.250 0.000 Â±0.000 0.000 Â±0.000 

MiniMax 0.560 Â±0.080 0.000 Â±0.000 0.000 Â±0.000 0.533 Â±0.164 0.400 Â±0.122 0.053 Â±0.016 0.467 Â±0.267 0.350 Â±0.062 0.247 Â±0.044 

CAMEL 0.360 Â±0.080 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.266 Â±0.133 0.000 Â±0.000 0.000 Â±0.000 

Multi-Agent-Majority 0.160 Â±0.080 0.000 Â±0.000 0.000 Â±0.000 0.133 Â±0.163 0.000 Â±0.000 0.000 Â±0.000 0.067 Â±0.133 0.000 Â±0.000 0.000 Â±0.000 

Multi-Agent Debate 0.320 Â±0.098 0.000 Â±0.000 0.000 Â±0.000 0.333 Â±0.211 0.175 Â±0.061 0.047 Â±0.016 0.266 Â±0.133 0.000 Â±0.000 0.000 Â±0.000 

TLVD 0.800 Â±0.126 0.886 Â±0.057 0.467 Â±0.112 0.734 Â±0.133 0.914 Â±0.070 0.350 Â±0.064 0.933 Â±0.133 0.900 Â±0.082 0.412 Â±0.043 

Removes the multi-agent LLM collaboration module and uses only a debate strategy. TLVD-I : Uses a single LLM to generate latent variables and their semantics. TLVD-R1 : Uses only the Action Like-lihood Reward. TLVD-R2 : Considers both the Action Likelihood Reward and the Uncertainty Reduction Reward. TLVD-R3 : Con-siders the Action Likelihood Reward, the Uncertainty Reduction Reward, and the Collaborative Contribution Reward. We conducted ablation experiments using WCHSU-Cancer (n=12) and WCHSU-Pain as examples. Additionally, we analyzed the impact of different LLM types on the results. As shown in Figure 8 and Table 7 (see Appendix D), we have the following observations: (i) TLVD achieves the best performance when all components are included. (ii) The Uncertainty Reduction Reward and the Collaborative Contribution Reward have a sig-nificant impact on model performance. (iii) There are noticeable performance differences among different LLMs, with GPT-oss-120B and DeepSeek-v3 performing relatively better. 

4.5 RQ4: Parameter Sensitivity 

In the WCHSU-Cancer ( ð‘› = 12 ) and WCHSU-Pain datasets, we further investigate the impact of the number of execution LLMs ð‘ 

and the discount factor ð›¾ on model performance. As shown in Figure 

Table 4: Performance of different configurations. WCHSU-Cancer ( ð‘› = 12 ) WCHSU-Pain Methods ACC CAcc ECit ACC CAcc ECit Homo. (2 Ã— LLaMA3.1 8B) 0.500 1.000 0.115 0.500 1.000 0.474                                

> Homo. (2 Ã—LLaMA3.1 70B) 0.833 1.000 0.462 0.850 1.000 0.737
> Hetero. (LLaMA3.1 8B, Qwen2.5 7B) 0.333 1.000 0.346 0.250 1.000 0.421
> Hetero. (LLaMA3.1 70B, Qwen2.5 72B) 0.500 1.000 0.346 0.750 1.000 0.632
> TLVD 0.833 0.900 0.415 0.800 0.886 0.453
> LLAMA-3.1-8B (Zero-shot) 0.000 0.000 0.000 0.250 0.191 0.057

9, the results indicate that when the number of execution LLMs exceeds six, performance gains become very limited, and in some cases, performance even declines. When using weaker LLMs (e.g., LLaMA 3.1 8B), the number of execution LLMs has little impact on model performance. In contrast, with stronger LLMs (e.g., LLaMA 3.1 70B), increasing the number of execution LLMs yields some performance improvements. We attribute this to the challenges faced by the coordinator LLM when managing too many execution LLMs, making it difficult to achieve effective coordination through information from additional agents. Moreover, as the discount factor 

ð›¾ increases, model performance shows an upward trend. WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates Huaming Du et al. : Non_small_cell_lung_cancer_antigen  

> : NLR : PLR

Input 

Execu1 

Execu2    

> L1ï¼šPathogenesis of non-small cell lung cancer (NSCLC)
> Evidence: Measurable elevations of NSCLC tumor antigens in the peripheral
> circulation provide the most direct evidence of the presence and metabolic
> activity of NSCLC .Answer1

Coordinator   

> L1ï¼šNSCLC Progression/Inflammation Factor .
> Evidence: NSCLC Progression/Inflammation Factor
> simultaneously drives the elevation of NLR and PLR
> through shared cytokines and signaling pathways, and leads
> to the release of tumor-specific antigens.

Output       

> L1ï¼šNSCLC Progression/Inflammation Factor
> Evidence: This pathophysiological state simultaneously drives the elevation
> of NLR and PLR through shared cytokines and signaling pathways, and leads
> to the release of tumor-specific antigens. Answer2
> Question: Latent variable L1simultaneously causes a patientâ€™s NLR
> (neutrophil-to-lymphocyte ratio), PLR (platelet-to-lymphocyte ratio),
> and Non-Small Cell Lung Cancer Antigen. Based on your knowledge,
> output the most likely latent variable and its description.
> Blief 1
> Blief 2
> https://arxiv.org/
> abs/2510.05920
> https://en.wikipedia.org/wiki/
> Adenocarcinoma_of_the_lung

Figure 5: Case study. 

4.6 RQ5: Case Studies 

We take the specific latent variable ð¿ 1 from the WCHSU-Cancer (ð‘› =22) dataset as an example to demonstrate the latent variables identified by our method and the supporting evidence. As shown in Figure 5, Executors 1 and 2 output ð¿ 1 and its semantics based on their own beliefs and observations. Through the latent variable verification process, retrieved information is returned. The coordi-nator aggregates all information to infer the final latent variables and simultaneously outputs the corresponding causal evidence. 

4.7 RQ6: Other In-Depth Analysis 

4.7.1 Impact of Data Sources. We further analyzed the impact of different data sources on TLVD to validate its applicability under varying conditions. We explored three scenarios on the WCHSU-Pain dataset: the W/O ARR model excludes literature from arXiv, the W/O WIKI model excludes literature from WIKIPEDIA, and the W/O DB model excludes medical-related knowledge graphs. As shown in Figure 6, we observe that articles on arXiv contain more medically relevant causal information, which aligns well with reality and is consistent with existing studies [ 8 ]. The influence of the databases on our model is relatively small, which is reason-able since the data we use contains knowledge that is not publicly available and is rarely stored or used in existing databases. 

4.7.2 Error Analysis. To further dissect our approach, we employed the MAST taxonomy [ 6 ] to attribute failures in multi-agent systems. Through the analysis of 100 execution traces, we computed the fre-quency of occurrence for each failure mode and category. Detailed definitions and examples for each failure mode can be found in existing studies [ 6 ]. As shown in Figure 7, the primary causes of multi-agent system failures are Specification Issues and Task Verifi-cation. For Specification Issues, failures stem from system design decisions and incomplete or ambiguous prompts, mainly includ-ing vague task instructions and unclear role definitions. For Task Verification, failures are primarily related to output quality control, such as insufficient verification or premature task termination. TLVD      

> W/O ARR
> W/O WIKI
> W/O DB
> TLVD
> W/O ARR
> W/O WIKI
> W/O DB
> TLVD
> W/O ARR
> W/O WIKI
> W/O DB
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 1.2
> Score  0.80
> 0.25
> 0.50
> 0.80 0.86
> 0.40
> 0.74 0.82
> 0.45
> 0.21
> 0.39 0.43
> ACC CAcc ECit

Figure 6: Impact of data sources on model performance. 0 5 10 15 20 25 

> Disobey Task Specification
> Disobey Role Specification
> Step Repetition
> Loss of Conversation History
> Unaware of Termination Conditions
> Conversation Reset
> Fail to Ask for Clarification
> Task Derailment
> Information Withholding
> Ignored Other Agent's Input
> Reasoning-Action Mismatch
> Premature Termination
> No or Incomplete Verification
> Incorrect Verification
> 15.00%
> 2.00%
> 11.00%
> 0.00%
> 9.00%
> 0.00%
> 2.00%
> 8.00%
> 6.00%
> 5.00%
> 9.00%
> 8.00%
> 10.00%
> 15.00%
> Poor Specification 37.00%
> Inter-Agent Misalignment 30.00%
> Task Verification 33.00%

Failure attribution analysis of our method 

Figure 7: Failure Attribution of Our Method. Note that the vertical axis represents specific failure modes, and the hori-zontal axis represents their respective proportions. 

5 Conclusion 

In this work, we propose TLVD, a multi-LLM collaborative frame-work for traceable latent variable discovery. TLVD first constructs a hierarchical coordination mechanism for multi-agent LLMs using game theory and reinforcement learning, enabling multiple exe-cution LLMs to perform distributed reasoning under the guidance of a coordinator LLM to infer latent variables and their semantic meanings. Subsequently, the framework conducts causal valida-tion of the inferred latent variables using multiple real-world data sources. Finally, we conduct extensive experiments on three real hospital datasets that we constructed, as well as two benchmark datasets, demonstrating the effectiveness of the TLVD in latent variable discovery. 

6 Acknowledgements 

This research is partially supported by funding from Xiangjiang Laboratory (25XJ02002), the National Natural Science Foundation of China (62376228, 62376227), the Science and Technology Innovation Program of Hunan Province (2024RC4008), the China Postdoctoral Science Foundation (2025M770766), Sichuan Provincial Postdoc-toral Research Project Special Funding (TB2025043) and Sichuan Science and Technology Program (2023NSFSC0032). Carl Yang is not supported by any funds from China. Traceable Latent Variable Discovery Based on Multi-Agent Collaboration WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates 

References 

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv:2303.08774 (2023). [2] Jamshaid Ahmad, Abdullah Eqal Al-Mazrooei, and Themistocles M Rassias. 2023. Common fixed point theorems with applications to theoretical computer science. 

International Journal of Nonlinear Analysis and Applications 14, 2 (2023), 1â€“10. [3] Sina Akbari, Ehsan Mokhtarian, AmirEmad Ghassami, and Negar Kiyavash. 2021. Recursive causal structure learning in the presence of latent variables and selection bias. NeurIPS 34 (2021), 10119â€“10130. [4] Animashree Anandkumar, Daniel Hsu, Adel Javanmard, and Sham Kakade. 2013. Learning linear bayesian networks with latent variables. In ICML . 249â€“257. [5] Barbara M Byrne. 2013. Structural equation modeling with Mplus: Basic concepts, applications, and programming . routledge. [6] Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ram-chandran, et al . 2025. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657 (2025). [7] Justin Chen, Swarnadeep Saha, and Mohit Bansal. 2024. ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. In ACL .7066â€“7085. [8] Xuanzhong Chen, Xiaohao Mao, Qihan Guo, Lun Wang, Shuyang Zhang, and Ting Chen. 2024. RareBench: can LLMs serve as rare diseases specialists?. In 

KDD . 4850â€“4861. [9] Zhengming Chen, Feng Xie, Jie Qiao, Zhifeng Hao, Kun Zhang, and Ruichu Cai. 2022. Identification of linear latent variable model with arbitrary distribution. In 

AAAI , Vol. 36. 6350â€“6357. [10] David Maxwell Chickering. 2002. Optimal structure identification with greedy search. Journal of machine learning research 3, Nov (2002), 507â€“554. [11] Xinshuai Dong, Biwei Huang, Ignavier Ng, Xiangchen Song, Yujia Zheng, Songyao Jin, Roberto Legaspi, Peter Spirtes, and Kun Zhang. 2024. A Versa-tile Causal Discovery Framework to Allow Causally-Related Hidden Variables. In ICLR .[12] Huaming Du, Yujia Zheng, Baoyu Jing, Yu Zhao, Gang Kou, Guisong Liu, Tao Gu, Weimin Li, and Carl Yang. 2025. Causal Discovery through Synergizing Large Language Model and Data-Driven Reasoning. In KDD . 543â€“554. [13] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2024. Improving factuality and reasoning in language models through multiagent debate. In ICML .[14] Chris Frith and Uta Frith. 2005. Theory of mind. Current biology 15, 17 (2005), R644â€“R645. [15] Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing function ap-proximation error in actor-critic methods. In ICML . PMLR, 1587â€“1596. [16] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al . 2025. DeepSeek-R1 in-centivizes reasoning in LLMs through reinforcement learning. Nature 645, 8081 (2025), 633â€“638. [17] Thilo Hagendorff, Sarah Fabi, and Michal Kosinski. 2023. Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. Nature Computational Science 3, 10 (2023), 833â€“838. [18] Elad Hazan et al . 2016. Introduction to online convex optimization. Foundations and Trends Â® in Optimization 2, 3-4 (2016), 157â€“325. [19] Samsad Afrin Himi, Markus BÃ¼hner, Matthias Schwaighofer, Anna Klapetek, and Sven Hilbert. 2019. Multitasking behavior and its related constructs: Executive functions, working memory capacity, relational integration, and divided attention. 

Cognition 189 (2019), 275â€“298. [20] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al .2024. MetaGPT: Meta programming for a multi-agent collaborative framework. In ICLR .[21] Biwei Huang, Charles Jia Han Low, Feng Xie, Clark Glymour, and Kun Zhang. 2022. Latent hierarchical causal structure discovery with rank constraints. Ad-vances in neural information processing systems 35 (2022), 5549â€“5561. [22] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al . 2025. A survey on hallucination in large language models: Principles, taxonomy, chal-lenges, and open questions. ACM TOIS 43, 2 (2025), 1â€“55. [23] Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. 2021. Learning latent causal graphs via mixture oracles. NeurIPS 34 (2021), 18087â€“18101. [24] Erich Kummerfeld and Joseph Ramsey. 2016. Causal clustering for 1-factor measurement models. In KDD . 1655â€“1664. [25] Hao Duong Le, Xin Xia, and Zhang Chen. 2024. Multi-agent causal discovery using large language models. arXiv preprint arXiv:2407.15073 (2024). [26] Ao Li, Yuexiang Xie, Songze Li, Fugee Tsung, Bolin Ding, and Yaliang Li. 2025. Agent-Oriented Planning in Multi-Agent Systems. In ICLR .[27] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: Communicative agents for" mind" exploration of large language model society. NeurIPS 36 (2023), 51991â€“52008. [28] Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and Deheng Ye. 2024. More Agents Is All You Need. Transactions on Machine Learning Research (2024). [29] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. In EMNLP . 17889â€“17904. [30] Yunhao Liang, Yuan Qu, Jingyuan Yang, Shaochong Lin, and Zuo-Jun Max Shen. 2025. Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games. arXiv:2508.02076 (2025). [31] Tongxuan Liu, Xingyu Wang, Weizhe Huang, Wenjiang Xu, Yuting Zeng, Lei Jiang, Hailong Yang, and Jing Li. 2024. Groupdebate: Enhancing the efficiency of multi-agent debate using group discussion. arXiv:2409.14051 (2024). [32] Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang, and Long Jin. 2024. Nodemixup: Tackling under-reaching for graph neural networks. In AAAI . 14175â€“14183. [33] Weigang Lu, Yibing Zhan, Binbin Lin, Ziyu Guan, Liu Liu, Baosheng Yu, Wei Zhao, Yaming Yang, and Dacheng Tao. 2024. SkipNode: On alleviating performance degradation for deep graph convolutional networks. TKDE (2024), 7030â€“7043. [34] Ehsan Mokhtarian, Sepehr Elahi, Sina Akbari, and Negar Kiyavash. 2025. Recur-sive causal discovery. JMLR 26, 61 (2025), 1â€“65. [35] Ignavier Ng, Xinshuai Dong, Haoyue Dai, Biwei Huang, Peter Spirtes, and Kun Zhang. 2024. Score-based causal discovery of latent variable causal models. In 

Forty-first International Conference on Machine Learning .[36] J Pearl. 2009. Causality: models, reasoning, and inference. 2nd edn Cambridge University Press. New York (2009). [37] Judea Pearl. 2019. The seven tools of causal inference, with reflections on machine learning. Commun. ACM 62, 3 (2019), 54â€“60. [38] Jonas Peters, Joris M Mooij, Dominik Janzing, and Bernhard SchÃ¶lkopf. 2014. Causal discovery with continuous additive noise models. The Journal of Machine Learning Research 15, 1 (2014), 2009â€“2053. [39] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al . 2024. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. In ICLR .[40] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. 2018. QMIX: Monotonic Value Function Factori-sation for Deep Multi-Agent Reinforcement Learning. In ICML . 4295â€“4304. [41] Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, and Jiajun Zhang. 2025. Towards scientific intelligence: A survey of llm-based scientific agents. 

arXiv preprint arXiv:2503.24047 (2025). [42] Yangjun Ruan, Chris J Maddison, and Tatsunori B Hashimoto. 2024. Observational scaling laws and the predictability of langauge model performance. Advances in Neural Information Processing Systems 37 (2024), 15841â€“15892. [43] Saber Salehkaleybar, AmirEmad Ghassami, Negar Kiyavash, and Kun Zhang. 2020. Learning linear non-Gaussian causal models in the presence of latent variables. Journal of Machine Learning Research 21, 39 (2020), 1â€“24. [44] Shohei Shimizu, Patrik O Hoyer, Aapo HyvÃ¤rinen, Antti Kerminen, and Michael Jordan. 2006. A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research 7, 10 (2006). [45] Peter Spirtes, Clark N Glymour, and Richard Scheines. 2000. Causation, prediction, and search . MIT press. [46] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science 18, 6 (2024), 186345. [47] Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, et al . 2025. WideSearch: Benchmarking Agentic Broad Info-Seeking. arXiv preprint arXiv:2508.07999 (2025). [48] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al . 2024. Autogen: Enabling next-gen LLM applications via multi-agent conversations. In COLM .[49] Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, and Kun Zhang. 2020. Generalized independent noise condition for estimating latent variable causal graphs. NeurIPS 33 (2020), 14891â€“14902. [50] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. 2023. Self-evaluation guided beam search for reasoning. 

NeurIPS 36 (2023), 41618â€“41650. [51] Xie Yi, Zhanke Zhou, Chentao Cao, Qiyu Niu, Tongliang Liu, and Bo Han. 2025. From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium. In ICML .[52] Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. 2025. G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems. In NeurIPS .[53] Wanjia Zhao, Mert Yuksekgonul, Shirley Wu, and James Zou. 2025. Sirius: Self-improving multi-agent systems via bootstrapped reasoning. arXiv:2502.04780 

(2025). [54] Lecheng Zheng, Zhengzhang Chen, Jingrui He, and Haifeng Chen. 2024. MULAN: multi-modal causal structure learning and root cause analysis for microservice WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates Huaming Du et al.    

> systems. In WWW . 4107â€“4116. [55] Yaochen Zhu, Yinhan He, Jing Ma, Mengxuan Hu, Sheng Li, and Jundong Li. 2024. Causal inference with latent variables: Recent advances and future prospectives. In KDD . 6677â€“6687.

A Reproducibility 

In this section, we provide more details about the datasets and evaluation metrics to facilitate the reproducibility of the results. Our code is available at https://github.com/HYJ9999/TLVD.git. 

WCHSU-Cancer : The WCHSU-Cancer dataset comes from the health management center of a large hospital in Asia, focusing on early screening for lung cancer, and includes health check-up data from a total of 200,000 participants. The original dataset contains 230 variables. WCHSU-Cancer (n=22) was curated by scoring the correlation between the original variables and the lung cancer variable of interest using LLMs, with scores ranging from 0 to 5. After filtering based on scores greater than 5, 22 relevant variables were selected. WCHSU-Cancer (n=12), on the other hand, was screened directly by lung cancer specialists from the same hospital. 

WCHSU-Pain : The WCHSU-Pain comes from the same large hospital in Asia and focuses on postoperative pain in patients. It includes perioperative and postoperative analgesic usage data from a total of 1,568 patients. The dataset contains 16 variables. Please noted that all datasets used were properly pre-processed (including the de-identification of all patient information) and IRB-approved. Next, we present the detailed computation of the evaluation metrics: (1) ACC represents the proportion of latent variables cor-rectly predicted by the model. For the WCHSU dataset which lacks ground-truth, we invited five experts from relevant fields at Asiaâ€™s largest hospital to perform a consistency evaluation of the latent variables inferred by the model. Specifically, five experts assessed the latent variables and the authenticity of the evidence based on their professional experience; the consensus reached through con-sultation was adopted as the ground truth for this experiment. (2) 

ð¶ð´ð‘ð‘ = ð‘› ð¸ðº , ð‘› is the number of edges with real evidence, and ð¸ðº 

is the number of edges for which evidence is found. (3) ð¸ð¶ð‘–ð‘¡ = ð‘› ð¸ð´ ,

ð¸ð´ is the number of edges in the causal graph where at least one of the nodes is a latent variable. 

B Theoretical Proof B.1 Proof of Theorem 3.1 

Proof. We aim to prove the existence of a Bayesian Nash Equilib-rium (BNE) in our multi-agent LLM framework under the specified conditions. Following existing studies, our proof primarily proceeds by verifying the conditions of Glicksbergâ€™s Fixed Point Theorem, which guarantees the existence of a fixed point in continuous games with finite-dimensional strategy spaces. 

Step 1: Define the Best Response Correspondence For each agent ð‘– , define the best response correspondence ðµð‘… ð‘– as: 

ðµð‘… ð‘– (ðœ‹ âˆ’ð‘– ) = {ðœ‹ ð‘– âˆˆ Î ð‘– | max 

> ðœ‹ ð‘–

ð‘ˆ ð‘– (ðœƒ ð‘– , ðœ‹ ð‘– , ðœ‹ âˆ’ð‘– )} ,

where Î ð‘– is the set of all admissible strategies for agent ð‘– , and ðœ‹ âˆ’ð‘– 

denotes the strategies of all other agents. 

Step 2: Verify the Conditions of Glicksbergâ€™s Fixed Point Theorem To apply Glicksbergâ€™s theorem, we need to verify the following conditions for each agent ð‘– :

(i) Compactness and Convexity of Strategy Space : The strat-egy space Î ð‘– consists of all measurable functions from the type space Î˜ð‘– to the action space ð´ ð‘– . By Tychonoffâ€™s theorem, since Î˜ð‘– 

and ð´ ð‘– are compact metric spaces, Î ð‘– is also compact. Convexity follows similarly. (ii) Continuity of the Payoff Function : For fixed ðœƒ ð‘– , since ð‘ˆ ð‘– is continuous in actions and strategies map con-tinuously from types to actions, the composition 

ð‘ˆ ð‘– (ðœƒ ð‘– , ðœ‹ ð‘– (ðœƒ ð‘– ), ðœ‹ âˆ’ð‘– (ðœƒ âˆ’ð‘– )) is continuous in (ðœ‹ ð‘– , ðœ‹ âˆ’ð‘– ). (III) Quasi-Conc avity of the Payoff Function : For each ðœƒ ð‘– and ðœ‹ âˆ’ð‘– , the mapping 

ðœ‹ ð‘– â†¦ â†’ ð‘ˆ ð‘– (ðœƒ ð‘– , ðœ‹ ð‘– , ðœ‹ âˆ’ð‘– ) is linear in the space of mixed strategies, en-suring quasi-concavity. 

Step 3: Establish Upper Hemicontinuity and Non-Empty, Convex-Valuedness of the Best Response Correspondence 

We need to show that ðµð‘… ð‘– (ðœ‹ âˆ’ð‘– ) is upper hemicontinuous with non-empty, convex values. 

(i) Non-Empty and Convex Values : The payoff function ð‘ˆ ð‘– is continuous and quasi-concave in ðœ‹ ð‘– . By the Weierstrass Theorem, the extreme value must exist (and thus be non-empty); convexity follows from the quasi-concavity of ð‘ˆ ð‘– . (ii) Upper Hemicontinu-ity According to Bergeâ€™s Maximum Theorem, if: (1)The constraint set Î ð‘– is compact. (2) The objective function ð‘ˆ ð‘– is continuous in 

(ðœ‹ ð‘– , ðœ‹ âˆ’ð‘– ), then the best response correspondence ðµð‘… ð‘– (ðœ‹ âˆ’ð‘– ) is guar-anteed to be upper hemicontinuous. 

Step 4: Application of Glicksbergâ€™s Fixed Point Theorem 

After verifying the above conditions, applying Glicksbergâ€™s Fixed Point Theorem in our framework leads to the following conclusion: if each playerâ€™s strategy set is compact and convex, and their payoff functions are continuous and quasi-concave in their own strategies, then the game has at least one mixed-strategy Nash equilibrium. That is, there exists a strategy profile ðœ‹ âˆ— = (ðœ‹ âˆ— 

> 1

, ðœ‹ âˆ— 

> 2

, . . . , ðœ‹ âˆ— 

> ð‘

) such that for each agent ð‘– : ðœ‹ âˆ— 

> ð‘–

âˆˆ ðµð‘… ð‘– (ðœ‹ âˆ—âˆ’ð‘– ), meaning that, given their beliefs about the types and strategies of other agents, no agent has an incentive to unilaterally deviate to improve their expected payoff. This strategy profile constitutes a Bayesian Nash Equilibrium in our TVLD framework. 

â–¡

B.2 Proof of Convergence to BNE 

Proof. We aim to show that by minimizing the TD loss of each agentâ€™s Q-network, the agentsâ€™ strategies converge to a BNE. 

Core Assumptions :1. The Q-network ð‘„ ð‘– (ð‘ , ð‘Ž ð‘– ; ðœƒ ð‘– ) is parameterized by prompt em-beddings ðœƒ ð‘– , and the mapping from ðœƒ ð‘– to ð‘„ ð‘– is continuously differ-entiable. 2. The exploration strategy ensures sufficient coverage of the stateâ€“action space. 3. The loss function ð¿ ð‘– (ðœƒ ð‘– ) is convex, or its gradient is Lipschitz continuous with respect to ðœƒ ð‘– . 4. The gradient 

âˆ‡ðœƒ ð‘– ð¿ ð‘– (ðœƒ ð‘– ) is Lipschitz continuous. 5. The learning rate ðœ‚ ð‘¡ satisfies the Robbinsâ€“Monro conditions: Ãâˆž 

> ð‘¡ =1

ðœ‚ ð‘¡ = âˆž and Ãâˆž 

> ð‘¡ =1

ðœ‚ 2 

> ð‘¡

< âˆž.

Step 1: Defining the TD Loss Function For agent ð‘– , the TD loss is: 

ð¿ ð‘– (ðœƒ ð‘– ) = E(ð‘ ,ð‘Ž ð‘– ,ð‘Ÿ ð‘– ,ð‘  â€² )âˆ¼ ð· ð‘– 

h ð‘Ÿ ð‘– + ð›¾ max 

> ð‘Ž â€²
> ð‘–

ð‘„ ð‘– (ð‘  â€², ð‘Ž â€² 

> ð‘–

; ðœƒ âˆ’ 

> ð‘–

) âˆ’ ð‘„ ð‘– (ð‘ , ð‘Ž ð‘– ; ðœƒ ð‘– )2 i

,

which measures the discrepancy between the predicted Q-value and the target Q-value based on the reward and estimated optimal future Q-value. Traceable Latent Variable Discovery Based on Multi-Agent Collaboration WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates 

Step 2: Convergence of Gradient Descent with TD Loss 

Under Assumptions 1â€“4, stochastic gradient descent converges almost surely to a stationary point ðœƒ âˆ— 

> ð‘–

of ð¿ ð‘– (ðœƒ ð‘– ):

lim  

> ð‘¡ â†’âˆž

ðœƒ (ð‘¡ ) 

> ð‘–

= ðœƒ âˆ— 

> ð‘–

and âˆ‡ðœƒ ð‘– ð¿ ð‘– (ðœƒ âˆ— 

> ð‘–

) = 0.

This follows from standard stochastic approximation theory. The gradient condition implies: 

E(ð‘ ,ð‘Ž ð‘– ,ð‘Ÿ ð‘– ,ð‘  â€² )âˆ¼ ð· ð‘– 

h ð‘Ÿ ð‘– + ð›¾ max 

> ð‘Ž â€²
> ð‘–

ð‘„ ð‘– (ð‘  â€², ð‘Ž â€² 

> ð‘–

; ðœƒ âˆ’ 

> ð‘–

) âˆ’ ð‘„ ð‘– (ð‘ , ð‘Ž ð‘– ; ðœƒ âˆ— 

> ð‘–

)

Â· âˆ‡ ðœƒ ð‘– ð‘„ ð‘– (ð‘ , ð‘Ž ð‘– ; ðœƒ âˆ— 

> ð‘–

)

i

= 0

If the function approximator is sufficiently expressive, this implies that the TD error vanishes in expectation: 

ð‘„ ð‘– (ð‘ , ð‘Ž ð‘– ; ðœƒ âˆ— 

> ð‘–

) = E

"

ð‘Ÿ ð‘– + ð›¾ max 

> ð‘Ž â€²
> ð‘–

ð‘„ ð‘– (ð‘  â€², ð‘Ž â€² 

> ð‘–

; ðœƒ âˆ— 

> ð‘–

) | ð‘ , ð‘Ž ð‘– 

#

.

Thus, ð‘„ ð‘– (Â· ; ðœƒ âˆ— 

> ð‘–

) satisfies the Bellman optimality equation. This en-sures that the agentâ€™s policy ðœ‹ ð‘– (ð‘Ž ð‘– |ð‘  ; ðœƒ âˆ— 

> ð‘–

) is a best response to the current policies of the other agents, as it maximizes expected cu-mulative rewards. 

Step 3: Establishing Bayesian Nash Equilibrium Since each agentâ€™s policy is a best response to others, the set of policies ðœ‹ âˆ—

> ð‘–

constitutes a BNE. At this equilibrium, each agent maximizes its expected utility given its beliefs about other agentsâ€™ types and strategies, thus fulfilling the definition of BNE. â–¡

C Detailed Proofs C.1 Proof of Lemma 3.1 

Proof. Consider the value functions under policies ðœ‹ â€² and ðœ‹ :

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³

ð‘‰ ðœ‹ â€² 

> ð‘–

(ð‘  ) = Eðœ‹ â€²

" âˆžâˆ‘ï¸ 

> ð‘˜ =0

ð›¾ ð‘˜ ð‘Ÿ ð‘– (ð‘  ð‘˜ , ð‘Ž ð‘˜ ) | ð‘  0 = ð‘  

#

,ð‘‰ ðœ‹ ð‘– (ð‘  ) = Eðœ‹ 

" âˆžâˆ‘ï¸ 

> ð‘˜ =0

ð›¾ ð‘˜ ð‘Ÿ ð‘– (ð‘  ð‘˜ , ð‘Ž ð‘˜ ) | ð‘  0 = ð‘  

#

.

Subtracting the two equations yields: 

ð‘‰ ðœ‹ â€² (ð‘  ) âˆ’ ð‘‰ ðœ‹ (ð‘  ) = Eðœ‹ â€²

" âˆžâˆ‘ï¸ 

> ð‘˜ =0

ð›¾ ð‘˜ ð‘Ÿ ð‘– (ð‘  ð‘˜ , ð‘Ž ð‘˜ )

#

âˆ’ Eðœ‹ 

" âˆžâˆ‘ï¸ 

> ð‘˜ =0

ð›¾ ð‘˜ ð‘Ÿ ð‘– (ð‘  ð‘˜ , ð‘Ž ð‘˜ )

#

=

> âˆž

âˆ‘ï¸ 

> ð‘˜ =0

ð›¾ ð‘˜ 

Eð‘  ð‘˜ âˆ¼ð‘‘ ðœ‹ â€²

> ð‘˜

[ð‘Ÿ ð‘– (ð‘  ð‘˜ , ð‘Ž ð‘˜ )] âˆ’ Eð‘  ð‘˜ âˆ¼ð‘‘ ðœ‹ ð‘˜ [ð‘Ÿ ð‘– (ð‘  ð‘˜ , ð‘Ž ð‘˜ )] 



.

Assuming the difference in state distributions is negligible, we focus on action differences. Using the Q-function definition: 

ð‘„ ðœ‹ (ð‘ , ð‘Ž ð‘– , ð‘Ž âˆ’ð‘– ) = ð‘Ÿ ð‘– (ð‘ , ð‘Ž ð‘– , ð‘Ž âˆ’ð‘– ) + ð›¾ Eð‘  â€²âˆ¼ð‘ƒ [ð‘‰ ðœ‹ (ð‘  â€²)] ,

we can write: 

ð‘‰ ðœ‹ â€² (ð‘  ) âˆ’ ð‘‰ ðœ‹ (ð‘  ) =

> âˆž

âˆ‘ï¸ 

> ð‘˜ =0

ð›¾ ð‘˜ Eð‘  ð‘˜ âˆ¼ð‘‘ ðœ‹ â€²

> ð‘˜

ð‘„ ðœ‹ (ð‘  ð‘˜ , ð‘Ž â€² 

> ð‘˜

) âˆ’ ð‘‰ ðœ‹ (ð‘  ð‘˜ ) .

Since ð‘‰ ðœ‹ (ð‘  ð‘˜ ) = Eð‘Ž ð‘˜ âˆ¼ðœ‹ (ð‘  ð‘˜ ) [ð‘„ ðœ‹ (ð‘  ð‘˜ , ð‘Ž ð‘˜ )] , we have: 

ð‘‰ ðœ‹ â€² (ð‘  ) âˆ’ ð‘‰ ðœ‹ (ð‘  ) =

> âˆž

âˆ‘ï¸ 

> ð‘˜ =0

ð›¾ ð‘˜ Eð‘  ð‘˜ âˆ¼ð‘‘ ðœ‹ â€²

> ð‘˜

h

Eð‘Ž â€²    

> ð‘˜ âˆ¼ðœ‹ â€²(ð‘  ð‘˜ )

ð‘„ ðœ‹ (ð‘  ð‘˜ , ð‘Ž â€² 

> ð‘˜

)âˆ’ Eð‘Ž ð‘˜ âˆ¼ðœ‹ (ð‘  ð‘˜ )ð‘„ ðœ‹ (ð‘  ð‘˜ , ð‘Ž ð‘˜ ) .

Switching the order of expectations and summing over ð‘˜ , we get: 

ð‘‰ ðœ‹ â€² (ð‘  ) âˆ’ ð‘‰ ðœ‹ (ð‘  ) = 11 âˆ’ ð›¾ Eð‘  âˆ¼ð‘‘ ðœ‹ â€²

ð‘„ ðœ‹ (ð‘ , ð‘Ž â€² 

> ð‘–

, ð‘Ž â€²âˆ’ð‘– ) âˆ’ ð‘„ ðœ‹ (ð‘ , ð‘Ž ð‘– , ð‘Ž âˆ’ð‘– ) .

â–¡

C.2 Bounding the Bayesian Regret 

To prove the regret bound in our framework is ð‘… (ð‘‡ ) = ð‘‚ 

 ð‘ âˆšð‘‡ 

> 1âˆ’ð›¾



,we employ the given lemma: 

ð‘‰ âˆ— 

> ð‘–

(ð‘  ð‘¡ ) âˆ’ ð‘‰ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ ) = 11âˆ’ð›¾ Eð‘Ž âˆ—ð‘¡ ð‘– ,ð‘Ž âˆ—ð‘¡   

> âˆ’ð‘– ,ð‘Ž ð‘¡ ð‘– ,ð‘Ž ð‘¡
> âˆ’ð‘–

ð‘„ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž âˆ—ð‘¡ ð‘– , ð‘Ž âˆ—ð‘¡  

> âˆ’ð‘–

) âˆ’ ð‘„ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž ð‘¡ ð‘– , ð‘Ž ð‘¡  

> âˆ’ð‘–

),

and combine it with the convergence properties of policy gradient methods. The detailed derivation is as follows: 

C.2.1 Advantage Function Bound. From the convergence theory of policy gradient methods (based on online convex optimization) [ 18 ], for each agent ð‘– , state ð‘  ð‘¡ and optimal action ð‘Ž âˆ—ð‘¡ ð‘– , the expected advantage function satisfies 

E ð´ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž âˆ—ð‘¡ ð‘– ) â‰¤ ð¶ 

âˆšð‘¡ , (15) where ð´ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž âˆ—ð‘¡ ð‘– ) = ð‘„ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž âˆ—ð‘¡ ð‘– ) âˆ’ ð‘‰ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ )) , and ð¶ = ð‘‚ 

 ð‘… max 

> 1âˆ’ð›¾



> 0

is a constant depending on the reward bound ð‘… max = 1 and the geometry of the policy space. 

C.2.2 Application of the Lemma. According to the lemma above, the value function gap is linked to the expected advantage function: 

ð‘‰ âˆ— 

> ð‘–

(ð‘  ð‘¡ ) âˆ’ ð‘‰ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ ) = 11 âˆ’ ð›¾ E ð´ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž âˆ—ð‘¡ ð‘– ) . (16) This follows from the linearity of expectation and the determinism of the optimal action (i.e., when ð‘Ž âˆ—ð‘¡ ð‘– is fixed, Eð‘Ž âˆ—ð‘¡ ð‘– [ð´ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž âˆ—ð‘¡ ð‘– )] =

ð´ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž âˆ—ð‘¡ ð‘– )). 

C.2.3 Bound on Value Function Difference. Combining Equation 15 and Equation 16, we obtain ð‘‰ âˆ— 

> ð‘–

(ð‘  ð‘¡ ) âˆ’ ð‘‰ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ ) â‰¤ 11âˆ’ð›¾ Â· ð¶ âˆšð‘¡ .

C.2.4 Individual Agent Regret Summation. The cumulative regret of agent ð‘– over ð‘‡ steps is 

ð‘… ð‘– (ð‘‡ ) = E

" ð‘‡ âˆ‘ï¸ 

> ð‘¡ =1

 ð‘‰ âˆ— 

> ð‘–

(ð‘  ð‘¡ ) âˆ’ ð‘‰ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ )#

â‰¤ ð¶ 

1 âˆ’ ð›¾ 

> ð‘‡

âˆ‘ï¸  

> ð‘¡ =1

1

âˆšð‘¡ .

C.2.5 Bounding the Harmonic Sum. Using a standard integral bound on the partial harmonic series with exponent 1/2: Ãð‘‡ ð‘¡ =1 1âˆšð‘¡ â‰¤ 2âˆšð‘‡ .

C.2.6 Global Regret Bound. The total regret is the sum over all agents: 

ð‘… (ð‘‡ ) =

> ð‘

âˆ‘ï¸ 

> ð‘– =1

ð‘… ð‘– (ð‘‡ ) â‰¤ 

> ð‘

âˆ‘ï¸ 

> ð‘– =1

2ð¶ âˆšð‘‡ 

1 âˆ’ ð›¾ = 2ð‘ð¶ âˆšð‘‡ 

1 âˆ’ ð›¾ = ð‘‚ ð‘ âˆšð‘‡ 

1 âˆ’ ð›¾ 

!

.

Note that this proof relies on the convergence property of pol-icy gradient methods, which ensures that the advantage function 

ð´ ðœ‹ ð‘¡  

> ð‘–

(ð‘  ð‘¡ , ð‘Ž âˆ—ð‘¡ ) decreases at a rate of ð‘‚ (1/âˆšð‘¡ ). This convergence rate can be derived from online convex optimization theory, such as mirror descent. WWW â€™26, April 13â€“17, 2026, Dubai, United Arab Emirates Huaming Du et al. 

Table 7: Comparison of performance using different LLMs. WCHSU-Cancer ( ð‘› = 12 ) WCHSU-Pain Methods ACC CAcc ECit ACC CAcc ECit Qwen3 235B 0.833 1.000 0.385 0.500 1.000 0.316 

Llama 4 Maverick 0.500 0.750 0.115 0.750 1.000 0.474 

GPT-oss-120B 0.667 1.000 0.269 1.000 1.000 0.632 

DeepSeek-v3 0.833 1.000 0.385 0.750 0.857 0.368 Cancer-ACC 

Cancer-CAcc 

Cancer-ECit 

Pain-ACC 

Pain-CAcc 

Pain-ECit 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

1.2 

> Score

TLVD-v 

TLVD-d 

TLVD-I 

TLVD-R1 

TLVD-R2 

TLVD-R3 

TLVD 

Figure 8: Performance of evaluation for ablation study on WCHSU-Cancer (n=12) and WCHSU-Pain. 2 3 4 5 6 7

Agent Number 

0.25 

0.50 

0.75 

1.00 

1.25 

> Performance
> 70B (ECit)
> 8B (ECit)
> 70B (ACC)
> 8B (ACC)

(a) WCHSU-Cancer 2 3 4 5 6 7

Agent Number 

0.25 

0.50 

0.75 

1.00 

1.25  

> Performance
> 70B (ECit)
> 8B (ECit)
> 70B (ACC)
> 8B (ACC)

(b) WCHSU-Pain 0.1 0.3 0.5 0.7 0.9 

0.25 

0.50 

0.75 

1.00 

1.25 

> Performance
> 70B (ECit)
> 8B (ECit)
> 70B (ACC)
> 8B (ACC)

(c) WCHSU-Cancer 0.1 0.3 0.5 0.7 0.9 

0.25 

0.50 

0.75 

1.00 

1.25  

> Performance
> 70B (ECit)
> 8B (ECit)
> 70B (ACC)
> 8B (ACC)

(d) WCHSU-Pain 

Figure 9: Performance comparison with different ð‘ and ð›¾ .50000 100000 200000 

Sample Size 

0

20000 

40000 

60000 

80000 

> Token
> Debate
> CAMEL
> Majority
> Latent Variable Proposal
> Latent Variable Validation

(a) WCHSU-Cancer 500 784 1568 

Sample Size 

0

20000 

40000 

60000 

80000  

> Token
> Debate
> CAMEL
> Majority
> Latent Variable Proposal
> Latent Variable Validation

(b) WCHSU-Pain 

Figure 10: The cost statistics of our model. Table 5: Performance comparison on benchmark datasets. Multitasking Behaviour Teacherâ€™s Burnout Study Methods ACC CAcc ECit ACC CAcc ECit GPT5 0.250 Â±0.047 0.083 Â±0.053 0.083 Â±0.053 0.145 Â±0.045 0.069 Â±0.029 0.032 Â±0.013 

Gemini-deepresearch 0.500 Â±0.000 0.657 Â±0.070 0.383 Â±0.041 0.364 Â±0.100 0.315 Â±0.038 0.146 Â±0.017 

OpenAI-deepresearch 0.350 Â±0.122 0.150 Â±0.062 0.150 Â±0.062 0.564 Â±0.068 0.762 Â±0.049 0.354 Â±0.023 

Qwen-deepresearch 0.050 Â±0.100 0.000 Â±0.000 0.000 Â±0.000 0.164 Â±0.068 0.154 Â±0.024 0.071 Â±0.011 

Doubao-deepresearch 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.127 Â±0.045 0.131 Â±0.039 0.061 Â±0.018 

Autogen 0.050 Â±0.100 0.000 Â±0.000 0.000 Â±0.000 0.255 Â±0.068 0.208 Â±0.039 0.096 Â±0.018 

MiniMax 0.150 Â±0.122 0.000 Â±0.000 0.000 Â±0.000 0.327 Â±0.109 0.300 Â±0.051 0.139 Â±0.026 

CAMEL 0.050 Â±0.100 0.000 Â±0.000 0.000 Â±0.000 0.236 Â±0.072 0.192 Â±0.024 0.089 Â±0.011 

Multi-Agent-Majority 0.100 Â±0.122 0.167 Â±0.105 0.083 Â±0.053 0.273 Â±0.141 0.185 Â±0.029 0.086 Â±0.013 

Multi-Agent Debate 0.000 Â±0.000 0.000 Â±0.000 0.000 Â±0.000 0.291 Â±0.134 0.162 Â±0.015 0.075 Â±0.007 

TLVD 0.500 Â±0.000 0.820 Â±0.095 0.283 Â±0.041 0.582 Â±0.045 0.769 Â±0.049 0.357 Â±0.023 

Table 6: Performance comparison of models using the Hier. rank [21] method. WCHSU-Cancer ( ð‘› = 12 ) WCHSU-Pain Methods ACC CAcc ECit ACC CAcc ECit GPT5 0.200 Â±0.245 0.125 Â±0.065 0.125 Â±0.065 0.300 Â±0.187 0.100 Â±0.033 0.100 Â±0.033 

Gemini-deepresearch 0.100 Â±0.200 0.000 Â±0.000 0.000 Â±0.000 0.100 Â±0.122 0.191 Â±0.021 0.163 Â±0.018 

OpenAI-deepresearch 0.500 Â±0.316 0.250 Â±0.258 0.250 Â±0.135 0.450 Â±0.100 0.450 Â±0.041 0.200 Â±0.018 

Qwen-deepresearch 0.200 Â±0.245 0.217 Â±0.027 0.208 Â±0.000 0.300 Â±0.100 0.443 Â±0.029 0.237 Â±0.018 

Doubao-deepresearch 0.100 Â±0.200 0.208 Â±0.037 0.250 Â±0.113 0.150 Â±0.122 0.000 Â±0.000 0.000 Â±0.000 

Autogen 0.200 Â±0.245 0.083 Â±0.026 0.083 Â±0.026 0.250 Â±0.158 0.000 Â±0.000 0.000 Â±0.000 

MiniMax 0.300 Â±0.245 0.216 Â±0.017 0.216 Â±0.017 0.350 Â±0.122 0.412 Â±0.037 0.259 Â±0.023 

CAMEL 0.400 Â±0.200 0.000 Â±0.000 0.000 Â±0.000 0.350 Â±0.122 0.000 Â±0.000 0.000 Â±0.000 

Multi-Agent-Majority 0.300 Â±0.245 0.000 Â±0.000 0.000 Â±0.000 0.300 Â±0.100 0.000 Â±0.000 0.000 Â±0.000 

Multi-Agent Debate 0.400 Â±0.200 0.286 Â±0.090 0.092 Â±0.017 0.500 Â±0.000 0.600 Â±0.122 0.104 Â±0.028 

TLVD 0.700 Â±0.245 1.000 Â±0.035 0.646 Â±0.012 0.750 Â±0.106 1.000 Â±0.172 0.471 Â±0.164 

D More experimental results 

We conducted additional experiments on two benchmark datasets with ground-truth labels: Multitasking Behaviour and Teacherâ€™s Burnout Study. As shown in Table 5, our method still achieves the best performance. Although these two datasets might have been included in the training data of LLMs, all the baselines we compared against are also LLM-based, making the comparison reasonably fair and still demonstrating the effectiveness of our method. Additionally, to further investigate the impact of different causal discovery algorithms on our framework, we employed Hier.rank [21 ] to uncover latent causal graph structures. As shown in Table 6, the results indicate that our method is robust and maintains optimal performance across different causal discovery algorithms. Although some edges still lack explicit causal evidence, based on the identifi-ability guarantees of the Hier.rank and RLCD [ 11 ] algorithms, the latent variables inferred by our framework are also highly likely to be correct. The results of the ablation experiments are shown in Table 7 and Figure 8. See Figure 9 for the parameter analysis. Finally, we also analyzed the token consumption of our method across different modules and sample sizes. As shown in Figure 10, the token consumption is higher during the latent variable validation phase, primarily due to the processing of papers and Wiki texts containing rich textual content. In contrast, the latent variable proposal phase requires relatively few tokens, mainly thanks to modeling agent interactions via the belief network. Although our method consumes slightly more tokens than the baseline models, this is acceptable given the performance improvements achieved. Moreover, the token usage of our method is primarily determined by the number of latent variables and the size of their corresponding Markov blankets.