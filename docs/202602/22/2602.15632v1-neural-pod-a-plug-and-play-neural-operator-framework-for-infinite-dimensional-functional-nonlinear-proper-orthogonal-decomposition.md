---
title: "Neural-POD: A Plug-and-Play Neural Operator Framework for Infinite-Dimensional Functional Nonlinear Proper Orthogonal Decomposition"
title_zh: Neural-POD：一种用于无穷维函数式非线性本征正交分解的即插即用神经算子框架
authors: "Changhong Mou, Binghang Lu, Guang Lin"
date: 2026-02-17
pdf: "https://arxiv.org/pdf/2602.15632v1"
tags: ["query:sr"]
score: 6.0
evidence: 用于科学发现和函数逼近的神经算子框架
tldr: 针对科学AI中传统POD受限于线性近似和特定网格离散化的问题，本文提出Neural-POD框架。该方法利用神经网络在无限维空间构建非线性正交基函数，通过模拟施密特正交化的残差最小化过程进行训练。Neural-POD具有分辨率不变性，支持任意范数优化，能有效捕捉复杂时空系统的非线性结构。作为一种即插即用的工具，它成功桥接了经典降阶模型与算子学习（如DeepONet），在流体力学模拟中表现出极高的鲁棒性与泛化能力。
motivation: 传统POD方法受限于线性子空间近似且依赖特定计算网格，难以处理复杂非线性系统及多分辨率任务。
method: 提出Neural-POD框架，通过神经网络迭代解决残差最小化问题，在无限维函数空间中学习非线性正交基。
result: 在Burgers和Navier-Stokes方程实验中，该方法实现了分辨率无关的特征提取，并显著提升了降阶模型的预测精度。
conclusion: Neural-POD为算子学习与经典降阶模型提供了高效的集成方案，是处理无限维非线性问题的强有力工具。
---

## 摘要
AI for Science 的快速发展常受限于“离散化”问题，即学习到的表示仍局限于训练期间使用的特定网格或分辨率。我们提出了神经本征正交分解（Neural-POD），这是一种即插即用的神经算子框架，利用神经网络在无穷维空间中构建非线性正交基函数。与受限于通过奇异值分解（SVD）获得线性子空间逼近的经典本征正交分解（POD）不同，Neural-POD 将基构建公式化为一系列通过神经网络训练解决的残差最小化问题。每个基函数通过学习表示数据中的剩余结构获得，遵循类似于施密特（Gram-Schmidt）正交化的过程。这种神经公式化方法相比经典 POD 具有几个关键优势：它支持在任意范数（如 $L^2$、$L^1$）下进行优化，学习无穷维函数空间之间分辨率无关的映射，能有效泛化到未见过的参数范围，并能本质地捕捉复杂时空系统中的非线性结构。所得基函数具有可解释性、可重用性，并能集成到降阶模型（ROM）和算子学习框架（如深度算子网络 DeepONet）中。我们通过包括 Burgers 方程和 Navier-Stokes 方程在内的不同复杂时空系统展示了 Neural-POD 的鲁棒性。我们进一步表明，Neural-POD 作为经典伽辽金（Galerkin）投影与算子学习之间的高性能、即插即用桥梁，实现了与基于投影的降阶模型及 DeepONet 框架的一致集成。

## Abstract
The rapid development of AI for Science is often hindered by the "discretization", where learned representations remain restricted to the specific grids or resolutions used during training. We propose the Neural Proper Orthogonal Decomposition (Neural-POD), a plug-and-play neural operator framework that constructs nonlinear, orthogonal basis functions in infinite-dimensional space using neural networks. Unlike the classical Proper Orthogonal Decomposition (POD), which is limited to linear subspace approximations obtained through singular value decomposition (SVD), Neural-POD formulates basis construction as a sequence of residual minimization problems solved through neural network training. Each basis function is obtained by learning to represent the remaining structure in the data, following a process analogous to Gram--Schmidt orthogonalization. This neural formulation introduces several key advantages over classical POD: it enables optimization in arbitrary norms (e.g., $L^2$, $L^1$), learns mappings between infinite-dimensional function spaces that is resolution-invariant, generalizes effectively to unseen parameter regimes, and inherently captures nonlinear structures in complex spatiotemporal systems. The resulting basis functions are interpretable, reusable, and enabling integration into both reduced order modeling (ROM) and operator learning frameworks such as deep operator learning (DeepONet). We demonstrate the robustness of Neural-POD with different complex spatiotemporal systems, including the Burgers' and Navier-Stokes equations. We further show that Neural-POD serves as a high performance, plug-and-play bridge between classical Galerkin projection and operator learning that enables consistent integration with both projection-based reduced order models and DeepONet frameworks.