Title: Discovery of Interpretable Physical Laws in Materials via Language-Model-Guided Symbolic Regression

URL Source: https://arxiv.org/pdf/2602.22967v1

Published Time: Fri, 27 Feb 2026 02:04:33 GMT

Number of Pages: 17

Markdown Content:
# Discovery of Interpretable Physical Laws in Materials via Language-Model-Guided Symbolic Regression 

## Yifeng Guan 1,2 , Chuyi Liu 1,2 , Dongzhan Zhou 1, Lei Bai 1,Wan-jian Yin 3,4 , Jingyuan Li 2,5* , Mao Su 1,6* 

> 1

Shanghai Artificial Intelligence Laboratory, Shanghai, 200232, China. 

> 2

School of Physics, Zhejiang University, Hangzhou, 310058, China. 

> 3

Soochow Institute for Energy and Materials Innovations (SIEMIS), Soochow University, Suzhou, 215006, China. 

> 4

Hefei National Laboratory,Hefei,230088,China. 

> 5

Institute for Advanced Study in Physics, Zhejiang University, Hangzhou, 310058, China. 

> 6

Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China. *Corresponding author(s). E-mail(s): jingyuanli@zju.edu.cn; sumao@pjlab.org.cn; 

Abstract 

Discovering interpretable physical laws from high-dimensional data is a funda-mental challenge in scientific research. Traditional methods, such as symbolic regression, often produce complex, unphysical formulas when searching a vast space of possible forms. We introduce a framework that guides the search pro-cess by leveraging the embedded scientific knowledge of large language models, enabling efficient identification of physical laws in the data. We validate our approach by modeling key properties of perovskite materials. Our method miti-gates the combinatorial explosion commonly encountered in traditional symbolic regression, reducing the effective search space by a factor of approximately 10 5.A set of novel formulas for bulk modulus, band gap, and oxygen evolution reac-tion activity are identified, which not only provide meaningful physical insights but also outperform previous formulas in accuracy and simplicity. 

1

> arXiv:2602.22967v1 [physics.comp-ph] 26 Feb 2026

# 1 Introduction 

The accurate prediction of physical properties is a key objective for many researchers. While deep learning methods, such as Graph Neural Networks (GNNs)[1, 2], have demonstrated exceptional capabilities in predicting properties in materials and bio-logical science[3–7], they inherently operate as black boxes[8, 9]. Consequently, these methods cannot explain underlying physical mechanisms or provide insights, which significantly limits their value for fundamental scientific discovery[10–12]. Ideally, one would use an explicit formula to describe target properties and provide physi-cal insights, rather than merely predicting numerical values[13, 14]. However, due to the complexity of real-world systems, identifying such formulas remains a significant challenge. Symbolic Regression (SR) method such as genetic programming [15], SINDy [16], and HI-SISSO [17], is a possible way to tackle the problem[18, 19] and these meth-ods have been used to model several physical properties[18, 20–23]. However, in the absence of experience and physical understanding of the data and physical process, one would use all input features and parameters to guarantee not missing the important ones. This turns the SR into a blind walking in a vast space. Consequently, it causes SR to merge physically irrelevant variables into the derived formulas. This results in equations that, while potentially accurate in fitting data, are physically incoherent and fail to reveal the true underlying mechanisms. In recent years, Large Language Models (LLMs) have demonstrated potential in science, but still remain at an exploratory stage in many domains like material science[24–26]. Many researchers have attempted to use LLMs as end-to-end symbolic regression engines, like Funsearch[27] and LLM-SR [28], hoping to leverage the pre-trained scientific prior knowledge in LLM to directly extract physical frameworks from data. Although this approach has achieved some success in simple low-dimensional problems, the performance of LLMs in handling high-dimensional complex data is constrained by their fundamental nature as language models[29]. Lacking the intrin-sic capability to process complex numerical patterns, they often fail to identify valid mathematical structures from such complex data[30]. To overcome these limitations, we propose LangLaw, a language-model-guided framework designed to discover governing physical laws from scientific data. Our approach integrates the robust search capability of SR and the scientific knowledge and reasoning ability of LLMs, enabling the LLM to intelligently guide the SR process by identifying physically relevant variables and pruning the search space. We evalu-ated our framework on three representative materials property datasets: Bulk Modulus (B0) for mechanical stability [17], Band Gap of lead-free perovskites for optoelectronic properties [31], and Oxygen Evolution Reaction (OER) activity for electrocatalytic performance [32]. It is worth noting that materials property data are often scarce due to experimental and computational challenges, which makes it more difficult to extract information using data-driven methods. The rich scientific knowledge embedded in LLMs provides critical assistance in overcoming this challenge, thereby enabling more effective formula discovery from limited data. 22 Results 

The LangLaw framework operates as an iterative loop, as shown in Fig. 1. First, the LLM analyzes the descriptions of input features (such as electron negativity, atomic radii, and ionization potential) to generate specific search parameters and choose fea-tures as inputs. This step mitigates combinatorial explosion and reduces the effective search space by a factor of approximately 10 5 (see Supplemental Figure S8). Here we used the Intern-S1 [33], which is a multimodal foundation model that enhances scientific reasoning capability, but other LLMs can also be used (see Supple-mentary Note S5). Guided by these instructions, the SR engine (implemented using PySR [34]) performs a search to find candidate mathematical formulas. We record the results of each iteration, including the derived formulas, parameters, and fitting errors, into an Experience Pool. The LLM then reviews this historical data to iden-tify effective variable combinations and refine the instructions for subsequent rounds. This feedback mechanism allows the system to progressively narrow down the search space and identify physically meaningful equations. In each step of the loop, the LLM analyzes the meaning of the input features and reviews the results from previous searches. Based on its scientific knowledge, the model suggests specific variables and parameters for the next search. It filters out features that do not make physical sense, even if they show a statistical correlation. The Symbolic Regression engine then performs the search using these constraints. Finally, it produces a set of formulas that balance high accuracy with low complexity. Comprehensive details regarding the symbolic regression settings, the specific structure of LLM prompting, reasoning examples, and the implementation mechanism of the experience pool are elaborated in the Supplementary Note S2. To evaluate the performance of LangLaw, we applied it to the task of modeling materials properties. We evaluated the discovered formulas based on prediction error and complexity. The formulas located on the Pareto front, as well as those identified in previous studies using other SR-based methods, are listed in Supplementary Table S4-S9. 

## 2.1 Perovskite Bulk Modulus 

The bulk modulus ( B0) measures a material’s resistance to uniform compression and is a key indicator of mechanical stability. Identifying the factors that govern B0 is critical for understanding intrinsic material properties and designing stable functional perovskites. Previous research has attempted to predict B0 using either empirical formulas or data-driven methods. Verma and Kumar proposed the following empirical relation[35] (point C in Fig. 2): 

BVK 0 = C0 + C1

(nAnB )C2

(a0)3.5 , (1) where a0 is the lattice parameter, nA and nB are the oxidation number of the A/B-site ions, and C0, C 1, C 2 are fitted constants. Alternatively, HI-SISSO [17] identified a formula from a set of 23 primary features, including valence orbital radius, electron 3affinity, ionization potential, and electronegativity (point B in Fig. 2): 

BHI-SISSO 0 = 2 .99 (IP B − EA B )0.419 (E0)0.964 

(a0 − 5.09 × 10 −4 EA B nA   

> |rcat
> s,B −rs,B |

)2.75 , (2) where IP B and EA B denote the ionization potential and electron affinity of the B-site ion, respectively. E0 represents the cohesive energy, while rs,B and rcat  

> s,B

correspond to the radii of the valence s-orbital of the B-site ions in its neutral and cationic (1+) states respectively. While Eq. (2) offers improved accuracy compared to Eq. (1), its multiple coupled terms and fitted exponents compromise physical interpretability. We utilized the same dataset and experimental settings as in HI-SISSO [17]. The results are presented in Fig. 2. LangLaw identified a set of formulas along the Pareto front, each keeping a balance between accuracy and simplicity. We selected the formula corresponding to point A for a detailed analysis: 

BLangLaw 0 = −

 EA B

IP B



+ 0 .51 

 nA + 25 .7

a0

− EN B



− 1.75 (3) where the newly emerged EN B denotes the electron negativity of the B-site atom. This formula provides a clear and interpretable linear relationship for the bulk modulus of perovskites: The first term −( EA B 

> IP B

) can measure the ”softness” of the electron cloud. A higher electron affinity EA B (stronger tendency to gain an electron) coupled with a lower ionization potential IP B (easier to lose an electron) indicates a more polarizable elec-tron cloud. Ions with highly polarizable electron clouds are more easily deformed under stress, which reduces the resistance to compression and results in a lower bulk modulus. The second term can be divided into two parts. The first part nA+25 .7 

> a0

resembles in form to the non-linear term (nAnB )C2

> a3.50

in the VK formula (Eq. 1). In ABO 3 perovskites, charge neutrality imposes the constraint nA + nB = 6. We also note that the lattice parameter a0 varies within a narrow range (centered around 4 ˚ A). Therefore, our derived term can be regarded as an effective linear proxy for the non-linear term in the VK formula (Eq. 1). The −EN B part performs an ionic correction. Higher EN B

weakens the B−O ionic bonds, which softens the lattice leading to lower bulk modulus. To evaluate the transferability of our model to out-of-distribution (OOD) data, we tested Eq. 3 on 10 perovskites selected from 7,308 single ( ABO 3) and double (A2BB ′O6) structures from the same study[17]. These specific materials were chosen as OOD because their bulk modulus values and the double perovskite structure are rare in the training set. As shown in Fig. 3, our linear formula achieves remarkably lower prediction errors than Eq. 2, demonstrating superior generalization to new material compositions. 42.2 Double Perovskite Band Gap 

Next, we applied LangLaw to the band gap prediction problem of lead-free double perovskites ( A2BB ′X6). Band gap is a core parameter for screening photovoltaic materials[36]. We employed a dataset containing 745 A2BB ′X6 materials and their band gaps extracted from Ref. [31], aiming to discover key physicochemical factors affecting the band gap. The results are shown in Fig. 4. Among the candidates found, we identified a highly interpretable formula that balances simplicity and accuracy (point A in Fig. 4): 

ELangLaw  

> g

= 0 .056 

 X3

> X

V 4

> B



+ 2.66 

RX VAX2

> B′

, (4) where VA and VB are the valence electrons of the A- and B-site atoms, respectively, 

RX is the ionic radius of the X-site anion, and XX and XB′ are the electronegativity of the X- and B’- site cation, respectively. We compare Eq. 4 with the formula derived by the SISSO method [37, 38] (point B in Fig. 4) on the same dataset, given in Eq. (5): 

ESISSO  

> g

= 0 .573 + 0 .053 

 X3

> X

V 4

> B



+ 3 .509 

 1

X2

> B′

√VA



− 0.154 

 √ZX

RB



,

(5) where ZX is atomic number of the X-site anion and RB is the ionic radii of the B-site cation. Interestingly, both formulas incorporate the term X3 

> X
> V4
> B

with nearly identical coeffi-cients. This suggests that these factors play a major role in determining the band gap. There is also a pair of similar terms, both containing XB′ and VA in the denomina-tor. The difference lies in that our formula includes an extra term RX . We analyzed the distribution of RX in the dataset and found that its variation is relatively small. By replacing RX with the mean value of RX in the dataset, we observed only a slight increase in prediction error. Furthermore, we noted that VA in the two formulas dif-fers by a square root. Numerical analysis reveals that this term takes only the values 1 or 11, meaning the difference introduced by the square root can also be treated as a constant. Consequently, this term in our two formulas becomes highly similar as well. However, the LangLaw formula (Eq. 4) has advantages in conciseness. 

## 2.3 OER Activity 

Finally, we applied LangLaw to search for new formulas for the oxygen evolution reac-tion (OER) activity of oxide perovskites. The OER is a key rate-limiting step that determining the overall efficiency in electrocatalysis[39, 40]. Oxide perovskites are con-sidered highly promising candidates for OER catalysts due to their rich compositional tunability, flexible electronic structure, and excellent chemical stability[41]. 5A previous study [32] proposed GPSR (symbolic regression with genetic program-ming) and derived a formula (point B in Fig. 5) to predict OER activity, measured by the potential versus the reversible hydrogen electrode ( VRHE ): 

V GPSR RHE = 1 .554 μt + 1 .092 , (6) where μ is the octahedral factor that reflects the local geometry of the BO 6 octahe-dron and t is the tolerance factor that describes the global distortion of the crystal structure. This model was trained on 18 synthesized ABO 3 perovskites, all measured under unified and comparable experimental conditions, ensuring data reliability and consistency. The formula was subsequently tested on 5 high-activity, stable perovskites screened from 3,545 perovskites. Using the same OER activity dataset, we identified a formula with higher accuracy (point A in Fig. 5): 

V LangLaw RHE = ( μ + 0 .127) × (3 .24 + 0.0016 

t − 1.1 ). (7) Our formula (Eq. 7) also relates the OER activity to two geometric factors μ and t.We note that the term involving t has a very small coefficient of 0.0016, suggesting that 

t likely has a limited influence on the outcome. Numerical analysis shows that the value of this term is only around 0.1, which is considerably smaller than the constant 3.24 added to it. To validate this assumption, we also identified some formulas containing only μ, such as those corresponding to points C (3 .810 μ) and D (log( μ) + 2 .623) in Fig. 5, whose prediction errors remain lower than that of the GPSR formula (Eq. 6). Using the three datasets, bulk modulus, band gap, and OER activity, we also com-pared LangLaw with other methods, as shown in Table 1. LLM-SR [28] is a recently proposed approach for scientific equation discovery that leverages the scientific knowl-edge and code generation capabilities of LLMs. The formulas identified by LLM-SR not only exhibit greater complexity but also result in higher predictive errors. Deep learning methods, including CGCNN [3] and ALIGNN [4], typically require a large amount of data and tend to overfit in small-data scenarios. For bulk modulus pre-diction, LangLaw outperforms both CGCNN and ALIGNN. Notably, on OOD data, LangLaw achieves the lowest RMSE of 0.0851, which is half that of ALIGNN (0.167) and five times lower than CGCNN (0.401). For band gap prediction, CGCNN and ALIGNN also yield higher predictive errors. We did not apply CGCNN or ALIGNN to the OER activity dataset, as it consists of only 18 data points. This comparison highlights the significant advantage of LangLaw in extracting robust and transferable physical laws from small datasets, where pure data-driven methods often struggle. 

# 3 Conclusion 

In conclusion, we introduced a Language-guided SR (LangLaw) framework that com-bines LLMs with SR to discover interpretable physical laws from high-dimensional data. By leveraging the scientific knowledge and reasoning capabilities of LLMs to 6guide the regression process, our method overcomes the key limitation of traditional SR that tends to produce complex, unphysical expressions, and efficiently uncovers formulas that are both accurate and insightful. Beyond advancing symbolic regression, this work significantly extends the role of LLMs in materials design. Rather than serving merely as predictors or text generators, LLMs here act as knowledge-guided search engines, directly shaping the discovery of fundamental physical relationships. This demonstrates a novel pathway for LLMs to contribute to interpretable, mechanism-driven materials design, offering researchers a practical and principled tool to extract governing scientific laws from complex, real-world data. 

# 4 Methods 

## 4.1 Implementation 

We implement the overall workflow of the L-SR framework based on the PySR library [34]. PySR employs a multi-island genetic programming algorithm, representing mathematical expressions as tree structures that evolve through tournament selection, crossover, and mutation within isolated populations. In each round, the LLM analyzes the dataset, textual descriptions, and an experience pool that stores the selected fea-ture sets, hyperparameters, and fitting errors from previous search rounds. Based on this information, the LLM identifies relevant physical variables and provides the fea-ture subset, the maximum tree depth, and the number of evolutionary iterations to be used in the current round of SR search. This design restricts the search space to physically relevant variables and avoids the generation of overly complex functional forms. During the search, continuous constants are optimized using gradient-based methods, and the algorithm identifies a Pareto front of formulas to balance accuracy and complexity. The output formula from each round of SR is evaluated using the scoring function described in Supplementary Note S4. The details of the PySR and LLM implementations are presented in Supplementary Notes S1-S2. For the selection of LLMs, we mainly used Intern-S1 [33], but other LLMs are also applicable. The LLM settings and comparisons are detailed in Supplementary Note S5. 

## 4.2 Experimental settings 

To evaluate LangLaw, we compared it with three categories of baselines: traditional SR methods (HI-SISSO, SISSO, and GPSR), recent LLM-based SR method (LLM-SR), and deep learning methods (CGCNN and ALIGNN). The parameters settings are provided below and detailed in Supplementary Notes S6-S7. The results reported for HI-SISSO, SISSO, and GPSR in Table 1 were calculated using the formulas in their respective original papers. For LLM-SR, we utilized the code released with the paper, adapting the prompts for our experimental context. In line with the LLM-SR paper, we employed GPT-4o as the LLM backbone and used the temperature of 0.8 for LLM inference. For CGCNN and ALIGNN, we adopted their publicly released implementations. CGCNN is trained using a hidden dimensions of 128, a batch size of 256, three message passing layers, a learning rate of 1e-2, a radius 7cutoff of 8.0 ˚ A, the nearest neighbors setting to 12, and is trained from scratch for 2000 epochs. ALIGNN follows its original setup with a learning rate of 1e-3, a batch size of 32, and is trained from scratch for 2000 epochs. In the PySR component of LangLaw, we set the populations to 31, the popula-tion size to 27 and the max size to 30. The L1 distance is selected as the loss function. The allowed operator set comprises +, −, ×, ÷, and exponentiation ˆ, with constrain parameter {-1, 1 } of the pow operator. For the LLMs module, the temperature is set to 1.0 for balancing performance. The experience pool retains the top-k historical for-mulas with k = 15. Example prompts are provided in Supplementary Note S2. The search ends when the error falls below a threshold of 0.001 or after a maximum of 100 rounds. 

# Data availability 

ALL datasets used in this study are publicly available (see Supplementary Note S3). 

# Code availability 

The code for reproducing the findings in this work is available at https://github.com/ nakonako4/langlaw. 

# Acknowledgments 

This work was supported by New Generation Artificial Intelligence-National Science and Technology Major Project(2025ZD0121802), Shanghai Committee of Science and Technology, China (Grant No. 23QD1400900), National Key Research and Develop-ment Program of China (Grant No. 2025YFC2311702 and 2025YFC2311703), the National Natural Science Foundation of China (Grant No. 12404291 and 32371299) and the Innovation Program for Quantum Science and Technology (Grant No. 2024ZD0300102). Y.G. did this work during his internship at Shanghai Artificial Intelligence Laboratory. 

# Author contributions 

M.S. conceived the idea and led the research. Y.G. developed the code and performed the experiments. W.Y., J.L., and M.S. analyzed the results. C.L., D.Z., and L.B. contributed technical ideas. Y.G. and M.S. wrote the first draft. All authors discussed the results and reviewed the manuscript. 

# Competing interests 

The authors declare no competing interests. 

# References 

[1] Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M. & Monfardini, G. The graph neural network model. IEEE Trans. Neural Netw. 20 , 61–80 (2009). 8[2] Kipf, T. N. & Welling, M. Semi-supervised classification with graph convolutional networks (2017). Paper published as a conference paper at ICLR 2017. [3] Xie, T. & Grossman, J. C. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. Phys. Rev. Lett. 

120 , 145301 (2018). [4] Choudhary, K. & DeCost, B. Atomistic Line Graph Neural Network for improved materials property predictions. npj Comput. Mater. 7, 185 (2021). [5] Stokes, J. M. et al. A deep learning approach to antibiotic discovery. Cell 180 ,688–702.e13 (2020). [6] Xue, D. et al. Accelerated search for materials with targeted properties by adaptive design. Nat. Commun. 7, 11241 (2016). [7] Lu, S. et al. Accelerated discovery of stable lead-free hybrid organic-inorganic perovskites via machine learning. Nat. Commun. 9, 3405 (2018). [8] Schmidt, J., Marques, M. R. G., Botti, S. & Marques, M. A. L. Recent advances and applications of machine learning in solid-state materials science. npj Comput. Mater. 5, 83 (2019). [9] Rodr´ ıguez-P´ erez, R. & Bajorath, J. Interpretation of machine learning models using shapley values: application to compound potency and multi-target activity predictions. J. Comput.-Aided Mol. Des. 34 , 1013–1026 (2020). [10] Roscher, R., Bohn, B., Duarte, M. F. & Garcke, J. Explainable machine learning for scientific insights and discoveries. IEEE Access 8, 42200–42216 (2020). [11] Karniadakis, G. E. et al. Physics-informed machine learning. Nat. Rev. Phys. 3,422–440 (2021). [12] Wang, H. et al. Scientific discovery in the age of artificial intelligence. Nature 

620 , 47–60 (2023). [13] Ghiringhelli, L. M., Vybiral, J., Levchenko, S. V., Draxl, C. & Scheffler, M. Big Data of Materials Science: Critical Role of the Descriptor. Phys. Rev. Lett. 114 ,105503 (2015). [14] Reuter, K., Stampf, C. & Scheffler, M. in Ab initio atomistic thermodynamics and statistical mechanics of surface properties and functions (ed.Yip, S.) Handbook of Materials Modeling: Methods 149–194 (Springer Netherlands, Dordrecht, 2005). [15] Schmidt, M. & Lipson, H. Distilling free-form natural laws from experimental data. Science 324 , 81–85 (2009). 9[16] Brunton, S. L., Proctor, J. L. & Kutz, J. N. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proc. Natl. Acad. Sci. 113 , 3932–3937 (2016). [17] Foppa, L., Purcell, T. A., Levchenko, S. V., Scheffler, M. & Ghiringhelli, L. M. Hierarchical Symbolic Regression for Identifying Key Physical Parameters Cor-related with Bulk Properties of Perovskites. Phys. Rev. Lett. 129 , 055301 (2022). [18] Wang, Y., Wagner, N. & Rondinelli, J. M. Symbolic regression in materials science. MRS Commun. 9, 793–805 (2019). [19] Koza, J. R. Genetic programming as a means for programming computers by natural selection. Stat. Comput. 4, 87–112 (1994). [20] Bartel, C. J. et al. Physical descriptor for the Gibbs energy of inorganic crystalline solids and temperature-dependent materials chemistry. Nat. Commun. 9, 4168 (2018). [21] Xie, S. R., Stewart, G. R., Hamlin, J. J., Hirschfeld, P. J. & Hennig, R. G. Func-tional form of the superconducting critical temperature from machine learning. 

Phys. Rev. B 100 , 174513 (2019). [22] Ouyang, R. Exploiting Ionic Radii for Rational Design of Halide Perovskites. 

Chem. Mater. 32 , 595–604 (2020). [23] Cao, G. et al. Artificial intelligence for high-throughput discovery of topological insulators: The example of alloyed tetradymites. Phys. Rev. Mater. 4, 034204 (2020). [24] Boiko, D. A., MacKnight, R., Kline, B. & Gomes, G. Autonomous chemical research with large language models. Nature 624 , 570–578 (2023). [25] Jablonka, K. M., Ai, Q., Al-Feghali, A., Badhwar et al. 14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. Digit. Discov 2, 1233–1250 (2023). [26] M. Bran, A. et al. Augmenting large language models with chemistry tools. Nat. Mach. Intell. 6, 525–535 (2024). [27] Romera-Paredes, B. et al. Mathematical discoveries from program search with large language models. Nature 625 , 468–475 (2024). [28] Shojaee, P., Meidani, K., Gupta, S., Farimani, A. B. & Reddy, C. K. Llm-sr: Scientific equation discovery via programming with large language models (2025). Paper published as a conference paper at ICLR 2025. 10 [29] Dziri, N. et al. Faith and fate: limits of transformers on compositionality (2023). Paper presented at the 37th International Conference on Neural Information Processing Systems, New Orleans, LA, USA, 10–16 December 2023. [30] Frieder, S. et al. Mathematical capabilities of chatgpt (2023). Paper presented at the 37th International Conference on Neural Information Processing Systems, New Orleans, LA, USA, 10–16 December 2023. [31] Gao, Z. et al. Screening for lead-free inorganic double perovskites with suitable band gaps and high stability using combined machine learning and dft calculation. 

Appl. Surf. Sci. 568 , 150916 (2021). [32] Weng, B. et al. Simple descriptor derived from symbolic regression accelerating the discovery of new perovskite catalysts. Nat. Commun. 11 , 3513 (2020). [33] Bai, L. et al. Intern-s1: A scientific multimodal foundation model (2025). Preprint at https://arxiv.org/abs/2508.15763. [34] Cranmer, M. Interpretable machine learning for science with pysr and symboli-cregression.jl (2023). Preprint at https://arxiv.org/abs/2305.01582. [35] Verma, A. & Kumar, A. Bulk modulus of cubic perovskites. J. Alloy. Compd. 

541 , 210–214 (2012). [36] MA, G. et al. Solar cell efficiency tables (version 57). Prog. Photovolt. 29 , 3–15 (2021). [37] Ouyang, R., Curtarolo, S., Ahmetcik, E., Scheffler, M. & Ghiringhelli, L. M. Sisso: A compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates. Phys. Rev. Mater. 2, 083802 (2018). [38] Baloch, A. A. B., Albadwawi, O., AlShehhi, B. & Alberts, V. Bandgap model using symbolic regression for environmentally compatible lead-free inorganic dou-ble perovskites (2022). Paper presented at the 2022 IEEE 49th Photovoltaics Specialists Conference, San Juan, Puerto Rico, 11–16 June 2023. [39] Suntivich, J., May, K. J., Gasteiger, H. A., Goodenough, J. B. & Shao-Horn, Y. A perovskite oxide optimized for oxygen evolution catalysis from molecular orbital principles. Science 334 , 1383–1385 (2011). [40] Wei, C. et al. Recommended practices and benchmark activity for hydrogen and oxygen electrocatalysis in water splitting and fuel cells. Adv. Mater. 31 , 1806296 (2019). [41] Yin, W.-J. et al. Oxide perovskites, double perovskites and derivatives for elec-trocatalysis, photocatalysis, and photovoltaics. Energy Environ. Sci. 12 , 442–462 (2019). 11 Figures 

Fig. 1 Overview of the LangLaw framework. The workflow is organized into four interconnected phases forming a closed loop: LLM inference (top left): The Large Language Model (LLM) acts as a reasoning agent. It analyzes the data description (e.g., crystal structure) and previous experience to generate search constraints, including feature selection, iteration counts, and tree depth. Regression (top right): These parameters act as control signals (purple dashed lines) to guide the Symbolic Regression (PySR) engine. The engine performs evolutionary searches using a parallel island model, evolving formula populations via genetic operations like crossover and mutation (details are shown in Supplementary Note S1). Evaluation (bottom right): Candidate formulas on the Pareto front are screened. The optimal formula is selected based on a score function that balances fitting loss and complexity[15]. Then if the error is lower enough or comes to the end round, the formula will be output, else the formula will be added into the Formulas Library. Experience (bottom left): The selected formulas and their performance metrics are stored in a formula library. This knowledge is formatted into prompts to update the LLM’s experience, refining the search strategy for subsequent rounds. 

12 Fig. 2 Performance comparison on the Perovskite Bulk Modulus dataset. This plot shows the complexity and Mean Absolute Error of formulas of Bulk Modulus found by different methods: Verma and Kumar’s formula (gray points), LangLaw (green points), HI-SISSO (blue points) and LLM-SR (yellow points). The gray line is the Pareto front. 

13 Fig. 3 Evaluation of out of distribution generalization capability. The bar chart compares the absolute prediction error of our discovered linear formula against the high-complexity HI-SISSO model on 10 screened perovskite materials not seen during training. Our method (green bars) consis-tently yields lower prediction errors across diverse compositions compared to HI-SISSO (blue bars), demonstrating superior transferability and robustness in data-scarce scenarios. 

14 Fig. 4 Performance comparison on the Band Gap dataset. This plot shows the complexity and Mean Absolute Error of formulas of Band Gap found by LangLaw (green points) and SISSO (blue points). The gray line is the Pareto front. 

15 Fig. 5 Performance comparison on the OER activity dataset. This plot shows the complexity and Mean Absolute Error of formulas of OER activity found by LangLaw (green points) and GPSR (blue points). The gray line is the Pareto front. 

16 Table 1  Performance comparison on three material science tasks.  This table reports the formula complexity and prediction errors for bulk modulus, band gap, and oxygen evolution reaction ( VRHE ). The selection of MAE or RMSE as the evaluation metric for each column follows the convention used in the respective source studies. HI-SISSO serves as the symbolic regression baseline for Bulk Modulus, while SISSO is the specific baseline for the Band Gap task and GPSR is for the OER task. These methods are compared alongside the LLM-based LLM-SR and deep learning models CGCNN and ALIGNN. ID refers to the test error on the in-distribution dataset, and OOD refers to the generalization error on out-of-distribution samples. Bold numbers indicate the best results. Bulk Modulus  Band Gap  VRHE 

> Methods  Complexity ↓ ID MAE(eV/˚ A3)↓
> OOD RMSE(eV/˚ A3)↓ Complexity ↓ RMSE(eV) ↓ Complexity ↓ ID MAE(eV) ↓
> OOD MAE(eV) ↓
> HI-SISSO[17]  26  0.090  0.411  — — — — —SISSO[37]  — — — 39  0.672  — — —GPSR[32]  — — — — — 7 0.0253  0.0209
> LLM-SR[28]  63  0.140  3.93  70  0.669  54  0.0147  0.108 CGCNN[3]  — 0.0903  0.401  — 1.053  — — —ALIGNN[4]  — 0.0784  0.167  — 1.114  — — —LangLaw  17  0.0739  0.0851  19  0.672  11  0.0187  0.0225

17