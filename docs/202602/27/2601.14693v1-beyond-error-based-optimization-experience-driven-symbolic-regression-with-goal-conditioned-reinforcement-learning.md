---
title: "Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning"
title_zh: 超越基于误差的优化：基于目标条件强化学习的经验驱动符号回归
authors: "Jianwen Sun, Xinrui Li, Fuqing Li, Xiaoxuan Shen"
date: 2026-01-21
pdf: "https://arxiv.org/pdf/2601.14693v1"
tags: ["query:sr-rl"]
score: 10.0
evidence: 用于符号回归和表达式发现的目标条件强化学习
tldr: 针对符号回归中仅依赖误差引导搜索导致结构歧义和收敛难的问题，本文提出EGRL-SR框架。该方法将符号回归建模为目标条件强化学习问题，利用事后经验回放和动作价值网络从历史轨迹中学习映射模式。通过设计全点满足二值奖励函数和结构引导启发式探索策略，EGRL-SR能更有效地引导搜索过程，在恢复率和鲁棒性上显著优于现有方法，尤其在处理复杂表达式时表现出色。
motivation: 传统基于误差的符号回归方法在面对具有相似误差但结构迥异的候选表达式时，容易出现搜索方向模糊和难以收敛到真实函数的问题。
method: 提出一种基于目标条件强化学习的框架，结合事后经验回放、二值奖励函数及结构引导探索策略，利用历史经验主动引导表达式搜索。
result: 在公开基准测试中，EGRL-SR在恢复率、鲁棒性以及处理复杂表达式的能力上均优于当前最先进的方法。
conclusion: 实验证明动作价值网络能有效引导搜索，且所提奖励函数和探索策略对提升符号回归性能至关重要。
---

## 摘要
符号回归旨在自动识别紧凑且可解释的数学表达式，以建模输入和输出变量之间的函数关系。大多数现有的基于搜索的符号回归方法通常依赖拟合误差来指导搜索过程。然而，在巨大的表达式空间中，许多候选表达式可能表现出相似的误差值，但在结构上却存在巨大差异，这导致了模糊的搜索方向，并阻碍了向底层真实函数的收敛。为了应对这一挑战，我们提出了一个名为 EGRL-SR（经验驱动的目标条件强化学习符号回归）的新型框架。与传统的误差驱动方法不同，EGRL-SR 引入了一个新视角：利用精确的历史轨迹并优化动作价值网络来主动引导搜索过程，从而实现更稳健的表达式搜索。具体而言，我们将符号回归建模为一个目标条件强化学习问题，并结合了事后经验回放（hindsight experience replay），使动作价值网络能够从多样的输入输出对中泛化出通用的映射模式。此外，我们设计了一个全点满足（all-point satisfaction）二值奖励函数，鼓励动作价值网络关注结构模式而非低误差表达式，并同时提出了一种结构引导的启发式探索策略，以增强搜索的多样性和空间覆盖率。在公共基准测试上的实验表明，EGRL-SR 在恢复率和稳健性方面始终优于现有最先进的方法，并且在相同的搜索预算下可以恢复更复杂的表达式。消融实验结果验证了动作价值网络能有效地引导搜索，且奖励函数和探索策略都发挥了关键作用。

## Abstract
Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.