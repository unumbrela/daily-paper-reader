Title: GENSR: Symbolic Regression Based in Equation Generative Space

URL Source: https://arxiv.org/pdf/2602.20557v1

Published Time: Wed, 25 Feb 2026 01:27:15 GMT

Number of Pages: 34

Markdown Content:
Published as a conference paper at ICLR 2026 

# GEN SR: SYMBOLIC REGRESSION BASED ON EQUATION GENERATIVE SPACE 

Qian Li 1,2, Yuxiao Hu 2,3, Juncheng Liu 4, Yuntian Chen 2âˆ—

> 1

Shanghai Jiao Tong University, Shanghai, China 

> 2

Eastern Institute of Technology, Ningbo, China 

> 3

The Hong Kong Polytechnic University, Hong Kong, China 

> 4

Imperial College London, London, England 

qianl01205@sjtu.edu.cn , yuxiao.hu@connect.polyu.hk ,

junchengliu23@imperial.ac.uk , ychen@eitech.edu.cn 

## ABSTRACT 

Symbolic Regression (SR) tries to reveal the hidden equations behind observed data. How-ever, most methods search within a discrete equation space, where the structural modifi-cations of equations rarely align with their numerical behavior, leaving fitting error feed-back too noisy to guide exploration. To address this challenge, we propose GenSR 1, a generative latent spaceâ€“based SR framework following the â€œmap construction â†’ coarse localization â†’ fine searchâ€ paradigm. Specifically, GenSR first pretrains a dual-branch Conditional Variational Autoencoder (CVAE) to reparameterize symbolic equations into a generative latent space with symbolic continuity and local numerical smoothness. This space can be regarded as a well-structured â€œmapâ€ of the equation space, providing di-rectional signals for search. At inference, the CVAE coarsely localizes the input data to promising regions in the latent space. Then, a modified CMA-ES refines the candi-date region, leveraging smooth latent gradients. From a Bayesian perspective, GenSR reframes SR task as maximizing the conditional distribution p(Equ .|Num .), with CVAE training achieving this objective through the Evidence Lower Bound (ELBO). This new perspective provides a theoretical guarantee for the effectiveness of GenSR. Extensive ex-periments show that GenSR jointly optimizes predictive accuracy, expression simplicity, and computational efficiency, while remaining robust under noise. 

## 1 INTRODUCTION 

Symbolic Regression (SR) is a core machine learning task aimed at discovering interpretable mathematical expressions that explain observed data. Unlike traditional regression, which assumes a fixed functional form (e.g., linear, polynomial), SR explores the equation space to identify functions that best fit the data. Formally, given N observation samples {xi, y i}Ni=1 , the objective is to find a mathematical expression f : x 7 â†’ y

that captures the underlying data distribution. Owing to its flexibility and interpretability, SR is widely used in scientific discovery and engineering (Xu et al., 2023; Chen et al., 2022), where understanding the underlying equations is as important as predictive accuracy. SR effectively bridges the gap between black-box models and interpretable scientific insight. Consequently, growing research efforts focus on developing SR algorithms that balance fitting accuracy, computational efficiency, and expression simplicity. 

> âˆ—

Corresponding author 

> 1

https://github.com/tokaka22/ICLR26-GENSR 

1

> arXiv:2602.20557v1 [cs.LG] 24 Feb 2026

Published as a conference paper at ICLR 2026 Many prior SR methods search for equations directly in a discrete symbolic space, using heuristic strategies such as genetic programming (GP) (Stephens et al., 2015; Virgolin et al., 2021), Monte Carlo Tree Search (MCTS) (Sun et al.), or other combinatorial optimization methods (Yu et al., 2025; Zhang et al., 2025). These methods iteratively adjust the symbolic structures of equations through various operations such as crossover, mutation, and tree expansion to search for better solutions. Unfortunately, such modifications do not reliably reduce fitting errors and may even lead the search into suboptimal regions. This is because edit distance and numerical behavior similarity are unrelated measures: equations with similar structures can exhibit drastically different numerical behaviors, and vice versa. Consequently, in the discrete equation space where similarity is measured by edit distance, the numerical error feedback neither corresponds to edit distance nor provides a reliable search direction. The search relies heavily on random mutations, combinations, and backtracking, resulting in an unstable search trajectory and high time complexity. Therefore, the discrete equation space with edit distance is inefficient and even unsuitable for SR. In this paper, we reparameterize the discrete equation space into a continuous latent space, which can be regarded as a â€œmapâ€ of the equation world (where equations are represented as vectors and dimensions naturally define search directions). To enable efficient search in this space, two key points must be addressed: (1) The space should be generative. A generative latent space, constructed by generative models, supports continuous sampling such that latent vectors decode into syntactically valid equations. This property enables smooth interpolation and fine-grained search in the equation latent space. In contrast, pretraining-based methods (Meidani et al.; Kamienny et al., 2022) typically learn discriminative embeddings for similarity or prediction, rather than modeling a generative distribution. Such discriminative spaces are fragmented and filled with non-decodable regions, hindering continuous search and producing invalid samples. (2) The space should be well-organized. Although some methods attempt to construct a generative latent space for equations (MeË‡ znar et al., 2023; Popov et al., 2023), the Euclidean distances within it reflect only edit distances and are unrelated to the equationsâ€™ numerical behavior. Consequently, numerical errors cannot be mapped into meaningful Euclidean distances for efficient optimization, and such latent spaces still cannot support smooth, efficient search. Ideally, equations with similar numerical behaviors should be close in the latent space, allowing smooth numerical error signals to guide exploration. Yet expressions like cos 2 x,

1 âˆ’ sin 2 x, and their Taylor expansions exhibit the same numerical behavior but entirely different symbolic forms. Placing such expressions in close proximity would force the decoder to map nearby vectors to drastically different expressions, destabilizing decoding. Thus, a key design challenge is balancing symbolic continuity and numerical smoothness. Inspired by human localizationâ€”first identifying a coarse region before precise targetingâ€”we construct a generative latent space with global symbolic continuity and local numerical smoothness. The former ensures stable decoding and induces clear structural clustering for coarse localization, while the latter allows fitting errors to provide directional signals for fine-grained search once the relevant region is located. Building on this insight, we propose GenSR, a novel framework that follows the â€œmap construction â†’

coarse localization â†’ fine searchâ€ paradigm. GenSR first pretrains a dual-branch Conditional Variational Autoencoder (CVAE): The posterior branch learns distributions over symbolic structures given symbolic and numerical inputs, whereas the prior branch learns distributions over numerical features from numer-ical inputs only. Joint optimization of reconstruction loss and KL divergence yields a Gen erative latent space with global symbolic continuity and local numerical smoothness for SR , serving as a â€œmapâ€ of the equation world. At inference, input data are mapped to an initial localization distribution by prior branch, which highlights promising latent regions. A modified CMA-ES algorithm then contracts this distribution toward the optimum, exploiting smooth directional signals in the latent space for efficient convergence. From a probabilistic view, GenSR reformulates SR as a Bayesian optimization problem, i.e., maximizing 

p(Equ .|Num .), with CVAE training corresponding to ELBO optimization. This perspective provides the theoretical guarantee for GenSR. Extensive experiments show that GenSR consistently achieves an optimal balance of predictive accuracy, expression simplicity, and runtime efficiency, while maintaining robustness under noisy conditions. 2Published as a conference paper at ICLR 2026 Our contributions come in four parts: (1) GenSR constructs a continuous generative latent space unifying symbolic continuity and local numerical smoothness, addressing the inefficiency of discrete equation spaces; (2) GenSR follows a new search paradigm of â€œmap construction â†’ coarse localization â†’ fine searchâ€ to achieve effective SR; (3) GenSR provides a new Bayesian perspective for SR and uses ELBO optimization to solve it; (4) Extensive experiments provide a thorough analysis of latent space properties and demonstrate that our method achieves a better balance among accuracy, equation complexity, and efficiency. 

## 2 RELATED WORK 

The field of SR has evolved through a variety of approaches. The most common SR methods rely on combinatorial optimization over a discrete equation space using heuristic search. Genetic Programming (GP) techniques (Stephens et al., 2015; Virgolin et al., 2021; Burlacu et al., 2020; Cranmer, 2023) evolve populations of expressions through selection and mutation, while Monte Carlo Tree Search (MCTS) meth-ods (Sun et al.) systematically explore the expression tree space. More recent neural-guided approaches have integrated reinforcement learning (Petersen et al.), Transformer-based planning (Shojaee et al., 2023), retrieval-augmented generation (Zhang et al., 2025), and minimum description length principles (Yu et al., 2025) to enhance search efficiency. Large language models have also been applied to generate equations through prompting techniques (Shojaee et al.; Grayeli et al., 2024); however, due to inherent hallucination issues, LLM-based methods still resemble enumeration in the discrete space and require many queries to reach high-quality solutions. Ultimately, symbolic regression over discrete equation spaces remains funda-mentally inefficient, as structural similarity poorly reflects numerical behavior, leaving search unguided and error feedback uninformative. Alternative strategies have sought to avoid discrete search entirely. Sequence-to-sequence models (Kami-enny et al., 2022; Li et al., 2022; Biggio et al., 2021) generate equations directly but do not enforce alignment between symbolic forms and numerical behavior during training. limiting their generalization. The SNIP framework (Meidani et al.) made progress by learning a shared latent space through contrastive learning, creating representations useful for similarity assessment. However, because SNIP learns a discriminative rather than generative space, many points in its latent space do not correspond to valid equations, making it inadequate for the generative search demands of SR. Our work addresses these limitations by learning a continuous, generative latent space using a dual-branch CVAE. This approach transforms the discrete equation space into a structured manifold where nearby points correspond to expressions with similar symbolic forms and numerical properties. The resulting space pro-vides meaningful gradients that enable an efficient three-stage search strategyâ€”map construction, coarse localization, and fine-grained searchâ€”directly addressing the core weaknesses of previous methods. 

## 3 GEN SR: SYMBOLIC REGRESSION BASED ON Gen ERATIVE LATENT SPACE 

As illustrated in Fig. 1, the core idea of GenSR is to construct a continuous generative latent space for equations (which can be regarded as a â€œmapâ€ of the equation world) using a dual-branch CVAE and to perform efficient search directly within this space. The dual-branch CVAE consists of a Transformer-based encoderâ€“decoder network, two branch-specific networks, and a feature fusion module. The CVAE is pre-trained on approximately 5 million synthetic equationâ€“sample pairs to ensure coverage of diverse functional forms. At inference, GenSR leverages the prior branch to produce an initial localization distribution in the latent space and refines the solution in the latent space using a degenerate version of CMA-ES (Hansen, 2016), enabling fast and precise search for the distribution of candidate equations. 3Published as a conference paper at ICLR 2026 Transformer                                  

> Encoder
> Posterior
> Network
> Prior
> Network
> ð‘ (ðœ‡ !,ðœŽ !
> ")
> ð‘ (ðœ‡ ",ðœŽ "
> ")
> Feature
> Fusion
> Transformer
> Decoder
> Transformer
> Encoder
> Prior Network
> Feature  Fusion
> Transformer
> Decoder
> Constants
> Refinement ð‘ (ðœ‡ ,ðœŽ !)
> ð‘¥ "
> "â‹¯ð‘¥ "
> ($)ð‘¦ "
> â‹®â‹±â‹®â‹®
> ð‘¥ &
> "â‹¯ð‘¥ &
> ($)ð‘¦ &
> ð‘“ (ð’™ )
> ð’› "
> Position
> Embedding
> ð‘¥ "
> "â‹¯ð‘¥ "
> ($)ð‘¦ "
> â‹®â‹±â‹®â‹®
> ð‘¥ &
> "â‹¯ð‘¥ &
> ($)ð‘¦ &
> KL Loss
> Reconstruction Loss
> Training ï¼šGenerative Space Construction
> Inference ï¼šSearch on the Generative Space
> Evaluation: Fitting ð‘“ ð’™ ,ð‘“ $ð’™ & Complexity

ð‘“ " ð’™ 

> Schematic of the evolution
> Update
> Tokenize & Embed
> Frozen
> Prefix Order
> Sampling
> ð’› '
> ð’› "
> ð’› '
> Degraded CMA -ES

Figure 1: The overview of GenSR. During training, the dashed lines denote the prior branch, while the solid lines indicate the posterior branch. During inference, only the prior branch is used. 3.1 PREPARATIONS 

To construct the generative latent space, we first prepare synthetic data and preprocess input numerical samples and equations, laying the foundation for dual-branch CVAE pre-training. 

Preparation of synthetic data. To construct the generative latent space, we pretrain the conditional VAE (Fig. 1) on a large synthetic dataset. Following Kamienny et al. (2022); Meidani et al., we construct a dataset in which each equation f is paired with m numerical examples (x, y )mi=1 âˆˆ RmÃ—(D+1) , where D denotes the dimensionality of the input variables. Note that we control the expression length to push complex equations toward the periphery of the latent space, mitigating overfitting to complex solutions during search. 

Pre-processing of numeric samples. Following Kamienny et al. (2022), we tokenize each numerical sample using base-10 floating-point representation, rounding to four significant digits and decomposing into sign, mantissa ( 0â€“9999 ), and exponent ( Eâˆ’100 â€“E100 ), e.g., 0.7895 is tokenized as [+ , 7895 , E âˆ’4] . The 

m samples for each equation f are represented as {(x, y )}mi=1 âˆˆ RmÃ—3( D+1) . A learnable embedder with dimensionality reduction maps these token embeddings to X âˆˆ RmÃ—d, which serves as the numerical input to the Transformer encoder (Kamienny et al., 2022). 

Pre-processing of equation. Each expression f is represented as a binary tree and linearized in prefix order (Lample & Charton). Operators, variables, and integers are mapped to dedicated tokens, while con-stants are tokenized in the same way as numeric samples. Special tokens [ âŒ©BOS âŒª] and [ âŒ©EOS âŒª] are inserted to mark the start and end of each sequence. Particularly, sequences are padded to a fixed length m to match the sequence length of numeric samples. Each token is embedded into a d-dimensional space with positional encoding, yielding F âˆˆ RmÃ—d as the equation input to the Transformer encoder. 4Published as a conference paper at ICLR 2026 3.2 PRE -TRAIN THE GENERATIVE LATENT SPACE FOR EQUATIONS 

We adopt the dual-branch CVAE shown in Fig. 1 to learn a well-organized generative latent space for equa-tions, which can be regarded as a â€œmapâ€ of the equation space. This map provides initial localization of high-probability regions and enables smooth directional search toward optimal solutions, achieving efficient equation discovery. The dual-branch CVAE consists of a posterior branch and a prior branch. Follow-ing Kingma & Welling (2013); Sohn et al. (2015), we assume the conditional distribution of latent variables is a multivariate independent Gaussian distribution. Next, we describe each branch and its training details. 

Posterior Branch (solid lines in Fig. 1): Comprises an 8-layer Transformer encoder, a posterior network, a feature fusion MLP, and an 8-layer Transformer decoder. It takes both X (numerical samples) and F

(symbolic equation) as inputs, learning the posterior distribution q(z|X, F ) = N (Î¼1, Ïƒ21 I) via the posterior network. Using the reparameterization trick, we sample n latent vectors Z = {zi = Î¼1 + Ïƒ1 âŠ™ Ïµ}ni=1 ,where Ïµ âˆ¼ N (0 , I ) and âŠ™ denotes element-wise multiplication. The feature fusion MLP aggregates salient information across sampled latent vectors, and the Transformer decoder reconstructs the original symbolic equation, ensuring the latent space captures the continuity of symbolic structures. 

Prior Branch (dashed lines in Fig. 1): The prior branch shares all modules (Transformer encoder/decoder, feature fusion MLP) with the posterior branch, except that the posterior network is replaced by a prior net-work. It takes only numerical samples X as input and learns the prior distribution p(z|X) = N (Î¼2, Ïƒ22 I),which captures the numerical behavior of equations. 

Training Process : During training, both branches are jointly optimized on paired data (F , X) using a combined objective: (1) Reconstruction loss Lrec enforces continuity of symbolic structures by accurately decoding equations from the latent space; (2) KL divergence DKL aligns the posterior distribution q(z|X, F )

with the prior distribution p(z|X), ensuring local numerical smoothness. This alignment of distributions regularizes the prior network to support reliable inference when the true equation is unknown. We apply KL annealing during training to mitigate posterior collapse. 3.3 ANALYSIS OF GENERATIVE LATENT SPACE PROPERTIES 

We further explain why GenSR in Sec. 3.2 yields a generative latent space that is globally continuous in symbolic structure and locally smooth in numerical features. 

Continuity of symbolic structure. Let q(1) (z) = N (Î¼(1) , Ïƒ(1)2 I) and q(2) (z) = N (Î¼(2) , Ïƒ(2)2 I)

denote the two output posterior distributions for input sample pairs (F (1) , X(1) ) and (F (2) , X(2) ), respec-tively. We define the high-confidence region for each distribution at confidence level Î± as: 

R(i) 

> Î±

=

ï£±ï£²ï£³z âˆˆ Rd :

> d

X

> j=1

(zj âˆ’ Î¼(i) 

> j

)2

(Ïƒ(i) 

> j

)2 â‰¤ Ï‡2

> d,Î±

ï£¼ï£½ï£¾ , i = 1 , 2,

where Ï‡2 

> d,Î±

is the Î±-quantile of the chi-squared distribution with d degrees of freedom. For two distributions, their n-sampled sets Z(j) = {z(j) 

> i

= Î¼(j) +Ïƒ(j) âŠ™Ïµ}ni=1 , where Ïµ âˆ¼ N (0 , I), j = 1 , 2, typically fall within the respective high-probability regions R(1)  

> Î±

and R(2)  

> Î±

. Here, unlike the traditional VAE framework that sam-ples only a single z, we employ repeated probabilistic sampling and a feature fusion module to ensure that the reconstruction loss influences the high-probability regions of the distribution, thereby strengthening the symbolic continuity of the latent space. If there exists zâˆ— âˆˆ R (1)  

> Î±

âˆ© R (2)  

> Î±

, it is shaped by the reconstruction objectives of both f (1) and f (2) , yielding a decoded structure that interpolates between them. This encour-ages a smooth symbolic transition in the latent space. Trained on diverse equations, such local interpolation generalizes globally, clustering structurally similar equations in adjacent regions and inducing a latent space with continuous symbolic structure. 5Published as a conference paper at ICLR 2026 

Smoothness of numerical feature. We leverage the prior branch to capture characteristics of numerical distributions and align them with the symbolic latent space through a KL divergence loss, imposing numer-ical constraints on the distribution. This design resembles contrastive learning in SNIP (Meidani et al.), but with a key distinction: we align entire distributions rather than individual points, which ensures local numer-ical smoothness within the generative latent space. Specifically, the KL loss DKL (q(z|X, F ) âˆ¥ p(z|X)) 

regularizes the posterior distribution to stay close to the prior distribution. For the independent Gaussian distributions assumed in our CVAE ( Î£1 = Ïƒ21 I, Î£2 = Ïƒ22 I), this divergence simplifies to: 

DKL (q âˆ¥ p) = 12

> d

X

> j=1

(Î¼1,j âˆ’ Î¼2,j )2

Ïƒ22,j 

+ Ïƒ21,j 

Ïƒ22,j 

âˆ’ ln Ïƒ21,j 

Ïƒ22,j 

âˆ’ 1

!

.

Minimizing this term in our dual-branch CVAE constrains the distance between the posterior and prior means while aligning their variances, ensuring that the high-probability regionsâ€”rather than isolated pointsâ€”are matched to preserve local numerical smoothness. Consequently, GenSR establishes a well-structured generative latent space for SR. We will provide more validation of the properties of the space in Sec. 5.3. 3.4 SEARCH IN THE GENERATIVE LATENT SPACE 

During inference, since the ground-truth equation is unavailable, only the prior branch is used. The inference process is illustrated in Fig. 1. For a given set of numerical samples {(x, y )i}mi=1 , we first preprocess them as described in Sec. 3.1. The prior branch then encodes them into an initial Gaussian distribution N (Î¼0, Ïƒ20 I) in the latent space. This distribution provides coarse localization, highlighting regions likely to contain equations that fit the data well. For example, high-probability regions align with plausible symbolic families (e.g., trigonometric, exponential) and input dimensionality, offering a structured starting point for fine-grained search. Since the prior and posterior latent spaces are only approximate, directly decoding the vector Î¼0 in an end-to-end manner may not yield the most accurate result. To refine the initial distribution N (Î¼0, Ïƒ20 I)

toward the optimal solution, we use a modified Covariance Matrix Adaptation Evolution Strategy (CMA-ES) (Hansen, 2016), leveraging the numerical smoothness of the latent space for efficient convergence. The algorithm is as follows: 1. Initialization : Set the initial search distribution as N (Î¼0, Ïƒ20 I) (with Î¼0 and Ïƒ0 are the output of the prior network). 2. Iteration (for each generation i): (a) Sample multiple latent vectors {zj } from N (Î¼i, Ïƒ2 

> i

I).(b) Decode {zj } into candidate equations {fj (x)} using the well-trained decoder. (c) Refine the constants of {fj (x)} via BFGS optimization. (d) Evaluate the fitness of candidate equations: Fitness = R2 âˆ’ Ï‰ Â· complexity. (e) Select the top-p candidates and update the distribution parameters Î¼i â†’ Î¼i+1 and Ïƒi â†’ Ïƒi+1 

using the CMA-ES update rules. This yields the updated distribution The prior branch provides a well-organized generative latent space for CMA-ES, thereby enabling faster and more stable convergence. However, CMA-ES becomes costly as the latent dimension d grows. To improve efficiency, we introduce two key modifications to standard CMA-ES: 1. Diagonal Covariance Assumption : Inspired by Ros & Hansen (2008), we restrict the covariance matrix Î£ to a diagonal form Ïƒ2I, updating only the variance of each latent dimension indepen-dently. This aligns with the latent variable assumption in VAEs (Kingma & Welling, 2013) (mod-eled as independent Gaussian components) and reduces the number of parameters to optimize. 6Published as a conference paper at ICLR 2026 2. Top-k Variance Update : Update only the top k latent dimensions with the largest variances (while setting others to zero). This focuses search on the most uncertain and relevant directions, acceler-ating convergence without compromising solution quality. These modifications enable rapid and precise contraction of the initial distribution, facilitating rapid local-ization of the target equation. Fig. 1 provides a schematic of the CMA-ES distribution contraction process. 

## 4 GEN SR: S YMBOLIC REGRESSION FROM A BAYESIAN PERSPECTIVE 

GenSR can be naturally interpreted as probabilistic inference, reformulating SR task as a Bayesian problem: given numerical samples X âˆˆ RmÃ—d, the goal is to infer a posterior distribution p(F |X) over equation space that best explains the data, i.e., maximize p(F |X). Unlike traditional SR tasks seeking a single optimum, this Bayesian formulation reasons over the entire posterior distribution of candidate equations, enabling principled uncertainty quantification and probabilistic comparison across candidates. To approximate the intractable distribution p(F |X), we introduce a variational distribution q(z|X, F ),where z âˆˆ Rd is a latent vector. Following Sohn et al. (2015), we derive the Evidence Lower Bound (ELBO) of the marginal log-likelihood log p(F |X) as follows. The full derivation is provided in Appendix B. 

log p(F |X) = 

Z

> z

q(z|X, F ) log p(F |X)dz

â‰¥

Z

> z

q(z|X, F ) log p(F , z|X)

q(z|X, F ) dz

= Eq(z|X,F ) [log p(F |X, z)] âˆ’ DKL (q(z|X, F ) || p(z|X)) . (1) Maximizing p(F |X) amounts to maximizing the ELBO 1. To optimize this objective, GenSR employs posterior encoder qÏ•,Ï† (z|X, F ), prior encoder pÏ•,Ïˆ (z|X), and decoder pÎ¸ (F |X, z) to parameterize 

q(z|X, F ), p(z|X), and p(F |X, z), respectively. Then, training our dual-branch CVAE with Lrec and 

DKL is equivalent to maximizing the ELBO 1: maximizing Eq(z|X,F )[log p(F |X, z)] corresponds to max-imizing the probability of reconstructing F using z sampled from the output of qÏ•,Ï† (z|X, F ), i.e., minimiz-ing the reconstruction loss Lrec ; and the second term in inequality 1 corresponds to the KL-divergence loss in Sec. 3.2 which aligns the variational distribution qÏ•,Ï† (z|X, F ) with the prior distribution pÏ•,Ïˆ (z|X).During inference, we sample z from the prior distribution pÏ•,Ïˆ (z|X) to reconstruct the equation F , which approximates the intractable expectation EqÏ•(z|X,F )[log pÎ¸ (F |X, z)] with EpÏˆ (z|X)[log pÎ¸ (F |X, z)] . To mitigate the inherent approximation error, we refine the prior distribution as explained in Sec. 3.4. Notably, GenSR differs from Holt et al., which approximates p(Token |Num .) at the token level without constructing a latent distribution over symbolic equations, and also from MeË‡ znar et al. (2023); Popov et al. (2023), which only models p(Equ .). GenSR explicitly formulates the SR task as maximizing the conditional distribution p(Equ .|Num .) from a Bayesian inference perspective. To the best of our knowledge, GenSR is the first framework that realize SR based on estimating and optimizing the ELBO of p(Equ .|Num .). Under GenSR framework, our dual-branch CVAE pretraining model corresponds directly to a ELBO, while the CMA-ES refinement process serves as an approximate optimization of the conditional variational distribu-tion. This not only provides GenSR with solid theoretical guarantee, but also introduces a new Bayesian perspective and technical route for SR. 

## 5 EXPERIMENT 

5.1 EXPERIMENTAL SETTINGS 

To validate our method, we conduct evaluations on the SRBench benchmark (La Cava et al., 2021; Cavalab, 2022), which consists of 119 Feynman equations, 14 ODE-Strogatz challenges, and 57 black-box regres-7Published as a conference paper at ICLR 2026 sion tasks without known underlying functions. We compare against 18 baseline algorithms, covering both pretraining-based methods and diverse heuristic-based methods. The evaluation metrics include accuracy 

R2, time complexity, and equation complexity. To better balance and visualize the trade-offs among these three metrics, we employ the Pareto front for representation. Comprehensive information on datasets, met-rics, and baseline methods can be found in Appendix G.1, G.2, and G.3. For clarity, this section presents only the key experimental results; the full results are deferred to Appendix G.5. Additional analyses, including the ablation study and other task based on our generative space, are provided in Appendix E, G.4. 5.2 COMPARISON OF GEN SR WITH OTHER METHODS 0 2 4 6 8 10 12 14 16 18 20           

> Mean RÂ² Rank
> 0
> 2
> 4
> 6
> 8
> 10
> 12
> 14
> 16
> 18
> 20
> Equation Complexity Rank
> GP-GOMEA
> SPL
> RSRM
> Ours
> NeurSR
> SNIP
> DSR
> MDL
> RAG-SR
> E2ESR
> AFP
> AFP-FE
> EPLEX
> BSR
> TPSR
> SBP-GP
> Operon
> GPlearn
> AIFeynman2
> FEAT 0246810 12 14 16 18 20
> Mean RÂ² Rank
> 0
> 2
> 4
> 6
> 8
> 10
> 12
> 14
> 16
> 18
> 20
> Time Complexity Rank
> NeurSR
> SNIP
> Operon
> RSRM
> AFP
> EPLEX
> SBP-GP
> SPL
> AIFeynman2
> MDL
> FEAT
> AFP-FE
> DSR
> GPlearn
> BSR
> E2ESR
> GP-GOMEA
> TPSR
> RAG-SR
> Ours

Figure 2: Pareto front results on the Feynman dataset. The x-axis shows the mean test R2 rank, while the y-axis shows equation complexity rank (left) and time complexity rank (right). Solid lines indicate the optimal Pareto front, and dashed lines show lower-ranked fronts from bottom-left to top-right. 0.80 0.85 0.90 0.95 1.00        

> RÂ² Score
> AFP
> FEAT
> AFP-FE
> TPSR
> SNIP
> SBP-GP
> Operon
> MDL
> GP-GOMEA
> RAG-SR
> Ours
> Target Noise
> 0.000
> 0.001
> 0.01
> 0.1
> 10 210 310 4
> Time Complexity(s)
> 020 40 60 80 100
> Equation Complexity

Figure 3: Comparison on the Strogatz dataset under different noise levels. Subplots (left to right) report R2

score, time complexity (s), and equation complexity. Noise levels are represented by blue circles (0.000), orange squares (0.001), green triangles (0.01), and red diamonds (0.1), with error bars indicating standard deviations. Only methods whose mean R2 across noise settings exceeds 0.9 are included. 

Pareto performance analysis. As shown in Fig. 2, our method achieves a balanced optimization across accuracy ( R2), equation complexity, and time complexity, and consistently lies on the rank-1 Pareto front, outperforming competing baselines overall. The superiority stems from the generative continuous latent space learned via CVAE training, which preserves global structural continuity while ensuring local numer-ical smoothness. Built upon this representation, the GenSR efficiently enables a hierarchical optimization 8Published as a conference paper at ICLR 2026 paradigm of â€œmap construction â†’ coarse localization â†’ fine search.â€ In contrast to discrete symbolic spaces that rely on heuristic mutations and backtracking, our well-organized generative latent space provides sta-ble directional signals, making the search both more efficient and less prone to overfitting. Consequently, GenSR achieves robust improvements in accuracy, expression brevity, and computational efficiency. 

Performance under noisy targets. As shown in Fig. 3, our method achieves the highest R2 across varying noise levels, with smaller variance than competing baselines, indicating stable and robust fitting under perturbations. Moreover, it requires significantly less runtime and yields equations of lower com-plexity. These results highlight that our generative latent space preserves structural organization and local smoothness even in the presence of noise, thereby providing search with stable optimization directions and avoiding the random oscillations and inefficient exploration typical of discrete symbolic spaces. Benefiting from this design, GenSR maintains a favorable balance of accuracy, efficiency, and compactness under noisy conditions, demonstrating strong robustness. 5.3 VISUALIZATION COMPARISON OF LATENT SPACE 

We compare GenSR with E2ESR (Kamienny et al., 2022) and SNIP (Meidani et al.). E2ESR employs a Transformer to predict target equations, while SNIP leverages contrastive pretraining to align symbolic forms with numeric data. Unlike GenSR, Etheir spaces are all discriminative rather than generative. 

Structural separability comparison of latent spaces. As shown in Fig. 4, different methods induce distinct distributional characteristics of equation latent vectors. The latent space of E2ESR fails to effectively separate function types and input dimensions: although it can partially distinguish two input variants of the 

log operator, the trig and exp components remain highly entangled, resulting in a mixed representation. SNIP achieves slightly better separation at the operator level, yet lacks intra-class compactness; for instance, the exp -5D cluster splits into two distinct regions, accompanied by noticeable overlaps across categories. In contrast, our GenSR exhibits a clearly structural disentangled latent space, where both different function types and input dimensionality are well separated. This space provides GenSR with favorable conditions for initial equation localization while ensuring decoding stability. exp-5D 

> exp-2D
> trig-5D
> trig-2D
> log-5D
> log-2D
> (a) E2ESR exp-5D
> exp-2D
> trig-5D
> trig-2D
> log-5D
> log-2D
> (b) SNIP exp-5D
> exp-2D
> trig-5D
> trig-2D
> log-5D
> log-2D
> (c) GenSR

Figure 4: 2D t-SNE visualization of latent variables from E2ESR, SNIP, and GenSR. The legend distin-guishes six categories, corresponding to equations from three representative function families, each evalu-ated under 2D and 5D input dimensionality, illustrating the clustering behavior of the learned latent spaces. 9Published as a conference paper at ICLR 2026 (a) exp-5D (b) exp-2D (c) trig-5D (d) trig-2D (e) log-5D (f) log-2D 0.00 

> 0.25
> 0.50
> 0.75
> 1.00
> Average Normalized y

Figure 5: 2D t-SNE visualization of GenSR latent variables for equations from three function families (exponential, trigonometric, logarithmic) under 2D and 5D input settings, shown in subplots (a)â€“(f). Colors indicate the average of normalized y values, as displayed in the accompanying color bar. 

Numerical continuity within function Classes. As for each observed sample {xi, y i}Ni=1 , we use the normalized mean of {yi}Ni=1 to represent the numerical feature. Fig. 5 visualizes the local numerical feature distribution of GenSR, where each subfigure shows the numerical feature distribution of one cluster in Fig. 4 (c). For each structural cluster, GenSR exhibits a smooth distribution of numerical features, which enables the search algorithm refine convergence along smooth numerical directions after localizing high probability areas. Details and additional latent space experiments are provided in the Appendix C. 

## 6 CONCLUSION AND FUTURE WORK 

We introduced GenSR, a generative latent space-based SR framework that maps the discrete symbolic equa-tion space into a continuous, well-structured space. GenSR first jointly enforces symbolic continuity and local numerical smoothness through a dual-branch CVAE, and then enables efficient symbolic regression through coarse localization and fine-grained search. This design bridges combinatorial symbolic search with continuous optimization, yielding a balanced trade-off among accuracy, complexity, and efficiency. More-over, GenSR organizes distinct equation families into separate latent regions, naturally supporting tasks such as equation classification, which we further demonstrate in the appendix. These results highlight the broader potential of generative latent spaces as a foundation for equation discovery and, more generally, for combina-torial scientific search. Future works include incorporating richer generative priors and physical constraints, and leveraging more advanced generative models to construct more powerful and generalizable equation spaces, thereby advancing interpretable and domain-aware machine learning for scientific discovery. 

## 7 ACKNOWLEDGEMENT 

This work was supported by the National Natural Science Foundation of China (12572266), and the High Performance Computing Centers at Eastern Institute of Technology, Ningbo, and Ningbo Institute of Digital Twin. 10 Published as a conference paper at ICLR 2026 

## REPRODUCIBILITY STATEMENT 

For the reproducibility of our results, we have provided a detailed description of our methods and experi-mental setups in Sec. 3.1, 3.2 and Appendix F. We also confirmed the robustness of our results through the experiment (Appendix G.5). In addition, to further facilitate the reproduction, we will release our codes and the checkpoints for the trained models. 

## REFERENCES 

Ignacio Arnaldo, Krzysztof Krawiec, and Una-May Oâ€™Reilly. Multiple regression genetic programming. In Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation, pp. 879â€“886, 2014. Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. Neu-ral symbolic regression that scales. In International Conference on Machine Learning, pp. 936â€“945. Pmlr, 2021. Bogdan Burlacu, Gabriel Kronberger, and Michael Kommenda. Operon c++ an efficient genetic pro-gramming framework for symbolic regression. In Proceedings of the 2020 genetic and evolutionary computation conference companion, pp. 1562â€“1570, 2020. Cavalab. Srbench: A living benchmark framework for symbolic regression, 2022. URL https: //cavalab.org/srbench/results/#symbolically-verfied-solutions . Accessed: 2024-09-26. Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785â€“794, 2016. Yuntian Chen, Yingtao Luo, Qiang Liu, Hao Xu, and Dongxiao Zhang. Symbolic genetic algorithm for discovering open-form partial differential equations (sga-pde). Physical Review Research, 4(2):023174, 2022. Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582, 2023. Fabricio Olivetti de Franca and Guilherme Seidyo Imai Aldeia. Interactionâ€“transformation evolutionary algorithm for symbolic regression. Evolutionary computation, 29(3):367â€“390, 2021. Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, and Benjamin Wild. Janusdna: A powerful bi-directional hybrid dna foundation model. arXiv preprint arXiv:2505.17257, 2025. Chen Feng, Ziquan Liu, Zhuo Zhi, Ilija Bogunovic, Carsten Gerner-Beuerle, and Miguel Rodrigues. Prosac: Provably safe certification for machine learning models under adversarial attacks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2933â€“2941, 2025. Chen Feng, Minghe Shen, Ananth Balashankar, Carsten Gerner-Beuerle, and Miguel RD Rodrigues. Noisy but valid: Robust statistical evaluation of LLMs with imperfect judges. In The Fourteenth International Conference on Learning Representations, 2026. URL https://openreview.net/forum?id= hEhxreaLdU .Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an applica-tion to boosting. Journal of computer and system sciences, 55(1):119â€“139, 1997. 11 Published as a conference paper at ICLR 2026 Arya Grayeli, Atharva Sehgal, Omar Costilla Reyes, Miles Cranmer, and Swarat Chaudhuri. Symbolic regression with a learned concept library. Advances in Neural Information Processing Systems, 37:44678â€“ 44709, 2024. Nikolaus Hansen. The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016. Samuel Holt, Zhaozhi Qian, and Mihaela van der Schaar. Deep generative symbolic regression. In The Eleventh International Conference on Learning Representations. Yuxiao Hu, Qian Li, Xiaodan Shi, Jinyue Yan, and Yuntian Chen. Domain knowledge-enhanced multi-spatial multi-temporal pm2. 5 forecasting with integrated monitoring and reanalysis data. Environment international, 192:108997, 2024. Yuxiao Hu, Qian Li, Dongxiao Zhang, Jinyue Yan, and Yuntian Chen. Context-alignment: Activating and enhancing llm capabilities in time series. arXiv preprint arXiv:2501.03747, 2025. Tao Huang, Rui Wang, Xiaofei Liu, Yi Qin, Li Duan, and Liping Jing. Detecting misbehaviors of large vision-language models by evidential uncertainty quantification. arXiv preprint arXiv:2602.05535, 2026. Yanhao Jia, Ji Xie, S Jivaganesh, Hao Li, Xu Wu, and Mengmi Zhang. Seeing sound, hearing sight: Un-covering modality bias and conflict of ai models in sound localization. arXiv preprint arXiv:2505.11217, 2025. Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, and Jian Guo. Bayesian symbolic regression. AAAI, 2020. Pierre-Alexandre Kamienny, StÂ´ ephane dâ€™Ascoli, Guillaume Lample, and Franc Â¸ois Charton. End-to-end symbolic regression with transformers. Advances in Neural Information Processing Systems, 35:10269â€“ 10281, 2022. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. William La Cava, Lee Spector, and Kourosh Danai. Epsilon-lexicase selection for regression. In Proceedings of the Genetic and Evolutionary Computation Conference 2016, pp. 741â€“748, 2016. William La Cava, Tilak Raj Singh, James Taggart, Srinivas Suri, and Jason H Moore. Learning concise representations for regression by evolving networks of trees. ICLR, 2019. William La Cava, Bogdan Burlacu, Marco Virgolin, Michael Kommenda, Patryk Orzechowski, FabrÂ´ Ä±cio Olivetti de Franc Â¸a, Ying Jin, and Jason H Moore. Contemporary symbolic regression meth-ods and their relative performance. Advances in neural information processing systems, 2021(DB1):1, 2021. Guillaume Lample and Franc Â¸ois Charton. Deep learning for symbolic mathematics. In International Conference on Learning Representations. Qian Li, Yuxiao Hu, and Yang Cao. Adaptive peronaâ€“malik model based on dynamical threshold for image multiâ€“noise removal with details preservation. Computers & Mathematics with Applications, 137:28â€“43, 2023a. 12 Published as a conference paper at ICLR 2026 Qian Li, Yuxiao Hu, Ye Liu, Dongxiao Zhang, Xin Jin, and Yuntian Chen. Discrete point-wise attack is not enough: Generalized manifold adversarial attack for face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 20575â€“20584, 2023b. Qian Li, Yuxiao Hu, Yinpeng Dong, Dongxiao Zhang, and Yuntian Chen. Focus on hiders: Exploring hidden threats for enhancing adversarial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24442â€“24451, 2024. Wenqiang Li, Weijun Li, Linjun Sun, Min Wu, Lina Yu, Jingyi Liu, Yanjie Li, and Songsong Tian. Transformer-based model for symbolic regression via joint supervised learning. In The Eleventh International Conference on Learning Representations, 2022. Kazem Meidani, Parshin Shojaee, Chandan K Reddy, and Amir Barati Farimani. Snip: Bridging mathemat-ical symbolic and numeric realms with unified pre-training. In The Twelfth International Conference on Learning Representations. Sebastian MeË‡ znar, SaË‡ so DË‡ zeroski, and LjupË‡ co Todorovski. Efficient generator of mathematical expressions for symbolic regression. Machine Learning, 112(11):4563â€“4596, 2023. Randal S Olson, William La Cava, Patryk Orzechowski, Ryan J Urbanowicz, and Jason H Moore. Pmlb: a large benchmark suite for machine learning evaluation and comparison. BioData mining, 10:1â€“13, 2017. Brenden K Petersen, Mikel Landajuela Larma, Terrell N Mundhenk, Claudio Prata Santiago, Soo Kyung Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In International Conference on Learning Representations. Sergei Popov, Mikhail Lazarev, Vladislav Belavin, Denis Derkach, and Andrey Ustyuzhanin. Symbolic expression generation via variational auto-encoder. PeerJ Computer Science, 9:e1241, 2023. Steven J Rigatti. Random forest. Journal of Insurance Medicine, 47(1):31â€“39, 2017. Raymond Ros and Nikolaus Hansen. A simple modification in cma-es achieving linear time and space complexity. In International conference on parallel problem solving from nature, pp. 296â€“305. Springer, 2008. Michael D Schmidt and Hod Lipson. Age-fitness pareto optimization. In Proceedings of the 12th annual conference on Genetic and evolutionary computation, pp. 543â€“544, 2010. Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. Llm-sr: Scientific equation discovery via programming with large language models. In The Thirteenth International Conference on Learning Representations. Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, and Chandan Reddy. Transformer-based planning for symbolic regression. Advances in Neural Information Processing Systems, 36:45907â€“45919, 2023. Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep condi-tional generative models. Advances in neural information processing systems, 28, 2015. Trevor Stephens et al. gplearn: Genetic programming in python, with a scikit-learn inspired api. Journal of Engineering for Gas Turbines and Power, 30, 2015. Steven H Strogatz. Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering. CRC press, 2018. 13 Published as a conference paper at ICLR 2026 Fangzheng Sun, Yang Liu, Jian-Xun Wang, and Hao Sun. Symbolic physics learner: Discovering gov-erning equations via monte carlo tree search. In The Eleventh International Conference on Learning Representations. Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, and Max Tegmark. Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. Advances in Neural Information Processing Systems, 33:4860â€“4871, 2020. Marco Virgolin, Tanja Alderliesten, and Peter AN Bosman. Linear scaling with and within semantic backpropagation-based genetic programming for symbolic regression. In Proceedings of the genetic and evolutionary computation conference, pp. 1084â€“1092, 2019. Marco Virgolin, Tanja Alderliesten, Cees Witteveen, and Peter AN Bosman. Improving model-based genetic programming for symbolic regression of small expressions. Evolutionary computation, 29(2):211â€“237, 2021. Hao Xu, Jinglong Lin, Dongxiao Zhang, and Fanyang Mo. Retention time prediction for chromatographic enantioseparation by quantile geometry-enhanced graph neural network. Nature Communications, 14(1): 3095, 2023. Yilong Xu, Yang Liu, and Hao Sun. Rsrm: Reinforcement symbolic regression machine. ICLR, 2024. Zihan Yu, Jingtao Ding, Yong Li, and Depeng Jin. Symbolic regression via mdlformer-guided search: from minimizing prediction error to minimizing description length. In The Thirteenth International Conference on Learning Representations, 2025. Hengzhe Zhang, Qi Chen, Wolfgang Banzhaf, Mengjie Zhang, et al. Rag-sr: Retrieval-augmented generation for neural symbolic regression. In The Thirteenth International Conference on Learning Representations, 2025. 14 Published as a conference paper at ICLR 2026 

## A THE USE OF LARGE LANGUAGE MODELS (LLM S)

Large Language Models (LLMs) were used solely as auxiliary tools for language polishing and minor edit-ing. They did not contribute to the research ideation, experimental design, or writing to an extent that would qualify them as contributors. All substantive research ideas, methodologies, and results presented in this paper are entirely the work of the authors. 

## B DERIVATION OF p(F |X)MMXIMIZATION 

We refer to Sohn et al. (2015) to approximate the intractable distribution p(F |X) using a variational dis-tribution q(z|X, F ) that can be any distribution (which is assumed as a multivariate independent Gaussian distribution in our work). The detailed derivation is as follows. 

log p(F |X) = 

Z

> z

q(z|X, F ) log p(F |X)dz

=

Z

> z

q(z|X, F ) log p(F , z|X)

p(z|X, F ) dz

=

Z

> z

q(z|X, F ) log 

 p(F , z|X)

q(z|X, F )

q(z|X, F )

p(z|F , X)



dz

=

Z

> z

q(z|X, F ) log 

 p(F , z|X)

q(z|X, F )



dz +

Z

> z

q(z|X, F ) log 

 q(z|X, F )

p(z|F , X)



dz

=

Z

> z

q(z|X, F ) log 

 p(F , z|X)

q(z|X, F )



dz + DKL (q(z|X, F )|| p(z|F , X)) .

Since DKL (q(z|X, F )|| p(z|F , X)) â‰¥ 0, thus, we have the lower bound of log p(F |X):

log p(F |X) â‰¥

Z

> z

q(z|X, F ) log 

 p(F , z|X)

q(z|X, F )



dz

=

Z

> z

q(z|X, F ) log 

 p(F |z, X)p(z|X)

q(z|X, F )



dz

=

Z

> z

q(z|X, F ) log p(F |z, X)dz +

Z

> z

q(z|X, F ) log 

 p(z|X)

q(z|X, F )



dz

=

Z

> z

q(z|X, F ) log p(F |z, X)dz âˆ’ DKL (q(z|X, F )|| p(z|X)) = Eq(z|X,F ) [log p(F |X, z)] âˆ’ DKL (q(z|X, F )|| p(z|X)) .

To optimize this lower bound, GenSR employ posterior encoder qÏ•,Ï† (z|X, F ), prior encoder pÏ•,Ïˆ (z|X),and decoder pÎ¸ (F |X, z) to parameterize q(z|X, F ), p(z|X), and p(F |X, z), respectively. Here, Ï•, Ï†, and 

Ïˆ denote the model parameters of the Transformer encoder, the posterior network, and the prior network, respectively, and Î¸ comprises the parameters of the Transformer decoder and the feature fusion module. Therefore, training the CVAE with reconstruction loss and KL divergence is equivalent to maximizing the lower bound, i.e., maximizing the conditional distribution p(F | X).15 Published as a conference paper at ICLR 2026 

## C T-SNE D ATASET CONSTRUCTION AND SUPPLEMENTARY EXPERIMENTS 

C.1 T-SNE D ATASET CONSTRUCTION 

In Fig. 4 and Fig. 5, we constructed a dedicated dataset to analyze the separability of function categories and the continuity of numerical features in the latent space. The dataset covers three representative classes of functions: exponential functions, trigonometric functions, and logarithmic functions. The exponential class includes the operator exp( Â·), the trigonometric class includes sin( Â·), cos( Â·), and tan( Â·), while the logarithmic class uses log( Â·).To systematically examine the impact of input dimensionality on latent space distribution, we generated two cases for each function class: 2-dimensional input ( D = 2 ) and 5-dimensional input ( D = 5 ). This setting allows us to verify whether the latent space maintains clear structural separability and numerical continuity under both low- and higher-dimensional inputs. The expression generation process is as follows: first, we specify the dimensionality of the variable x, and then combine the basic operators +, âˆ’, Ã—, Ã· with the corresponding function operators from each cate-gory. To ensure comparability, we constrain the expression length such that the number of tokens (including coefficients, operators, and variables) is less than 25. In terms of scale, for each function class and dimensionality combination, we generated 5000 equations, and for each equation, 200 data points were sampled to construct the corresponding dataset. This design provides sufficient statistical coverage and ensures that the t-SNE visualization results are representative. In summary, this dataset systematically spans function categories, input dimensionalities, expression struc-tures, and sample sizes, serving as a solid basis for validating the global structural separability and local numerical feature continuity of the latent space. C.2 INTERPOLATION RESULTS IN LATENT SPACE 

Table 1: Linear interpolation between two expressions (1 â†’2) in latent space across methods. For each pair, we decode from z(ratio ) = (1 âˆ’ ratio )z1 + ratio z2 with ratio âˆˆ { 0, 0.25 , 0.5, 0.75 , 1}. Expressions marked in red are syntactically invalid.                                                                                                                                       

> ratio Ours E2ESR SNIP
> Pair 1: Expression 1: cexp( x)Â·sin( xâˆ’c)â†’Expression 2: log( x+c)Â·cos( x)
> 0cexp( x) sin( xâˆ’c)cexp( x) sin( xâˆ’c)cexp( x) sin( xâˆ’c)
> 0.25 cexp( x) sin( xâˆ’c) + csin( x)Invalid exp( x+c) sin( x) + csin( x)
> 0.5 cexp( cÂ·c) sin( x) + log( cÂ·x) cos( c)Invalid exp( x) cos( x) + ccos( x+c)
> 0.75 exp( c) + log( x+c) cos( x)log( x) sin( x)Â·cos( x)Invalid 1log( x+c) cos( x)log( x+c) cos( x)log( x+c) cos( x)
> Pair 2: Expression 1: xcos( c x ) + câ†’Expression 2: xâˆ’sin( xâˆ’c)
> 0xcos( c x ) + cxcos( c x ) + cxcos( c x ) + c
> 0.25 xcos( c x ) + câˆ’cos( x)Invalid xcos( c) + csin( x)
> 0.5 ccos( cÂ·x) + câˆ’cos( xâˆ’c)Invalid Invalid 0.75 cos (cÂ·x)âˆ’sin( xâˆ’c)Invalid Invalid 1xâˆ’sin( xâˆ’c)xâˆ’sin( xâˆ’c)xâˆ’sin( xâˆ’c)

We compare the continuous interpolation capabilities of generative (our GenSR) and discriminative (such as SNIP and E2E) latent spaces through interpolation experiments in the latent space. For each method, we 16 Published as a conference paper at ICLR 2026 first feed the numerical samples of Expression 1 into its numeric encoding branch. In GenSR, this branch corresponds to the prior branch that only uses numeric inputs, while in E2ESR and SNIP it is implemented as their respective numeric encoders. The output of this branch is taken as the latent representation z1.Expression 2 is handled in the same manner to obtain z2.We then construct intermediate latent points by linear interpolation, 

z(ratio ) = (1 âˆ’ ratio )z1 + ratio z2

with ratio âˆˆ { 0, 0.25 , 0.5, 0.75 , 1}. Each interpolated latent vector z(ratio ) is subsequently passed through the corresponding decoder to generate the intermediate expression. This unified procedure enables a fair comparison of different methods in terms of structural smoothness, and decoding robustness under latent-space interpolation, and allows us to assess whether their latent representations support continuous interpo-lation between equations. Table 1 reports the linear interpolation between two expressions (1 â†’2) in the latent space. Our method consistently produces syntactically valid and meaningful expressions at each interpolation step, thanks to the design of a continuous generative latent space that preserves the decodability of equations under inter-polation. In contrast, E2ESR and SNIP frequently yield syntactically invalid expressions (marked in red), indicating that their discriminative latent spaces lack sufficient syntactic regularization and often degenerate into structures that cannot be converted into valid prefix forms. Moreover, the transitions generated by our method are smooth: expressions gradually transform from 1 to 2, demonstrating the structural continuity of the latent space. By comparison, E2ESR and SNIP exhibit drastic structural changes even under small variations in the latent variables, suggesting less stable latent representations. For clarity, all constants are uniformly denoted by c to eliminate the influence of specific numeric values. 

## D RECONSTRUCTION PERFORMANCE OF POSTERIOR VS . P RIOR BRANCH 

Table 2: Reconstruction error on synthetic expression datasets. We report Levenshtein (edit) distance 

(mean Â± std; lower is better) between an expression and its reconstruction. Two operator sets are considered: (i) LogExp = {+, âˆ’, Ã—, Ã·, log , exp }; (ii) Trig = LogExp âˆª { sin , cos , tan }. Each dataset has 2k samples; the maximum expression length is shown in the name. 

Dataset (ops, length, vars) Posterior branch Prior branch 

LogExp-15 (vars â‰¤ 3) 0.094 (Â±0.023) 0.106 (Â±0.031) 

LogExp-20 (vars â‰¤ 5) 0.095 (Â±0.027) 0.112 (Â±0.029) 

LogExp-25 (vars â‰¤ 5) 0.107 (Â±0.018) 0.120 (Â±0.022) 

Trig-15 (vars â‰¤ 3) 0.115 (Â±0.032) 0.137 (Â±0.019) 

Trig-20 (vars â‰¤ 5) 0.113 (Â±0.026) 0.142 (Â±0.043) 

Trig-25 (vars â‰¤ 5) 0.146 (Â±0.041) 0.171 (Â±0.038) 

Synthetic dataset construction. To systematically evaluate the reconstruction capability of our varia-tional autoencoder, we construct six synthetic datasets under two operator sets: (i) LogExp , which includes basic arithmetic operators {+, âˆ’, Ã—, Ã·} along with log and exp , and (ii) Trig , which extends LogExp by adding trigonometric functions {sin , cos , tan }. For each operator set, we generate three datasets with max-imum expression lengths of 15, 20, and 25 tokens, constraining the number of variables to 3, 5, and 5,respectively. This range covers expression lengths slightly longer than those commonly seen in real-world benchmarks (the Feynman dataset has an average length of 13 .56 ), allowing us to evaluate reconstruction performance under increasing structural complexity. Each dataset contains 2k randomly synthesized sam-ples. 17 Published as a conference paper at ICLR 2026 We quantify reconstruction fidelity using the Levenshtein (edit) distance, which counts the minimum number of insertions, deletions, and substitutions required to transform the reconstructed expression into the ground-truth one. This metric directly measures symbolic accuracy rather than numeric approximation quality, making it a natural choice for evaluating exact expression reconstruction. 

Results analysis. Table 2 reports the reconstruction performance using latent codes from both the poste-rior and prior branches. We summarize three consistent observations: â€¢ Posterior branch consistently outperforms the prior branch. This is expected, as the posterior branch is trained by directly maximizing the expectation EqÏ•(z|X,F )[log pÎ¸ (F | X, z)] using the ground-truth latent variables as supervision, resulting in lower edit distances and more accurate reconstructions. â€¢ Prior branch performs slightly worse but remains competitive. Unlike the posterior branch, the prior branch is not trained to maximize the above expectation directly. Instead, it is optimized by aligning its output distribution with the posterior through the KL divergence loss, which leads to slightly higher edit distances. Nevertheless, the reconstruction quality remains reasonably good. This is crucial, since during inference we can only sample from the prior distribution. The results indicate that the KL loss effectively aligns the prior with the posterior, enabling accurate recon-struction even without access to ground-truth latent variables. â€¢ Expression complexity has a controlled impact on reconstruction. As expression length in-creases and trigonometric operators are introduced, the reconstruction error grows gradually but remains well-behaved. The posterior branch maintains near-exact reconstruction even for length-25 expressions, validating that the learned latent space preserves sufficient structural information. Overall, these results demonstrate that the learned latent space is both expressive and well-regularized: it enables faithful equation reconstruction from posterior samples, while the prior remains sufficiently aligned to support reliable generation at inference time. 

## E ABLATION STUDIES 

To better understand the design choices in GenSR, we conduct ablation studies on both the latent space configuration and the CMA-ES search procedure. These experiments reveal how different hyperparameters affect accuracy, equation complexity, and runtime, and guide our choice of robust default settings. 18 Published as a conference paper at ICLR 2026 E.1 LATENT DIMENSION OF THE GENERATIVE SPACE 64 128 256 512 768 

> Latent dimension
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> R2

Noise-Free     

> R2
> Equation Complexity
> 12
> 15
> 18
> 21
> 24
> 27
> Equation Complexity 64 128 256 512 768
> Latent dimension
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> R2

Noise: 1e-2     

> R2
> Equation Complexity
> 12
> 15
> 18
> 21
> 24
> 27
> Equation Complexity 64 128 256 512 768
> Latent dimension
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> R2

Noise: 1e-1 

> R2
> Equation Complexity
> 12
> 15
> 18
> 21
> 24
> 27
> Equation Complexity

Figure 6: Effect of latent dimension in GenSR. The x-axis shows the latent dimension of the pretrained model (64, 128, 256, 512, 768), while the left and right y-axes report R2 and equation complexity. Results are reported under three noise levels: Noise-Free, 1 Ã— 10 âˆ’2, and 1 Ã— 10 âˆ’1.Fig. 6 presents the results of pretraining GenSR with different latent dimensions. The results in Fig. 6 To isolate the evaluation of the latent space itself, we did not apply CMA-ES optimization and instead directly compared the outputs of the pre-trained model. In the Noise-Free setting (left subplot), R2 improves steadily as the latent dimension increases, reaching its maximum at 512 before showing a slight decline at 768. Equation complexity grows moderately for smaller dimensions but exhibits a marked surge beyond 512, indicating that excessively large latent spaces may encourage overly complex expressions. Under noisy conditions, the 768-dimensional model suffers from a more pronounced drop in accuracy and further inflated complexity, with performance degradation exacerbated as noise levels increase. Based on these observations, we adopt a 512-dimensional latent space for pretraining, as it offers the best trade-off and ensures robustness under noise. E.2 CMA-ES H YPERPARAMETERS 64 128 256 512 

> k
> 0.94
> 0.96
> 0.98
> R2
> R2
> Time Complexity(s)
> 170
> 180
> 190
> 200
> 210
> 220
> Time Complexity(s)

Figure 7: Effect of k on CMA-ES performance. 

(1) Effect of updating principal dimensions. In the CMA algorithm, we adopt a diagonal approximation by retaining only the variance terms of the covariance matrix, and further update only the top-k principal dimensions with the largest variances to reduce computational cost. Fig. 7 illustrates the effect of different 

k values. We observe that R2 improves as k increases and reaches a near-saturation point at k = 256 , with almost no further gains beyond this dimension. In contrast, the search time grows approximately linearly 19 Published as a conference paper at ICLR 2026 with k, leading to a significant increase in computational cost for larger k. Balancing accuracy and efficiency, we therefore fix k = 256 in all subsequent experiments, as it provides the best trade-off. 

(2) Other hyperparameters. We further study three intrinsic hyperparameters of CMA-ES: population size s, initial step size t, and the fitness weight Ï‰ in the objective Fitness = R2 âˆ’ Ï‰ Â· complexity. 10 20 30 40 50 60 70              

> s
> 0.95
> 0.96
> 0.97
> 0.98
> 0.99
> R2
> R2
> Time Complexity(s) 50
> 100
> 150
> 200
> 250
> 300
> Time Complexity(s) 0.3 0.5 0.7 0.9 1.1 1.3 1.5
> t
> 0.92
> 0.94
> 0.96
> 0.98
> R2
> R2
> Equation Complexity
> 15
> 18
> 21
> 24
> 27
> Equation Complexity 275 300 325 350 375 400 425
> 0.970
> 0.975
> 0.980
> 0.985
> 0.990
> 0.995
> R2
> R2
> Equation Complexity
> 16
> 18
> 20
> 22
> 24
> 26
> 28
> Equation Complexity

Figure 8: Ablation results on CMA-ES hyperparameters. Left: effect of population size s on R2 and runtime. Middle: effect of initial step size t on R2 and equation complexity. Right: effect of weight Ï‰ in the fitness function. 

Population size s. As s increases, R2 steadily improves from 0.95 to nearly 0.99 but saturates after s = 50 ,indicating that larger populations enhance global exploration yet exhibit diminishing returns. Meanwhile, runtime (i.e., time complexity) grows almost linearly from tens of seconds to nearly 300 seconds, showing a significant computational cost. Thus, medium-sized populations strike a better trade-off between accuracy and efficiency. 

Initial step size t. The value of R2 rises and then falls with increasing t, achieving the best performance around t = 1 .1, while equation complexity grows monotonically. Small step sizes constrain the search within local regions, whereas excessively large step sizes cause unstable convergence. A moderate t there-fore provides a preferable balance between accuracy and complexity. 

Fitness weight Ï‰. In the fitness function Fitness = R2 âˆ’ Ï‰ Â· complexity, Ï‰ governs the trade-off between accuracy and complexity. Smaller Ï‰ emphasizes maximizing R2 but yields overly complex equations, while larger Ï‰ favors simpler forms at the cost of accuracy. A moderate choice of Ï‰ effectively balances these two aspects, avoiding both overfitting and underfitting. 

Summary. Overall, our ablations show that optimal performance is obtained with a 512-dimensional latent space, k = 256 for covariance updates, and moderate settings for s, t, and Ï‰. These choices consistently yield robust trade-offs between accuracy, complexity, and runtime. 

## F MODEL ARCHITECTURE AND TRAINING CONFIGURATION 

F.1 MODEL ARCHITECTURE 

As illustrated in Fig. 1, GenSR consists of two input embedders (for numerical values and equations), a shared Transformer encoder, posterior and prior MLPs, a feature fusion linear layer, and a Transformer decoder. 20 Published as a conference paper at ICLR 2026 The numerical input {(x, y )i}mi=1 âˆˆ RmÃ—(D+1) is first tokenized into {(x, y)i}mi=1 âˆˆ RmÃ—3( D+1) , then mapped by the numerical embedder to Xe âˆˆ RmÃ—3( D+1) Ã—dn , where dn = 64 denotes the numerical embed-ding dimension. A reshape and linear projection yield the final X âˆˆ RmÃ—d. Similarly, equation expressions are padded to the maximum sequence length m and embedded into F âˆˆ RmÃ—d with d = 512 .For the posterior branch, X and F are concatenated and passed through an 8-layer Transformer encoder (8 heads, 512 hidden units), followed by an MLP to produce the posterior distribution N (Î¼1, Ïƒ21 ). For the prior branch, X alone (with masking applied) is fed into the same encoder to produce the prior distribution 

N (Î¼2, Ïƒ22 ). Notably, the numerical input X does not include positional encodings, ensuring that only the functional relations, rather than ordering artifacts, are modeled. The KL divergence between posterior and prior distributions serves as a regularization term: 

LKL = DKL 

 N (Î¼1, Ïƒ21 ) âˆ¥ N (Î¼2, Ïƒ22 ) .

During training, m samples are drawn from the posterior N (Î¼1, Ïƒ21 ) to form Z1 âˆˆ RmÃ—d, which is pro-cessed by the feature fusion linear layer and decoded by a symmetric 8-layer Transformer decoder (8 heads, 512 hidden units). The decoder generates logits of symbolic expressions, which are compared with the ground-truth equations through a reconstruction loss: 

LCE = âˆ’

> m

X

> i=1

log pÎ¸ (fi | zi

> 1

),

where fi denotes the i-th token of the target equation and Î¸ the decoder parameters. The overall training objective combines these two terms: 

L = LCE + Î» LKL ,

where Î» is a balancing hyperparameter. We employ KL annealing to gradually increase Î» from 0 to its full value during training. This prevents the KL term from dominating the optimization in the early stages and mitigates posterior collapse, while ensuring a well-structured latent space in later stages. During the inference phase, we use the prior distribution N (Î¼2, Ïƒ22 ) to form latent variables Z2 âˆˆ RmÃ—d,which are then fed into the Transformer decoder to generate candidate equations. Each equation is evaluated with a fitness score, which is subsequently utilized by the CMA-ES optimization procedure to iteratively refine the search and obtain the final predicted equation. F.2 TRAINING CONFIGURATION 

We adopt the Adam optimizer with a batch size of 256 and train the model for 200 epochs, each consist-ing of 1000 steps. The learning rate is set to 1.0 Ã— 10 âˆ’3 and follows the Noam schedule with w = 8000 

warm-up steps. This schedule allows the learning rate to increase linearly during warm-up and then decay proportionally to the inverse square root of the step number. For the KL divergence weight Î», we apply an an-nealing strategy: Î» is linearly increased from 0 to 1.0 during the first 50% of training steps and kept constant thereafter. This prevents the KL term from dominating optimization at the beginning and mitigates poste-rior collapse, while ensuring a structured latent space in later stages. The model comprises 162.1 million trainable parameters. All experiments are conducted using two NVIDIA H800 GPUs (Hopper architecture, 80GB memory each), with each epoch requiring approximately one hour of training time. 21 Published as a conference paper at ICLR 2026 

## G DETAILED EXPERIMENT RESULTS 

G.1 DETAILED DESCRIPTION OF EVALUATION DATASETS 

For our experimental evaluation, we adopt the SRBench benchmark (La Cava et al., 2021), a widely used and challenging collection of datasets for symbolic regression. SRBench consists of three representative groups: the Feynman dataset, the Strogatz dataset, and the Black-box dataset. These datasets differ in input dimen-sionality, sample size, functional properties, and task difficulty, jointly forming a comprehensive evaluation framework for symbolic regression methods and providing a standardized platform for comparison. 

Feynman dataset. The Feynman dataset (Udrescu et al., 2020) is derived from the Feynman Lectures on Physics and contains 119 equations spanning diverse domains such as mechanics, electromagnetism, and quantum physics. These equations are structurally diverse and grounded in real physical laws, making them a key benchmark for evaluating symbolic regression in scientific discovery. To ensure consistency across tasks, the dataset constrains the input dimensionality to D â‰¤ 10 . A unique advantage is that the true underlying functions are fully available, eliminating the ambiguity present in black-box tasks. With a cumulative size of approximately 10 5 data points, this dataset allows us to assess both large-scale regression accuracy and generalization across diverse physical laws. 

Strogatz dataset. The Strogatz dataset (Strogatz, 2018) originates from nonlinear dynamical systems, in-spired by the classical ODE-Strogatz collection. Each task corresponds to a two-dimensional dynamical system modeling problem, with input dimensionality fixed at D = 2 . Unlike Feynman, which emphasizes physical interpretability, the Strogatz dataset focuses on dynamic behaviors and is particularly suited to test-ing the ability of methods to handle nonlinearity and temporal dependencies. Each sub-dataset consists of approximately N = 400 samples, with true underlying functions provided to enable evaluation within a well-defined dynamical framework. This dataset is thus critical for benchmarking performance under small-sample and high-nonlinearity conditions. 

Black-box dataset. The Black-box dataset (Olson et al., 2017) highlights the applicability of symbolic regression to complex real-world scenarios. This group is primarily sourced from the PMLB repository and has been widely adopted in SRBench. Unlike Feynman and Strogatz, the target functions in this dataset are unknown, mimicking real-world regression tasks. These datasets often contain higher levels of noise and uncertainty, making them an effective test of robustness. To ensure comparability, input dimensionality is limited to D â‰¤ 10 . The collection comprises 57 sub-datasets, covering both real-world and synthetic cases, with dataset sizes ranging from just a few dozen to tens of thousands of samples. This wide variation tests not only the accuracy of algorithms but also their stability and generalization under noisy conditions. In summary, the Feynman dataset emphasizes physical interpretability, the Strogatz dataset captures the complexity of nonlinear dynamical systems, and the Black-box dataset stresses robustness under real-world uncertainty and noise. Together, these three datasets form a complementary and multi-faceted evaluation environment for symbolic regression. Under this benchmark, we are able to comprehensively examine trade-offs in accuracy, efficiency, and complexity, while highlighting the robustness and generality of our proposed approach across diverse tasks. G.2 EVALUATION METRICS 

To evaluate the performance of symbolic regression methods, we consider three key metrics that capture accuracy, efficiency, and interpretability: 22 Published as a conference paper at ICLR 2026 â€¢ Coefficient of determination ( R2): This metric quantifies how well the discovered symbolic ex-pression explains the variance of the target data. It is formally defined as: 

R2 = 1 âˆ’

Pni=1 (yi âˆ’ Ë†yi)2

Pni=1 (yi âˆ’ Â¯y)2 , (2) where yi denotes the ground-truth values, Ë†yi the predicted values, and Â¯y the mean of the ground-truth values. A higher R2 indicates better predictive accuracy, with R2 = 1 representing a perfect fit and R2 = 0 implying no improvement over the mean predictor. â€¢ Time complexity: This metric measures the wall-clock time required for the algorithm to complete the symbolic regression task and output a final expression, reported in seconds (s). It reflects the computational efficiency of the method, where shorter search times indicate faster convergence and reduced resource consumption. â€¢ Equation complexity: This metric evaluates the interpretability and compactness of the discovered expressions. We define complexity as the total number of tokens in the final equation: 

Complexity( f ) = 

> m

X

> j=1

I(tj ), (3) where f denotes the discovered equation, {tj }mj=1 are its tokens (including coefficients, operators, and variables), and I(Â·) is an indicator function that counts the presence of each token. Lower complexity corresponds to simpler and more human-readable expressions, which are less prone to overfitting and closer to the underlying physical or mathematical laws. G.3 BASELINES 

Evolutionary algorithm-based symbolic regression. We include several genetic programming variants: GPlearn (Stephens et al., 2015), a scikit-learnâ€“style GP framework; AFP (Schmidt & Lipson, 2010), which maintains diversity via age-fitness Pareto optimization; ITEA (de Franca & Aldeia, 2021), which exploits interactionâ€“transformation operators; EPLEX (La Cava et al., 2016), based on epsilon-lexicase selection; MRGP (Arnaldo et al., 2014), combining multiple regression with GP; SBP-GP (Virgolin et al., 2019), using semantic backpropagation with linear scaling; GP-GOMEA (Virgolin et al., 2021), a model-based GP with efficient linkage learning; Operon (Burlacu et al., 2020), a high-performance C++ SR framework; and FEAT (La Cava et al., 2019), which evolves networks of trees to learn concise regression models. 

Deep/Neural and reinforcement learning-based symbolic regression. We further consider neural and RL-driven methods: SPL (Sun et al.), which employs Monte Carlo tree search for physics discovery; DSR (Petersen et al.), a reinforcement learning method with risk-seeking policy gradients; RSRM (Xu et al., 2024), a reinforcement symbolic regression machine; TPSR (Shojaee et al., 2023), which formulates sym-bolic regression as a Transformer-based planning problem; E2ESR (Kamienny et al., 2022), an end-to-end Transformer framework; SNIP (Meidani et al.), which unifies symbolic and numeric pretraining; RAG-SR (Zhang et al., 2025), leveraging retrieval-augmented generation; MDLformer (Yu et al., 2025), guided by the minimum description length principle; and NeurSR (Biggio et al., 2021), a scalable neural symbolic regression approach. 

Physics- and probabilistic-inspired methods. We also evaluate AI Feynman (Udrescu et al., 2020), which exploits physics priors and graph modularity for symbolic discovery, and BSR (Jin et al., 2020), a Bayesian symbolic regression approach that provides uncertainty estimation. 23 Published as a conference paper at ICLR 2026 

Conventional machine learning baselines. Finally, we compare against strong non-symbolic baselines, including tree-based ensemble methods: XGBoost (Chen & Guestrin, 2016), LightGBM (Ke et al., 2017), AdaBoost (Freund & Schapire, 1997), and Random Forest (Rigatti, 2017). G.4 FUNCTION FAMILY CLASSIFICATION TASK E2ESR SNIP Ours 

> Method
> 0.5
> 0.6
> 0.7
> 0.8
> ACC
> 0.661
> 0.723
> 0.849

Figure 9: Accuracy comparison of different methods on the classification task. GenSR (Ours) achieves the highest accuracy (0.849), significantly outperforming E2ESR (0.661) and SNIP (0.723). To further examine the transferability of the learned generative latent space beyond symbolic regression, we design a function family classification task. The goal of this experiment is to assess whether the latent space representations learned by GenSR capture not only structural continuity and numerical smoothness, but also discriminative features that support downstream classification. 

Dataset construction. We synthesize a dataset covering five representative function families: exponential, logarithmic, sine, cosine, and tangent. To increase diversity and difficulty, each family is instantiated under four different input dimensionalities (2D, 3D, 4D, and 5D), leading to a total of 5 Ã— 4 = 20 classes. For each class, we randomly generate 2000 samples, where each sample consists of input points and their corre-sponding function values. The dataset is split into training, validation, and test sets with a ratio of 3:1:1 to ensure fair and stable evaluation. 

Model and training setup. We adopt the GenSR encoder pretrained on symbolic regression, which pro-duces 512-dimensional latent vectors. On top of the encoder, we attach a lightweight classification head composed of two fully connected layers: the first layer maps the latent vector to a hidden representation with ReLU activation, and the second layer outputs a 20-dimensional logits vector followed by a softmax layer to obtain class probabilities. We fine-tune the entire network using cross-entropy loss with the Adam optimizer, a learning rate of 1 Ã— 10 âˆ’3, and a batch size of 128. Training is run for at most 100 epochs, with early stopping if no improvement is observed on the validation set for five consecutive epochs. At test time, the class with the highest probability is selected as the prediction, and overall accuracy is reported. 

Comparison and results. For a fair comparison, we apply the same classification head design, data split, and training procedure to the encoders of E2ESR (Kamienny et al., 2022) and SNIP (Meidani et al.). As shown in Fig. 9, GenSR achieves significantly higher classification accuracy than both baselines. This result demonstrates that the latent space learned by GenSR is not only generative and structurally continuous, but also discriminative and generalizable, providing a strong foundation for downstream tasks beyond symbolic regression. In other words, GenSR yields representations that are simultaneously useful for generation, search, and classification, highlighting its potential for broader applications. 24 Published as a conference paper at ICLR 2026 In the future, the latent space obtained by the CVAE also can be applied to a variety of domains, such as biological contexts (Duan et al., 2025), psychological studies (Jia et al., 2025), energy systems and meteo-rological forecasting where spatiotemporal patterns play a critical role (Hu et al., 2024; 2025), and broader areas of artificial intelligence where interpretability and robustness are of particular importance (Li et al., 2024; 2023b;a; Huang et al., 2026; Feng et al., 2026; 2025). G.5 COMPLETE NUMERICAL RESULTS 

Tables 4, 5, 6, and 7 report the performance of different methods on the Strogatz (Strogatz, 2018) and Feynman datasets (Udrescu et al., 2020) under varying noise levels, while Table 3 summarizes the results on the Black-box datasets (Olson et al., 2017). Across all these benchmarks, our method demonstrates a consistently strong balance among the three key metricsâ€”accuracy ( R2), search efficiency (time), and equation complexity. Notably, even under noisy conditions, our approach maintains high R2 scores with smaller variance, achieves shorter runtime, and yields simpler equations, highlighting its robustness and overall superiority compared to existing baselines. 25 Published as a conference paper at ICLR 2026 Table 3: Complete experimental results of black-box dataset. 

Method Test R2 â†‘ Complexity â†“ Time (s) â†“

Operon 0.7945 65.69 2974 GP-GOMEA 0.7381 30.27 9636 MDL 0.6258 29.88 541.7 DSR 0.5625 9.465 36852 SBP-GP 0.7869 634 149344 FEAT 0.7621 82.49 6432 EPLEX 0.7372 53.14 15796 AFP-FE 0.6400 36.04 6184 AFP 0.6333 34.89 6033 GPlearn 0.5390 19.06 24254 Linear 0.4437 17.4 0.2447 NeurSR 0.1228 13.33 11.7 XGB 0.7496 20186 236 AdaBoost 0.6939 9481 65.12 LGBM 0.6410 5734 29.9 ITEA 0.6295 116.7 12183 SNIP 0.3335 38.91 3.286 BSR 0.2725 22.52 59822 RandomForest 0.6615 1517178 120.4 KernelRidge 0.5952 1824 39.19 FFX 0.5575 1562 244.3 E2ESR 0.3612 61.09 7.101 MRGP 0.5300 10802 165007 MLP 0.5238 3882 30.49 AIFeynman2 0.2110 2240 86854 SPL 0.5472 12.96 2047 RSRM 0.3324 9.23 1752 

Ours 0.8416 35.05 2725 

> 26

Published as a conference paper at ICLR 2026 Table 4: Complete experimental results at a noise level Ïµ = 0 .0. The results include test set R2, search time, and formula complexity of different methods in the Feynman and Strogatz datasets. 

Method Strogatz Dataset ( Ïµ = 0 .0) Feynman Dataset ( Ïµ = 0 .0)

R2â†‘ Time (s) â†“ Complx â†“ R2â†‘ Time (s) â†“ Complx â†“

FEAT 0.9210 (Â±0.054) 1135 (Â±970) 119 (Â±16) 0.9190 (Â±0.014) 2417 (Â±1737) 205.3 (Â±13) 

NeurSR 0.5206 (Â±0.029) 14.82 (Â±1.7) 11.3 (Â±0.51) 0.3958 (Â±0.013) 24.18 (Â±2.2) 13.28 (Â±0.15) 

E2ESR 0.5341 (Â±0.043) 3.729 (Â±2) 32.49 (Â±2.9) 0.8570 (Â±0.013) 4.062 (Â±1.3) 36.02 (Â±0.88) 

SNIP 0.9952 (Â±0.002) 509 (Â±21) 29 (Â±1.2) 0.985 (Â±0.004) 1203.5 (Â±145) 31.71 (Â±1.8) 

GPlearn 0.7689 (Â±0.067) 1195 (Â±592) 28.96 (Â±2.8) 0.8809 (Â±0.027) 3900 (Â±1052) 72.43 (Â±29) 

AFP 0.9248 (Â±0.041) 192.2 (Â±106) 38.17 (Â±2.8) 0.9590 (Â±0.005) 3655 (Â±1999) 36.87 (Â±2.2) 

AFP-FE 0.9442 (Â±0.045) 11041 (Â±14277) 46.16 (Â±4.1) 0.9806 (Â±0.007) 17817 (Â±530) 39.97 (Â±1.9) 

EPLEX 0.8125 (Â±0.065) 548.2 (Â±258) 50.09 (Â±2.7) 0.9869 (Â±0.006) 12771 (Â±4998) 52.95 (Â±1.3) 

SBP-GP 0.9812 (Â±0.016) 17591 (Â±1765) 712.2 (Â±39) 0.9945 (Â±0.001) 28901 (Â±37) 489.4 (Â±16) 

GP-GOMEA 0.9925 (Â±0.009) 2760 (Â±1258) 36.43 (Â±2.4) 0.9956 (Â±0.003) 5030 (Â±967) 34.57 (Â±1.5) 

Operon 0.9878 (Â±0.023) 66.58 (Â±10) 59.23 (Â±2.1) 0.9889 (Â±0.006) 2174 (Â±373) 69.88 (Â±1.8) 

SPL 0.7390 (Â±0.047) 322.1 (Â±180) 14.55 (Â±2.5) 0.7073 (Â±0.011) 209 (Â±145) 12.88 (Â±0.57) 

DSR 0.7602 (Â±0.086) 1858 (Â±2617) 15.6 (Â±1.7) 0.8441 (Â±0.091) 1733 (Â±3105) 14.86 (Â±1) 

RSRM 0.5501 (Â±0.103) 121.9 (Â±36) 13.09 (Â±2.3) 0.8003 (Â±0.013) 116.3 (Â±31) 13.17 (Â±0.47) 

AIFeynman2 0.6459 (Â±0.039) 762.1 (Â±424) 22.26 (Â±1.7) 0.9314 (Â±0.016) 854.3 (Â±24) 124.5 (Â±16) 

BSR 0.8455 (Â±0.044) 31380 (Â±23952) 38.98 (Â±5.4) 0.6609 (Â±0.018) 29065 (Â±765) 25.5 (Â±0.23) 

MDL 0.9900 (Â±0.009) 186.6 (Â±35) 14.07 (Â±1.9) 0.9171 (Â±0.005) 467.3 (Â±415) 23.4 (Â±1.2) 

TPSR 0.965 (Â±0.024) 291 (Â±53) 56.2 (Â±1.4) 0.9921 (Â±0.002) 236 (Â±39) 57.27 (Â±1.9) 

RAG-SR 0.9914 (Â±0.006) 1477 (Â±305) 46.5 (Â±3.1) 0.9926 (Â±0.002) 3156 (Â±305) 46.51 (Â±1.4) 

Ours 0.9918 (Â±0.003) 33 (Â±5.7) 20.53 (Â±0.9) 0.9872 (Â±0.003) 192.3 (Â±21) 22.94 (Â±0.5) 

27 Published as a conference paper at ICLR 2026 Table 5: Complete experimental results at a noise level Ïµ = 0 .001 . The results include test set R2, search time, and formula complexity of different methods in the Feynman and Strogatz datasets. 

Method Strogatz Dataset ( Ïµ = 0 .001 ) Feynman Dataset ( Ïµ = 0 .001 )

R2â†‘ Time (s) â†“ Complx â†“ R2â†‘ Time (s) â†“ Complx â†“

FEAT 0.9244 (Â±0.032) 594.4 (Â±181) 106.7 (Â±15) 0.9207 (Â±0.006) 1726 (Â±242) 196.5 (Â±12) 

NeurSR 0.5219 (Â±0.031) 15.07 (Â±1.7) 11.41 (Â±0.31) 0.3979 (Â±0.013) 24.51 (Â±1.7) 13.24 (Â±0.13) 

E2ESR 0.5105 (Â±0.060) 3.436 (Â±0.75) 33.83 (Â±4.4) 0.8585 (Â±0.010) 3.894 (Â±0.98) 35.85 (Â±1.5) 

SNIP 0.9667 (Â±0.024) 277.9 (Â±18.4) 27.57 (Â±1.3) 0.9904 (Â±0.002) 1365.07 (Â±104) 31.43 (Â±0.23) 

GPlearn 0.7955 (Â±0.067) 913.3 (Â±121) 29.59 (Â±2.5) 0.8902 (Â±0.008) 3316 (Â±540) 60.49 (Â±12) 

AFP 0.9172 (Â±0.052) 143.4 (Â±27) 38.75 (Â±4.6) 0.9606 (Â±0.006) 3711 (Â±457) 39.33 (Â±1.6) 

AFP-FE 0.9447 (Â±0.042) 8108 (Â±584) 48.74 (Â±3) 0.9805 (Â±0.007) 26160 (Â±157) 46.47 (Â±1.2) 

EPLEX 0.8488 (Â±0.053) 416.1 (Â±81) 49.26 (Â±4.7) 0.9866 (Â±0.007) 12341 (Â±436) 56.03 (Â±1.3) 

SBP-GP 0.9879 (Â±0.015) 19596 (Â±1233) 820.5 (Â±41) 0.9953 (Â±0.001) 28940 (Â±20) 574.4 (Â±13) 

GP-GOMEA 0.9914 (Â±0.009) 804 (Â±603) 41.14 (Â±2.9) 0.9962 (Â±0.001) 2904 (Â±146) 45.23 (Â±0.71) 

Operon 0.9843 (Â±0.036) 75.68 (Â±14) 67.03 (Â±5.5) 0.9916 (Â±0.005) 2195 (Â±404) 69.67 (Â±1.6) 

SPL 0.7526 (Â±0.049) 358.8 (Â±211) 14.4 (Â±2.5) 0.7073 (Â±0.016) 275.5 (Â±206) 13.15 (Â±0.44) 

DSR 0.8067 (Â±0.048) 500.8 (Â±317) 18.66 (Â±2.2) 0.8764 (Â±0.003) 830.1 (Â±282) 16.04 (Â±0.31) 

RSRM 0.5447 (Â±0.105) 129.7 (Â±8.1) 12.23 (Â±0.65) 0.8104 (Â±0.025) 128.2 (Â±3.8) 13.04 (Â±0.43) 

AIFeynman2 0.6855 (Â±0.091) 84.19 (Â±74) 25.64 (Â±3) 0.9177 (Â±0.008) 638 (Â±21) 130.6 (Â±17) 

BSR 0.8224 (Â±0.121) 24299 (Â±3478) 37.68 (Â±2.2) 0.6538 (Â±0.023) 30255 (Â±4770) 25.85 (Â±0.57) 

MDL 0.9965 (Â±0.004) 171.5 (Â±30) 21.38 (Â±1.4) 0.9079 (Â±0.008) 428.9 (Â±261) 32.35 (Â±1.1) 

TPSR 0.9216 (Â±0.052) 193.22 (Â±24.8) 55.64 (Â±3.1) 0.992 (Â±0.001) 190.2 (Â±7.9) 59.75 (Â±2.6) 

RAG-SR 0.9908 (Â±0.008) 591.4 (Â±38.4) 49.29 (Â±2.1) 0.9917 (Â±0.003) 3590 (Â±235) 49.42 (Â±1.1) 

Ours 0.9951 (Â±0.007) 41.56 (Â±6.1) 20.45 (Â±0.8) 0.9883 (Â±0.001) 279.31 (Â±32.6) 23.5 (Â±0.7) 

28 Published as a conference paper at ICLR 2026 Table 6: Complete experimental results at a noise level Ïµ = 0 .01 . The results include test set R2, search time, and formula complexity of different methods in the Feynman and Strogatz datasets. 

Method Strogatz Dataset ( Ïµ = 0 .01 ) Feynman Dataset ( Ïµ = 0 .01 )

R2â†‘ Time (s) â†“ Complx â†“ R2â†‘ Time (s) â†“ Complx â†“

FEAT 0.9244 (Â±0.043) 472.9 (Â±90) 95.61 (Â±16) 0.9212 (Â±0.010) 1464 (Â±365) 167.1 (Â±6.5) 

NeurSR 0.5179 (Â±0.042) 15.48 (Â±1.4) 11.63 (Â±0.34) 0.3942 (Â±0.011) 24.62 (Â±1.7) 13.26 (Â±0.12) 

E2ESR 0.5031 (Â±0.034) 3.392 (Â±0.72) 35.94 (Â±1.8) 0.8345 (Â±0.007) 4.274 (Â±0.58) 40.07 (Â±0.90) 

SNIP 0.9844 (Â±0.025) 237.61 (Â±32) 29.35 (Â±1.8) 0.987 (Â±0.007) 1210.1 (Â±154) 32.62 (Â±1.3) 

GPlearn 0.7956 (Â±0.059) 907.4 (Â±110) 30.59 (Â±4.3) 0.8890 (Â±0.009) 3351 (Â±437) 60.07 (Â±19) 

AFP 0.9153 (Â±0.053) 152.2 (Â±15) 38.62 (Â±7.1) 0.9610 (Â±0.005) 4090 (Â±758) 40.86 (Â±1.1) 

AFP-FE 0.9582 (Â±0.041) 8898 (Â±579) 48.8 (Â±5.4) 0.9819 (Â±0.008) 27763 (Â±297) 46.92 (Â±2.3) 

EPLEX 0.8562 (Â±0.040) 437.6 (Â±64) 53.07 (Â±3.8) 0.9910 (Â±0.002) 11043 (Â±718) 54 (Â±0.68) 

SBP-GP 0.9813 (Â±0.015) 20783 (Â±776) 850.9 (Â±34) 0.9950 (Â±0.001) 28954 (Â±15) 595.5 (Â±12) 

GP-GOMEA 0.9783 (Â±0.029) 765.6 (Â±1005) 42.64 (Â±4.5) 0.9967 (Â±0.001) 3020 (Â±360) 44.67 (Â±1.1) 

Operon 0.9829 (Â±0.031) 94.92 (Â±15) 81.68 (Â±1.2) 0.9878 (Â±0.010) 3165 (Â±549) 87.96 (Â±1.3) 

SPL 0.7388 (Â±0.060) 413.3 (Â±295) 14.71 (Â±1.8) 0.7133 (Â±0.007) 295.1 (Â±175) 13.43 (Â±0.58) 

DSR 0.8199 (Â±0.055) 492.9 (Â±287) 18.51 (Â±1.2) 0.8782 (Â±0.004) 929.8 (Â±422) 16.2 (Â±0.41) 

RSRM 0.5969 (Â±0.077) 142.2 (Â±21) 14.22 (Â±1.5) 0.8092 (Â±0.015) 131.6 (Â±14) 12.97 (Â±0.34) 

AIFeynman2 0.7753 (Â±0.047) 85.17 (Â±75) 32.41 (Â±4.1) 0.8732 (Â±0.021) 629.4 (Â±5.9) 155.2 (Â±8) 

BSR 0.8127 (Â±0.070) 23622 (Â±554) 38.74 (Â±2.6) 0.6734 (Â±0.018) 30411 (Â±4711) 28.03 (Â±0.49) 

MDL 0.9718 (Â±0.057) 505.1 (Â±34) 20.31 (Â±0.81) 0.9140 (Â±0.009) 844.3 (Â±561) 31.15 (Â±1.5) 

TPSR 0.9798 (Â±0.028) 175.09 (Â±19) 56.14 (Â±3.5) 0.9911 (Â±0.004) 217.06 (Â±34) 64.27 (Â±2.6) 

RAG-SR 0.9867 (Â±0.019) 509 (Â±40) 49.43 (Â±1.7) 0.9905 (Â±0.007) 3241 (Â±417) 72.41 (Â±2.2) 

Ours 0.9936 (Â±0.007) 44.58 (Â±24) 20.42 (Â±0.6) 0.9872 (Â±0.002) 486.83 (Â±48) 23.36 (Â±0.7) 

29 Published as a conference paper at ICLR 2026 Table 7: Complete experimental results at a noise level Ïµ = 0 .1. The results include test set R2, search time, and formula complexity of different methods in the Feynman and Strogatz datasets. 

Method Strogatz Dataset ( Ïµ = 0 .1) Feynman Dataset ( Ïµ = 0 .1)

R2â†‘ Time (s) â†“ Complx â†“ R2â†‘ Time (s) â†“ Complx â†“

FEAT 0.9228 (Â±0.027) 446.9 (Â±73) 84.2 (Â±15) 0.9195 (Â±0.006) 777.9 (Â±102) 99.48 (Â±5.6) 

NeurSR 0.5054 (Â±0.058) 17.47 (Â±1.6) 12.74 (Â±0.37) 0.3823 (Â±0.020) 25.81 (Â±1.7) 13.54 (Â±0.16) 

E2ESR 0.5152 (Â±0.038) 5.621 (Â±4.9) 38.49 (Â±2.7) 0.7714 (Â±0.014) 6.183 (Â±0.83) 44.09 (Â±0.61) 

SNIP 0.9187 (Â±0.042) 5650.7 (Â±35) 39.35 (Â±2.3) 0.9917 (Â±0.020) 6514.8 (Â±821) 37.71 (Â±2.4) 

GPlearn 0.8228 (Â±0.052) 894.6 (Â±108) 25.84 (Â±2.9) 0.8911 (Â±0.007) 2938 (Â±543) 48.83 (Â±9.5) 

AFP 0.9110 (Â±0.065) 161.4 (Â±28) 44.44 (Â±5.3) 0.9577 (Â±0.007) 3886 (Â±341) 40.79 (Â±1.5) 

AFP-FE 0.9496 (Â±0.037) 10082 (Â±565) 50.96 (Â±4) 0.9826 (Â±0.005) 28812 (Â±0.51) 48.87 (Â±1.4) 

EPLEX 0.8822 (Â±0.078) 405.8 (Â±43) 53.46 (Â±2.3) 0.9901 (Â±0.004) 10283 (Â±532) 45.62 (Â±1.3) 

SBP-GP 0.9323 (Â±0.050) 21886 (Â±782) 901.2 (Â±34) 0.9905 (Â±0.007) 28932 (Â±83) 621.9 (Â±6.6) 

GP-GOMEA 0.9668 (Â±0.031) 402.9 (Â±802) 43.71 (Â±2.1) 0.9957 (Â±0.003) 3186 (Â±454) 46.43 (Â±0.91) 

Operon 0.9380 (Â±0.042) 97.13 (Â±15) 83.44 (Â±1.2) 0.9847 (Â±0.008) 3090 (Â±330) 89.23 (Â±0.65) 

SPL 0.7715 (Â±0.044) 355.3 (Â±59) 13.91 (Â±1.6) 0.7109 (Â±0.013) 270 (Â±168) 13.54 (Â±0.50) 

DSR 0.8086 (Â±0.047) 500 (Â±300) 18.51 (Â±2.2) 0.8779 (Â±0.002) 814.9 (Â±197) 16.03 (Â±0.46) 

RSRM 0.5553 (Â±0.057) 138.3 (Â±6.4) 13.54 (Â±1.1) 0.8104 (Â±0.016) 133.8 (Â±6) 12.81 (Â±0.41) 

AIFeynman2 0.3170 (Â±0.082) 65.97 (Â±19) 23.53 (Â±4.6) 0.2248 (Â±nan) 710.7 (Â±nan) 176.6 (Â±nan) 

BSR 0.7190 (Â±0.076) 23292 (Â±510) 49.54 (Â±4.9) 0.6567 (Â±0.024) 32497 (Â±7914) 28.77 (Â±1.1) 

MDL 0.9686 (Â±0.057) 522 (Â±43) 20.5 (Â±0.62) 0.9097 (Â±0.009) 962.9 (Â±918) 30.86 (Â±1.4) 

TPSR 0.9707 (Â±0.022) 159.1 (Â±23) 56.28 (Â±4.1) 0.9836 (Â±0.005) 184.01 (Â±8.3) 66.96 (Â±1.9) 

RAG-SR 0.9693 (Â±0.043) 385.3 (Â±37) 46.23 (Â±2.7) 0.9849 (Â±0.006) 2957 (Â±306) 75.12 (Â±2.6) 

Ours 0.9773 (Â±0.029) 257.64 (Â±11) 20.21 (Â±1.3) 0.9886 (Â±0.005) 618.9 (Â±64) 23.62 (Â±0.8) 

30 Published as a conference paper at ICLR 2026 

## H COMPARISON WITH THE STANDARD BAYESIAN OPTIMIZATION (BO)  

> ALGORITHM ON

## FEYNMAN DATASET 

Table 8: Comparison of CMA and standard BO under different k with 512-dimensional latent space 

Method R2â†‘ Time (s) â†“ Complx â†“

k=256 CMA 0.9872 192.3 22.94 standard BO 0.9274 295.8 21.37 k=128 CMA 0.9675 179.1 23.26 standard BO 0.9161 237.5 22.55 k=64 CMA 0.9388 173.2 23.93 standard BO 0.8974 165.6 23.07 Table 8 reports the comparison between CMA-ES and standard Bayesian Optimization (BO) on GenSR with the 512-dimensional latent space. In our main experiments, GenSR adopts k = 256 , corresponding to the top-k principal latent dimensions with the largest variance. We therefore first conduct BO experiments under this same setting. As shown in the table, BO attains slightly lower expression complexity than CMA-ES, but its performance in terms of accuracy ( R2) and optimization time is substantially worse. This may be attributed to the challenges that BO potentially faces when dealing with high-dimensional optimization problems. To further investigate the effect of reducing the number of optimized dimensions, we additionally evaluate 

k = 128 and k = 64 (a setting already examined for CMA-ES in the k ablation study reported in Appendix 7). Although decreasing k improves the runtime of BO, its accuracy does not increase accordingly. At the same time, the accuracy of CMA-ES drops significantly under smaller k values. These results suggest that overly small k leads to an excessive loss of important latent dimensions, thereby limiting the expressiveness of the search space and degrading the performance of both optimization methods. Table 9: Comparison of CMA and standard BO under different k with 256-dimensional latent space 

Method R2â†‘ Time (s) â†“ Complx â†“

k=256 CMA 0.6984 181.4 18.23 standard BO 0.6241 206.6 17.12 k=128 CMA 0.4935 174.5 19.08 standard BO 0.4797 178.3 18.67 We further evaluate another pretrained model used in Appendix E.1, namely the GenSR variant with the 256-dimensional latent space, and conduct experiments with k = 256 and k = 128 . As shown in Table 9, both CMA-ES and standard BO exhibit relatively low accuracy under this lower-dimensional latent rep-resentation, which may suggest that the expressive capacity of the latent space is notably reduced in such settings. Moreover, the performance gap between BO and CMA-ES becomes less pronounced. Overall, these observations imply that BO may face certain limitations when dealing with high-dimensional optimization, whereas CMA-ES appears relatively more robust for our task. Attempts to improve BO by re-ducing the latent dimensionality or decreasing the number of optimized dimensions may still leave accuracy constrained, potentially due to insufficient latent expressiveness or the omission of important dimensions. 31 Published as a conference paper at ICLR 2026 

## I ABLATION ANALYSIS OF THE POSTERIOR SYMBOLIC ENCODER BRANCH 

Table 10: Comparison between GenSR and its disrupted variant 

Method R2â†‘ Complx â†“

GenSR 0.921 19.04 GenSR w/o Posterior 0.752 26.41 To examine the role of the dual-branch framework in shaping the latent space, we construct a variant that explicitly disrupt the posterior branch, denoted as GenSR w/o Posterior . While keeping the overall two-branch architecture intact, we pair numerical samples X with randomly sampled equations F as inputs, and replace the reconstruction loss of the posterior branch with a cross-entropy loss computed against the target expression corresponding to X. In addition, to directly evaluate the inherent performance of the model under this setting, we remove the CMA-ES optimization step and rely solely on the initial localization produced by the pretrained model. As shown in Table I, disabling the posterior branch leads to a substantial drop in performance ( R2 decreases from 0.921 to 0.752, and the complexity notably increases), indicating that the underlying latent-space struc-ture is significantly disrupted. Consequently, the initial localization becomes unreliable, demonstrating the critical role of the dual-branch framework in maintaining a well-structured latent representation and overall model performance. 

## J COMPARISON WITH THE STANDARD BAYESIAN OPTIMIZATION (BO) ALGORITHM 0 25 50 75 100 125 150 175 200 

> Epoch
> 0
> 10
> 20
> 30
> KL Term
> KL Term
> KL Weight ( )
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> KL Weight ( )
> KL Term and Weight ( ) During Training

Figure 10: The horizontal axis denotes the epoch, while the left and right vertical axes correspond to the KL term and the KL weight, respectively. The KL weight annealing strategy used here increases the weight from 0 to 1.0 during the first 50% of the training epoches and keeps it constant thereafter. 32 Published as a conference paper at ICLR 2026 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15                        

> Mean
> 50
> 100
> 150
> 200
> 250
> 300
> 350
> Frequency
> Posterior Mean Distribution 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15
> Mean
> 50
> 100
> 150
> 200
> 250
> 300
> 350
> Frequency
> Prior Mean Distribution 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
> Variance
> 100
> 200
> 300
> 400
> 500
> Frequency
> Posterior Variance Distribution 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
> Variance
> 100
> 200
> 300
> 400
> 500
> Frequency
> Prior Variance Distribution

Figure 11: Estimated distributions of prior and posterior means and variances obtained from 5,000 randomly sampled inputs in GenSR. As described in Appendix F.2, we adopt a KL weight annealing strategy during training. As shown in Fig. 10, the KL weight gradually increases from 0 to 1 in the first half of training, while the KL term eventually settles into a relatively stable range in the later stages, indicating that no collapse occurs. In addition, in GenSR, the prior distribution is learnable rather than fixed to a standard Gaussian. During training, the continual updates of the prior prevent the posterior distribution from trivially collapsing toward it, which may help reduce the likelihood of posterior collapse. Fig. 11 shows the distributions of the prior and posterior means and variances computed from 5,000 randomly generated samples using the final pretrained model. Clear discrepancies are observed in both the mean and variance distributions, indicating that the prior and posterior branches have not collapsed into a single distribution. If posterior collapse had occurred, the posterior would converge toward the prior; however, the differences shown here demonstrate that the model maintains a non-degenerate and informative posterior. 

## K COMPARING GENERATIVE AND DISCRIMINATIVE LATENT SPACES 

Contrastive learning methods (e.g., SNIP) construct a discriminative latent space that is fragmented and contains numerous â€œholes,â€ such that latent vectors sampled between two valid points often decode to empty or otherwise invalid strings. In contrast, our GenSR constructs a generative space (a continuous manifold). It learns the distribution of equations â€” meaning we can sample continuously and decode valid equations. This allows for â€smooth interpolation,â€ which is a prerequisite for the fine-grained search (CMA-ES) we perform. Table K provides a clear summary of the differences between the generative and discriminative 33 Published as a conference paper at ICLR 2026 latent spaces. Besides, the interpolation experiments (Appendix C.2) confirm that SNIP often falls into semantically invalid areas during search. Table 11: Comparison between Generative latent space and Discriminative latent space Property Generative latent space (GenSR) Discriminative latent space (contrastive)       

> Learning Objective Equation distribution Pointwise alignment Continuous latent sampling Supported Not supported Suitability for search High Low: limited with invalid regions

34