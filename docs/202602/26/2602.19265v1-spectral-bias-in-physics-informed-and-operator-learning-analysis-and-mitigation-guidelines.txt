Title: Spectral bias in physics-informed and operator learning: Analysis and mitigation guidelines

URL Source: https://arxiv.org/pdf/2602.19265v1

Published Time: Tue, 24 Feb 2026 02:19:58 GMT

Number of Pages: 68

Markdown Content:
# Spectral bias in physics-informed and operator learning: Analysis and mitigation guidelines 

Siavash Khodakarami a, Vivek Oommen b, Nazanin Ahmadi Daryakenari b,Maxim Beekenkamp, George Em Karniadakis a, ∗

> aDivision of Applied Mathematics, Brown University, Providence, RI 02912, USA
> bSchool of Engineering, Brown University, Providence, RI 02912, USA

Abstract 

Solving partial differential equations (PDEs) by neural networks as well as Kolmogorov-Arnold Networks (KANs), including physics-informed neu-ral networks (PINNs), physics-informed KANs (PIKANs), and neural opera-tors, are known to exhibit spectral bias, whereby low-frequency components of the solution are learned significantly faster than high-frequency modes. While spectral bias is often treated as an intrinsic representational limita-tion of neural architectures, its interaction with optimization dynamics and physics-based loss formulations remains poorly understood. In this work, we provide a systematic investigation of spectral bias in physics-informed and operator learning frameworks, with emphasis on the coupled roles of network architecture, activation functions, loss design, and optimization strategy. We quantify spectral bias through frequency-resolved error metrics, Barron-norm diagnostics, and higher-order statistical moments, enabling a unified analysis across elliptic, hyperbolic, and dispersive PDEs. Through diverse benchmark problems, including the Korteweg-de Vries, wave and steady-state diffusion-reaction equations, turbulent flow reconstruction, and earthquake dynamics, we demonstrate that spectral bias is not simply representational but funda-mentally dynamical. In particular, second-order optimization methods sub-stantially alter the spectral learning order, enabling earlier and more accurate recovery of high-frequency modes for all PDE types. For neural operators, we further show that spectral bias is dependent on the neural operator ar-chitecture and can also be effectively mitigated through spectral-aware loss formulations without increasing the inference cost. 

Keywords: Spectral bias, PINN, PIKAN, Neural operator, second-order optimization 1

> arXiv:2602.19265v1 [cs.LG] 22 Feb 2026

1. Introduction 

The simulation of partial differential equations (PDEs) remains the corner-stone of modern engineering and scientific discovery, driving progress in fields such as fluid dynamics, thermal transport, and structural mechanics. While traditional discretization schemes including Finite Element (FEM) and Fi-nite Difference (FDM) methods offer high reliability, they are often con-strained by high computational costs and the necessity for time-intensive mesh generation. These bottlenecks become particularly restrictive in iter-ative contexts, such as parametric optimization, uncertainty quantification, and inverse problems, where repeated high-fidelity simulations are required. Recent progress in scientific machine learning has given rise to neural PDE solvers, which employ neural networks to approximate solutions to PDEs while directly incorporating physical constraints into the training pro-cess. Among these methods, physics-informed neural networks (PINNs) [1] have attracted considerable interest for their capacity to enforce governing equations, boundary conditions, and initial conditions via automatic differ-entiation. By minimizing the residuals of the PDEs across a continuous domain, PINNs offer a mesh-free approach that seamlessly handles complex or irregular geometries, limited data, and inverse problems. Consequently, PINNs have been effectively applied to both forward and inverse problems in a variety of physical areas such as fluid mechanics, heat transfer, wave propa-gation, and coupled multiphysics systems [2–7]. In parallel, a distinct class of learners known as neural operators (e.g., Fourier neural operator (FNO) [8] and DeepONet [9]) has emerged. Unlike PINNs, which optimize for a single solution instance, neural operators aim to learn the underlying mapping be-tween infinite-dimensional function spaces and learning the solution operator of the PDE. These methods have demonstrated strong performance in surro-gate modeling, uncertainty quantification, and real-time prediction scenarios. Despite their expressive power, neural PDE solvers are fundamentally in-fluenced by the spectral bias inherent to neural network architectures and 

> ∗Corresponding author. Email: george_karniadakis@brown.edu

training dynamics. Spectral bias refers to the tendency of neural networks to learn low-frequency (smooth) components of a target function more rapidly than high-frequency or multiscale features [10]. In the context of PINNs, this bias manifests itself as an imbalance in the learning of solution modes, where smooth global structures are captured early during training while sharp gra-dients, boundary layers, and localized phenomena remain under-resolved. As a result, PINNs may satisfy PDE residuals in an averaged sense while failing to accurately represent fine-scale physics. Spectral bias also plays a central role in the performance of neural op-erators [11, 12]. Architectures such as the Fourier neural operator explicitly operate in spectral space and typically truncate or attenuate high-frequency modes to improve numerical stability and computational efficiency. Although this design enables strong generalization and rapid inference, it can limit the operator’s ability to resolve small-scale structures, sharp interfaces, and lo-calized discontinuities. Even in operator learning frameworks that do not ex-plicitly employ Fourier transforms, spectral bias arises from the smoothness-inducing nature of common neural network parameterizations and optimiza-tion schemes. The impact of spectral bias is further amplified in multiscale and multiphysics problems, where accurate solution representations require simultaneous resolution of widely separated spatial and temporal frequencies. In such settings, both PINNs and neural operators may converge to solutions that satisfy coarse-scale physics while systematically under-representing fine-scale dynamics. Recognizing the adverse impact of spectral bias on learning multiscale functions, a growing body of work has proposed spectral bias mitigation strategies across neural function approximation, physics-informed learning, and operator learning frameworks. For function approximation, early stud-ies demonstrated that standard multilayer perceptron (MLP) trained with first-order gradient-based optimization exhibits a strong preference for low-frequency components. To counteract this effect, techniques such as Fourier feature embeddings [13], multi-scale neural networks [14], and enhanced acti-vation functions [15] have been introduced to explicitly enrich the representa-tional capacity of neural networks at higher frequencies. In parallel, operator learning networks have adopted complementary approaches to mitigate spec-tral bias at the operator level. FNOs explicitly control spectral resolution through mode truncation and filtering, striking a balance between numerical 3stability and expressive power. Multi-scale DeepONet [16], high frequency scaling [11], and integration of neural operators with generative AI [17] are among the recent methods for neural operator spectral bias mitigation. Within physics-informed neural networks, spectral bias presents addi-tional challenges due to the coupling between PDE residual minimization and other loss terms (e.g., boundary conditions). Several studies have shown that standard PINN formulations tend to prioritize low-frequency residual re-duction, leading to physically consistent yet spectrally incomplete solutions for problems such as high-Reynolds-number fluid flows or wave-like prob-lems [18, 19]. To address this issue, various strategies have been proposed, including adaptive collocation point sampling based on residual magnitude [20], spatially-adaptive Fourier feature encoding [21], and separated-variable spectral neural networks [22]. Architectural enhancements such as Fourier-featured PINNs [19], exponential and sinusoidal input feature layers [23, 24], and adaptive activation functions [25] have also been explored to improve high-frequency representation while preserving physical constraints. In this work, we systematically investigate the role of neural architecture and optimization scheme in learning high-frequencies. Our aim is to move spectral bias characterization from an empirical phenomenon to a more an-alyzable one through proper quantification across different physics-informed and data-driven learning tasks. We show that spectral bias is not only rep-resentational but also dynamical, meaning that is strongly impacted by the training strategies and optimization procedure. While enhanced neural ar-chitectures can increase the representational power of the neural network, proper optimization is necessary to change the dynamics of spectral learning during the training. This is particularly important for high-dimensional neu-ral operator learning and physics-informed learning with multiple coupled loss components. Specifically, we compare the performance of a first-order optimizer (e.g., Adam), with a quasi second-order optimizer (SOAP) and a second-order optimizer (self-scaled Broyden (SS-Broyden)). The following are the specific questions we pose and answer in this paper: 

• What is the role of optimizer in spectral bias mitigation? 

• What is the role of network architecture (e.g., MLP vs KAN) in spectral bias mitigation in physics-informed networks? 4• What is the role of activation functions in PINNs and univariate func-tions in PIKANs in spectral bias mitigation? 

• Can spectral bias be mitigated in the same manner for dispersive, hy-perbolic, and elliptic equations? 

• How is spectral bias manifested by first, second, third, and fourth mo-ments? 

• For neural operators, how different neural architectures (e.g., Deep-ONet, DeepOKAN, FNO, and CNO) and different loss functions (e.g., MSE and binned spectral loss [26]), as well as optimizers are compared in solving problems with high-frequencies. The paper is organized as follows: In section 2, spectral bias of neural net-works, theoretical analysis on the role of activation function and optimization scheme, and the proposed metrics to investigate spectral bias quantitatively are described. In section 3, the representational methods including PINN, PIKAN, and neural operators are described. In section 4, we show the re-sults of effect of optimization, neural architecture, and activation functions in spectral bias of PINNs and PIKANs solving multiple PDEs, followed by the results of data-driven neural operator learning settings for turbulent jet reconstruction and earthquake dynamic prediction problems. In section 5, we summarize our results and findings. 

2. Spectral bias: Theoretical analysis and metrics 

Let Ω be a bounded domain and u : Ω → R, u(x) can be represented as follows: 

u(x) = 

> ∞

X

> k=1

uk ϕk(x), (1) where ϕk(x) are the orthonormal basis of L2(Ω) ordered by increasing spa-tial frequency. The error between the ground truth solution and the neural network solution ( uθ) can be written as follows: 

e(x) = uθ(x) − u(x) = 

> ∞

X

> k=1

(ek) ϕk(x), (2) 5where ek = uθ,k −uk. Assuming that basis are obtained through Fourier series expansion, ek will be the Fourier coefficient of the error at frequency k. Aneural PDE solver exhibits spectral bias if the training error associated with low-frequency modes converges significantly faster than the error of high-frequency modes. Note that by using Parseval’s theorem, the common L2

neural training loss can be written in form of Fourier coefficients: 

∥u − uθ∥2 

> L2(Ω)

= X

> k∈Zd

ˆu(k) − ˆuθ(k) 2 = X

> k∈Zd

|ˆek|2, (3) where ˆu(k) and ˆuθ(k) are the Fourier representation of the ground truth and neural network solutions. Since for most physical systems |ˆek| > |ˆek∗ | if 

k < k ∗ at the start of the training, then the low-frequency modes have larger energies and contribute more to the total L2 loss. Therefore, the optimizer of the neural network tends to learn low-frequency modes first, and higher modes at later stages of training if learned at all. In fact, using neural tangent kernel (NTK) theory [27], it can be shown that the learning dynamics for each mode ( k) is proportional to |ˆek| [26]. 

2.1. Effect of activation function on spectral bias 

In this work, we study the impact of activation function, neural represen-tation, and optimization scheme separately as well as their cross-correlated combined effect on the neural network spectral bias. Activation functions in neural networks have significant impact on the training dynamics and thereby on the spectral bias of the neural network. Consider a one hidden layer neural network in one dimension where each neuron contributes a ridge function. The Fourier transform of a single ridge function is shown below: 

\σ(w.x + b)( k) = 1

|w|eibk  

> |w|

ˆσ( k

|w|), (4) where w, and b are the learnable weights and biases, ˆσ is the Fourier trans-formation of the activation function ( σ). The hyperbolic tangent (Tanh) activation function has an exponentially decaying Fourier transform. 

| \T anh (k)| ∼ πsinh (π.k  

> 2

) = 2

eπ.k  

> 2

− e−π.k 

> 2

, (5) Thus, for higher frequencies as k → ∞ , \T anh ( k 

> |w|

) ∼ 2πe −π.k  

> 2|w|

. This demon-strates that high frequency components are exponentially suppressed and 6large weights |w| are required to represent oscillatory features. This explains why Tanh-based PINNs struggle with sharp gradients, high-frequency solu-tions, and multiscale PDEs. On the other hand, sine activations are used in the SIREN networks which have shown superior performance in recov-ering the high-frequencies and mitigate blurriness in computer vision tasks [28]. SIREN networks use the activation σ(x) = sin (w0x), where w0 > 0 is a fixed frequency scaling that is pre-determined separately for each layer of the network. The Fourier transform of a sine activation can be written as follows: \sin (k) = πi [δ(k − w0) − δ(k + w0)] , (6) where δ is the Dirac delta function. Note that the | \sin (k)| has no decay. Therefore, each neuron can contribute a sharply localized frequency, and varying the w and b (learnable parameters) shifts and modulates these fre-quencies without attenuation. 

2.2. Effect of optimization on spectral bias 

Additionally, we investigate the role of optimization scheme in spectral bias mitigation in physics-informed networks and neural operators for solving mul-tiple PDEs and also data-driven learning in multi-scale problems. Here, we conduct a linearized analysis for the impact of first-order and second-order optimizations in the training dynamics for different frequencies. Let f (x; θ)

be a neural network parameterized by θ ∈ Rp, trained to approximate a tar-get function f ⋆(x) on a domain Ω. Consider the commonly used L2 loss as the objective function: 

L(θ) = 12

Z

> Ω

 f (x; θ) − f ∗(x)2 dx. (7) Let θ0 be a reference parameter vector (e.g., initialization of the network or a set of parameter along training). Linearizing the network around θ0

gives the following: 

f (x; θ) ≈ f (x; θ0) + J(x; θ0) ( θ − θ0), (8) where J(x; θ0) = ∇θf (x; θ0) is the Jacobian showing the sensitivity of the network outputs to the parameters. Substituting this linearized approxima-tion into the Eq. 7, and calculating the gradient of the loss function at θ = θ0,7we get: 

∇θL =

Z

> Ω

JT (x; θ0) e(x) dx, (9) where e(x) = f (x; θ0) − f ⋆(x) is the neural network estimation error with parameters θ0. Under the first-order optimization scheme (e.g., gradient descent), the training dynamics becomes: 

˙θ = −η∇θL = −η

Z

> Ω

JT (z; θ0) e(z) dz, (10) where η is the learning rate and z is the dummy variable for integration. The training dynamics in parameter space can be transferred to function space using the chain rule ˙f (x) = J(x) ˙θ. Therefore, using Eq. 10 the induced dynamics in function space can be written as: 

˙f (x) = −η

Z

> Ω

J(x) · JT (z) e(z) dz = −η

Z

> Ω

K(x, z ) e(z) dz, (11) where K(x, z ) is the neural tangent kernel (NTK) [27]. The neural network estimation error at training time t can be expanded in the basis ϕk with coefficients ek(t):

e(z, t ) = X

> k

ek(t) ϕk(z). (12) Substituting Eq. 12 into Eq. 11, we get: 

˙f (x) = −η X

> k

ek(t)

Z

> Ω

K(x, z ) ϕk(z) dz. (13) By using the definition of eigenfunction, Eq. 13 can be written as a function of eigenvalues ( λk). 

Z

> Ω

K(x, z ) ϕk(z) dz = λk ϕk(x) (14) 

˙f (x) = −η X

> k

λk ϕk(x) ek(t). (15) Based on the definition of e(x) = f (x) − f ⋆(x), the learning dynamics at training time t in function space can be also written as: 

˙f (x) = X

> k

˙ek(t) ϕk(x). (16) 8Using Eqs. 15 and 16, we obtain the decoupled dynamics for the estimation error: 

˙ek(t) = −η λ k ek(t). (17) Thus, convergence rates (error decays) are proportional to λk. For typical activations (e.g., Tanh, ReLU), λk decays rapidly with frequency, causing spectral bias. Next, we show that such a decay with frequency does not exist for second-order optimization (e.g. SS-Broyden). In a second-order optimization scheme, the training dynamics for network parameters becomes: 

˙θ = −H−1 ∇θ(L), (18) where H = R 

> Ω

JT (z) J(z) dz is the Hessian matrix. Therefore, the induced dynamics in function space is modified from Eq. 11 by incorporating the Hessian: 

˙f (x) = −J(x)[ 

Z

> Ω

JT (z) J(z) dz ]−1

Z

> Ω

JT (z) e(z) dz (19) For simplicity, we consider the vector form of Eq. 19. 

˙f = −P · e, (20) where P = −J(JT J)−1JT is the projection matrix onto the range of the Jacobian ( J). This structure is analogous to the normal equations in linear regression, where (JT J)−1JT represents the Moore-Penrose pseudoinverse [29]. Given an over-parameterized neural network which is almost always the case in deep learning, we can assume that the target function ( ˙f ) is realizable by the network. This implies that the residual vector ( e) exists within the column space of J, meaning that the orthogonal component of the projection is zero. Consequently, 

P · e ≈ e → ˙f ≈ − e. (21) Projecting Eq. 21 into eigenbasis ϕk yields: 

˙ek(t) = −ek(t). (22) This demonstrates that the convergence rate for each frequency is indepen-dent of λk and all modes converge at comparable rates. 9It is important to investigate the coupled effect of activation function and optimization scheme on spectral bias. The activation function influences the smoothness and decay of NTK spectrum ( λk) and gradient magnitudes as-sociated with high-frequency modes. It can also affect the conditioning of the Hessian in second-order optimization methods. Under first-order opti-mization methods, these effects directly translate into frequency-dependent convergence rates and play a direct role in spectral bias. Under second-order optimization methods, the approximated curvature information can mitigate this dependence, explaining why second-order methods are less sensitive to the choice of activation function. This analysis explains why: 

• Spectral bias is prominent with first-order optimizers. 

• SIREN neural networks outperform networks with Tanh activation un-der first-order optimization for high-frequency problems. 

• The effect of activation function is less significant with quasi-second order optimizer (e.g., SOAP), and is very small with second-order op-timizer (e.g., SS-Broyden). Our analysis and experiments suggest that spectral bias is primarily an optimization-induced phenomenon rather than a representational problem, although the inductive biases such as proper choice of activation function for learning high-frequencies in the representational model can help with the mit-igation of spectral bias. Spectral bias is mainly caused from ill-conditioned training dynamics. Second-order optimization effectively preconditions these dynamics, equalizing convergence rates across all frequency models and sub-stantially helps with spectral bias mitigation. This theoretical analysis is supported by empirical results in section 4. 

2.3. Spectral bias metrics 

When studying neural networks spectral bias, it is important to consider appropriate metrics for visualizing and quantifying it. Intuitively, comparing the results in frequency domain reveal more information compared to physical space. Thus, the following unified metric can be used to quantify the error at each wavenumber of the predicted solution, its gradient, and Laplacian: 

eF,p = 1

N 2

> x

X

> k

|k|p|ˆek|2 (23) 10 where Nx is the grid size, k is the frequency in physical space, and the er-ror L2, gradient, and Laplacian errors in frequency domain correspond to 

p = 0 , p = 2 , and p = 4 , respectively. Note that the errors due to spectral bias generally happen around regions with high-frequency features and sharp gradients. Hence, the prediction errors will be magnified in the gradient and Laplacian of the solutions rather than the solution itself. Another potential metric for analysis of spectral bias is to measure the amount of oscillations within the solution and the predictions. The Barron norm measures how much a function oscillates and is calculated as the average of the norm of the frequency vector weighted by the Fourier magnitude [30]. Assume ˆf (ω) is the Fourier representation of function f : Rd → R. The Barron norm of f is defined as follows, provided that the integral is finite: 

∥f ∥B := 

Z

> Rd

∥ω∥2 | ˆf (ω)| dω, (24) where ω is the wavenumber. We employ the Barron norm to quantify the os-cillatory content of the ground-truth solution and the corresponding physics-informed network and neural operator predictions, thereby providing a mea-sure of spectral bias in the learned solutions. Since the Barron norm weights the Fourier spectrum by frequency, functions with smaller Barron norms are generally easier for neural networks to approximate. Consequently, an underestimated Barron norm in the model predictions indicates a loss of high-frequency content and increased spectral bias. Additionally, looking into the first four statistical moments can provide insight into the failure modes of the predictions and connections to the spec-tral bias. The first two moments are the mean and variance, respectively, and are dominated by low-frequency modes in most physical systems. The third moment shows the skewness in the distribution, making it sensitive to higher modes. The fourth moment (Kurtosis) measures the peakedness in the distribution and is associated with localized and sharp features. These measures for a time-dependent problem are defined in appendix A. 11 3. Methods 

3.1. PINN and PIKAN 

In the general form, consider a physical system governed by a set of PDEs defined over a spatiotemporal domain Ω × T , where x ∈ Ω ⊂ Rd denotes the spatial coordinates and t ∈ T denotes time. The governing equations are written in the general form as follows: 

N  u(x, t ) = f (x, t ), (x, t ) ∈ Ω × T , (25) 

B u(x, t ) = g(x, t ), (x, t ) ∈ ∂Ω × T , (26) 

u(x, 0) = u0(x), x ∈ Ω, (27) where N (·) denotes a differential operator defined in x ∈ Ω, and B(·) repre-sents the boundary operators defined in x ∈ ∂Ω.Physics-informed learning incorporates the governing equations, bound-ary conditions, and initial conditions (if any) directly into the training pro-cess. Let uθ(x, t ) denote a neural network parameterized by θ. The total physics-informed loss function is constructed as a weighted sum of multiple components as shown in Eq. 28: 

L(θ) = λpde LPDE + λbLBC + λic LIC + λdata Ldata , (28) where λi is the weights for ith loss component. Note that in this study no sensor data is used for training the networks and therefore Ldata is discarded. 

Lpde is the governing PDE residual, LBC is the error at boundary locations, and LIC is the error for the initial condition, each calculated on the corre-sponding collocation points as shown in the following equations. 

LPDE = 1

Npde Npde 

X

> i=1

N [uθ]( xif , t if ) − f (xif , t if ) 2 (29) 

LBC = 1

NbNbX

> i=1

B[uθ]( xib, t ib) − g(xib, t ib) 2 (30) 

LIC = 1

NiNiX

> i=1

uθ(xii, 0) − u0(xii) 2 (31) 12 In the PINN framework, the solution is represented by a feedforward neural network composed of fully connected layers with non-linear activation func-tions. The parameters include weights ( Wi) and biases ( bi) as shown in Eq. 32 for a neural network with L layers. While usually fixed, it is possible to make the activation functions ( σi) adaptive with learnable parameters vary-ing across each layer or even across neurons to accelerate the convergence in PINNs [25]. 

uθ(x)N N = σL (WL (σL−1 (. . . σ 1(W1x + b1) . . . )) + bL) (32) In the physics-informed Kolmogorov-Arnold network (PIKAN) framework, a KAN structure is used as the function approximator instead of a MLP. However, the mathematical integration of physical losses remains similar to the PINN. In KAN, univariate functions with learnable coefficients are used in the edges while only summation operation is done on nodes. Therefore, similar to MLP, a KAN can be formulated as shown in Eq. 33 [31]:          

> uθ(x)KAN =PnL−1
> iL−1=1 ϕL−1,i L,i L−1
> PnL−2
> iL−2=1 · · ·
>  Pn2
> i2=1 ϕ2,i 3,i 2
>  Pn1
> i1=1 ϕ1,i 2,i 1
>  Pn0
> i0=1 ϕ0,i 1,i 0(xi0)
>  !

(33) where L is the number of layers, nj is the number of neurons in the j th layer, and ϕk,j,i are the univariate activation functions. In this work, we explore B-Splines, radial basis functions (RBFs), and Chebyshev polynomials as the univariate functions. Building on the PIKAN formulation above, we also consider a stabilized Chebyshev-based variant, referred to as Tanh-cPIKAN, originally introduced in [32]. In this architecture, Chebyshev polynomial expansions are aug-mented with additional nonlinear contractions using Tanh activation, which maps intermediate representations to a bounded interval. The Tanh nonlin-earity is applied both prior to and after each Chebyshev expansion, resulting in a nested nonlinear–polynomial composition. For an L-layer network, the resulting approximation can be written as                

> uθ(x)Tanh-cKAN =W·σPnL−1
> iL−1=1
> PDdL=0 c(L)
> iL,i L−1,d LTdL
> 
> σ · · · Pn0
> i0=1
> PDd1=0 c(1)
> i1,i 0,d 1Td1(σ(xi0)) · · · !

(34) where Td(·) denotes the Chebyshev polynomial of degree d, D is the max-imum polynomial degree, {nℓ}Lℓ=0 are the layer widths (with n0 being the 13 input dimension), σ(·) refers to the Tanh activation function, xi0 represents the i0-th feature of the input vector x, and the trainable parameters θ consist of the final linear weight matrix W and the Chebyshev coefficients {c(ℓ) 

> iℓ,i ℓ−1,d ℓ

},where each coefficient corresponds to the d-th degree polynomial on the edge connecting node iℓ−1 of the previous layer to node iℓ of the current layer. The repeated application of Tanh enforces a contraction of intermediate activa-tions, thereby limiting the growth of high-order polynomial contributions and reducing the tendency of polynomial expansions to amplify high-frequency components. In the final layer, a linear readout is applied to the Tanh-activated hidden representation, allowing the output scale to be adjusted via 

W while preserving bounded intermediate features throughout the network. Beyond bounding intermediate activations, the Tanh nonlinearity directly modifies the optimization geometry. Let z(θ) denote the output of a Cheby-shev expansion, and define the contracted representation 

˜z = T anh (z).

By the chain rule, the Jacobian with respect to the parameters θ becomes 

∂˜z

∂θ = diag  1 − T anh 2(z)∂z

∂θ .

Thus, Tanh inserts a diagonal contraction matrix with entries in (0 , 1] into the Jacobian. For the squared residual loss L(θ) = ∥r(θ)∥2, the dominant curvature term is the Gauss–Newton matrix HGN = J⊤J, where J = ∂r/∂θ .With Tanh activations, this becomes 

HGN = J⊤ 

> 0

diag  1 − T anh 2(z)2J0,

where J0 denotes the Jacobian of the polynomial expansion without contrac-tion. Therefore, Tanh rescales curvature directions and attenuates those asso-ciated with large-magnitude activations. In Chebyshev-based networks, high-degree polynomial interactions may amplify derivatives and induce sharp curvature in the loss landscape. The repeated contraction induced by Tanh mitigates this effect by limiting activation growth and reducing curvature anisotropy, thereby improving numerical stability during physics-informed training. For first-order optimizers such as Adam, this contraction leads to more balanced gradient magnitudes across parameters, reducing oscillations caused by high-frequency components and promoting more stable conver-gence. 14 3.2. Neural operators 

Neural operators aim to directly learn mappings between spaces of functions, and is based on the universal operator approximation theorem by Chen and Chen [33]. Let U and V be Banach spaces of functions defined on Ω, and let the ground-truth operator be 

N : U 7 → V , v = N (u), (35) where u ∈ U denotes the input function and v ∈ V denotes the output func-tion. Given a training set of paired samples {(ui, v i)}Ni=1 , a neural operator Gθ

is trained to approximate N by minimizing a data misfit in function space, most commonly an L2 objective: 

θ⋆ = arg min 

> θ

1

N

> N

X

> i=1

Gθ(ui) − vi L2(Ω) . (36) We consider four representative neural operator families that differ in the way they represent the operator and propagate information across function space. Deep Operator Network (DeepONet) [9] represents Gθ through a sepa-rable branch and trunk construction, in which the input function is encoded into coefficients by the branch network and combined with a coordinate-dependent basis learned by the trunk network. A standard form is 

 Gθu(x) = 

> p

X

> j=1

bθb,j (u) tθt,j (x), (37) where bθb,j are the branch outputs and tθt,j are the trunk outputs evaluated at x. DeepOKAN follows the same operator-learning principle but replaces the MLP components with KAN style parameterizations to increase expres-sivity in function approximation [31]. The Fourier Neural Operator (FNO) learns an operator by alternating pointwise mixing with global convolutions implemented in Fourier space [8]. In a typical layer, 

vℓ+1 (x) = σ



W v ℓ(x) + F−1 Rℓ(k) F(vℓ)( k)(x)



, (38) where F denotes the Fourier transform, Rℓ is a learned spectral multiplier restricted to a finite set of modes, and σ is a nonlinearity. The Convo-lutional Neural Operator (CNO) learns the operator using multi-resolution 15 convolutional blocks that combine local receptive fields with coarse-scale con-text, but unlike a standard UNet it is formulated as an operator on function spaces with resolution-invariant layers, so the same learned mapping can be applied across discretizations without relying on explicit Fourier trunca-tion [34]. These distinct inductive biases suggest potentially different failure modes under spectral bias, especially for flows with slowly decaying spectra and sharp density-gradient features. A central question in this work is how much spectral bias persists in neu-ral operator surrogates across architectures, and whether it can be mitigated by modifying the standard L2 training objective in Eq. 36. While oper-ator architectures such as FNO explicitly use Fourier representations, the optimization is still typically driven by an L2 error that is dominated by low-frequency content, which can lead to underestimation of high-wavenumber power in the predictions. To investigate this, we evaluate each model using complementary diagnostics in physical and spectral domains, including field error, energy-spectrum error, and Barron-norm error. We also investigate an alternative to the plain L2 objective by augmenting training with a binned spectral power (BSP) loss [26]. We train operator surrogates both with the baseline objective in Eq. 36 and with BSP augmentation. Then, we com-pare how different architectures trade field accuracy for spectral fidelity in a setting where high-wavenumber content is essential. 

4. Results 

We begin by analyzing the spectral learning behavior in purely data-driven function approximation to isolate architectural effects (Section 4.1). We then examine the role of optimization in mitigating spectral bias in physics-informed networks (Section 4.2), followed by a detailed study of activation functions and representation models (Section 4.3). Finally, we assess the cou-pled effects of optimization and representation across hyperbolic and elliptic PDEs and extend the analysis to neural operator learning (Section 4.4). 

4.1. Effect of network architecture on data-driven high- and multi-frequency learning 

In this section, we investigate how different neural network architectures influence spectral learning behavior in purely data-driven settings. The pri-mary objective is to assess the ability of each architecture to capture high-16 frequency components, multi-scale spectral content, and sharp transitions when trained solely on observational data, without physics-informed con-straints. We consider several neural network architectures that span a range of rep-resentational and spectral biases. Specifically, we evaluate standard MLPs with Tanh activation functions (MLP–Tanh), MLPs equipped with sinusoidal activation functions (MLP–SIREN), Chebyshev Kolmogorov–Arnold Net-works (cKAN), and Tanh- Chebyshev Kolmogorov–Arnold Networks (Tanh-cKAN), introduced in [32]. All architectures are configured to have com-parable depth and width to isolate the effect of representation rather than parameter count. The MLP-based models rely on fixed nonlinear activation functions, while the cKAN and Tanh-cKAN architectures employ adaptive Chebyshev polynomial expansions that enable localized spectral represen-tations. The Tanh-cKAN variant further combines polynomial bases with nonlinear activation to enhance expressivity across frequency scales. Two data-driven benchmark problems are considered, each designed to probe a different manifestation of spectral bias in neural network training. 

Case 1: Discontinuous function with high-frequency content. The first bench-mark consists of a piecewise-defined target function with a jump discontinu-ity, defined as 

f (x) = 



5 + 

> 4

X

> k=1

sin( kx ), x < 0,

cos(10 x), x ≥ 0.

The presence of a discontinuity introduces broad-band high-frequency com-ponents in the Fourier domain, making this problem particularly challenging for neural networks that exhibit a low-frequency learning bias. For this case, 

80 uniformly spaced training samples are drawn from the interval [−π, π ].

Case 2: Smooth multi-frequency, multi-scale function. The second bench-mark considers a smooth target function composed of multiple sinusoidal components with well-separated frequencies and amplitudes: 

f (t) = sin(2 π · 0.01 t) + 0 .5 sin(2 π · 0.05 t) + 0 .2 sin(2 π · 0.1 t).

17 Although the function is smooth, the coexistence of multiple frequency scales makes it a canonical benchmark for evaluating a model’s ability to simul-taneously capture low- and high-frequency modes without non-smoothness-induced artifacts. This function is sampled using 300 uniformly spaced points over the temporal interval [0 , 300] .

Table 1: Data-driven spectral benchmark errors. Comparison of neural network architectures for Case 1 (discontinuous function) and Case 2 (smooth multi-frequency function). Performance is evaluated using physical- and frequency-domain error metrics. 

Architecture Rel. L2

Error Barron Norm Rel. Error log( eF,p =0 ) log( eF,p =2 ) log( eF,p =4 )

Case 1: Discontinuous Function 

MLP–Tanh 4.96 × 10 −4 2.35 × 10 −4 −3.72 1.31 6.38 

MLP–SIREN 4.68 × 10 −4 6.48 × 10 −4 −3.78 0.01 4.78 

cKAN 3.67 × 10 −2 1.62 × 10 −2 0.01 5.07 10 .17 

Tanh-cKAN 5.19 × 10 −4 3.28 × 10 −4 −3.68 −0.04 4.86 

Case 2: Smooth Multi-Frequency Function 

MLP–Tanh 5.26 × 10 −2 6.94 × 10 −2 −0.27 3.70 8.06 

MLP–SIREN 7.93 × 10 −4 1.13 × 10 −3 −3.91 −0.56 3.82 

cKAN 2.61 × 10 −1 1.52 × 10 −1 1.123 4.42 8.45 

Tanh-cKAN 2.94 × 10 −2 1.97 × 10 −1 −0.78 3.44 7.98 

18 MLP - tanh MLP -SIREN cKAN Tanh -cKAN      

> Epoch 15000 Epoch 0 Epoch 5000 Epoch 15000 Epoch 25000 Epoch 35000 Epoch 40000

Figure 1: Case 1: Training-time spectral evolution for the discontinuous benchmark. Comparison of MLP–Tanh, MLP–SIREN, cKAN, and Tanh-cKAN on the piecewise-discontinuous target function. Columns correspond to different training epochs, illustrating the evolution of the predicted signal in physical space (top row of each block) and the corresponding Fourier amplitude spectrum (bottom row of each block). The ground-truth solution is shown in black, while network predictions are shown in red. The discontinuity induces broad-band high-frequency content in the frequency domain, highlighting differences in how each architecture and optimization strategy capture sharp transitions and recover high-frequency modes over training. 

The quantitative results for both benchmarks are summarized in Table 1, while the training-time evolution in physical and frequency domains is illus-trated in Figures 1 and 2. For Case 1, which involves a discontinuous target function, MLP-based architectures with Tanh and SIREN activations achieve 19 low relative L2 errors; however, notable differences emerge in the frequency-domain metrics. In particular, the MLP–Tanh model exhibits large gradient-and Laplacian-weighted spectral errors. This indicates difficulty in accurately recovering high-frequency components induced by the discontinuity. By con-trast, MLP–SIREN and Tanh-cKAN substantially reduce higher-order spec-tral errors, consistent with the improved recovery of high-frequency modes observed in the Fourier spectra in Fig. 1. The cKAN model shows signifi-cantly larger errors across all spectral metrics. This suggests that polyno-mial bases alone, as configured here, are insufficient to capture sharp spectral transitions in the absence of additional nonlinear expressivity. For Case 2, which corresponds to a smooth but multi-frequency signal, the differences between architectures become more pronounced. The MLP–SIREN model achieves the lowest errors across both physical and spectral metrics. While the MLP–Tanh model captures the dominant low-frequency component, it suffers from substantial errors in higher-order spectral norms. This reflects in-complete learning of higher-frequency modes. The Tanh-cKAN architecture improves upon standard cKAN by reducing the physical-space error; however, its frequency-domain errors remain larger than those of MLP–SIREN, indi-cating residual spectral imbalance. These trends are clearly reflected in the frequency-domain plots in Fig. 2, where SIREN-based models exhibit faster and more uniform recovery of all frequency components during training. The combined evidence from Table 1 and Figs. 1-2 highlights that architectural choices play an important role in mitigating spectral bias in data-driven set-tings. 20 MLP - tanh MLP -SIREN cKAN Tanh -cKAN      

> Epoch 15000 Epoch 0 Epoch 5000 Epoch 15000 Epoch 25000 Epoch 35000 Epoch 40000

Figure 2: Case 2: Training-time spectral evolution for the smooth multi-frequency benchmark. Comparison of MLP–Tanh, MLP–SIREN, cKAN, and Tanh-cKAN on the multi-scale multi-frequency target function. Columns correspond to different training epochs to illustrate the evolution of the predicted signal in physical space (top row of each block) and the corresponding Fourier amplitude spectrum (bottom row of each block). The ground-truth solution is shown in black, while network predictions are shown in red. Although the target function is smooth, the presence of multiple well-separated fre-quency components makes this benchmark sensitive to spectral bias to reveal differences in how each architecture captures and balances low- and high-frequency modes during training. 

4.2. Effect of optimization on spectral bias in PINNs and PIKANs 

Recent studies have shown promising results in solving forward problems with PINNs optimized with second-order optimizers [32, 35, 36]. During the optimization of the neural network, it is important to determine the optimal 21 direction and step size for each parameter update. Commonly used gradient descent method only uses the first-order derivative information for the update direction (negative gradient direction). However, this may not be sufficient for multi-objective optimizations such as those occurring in physics-informed network training. In these cases, the network gets stuck in a local minima and an early flattened training loss history is observed. A common practice in training PINNs is to start with Adam optimizer and switch to low-memory BFGS (L-BFGS) optimizer in later iterations for faster convergence. Both of these optimizers are from a family of line search optimization methods [37], where the parameters are iteratively updated as follows: 

θk+1 = θk + αkpk, (39) where θ is the network parameters, α is the step size and pk is the direction at step k. In gradient descent method, pk = −∇ L(θ). Another example is Newton’s method with quadratic rate of convergence near the solution. In Newton’s method, both first order and second order derivatives are used in determining the optimal update direction, pk = −H −1 

> k

∇L(θk), where H is the Hessian matrix. Calculation of inverse Hessian can become computationally intractable for high-dimensional problems in deep learning and PINNs. As a result, quasi-Newton methods have emerged as alternatives to approximate the inverse Hessian. Example of such optimizers are BFGS and self-scaled Broyden (SS-Broyden) [32, 35, 38, 39]. Consider the one-dimensional Korteweg–de Vries (KdV) equation that models the propagation of waves in nonlinear dispersive media: 

ut + η u u x + μ

2 uxxx = 0 , t ∈ (0 , 1) , x ∈ (−1, 1) , (40) subject to the initial condition 

u(x, 0) = cos( πx ), (41) and periodic boundary conditions 

u(t, −1) = u(t, 1) . (42) Here, u denotes the wave amplitude (or free-surface elevation), η character-izes the strength of the nonlinear interaction, and μ determines the level of 22 dispersion. In this study, we used the classical values of η = 1, and μ = 0.022 [40]. Our results demonstrate that the choice of optimizer plays a dominant role in the performance of PINNs, particularly in mitigating spectral bias and enabling the learning of high-frequency solution components. We compare four optimization strategies of Adam, L-BFGS, SOAP, and SS-Broyden for training the PINN in solving the forward KdV equation. The first-order Adam optimizer consistently exhibits early stagnation dur-ing training, characterized by flattened loss histories and entrapment in local minima. While Adam is effective at rapidly reducing the loss during the ini-tial training stages, it struggles to further minimize the residual once higher-frequency components become dominant. This behavior results in limited accuracy and poor convergence, especially for problems with stiff or highly oscillatory dynamics. Adam weak performance can be enhanced by switch-ing to L-BFGS after adequate warm-up iterations with Adam. In contrast, the quasi-second-order SOAP optimizer substantially improves convergence behavior and final accuracy without requiring any warm-up steps. SOAP effectively handles the coupled loss components from the very beginning of training, leading to stable convergence and approximately three and two or-ders of magnitude improvement in error compared to Adam- and L-BFGS-based training, respectively. A second-order optimizer such as SS-Broyden can significantly improves the convergence and accuracy. This demonstrates that curvature-aware optimization can significantly alleviate spectral bias without relying on carefully staged optimization schedules. Note that all the SS-Broyden results in this work are based on Wolfe line search [35]. Among all tested optimizers, SS-Broyden consistently achieves the best performance. By more accurately approximating curvature dynamics, SS-Broyden attains approximately three and five of magnitude lower error com-pared to SOAP and Adam, respectively, while requiring similar or lower computational time. Although each SS-Broyden iteration incurs a higher computational cost due to the estimation of second-order information, the dramatic reduction loss at each iteration leads to superior overall efficiency and accuracy. These results highlight the critical importance of second-order optimization strategies for training PINNs in problems with high-frequencies. Figure 3 shows the training history of the PINN with different optimizers, demonstrating how the second-order and quasi-second order optimizer losses start decaying right after the beginning of the training while Adam is stuck. 23 SOAP (SIREN) 

SOAP (Tanh) 

Adam (Tanh) 

Adam (SIREN) 

Adam (SIREN) 

> L-BFGS

SSBroyden 

(Tanh) 

SSBroyden 

(SIREN) Figure 3: KdV equation: PINN training history with different optimizers (Adam, L-BFGS, SOAP, and SS-Broyden) and activation functions (Tanh, and SIREN) for KdV equation. The sub-figure shows the zoomed-in training history for the SS-Broyden opti-mizer. 

When looking into the first four statistical moments of the PINN predic-tions, it is obvious that while Adam can approximately recover the first two moments, it completely fails to recover the third and fourth moments. On the other hand, SOAP and SS-Broyden recover all the four moments accu-rately with SS-Broyden being more accurate (Fig. 4). The Barron norm which measures the amount of oscillations in the solutions also confirm the same findings (Fig. 4d). The SOAP and SS-Broyden results follow the same norm as the ground truth solutions while Adam results demonstrate smaller 24 Barron norm as the wave propagates in time. Note that L-BFGS improves the results of Adam but still shows smaller Barron norm, suggesting that the conventional optimizers systematically fail to capture high-frequencies in PINNs. Adam (Tanh)  Soap / SS -Broyden (a)  (b)         

> (c) (d) Adam L-BFGS SOAP SS -Broyden Adam L-BFGS SOAP SS -Broyden

Figure 4: KdV equation: Analysis of the predictions. (a, b) First four moments (mean, variance, skewness, and kurtosis) of the PINN predictions with (a) Adam optimizer and Tanh activation, and (b) Soap or SS-Broyden optimizer with Tanh activations. (c) Time-averaged absolute error at each moment for PINN predictions with different optimiz-ers and activations. (d) Barron norm at each time-step of PINN predictions with different optimizers. 

4.3. Effect of activation functions and representation models on spectral bias 

We investigate how the activation functions in PINNs and univariate func-tions in PIKANs performs differently in solving PDEs with high-frequencies. It is important to investigate the effect of the neural architecture in con-junction with the optimizer. In PINN, we investigate how changing the activation function from Tanh to oscillatory and periodic Sine function can help with the capture of high-frequencies in the solution. Particularly, we used sin (ωi · W X + b) as the activation function, where W and b are weight 25 and bias in the network, and ωi is the scaling factor for the ith layer of the network. Similar to the original SIREN neural network [28], we found out that using ω0 = 30 and ωi = 1 for the rest of the layers, provide the best performance while keeping the training stable.  fGT (x, t)                

> Soap,
> SIREN
> Soap,
> Tanh
> ||
> fGT (x, t)||
> L-BFGS,
> SIREN
> Adam,
> SIREN
> fGT (x, t )
> GT
> fGT (x, t )-f(x, t)
> fGT (x, t) -
> f(x, t)
> SS -Broyden ,
> SIREN
> ||
> fGT (x, t)|| -||
> f(x, t)||
> SS -Broyden ,
> Tanh
> L-BFGS,
> Tanh
> Adam,
> Tanh

Figure 5: KdV equation: Effect of optimizer and activation function. First row: Ground truth (GT) solution, gradient of the GT solution, and Laplacian of the GT solution. The other rows in first, second and third columns show the errors in PINN solution, errors in gradient magnitude of the solution, and errors in Laplacian of the solution, respectively. The rows are organized from the most to the least accurate results (top to bottom). The axis ranges ( x ∈ [−1, 1] , t ∈ [0 , 1] ) shown in top left subplot is applicable to all. 

26 Fig. 5 demonstrates the KdV equation predictions of PINNs with different activation functions trained with different optimizers. It can be seen that both activation function and the optimizer are effective in spectral bias mit-igation, with optimizer playing a more dominant role. Each optimizer and each activation function demonstrates a different dynamics during the train-ing. For example, when using Adam optimizer, SIREN layers can help getting out of the local minima, breaking the flattened loss history curve (Fig. 3). However, for quasi-second-order and second-order optimizers, SIREN pro-vides more representational power with negligible additional computational cost that can be useful for capturing the high-frequencies. Also, note that the impact of activation function reduces with the use of second-order opti-mizers. Therefore, the impact is rather small for SS-Broyden compared to SOAP and Adam (Fig. 3). For a more comprehensive analysis, we examined the learning dynamics and related them to spectral bias by tracking the prediction of the statisti-cal moments during training. The results in Figure 6 demonstrate that the slope of learning of the first two moments using Tanh with Adam is very small and only begins after approximately 10,000 iterations, and there is almost no learning for the third and fourth moments. When using SIREN with Adam, the slope of learning increases from a negligible value to a small but noticeable one. Using the SOAP optimizer, learning begins from the first iteration. Interestingly, SOAP with Tanh initially has faster error drop, how-ever, SOAP with SIREN surpasses it in the middle of training showing larger learning slope in later iterations. This is potentially due to the higher high-frequency representational power of SIREN compared to Tanh accompanied with harder optimization with SIREN in the early stages which explains the faster drop with Tanh. Notably, the intersection point between the Tanh and SIREN with SOAP curves shifts to earlier iterations from the first to fourth moments. For example, the curves intersect around iteration 30,000 for the first moment and around 11,000 for the fourth moment. This results indi-cate that SIREN is more effective in capturing high-frequency features and is directly effective for spectral bias mitigation. When using SS-Broyden, the effect of the activation function on learning dynamics is reduced, although SIREN still provides greater representational capacity for high frequencies. As a result, the first four moments are learned at similar rates with Tanh or SIREN networks trained with SS-Broyden. However, at the final steps of the training, the Tanh network plateaus, while the SIREN one keeps de-27 creasing for roughly another order of magnitude. Therefore, while activation function can significantly impact the high-frequency learning dynamics when optimized with Adam and SOAP, its impact is reduced when optimized with SS-Broyden. In this case, both networks follow similar learning trajectory, with SIREN providing additional improvement in the final training stage. Interestingly, a similar pattern of a slightly faster initial error decrease (early in the training) with Tanh is also observed with SS-Broyden, as seen with SOAP. However, this difference is much smaller with SS-Broyden, indicating weaker sensitivity to the choice of activation function. (a)  (b)  

> (d) (c)
> (e) Optimization gain
> Representation gain

Figure 6: KdV equation: Spectral bias dynamics during the training. (a-d) Errors of first four statistical moments during the training with different optimizers and activation functions. (e) Error in the frequency domain during the training. The small red arrows show the approximate slope of learning the high-frequencies (shown in third and fourth moments). The dotted black lines in the four moments shows the iteration at which the Tanh and SIREN networks trained with SOAP intersect. Note how the intersection happens earlier at higher moments. The legends in (a) are applicable to all. 

The problem with Tanh PINN trained with Adam can be mitigated by adopt-ing adaptive Tanh activations. While adaptive Tanh helps prevent premature convergence to poor local minima early in the training, the proper initializa-tion of the adaptive parameters remains crucial. Additionally, we observed 28 that employing slope recovery with layer-wise adaptive activations can sub-stantially accelerate convergence. Introduced in a previous work by Jagtap et al. [41], slope recovery is an additional loss term and can be defined by Eq. 43 for layerwise adaptive activations: 

L(α) = L − 1

PLi=1 exp( αi), (43) where L is the number of layers and α is the adaptive parameter of the ac-tivation function. Initializing the adaptive parameters to unity, effectively mirroring a standard Tanh activation, while employing a negligible weight for the slope recovery loss term ( wsr ) often leads to early entrapment in the same local minima as fixed Tanh architectures. However, strategically increasing wsr significantly accelerates convergence by encouraging the acti-vation functions to adapt their slopes earlier in the training (Fig. B.1). It is critical to note that an excessively high weight (typically wsr ≳ 1) causes this term to dominate the objective function, resulting in numerical insta-bility during training. While adaptive Tanh activations facilitate gradient flow and assist the Adam optimizer in bypassing local minima, the quasi-second order or second order optimizers demonstrate inherent robustness to these challenges. Consequently, we observed no significant performance gains when integrating adaptive Tanh or Sine activations into PINNs trained via the SOAP optimizer or SS-Broyden. Previous studies have shown that KANs may suffer less from spectral bias and handle high-frequencies better compared to MLPs [42]. Here we ex-plore how KANs constrained by physical laws (PIKANs) compare to PINNs in terms of spectral bias. Also, how changing the univariate function in the KAN architecture can improve the results. We studied three different variants of KANs by changing the univariate functions, including B-Splines, radial basis functions (RBFs), and Chebyshev polynomials with degrees of three, five, or seven. Except for the B-spline PIKAN where the model stuck in a local minima and did not show proper convergence behavior. The rest of the PIKANs showed comparable results to the PINN with SIREN. Among the PIKANs, Chebyshev PIKAN (C-PIKAN) achieved the best result, while having the highest computational time. Note that in general, PIKANs took two to four times longer to converge compared to PINNs for solving the KdV equation. By increasing the polynomial degree in C-PIKAN, the results are 29 only slightly improved while the computational time increases significantly for the same number of parameters. Therefore, C-PIKAN with degree three or five seems to be the most viable option within the PIKANs. Nevertheless, for the same optimizer, PINN with SIREN provides more accurate results with lower computational cost compared to the best PIKAN for KdV equa-tion. Summary of the quantitative results including relative errors, Barron norm errors, errors in frequency domain, and convergence time are shown in Table 2. 

Table 2: KdV equation prediction errors. Comparison of the performance of PINNs with different activation functions and PIKANs with different univariate functions trained with different optimizers. All models except those trained with SS-Broyden have ∼ 40,000 parameters. The models trained with SS-Broyden have ∼ 12,900 parameters. 

Rel. L2

Error Barron Norm Rel. L2 Error Log (eF,p =0 ) Log (eF,p =2 ) Log (eF,p =4 ) Convergence Time [hr.] Time to 1% Error 

PINNs 

Adam (Tanh) 2.47 × 10 −1 1.58 × 10 −1 -1.52 0.77 5.34 0.056 NA Adam+LBFGS (Tanh) 8.12 × 10 −2 4.14 × 10 −2 -2.48 -0.15 4.35 0.056+0.21 NA Adam (SIREN) 6.35 × 10 −2 1.06 × 10 −2 -2.69 -0.19 4.37 0.60 NA Adam+LBFGS (SIREN) 1.76 × 10 −3 1.28 × 10 −3 -5.48 -2.94 1.37 0.60+0.045 0.29 SOAP (Tanh) 7.05 × 10 −4 5.62 × 10 −4 -6.59 -3.91 0.39 0.40 0.033 SOAP (SIREN) 1.06 × 10 −4 5.92 × 10 −5 -8.25 -5.58 -1.32 0.43 0.11 SS-Broyden (Tanh) 8.69 × 10 −7 6.47 × 10 −7 -12.42 -9.37 -5.53 0.70 0.022 SS-Broyden (SIREN) 7.54 × 10 −8 5.51 × 10 −8 -14.41 -11.37 -7.66 1.57 0.021 PIKANs 

SOAP (B-Spline) 9.20 × 10 −1 2.46 × 10 −1 -0.37 1.85 6.38 NA NA SOAP (RBF) 5.99 × 10 −4 4.19 × 10 −4 -6.74 -4.17 0.066 0.94 0.14 SOAP (C7) 2.62 × 10 −4 2.45 × 10 −4 -7.46 -4.78 -0.44 1.67 0.58 SOAP (C5) 2.44 × 10 −4 2.46 × 10 −4 -7.52 -4.84 -0.63 1.71 0.4 SOAP (C3) 2.80 × 10 −4 2.55 × 10 −4 -7.40 -4.73 -0.47 1.35 0.21 SS-Broyden (C7) 2.20 × 10 −7 2.05 × 10 −7 -13.59 -10.48 -6.67 1.53 0.026 SS-Broyden (C5) 1.48 × 10 −7 1.38 × 10 −7 -13.92 -10.81 -7.00 1.07 0.025 SS-Broyden (C3) 2.280 × 10 −7 2.44 × 10 −7 -13.43 -10.33 -6.53 0.71 0.024 

4.4. Other experiments: Coupled effect of optimizer and activation function in hyperbolic and elliptic PDEs 

To further investigate the role of optimizers and activation functions, we examine how spectral bias manifests itself in hyperbolic equations such as wave equation and elliptic equations, such as steady-state diffusion-reaction equation. We designed a systematic study with the hyperbolic wave equation to ana-lyze the effect of different activation functions and optimizers in spectral bias 30 of PINNs by gradually adding more frequencies to the solution and making the problem harder. Unlike the KdV equation, which exhibits either disper-sive behavior, the wave equation introduces explicit propagation dynamics. The problem of interest involves a resting system with a time-dependent boundary condition where we systematically inject sinusoidal functions with multiple frequencies. The 1D wave equation is expressed as: 

 

> ∂2u∂t 2

− c2 ∂2u∂x 2 = 0 x ∈ (0 , 1) , t ∈ (0 , T ]

u(x, 0) = 0 , ∂u ∂t (x, 0) = 0 

u(0 , t ) = f (t), u(1 , t ) = 0 ,

(44) where c = 1 , T = 1 , and f (t) is the time-dependent boundary condition. To systematically control the spectral complexity of the solution, we define f (t)

as a superposition of sinusoidal modes: 

f (t) = r(t; τ ) exp 



−χ (t − μ)2

2σ2

 NX

> i=1

Ai sin(2 πf it) , (45) where r(t; τ ) is the smooth ramp function ensuring compatibility with the initial conditions, χ activates the Gaussian envelope and removes it when 

χ = 0 , Ai are the amplitudes, and fi are the frequencies. We consider the following four equations: (i) A1 = 1 , f 1 = 1 , χ = 0 .

(ii) A1 = 1 , f 1 = 10 , χ = 0 .

(iii) {Ai}4 

> i=1

= {0.25 , 0.25 , 0.25 , 0.25 }, {fi}4 

> i=1

= {1, 5, 10 , 20 }, χ = 0 .

(iv) {Ai}5 

> i=1

= {0.25 , 0.1, 0.25 , 0.5, 0.4}, {fi}5 

> i=1

= {1, 5, 10 , 20 , 40 }, χ =1.

In all cases, r(t; τ = 0 .05) is defined as follows: 

r(t; τ ) = 



0, t ≤ 0,s3 (10 − 15 s + 6 s2) , 0 < t < τ, s = tτ ,

1, t ≥ τ. 

(46) 31 Note that from case (i) to (iv) the frequencies injected at the boundary are increased, leading to monotonic increase in problem difficulty. Figure 7 shows how prediction errors increase with spectral complexity. The results demonstrate that the networks struggle more in the high-frequency regime (e.g., case (iv)). For low-frequency excitations (case (i)), first-order optimiza-tion with Adam is sufficient to achieve accurate solutions regardless of the activation function. As higher frequencies are introduced, SIREN provides some improvements when used with Adam. However, this benefit diminishes for more complex cases, where Adam ultimately fails to converge. In con-trast, both SOAP and SS-Broyden exhibit significantly improved robustness as spectral complexity increases. Notably, SS-Broyden achieves two to four orders of magnitude lower relative L2 errors compared to SOAP. Addition-ally, higher-order statistical diagnostics reveal clear differences. SS-Broyden achieves much lower errors in the third and fourth moments as well as in frequency errors, indicating superior recovery of high-frequency components (Fig. 7). The quantitative results for the case with largest frequencies (the most complex problem) is shown in Table 3. The results of other cases are shown in the appendix C.1. 32 Solid line: Tanh  Dashed line: SIREN (a)  Solid line: Tanh  Dashed line: SIREN (b)   

> SS -Broyden (SIREN) (d) SOAP (SIREN) (c)
> GT
> PINN
> GT
> PINN

Figure 7: Wave equation results with different optimizers and activation func-tions (a-b). Relative L2 error and fourth moment error of each of the wave problems trained with different optimizers and activation functions. (c) Skewness and Kurtosis of PINN prediction with SOAP optimizer and SIREN activation. (d) Skewness and Kurtosis of PINN prediction with SS-Broyden optimizer and SIREN activation. 

Note that the choice of activation function has a strong impact when using the Adam optimizer; however, this dependence diminishes when using SOAP and becomes nearly negligible for SS-Broyden, which exhibits minimal sen-sitivity to the activation function. Additionally, the appropriate choice of activation function depends on the frequency content of the problem. For example, SIREN is better suited for case (iv) of the wave equation, where high-frequency components are prominent. In contrast, Tanh may be more suitable for problems dominated by lower-frequency content, unless SIREN is adjusted with a smaller frequency scaling parameter (see appendix C.1). This can be interpreted as an inductive bias of the network: for problems with strong high-frequency content, an activation function with inherently oscillatory, high-frequency behavior is advantageous, whereas for smoother, low-frequency problems, a monotonic activation such as Tanh may be more appropriate. Similarly, this choice becomes less significant with SS-Broyden. 33 Table 3: Wave equation case (iv) prediction errors. Comparison of PINN and PIKAN performance for the high-frequency wave problem. Results for simpler cases (i)–(iii) are reported in the appendix. All models except those trained with SS-Broyden have ∼ 40,000 parameters. The models trained with SS-Broyden have ∼ 12,900 parame-ters. 

Method Rel. L2

Error Barron Norm Rel. L2 Error log( eF,p =0 ) log( eF,p =2 ) log( eF,p =4 )

PINN 

Adam (Tanh) 1.00 0.99 -1.65 2.23 6.80 Adam (SIREN) 1.00 0.95 -1.65 2.23 6.80 SOAP (Tanh) 4.03 × 10 −2 2.12 × 10 −2 -4.44 -0.69 3.92 SOAP (SIREN) 4.34 × 10 −3 2.54 × 10 −3 -6.37 -2.27 2.28 SS-Broyden (Tanh) 3.05 × 10 −5 2.37 × 10 −4 -10.67 -5.22 0.56 SS-Broyden (SIREN) 1.16 × 10 −5 4.37 × 10 −5 -11.52 -6.12 -0.37 PIKAN 

SOAP (C3) 3.36 × 10 −2 2.07 × 10 −2 -4.59 -1.16 3.93 SOAP (C5) 3.22 × 10 −2 2.23 × 10 −2 -4.63 -0.75 4.03 SOAP (C7) 3.12 × 10 −2 1.83 × 10 −2 -4.66 -0.98 3.87 SS-Broyden (C3) 3.80 × 10 −5 1.84 × 10 −4 -10.46 –5.05 0.73 SS-Broyden (C5) 1.18 × 10 −5 3.88 × 10 −5 -11.50 -6.18 -0.41 

SS-Broyden (C7) 1.75 × 10 −5 7.42 × 10 −5 -11.16 -5.71 0.065 

For an elliptic PDE, we consider a nonlinear steady reaction–diffusion equation posed on a two-dimensional periodic domain, 

∆u(x, y ) − kru(x, y )2 = f (x, y ), (x, y ) ∈ Ω := [ −1, 1] 2, (47) where ∆ denotes the Laplacian operator and kr > 0 is a reaction coefficient controlling the strength of the nonlinear sink term. In all experiments, we fix 

kr = 0 .1. This problem serves as a representative nonlinear elliptic PDE with smooth but oscillatory solutions, commonly encountered in chemical kinetics and reaction–diffusion systems. To enable quantitative error assessment, the forcing term f (x, y ) is con-structed via the method of manufactured solutions. Specifically, we prescribe the exact solution 

u∗(x, y ) = sin(3 πx ) cos(3 πy ), (48) which exhibits moderate spatial oscillations in both coordinate directions. Substituting u∗ into the governing equation yields the forcing 

f (x, y ) = ∆ u∗(x, y ) − kr

 u∗(x, y )2 = −18 π2u∗(x, y ) − kr

 u∗(x, y )2. (49) 34 We impose periodic boundary conditions in both spatial dimensions, 

u(−1, y ) = u(1 , y ), u(x, −1) = u(x, 1) , (50) along with periodicity of all derivatives. Rather than enforcing these con-straints explicitly through boundary residuals, periodicity is embedded di-rectly into the neural representation using a trigonometric feature map, en-suring the learned solution is periodic by construction. Table 4 reports the prediction errors for the reaction–diffusion problem across PINN and PIKAN architectures under different optimizers and activa-tion functions. We matched the number of trainable parameters between the MLP and Chebyshev-based PIKAN with degree 5, resulting in approximately 

14 .6 × 10 3 parameters for this problem. For degrees 3 and 7, the parameter counts differ due to the polynomial basis expansion. Among PINN archi-tectures, SS-Broyden with Tanh activation achieves the best overall perfor-mance, yielding the lowest relative L2 error and consistently strong spectral metrics while maintaining short convergence time. In contrast, Adam ex-hibits significantly larger errors, particularly in higher-order spectral norms, reflecting the limitations of purely first-order gradient updates in resolving os-cillatory residual structure. SOAP improves over Adam in the Tanh setting, consistent with its geometry-aware preconditioning that enhances gradient scaling and partially accounts for curvature effects. However, this advan-tage does not uniformly extend to SIREN activations. This behavior likely arises because SIREN’s sinusoidal activation already embeds strong oscilla-tory structure and provides smoother gradient propagation across frequency modes. In such cases, additional preconditioning may offer limited benefit and can even damp useful high-frequency gradient components. Within the PIKAN family, SS-Broyden achieves the strongest performance overall, with the best results observed for the Tanh-cPIKAN representation at higher poly-nomial degree. When parameter counts are matched, the Chebyshev-based PIKAN achieves comparable accuracy to the PINN with Tanh activation while requiring shorter time. Increasing the polynomial degree to 5 or 7 does not uniformly improve performance; although higher-degree polynomials in-crease expressivity, they also introduce greater curvature complexity and am-plify higher-order spectral components within the loss landscape. Because the network depth is kept fixed across polynomial degrees, this additional cur-vature is not automatically balanced, leading to sensitivity in optimization dynamics. These observations suggest that performance is not determined 35 solely by representational capacity, but rather by the interaction between polynomial degree, optimizer behavior, and the spectral structure induced by the Fourier feature embedding at the network input. In this context, the influence of the proposed Tanh-cKAN (and its physics-informed counterpart, Tanh-cPIKAN) is governed by the interplay between these elements. 

Table 4: Steady state diffusion-reaction equation prediction errors. Comparison of PINN and PIKAN performance for the diffusion-reaction problem.                                                                                                                                                                                       

> Method Rel. L2
> Error Barron Norm Rel. L2Error log( eF,p =0 ) log( eF,p =2 ) log( eF,p =4 )#params KConvergence Time (hr)
> PINN
> Adam (Tanh) 1.41 ×10 −11.21 ×10 −41.69 -2.39 1.99 14.6 0.18 Adam (SIREN) 3.14 ×10 −32.66 ×10 −3-1.60 0.23 3.44 14.6 0.11 SOAP (Tanh) 6.31 ×10 −23.17 ×10 −50.99 -3.26 -0.56 14.6 0.38 SOAP (SIREN) 1.34 ×10 −23.93 ×10 −6-0.35 -3.99 -1.39 14.6 0.29 SS-Broyden (Tanh) 1.27 ×10 −67.08 ×10 −10 -8.39 -12.63 -10.11 14.6 0.06 SS-Broyden (SIREN) 6.05 ×10 −63.48 ×10 −9-7.04 -11.28 -8.74 14.6 0.05 PIKAN
> Adam (C3) 1.33 ×10 −19.73 ×10 −51.65 -1.73 1.82 9.7 0.07 Adam (C5) 1.81 ×10 −13.40 ×10 −51.91 -1.82 1.43 14.6 0.11 Adam (C7) 1.51 ×10 −14.21 ×10 −51.75 -2.07 1.39 19.5 0.15 SOAP (C3) 1.00 ×10 −12.93 ×10 −51.40 -2.68 0.54 9.7 0.09 SOAP (C5) 9.29 ×10 −28.11 ×10 −61.33 -2.69 0.91 14.6 0.14 SOAP (C7) 9.10 ×10 −22.24 ×10 −61.32 -2.48 0.71 19.5 0.18 SS-Broyden (C3) 1.09 ×10 −36.15 ×10 −7-2.52 -6.77 -4.26 9.7 0.04
> SS-Broyden (C5) 1.15 ×10 −46.49 ×10 −8-4.48 -8.72 -6.20 14.6 0.06 SS-Broyden (C7) 9.41 ×10 −45.59 ×10 −7-2.61 -6.85 -4.34 19.5 0.10 SS-Broyden (Tanh- C7) 1.94 ×10 −51.05 ×10 −8-6.03 -10.28 -7.71 19.5 0.11

Since the exact solution is periodic and dominated by a small number of Fourier modes, the Fourier feature layer spans the true spectral support of the solution, substantially mitigating spectral bias at the representation level [13]. Consequently, Tanh-cKAN does not increase expressivity; Chebyshev polynomials act on already oscillatory coordinates, and the additional Tanh merely rescales and saturates intermediate representations. Its role is there-fore limited to (i) regularizing optimization geometry through improved Hes-sian conditioning and reduced curvature anisotropy, and (ii) controlling high-order mode amplification, including suppression of spurious Chebyshev com-ponents and residual-induced harmonics from the nonlinear reaction term. These mechanisms matter only when the optimizer–basis interaction becomes ill-conditioned. For first-order methods such as Adam, which do not exploit curvature information, improved conditioning provides no direct advantage. In contrast to data-driven settings without Fourier features, where spectral bias dominates and Tanh-cPIKAN can significantly aid Adam, the present 36 PDE setup is geometry-limited rather than spectrum-limited, and no mea-surable improvement is observed under Adam. For SS-Broyden combined with high-degree Chebyshev expansions (C7), the polynomial basis induces strong curvature anisotropy that destabilizes quasi-Newton updates. Here, Tanh-cKAN compresses the Hessian spectrum and stabilizes inverse-Hessian approximations, yielding substantial accuracy gains. When curvature is al-ready well-conditioned (C3–C5) or explicitly moderated by SOAP, the Tanh modification neither improves nor degrades performance. Accordingly, Tanh-cKAN results for those configurations are excluded from Table 4 . Overall, Tanh-cKAN and Tanh-cPIKAN act as targeted mechanisms for mitigating curvature-induced optimization pathologies rather than as universal expres-sivity enhancements. Figure 8 complements Table 4 by visualizing the field error, gradient er-ror, and Laplacian error for each optimizer–activation configuration. While Table 4 quantifies global error norms, the figure reveals how these errors distribute spatially and across derivative orders. For Adam with Tanh ac-tivation, the field error exhibits smooth low-frequency distortions, and the Laplacian shows noticeable high-frequency artifacts, indicating incomplete recovery of higher modes. Adam with SIREN reduces the field error mag-nitude but introduces structured oscillations in the gradient and Laplacian, reflecting enhanced high-frequency excitation without stable derivative con-sistency. SOAP produces more spatially coherent patterns than Adam due to its geometry-aware preconditioning. With Tanh, this mainly reduces dif-fuseness without altering spectral character, whereas SOAP–SIREN yields organized periodic residuals aligned with dominant Fourier modes, though derivative-level oscillations persist. In contrast, SS-Broyden yields nearly uniform and significantly smaller errors across field, gradient, and Laplacian levels. The residual patterns appear homogeneous and close to numerical noise. It confirms that curvature-aware updates resolve both low- and high-frequency components simultaneously. 37 SS-Broyden, Tanh 

> SS-Broyden, SIREN
> SS-Broyden (Tanh-C7)
> Adam, SIREN
> SOAP, SIREN
> SOAP, Tanh
> Adam, Tanh
> Adam, Tanh SOAP, SIREN
> Adam, SIREN
> SOAP, Tanh SSbroyden, Tanh n,
> Adam, Tanh SOAP, SIREN
> Adam, SIREN
> SOAP, Tanh SSbroyden, Tanh n,
> Adam, Tanh SOAP, SIREN
> Adam, SIREN
> SOAP, Tanh SSbroyden, Tanh broyden, SIREN
> Adam, Tanh SOAP, SIREN
> Adam, SIREN
> SOAP, Tanh SSbroyden, Tanh broyden, SIREN

Figure 8: Steady-state diffusion–reaction equation: Effect of optimizer and activation function. Comparison of ground-truth field, gradient magnitude, and Lapla-cian (top row) with corresponding prediction errors (bottom row) for representative PINN and PIKAN configurations. The rows are organized from the most to the least accurate results (top to bottom). The error maps illustrate qualitative differences in spatial and derivative accuracy that are not fully captured by scalar norms alone. These visualizations complement the quantitative metrics reported in Table 4. 

38 4.5. Spectral bias in neural operators 

Here, we consider two different applications. First, we consider a sonic jet at low resolution and we aim to reconstruct it at high resolution. Next, we consider earthquake dynamics and its effect on a six-story reinforced concrete frame building subjected to ground acceleration. 

4.5.1. High-speed Schlieren imaging 

High-speed Schlieren imaging is a standard tool for visualizing compress-ible flows by converting line-of-sight refractive-index gradients (and hence density-gradient features) into intensity variations [43, 44]. Here, we consider an impinging-jet experiment in which an under-expanded air jet (nozzle-exit Mach number Me ≈ 1) is directed onto a flat plate at ambient conditions us-ing a conventional Z-type Schlieren setup. The resulting flow exhibits coher-ent shock structures in the near field together with shear-layer instabilities and a turbulent wall-jet after impingement, producing abundant fine-scale features in the Schlieren field. These multiscale structures carry significant energy into high spatial wavenumbers, leading to a relatively slow-decaying spatial energy spectrum and making the dataset a stringent testbed for study-ing spectral bias in learned surrogates. The dataset consists of 1000 Schlieren snapshots at a spatial resolution of 

[128 , 256] over the domain [−0.8, 5.6] × [−1.45 , 1.75] , with successive frames separated by τ = 4 .76 μs . We use the first 800 snapshots for training, the next 100 for validation, and the final 100 for testing. Let uHR (x, t ) denote the high-resolution Schlieren field. In this work we focus on spatial super-resolution only: the low-resolution observation uLR is obtained by spatially subsampling uHR by a factor of 8, and the surrogate is trained to learn the mapping uLR 7 → uHR . This isolates the reconstruction challenge to recover-ing fine spatial detail - including high-wavenumber content associated with shock-turbulence interactions - from heavily downsampled Schlieren measure-ments [45, 46]. Results are summarized in Figure 9 and Table 5. Across architectures, standard L2 error minimization training recovers the dominant shock topol-ogy and large-scale flow organization, but exhibits clear spectral bias: high-wavenumber content is systematically attenuated, yielding spectra E(k) that deviate rapidly relative to the ground truth. This loss of small-scale power 39 is consistent with the qualitative diagnostics in Figure 9, where baseline pre-dictions appear overly smooth and the corresponding gradient and Laplacian fields under-represent sharp density-gradient features that are prominent in the Schlieren images. In contrast, incorporating the minimization of binned spectral power (BSP) loss[26] during training substantially improves fidelity at fine scales. We implemented a log-transformed variant of the BSP loss as described below. 

L =

> N

X

> i=1

∥vi − ˆvi∥2

| {z }

> Field error

+ ∥B (vi) − B (ˆ vi)∥2

| {z }

> BSP error

, (51) where v = uHR , ˆv = Gθ(uLR ) and B(·) denotes log of the binned energy spec-trum representation[26]. To ensure that both the loss terms have a similar or-der of magnitude, both the field and BSP errors are min-max normalized with respect to min and max computed from the training dataset. The predicted spectra follows the ground truth much more closely into the high-k regime, and the reconstructed fields retain the fine-scale turbulence structures, re-flected by more accurate gradients and Laplacians. Quantitatively, BSP yields large reductions in spectral and Barron-norm errors while leaving the field normalized RMSE (nRMSE) largely unchanged (Table 5). The energy-spectrum nRMSE drops by 3.1× for DeepOKAN (0.1289 →0.0416), 3.4× for FNO (0.0865 →0.0256), and 5.6× for CNO (0.1172 →0.0211), with commen-surate reductions in Barron-norm error of roughly 2.2-4.1× (DeepOKAN: 0.3659 →0.1662; FNO: 0.2641 →0.0828; CNO: 0.3229 →0.0792). These im-provements are consistent with mitigating spectral bias rather than simply rescaling the output. BSP primarily targets the high-frequency tail and associated sharp features, which are poorly captured by the baseline loss. DeepONet is comparatively insensitive to BSP in this setting, showing no improvement in spectral error and only a modest reduction in Barron-norm error. Finally, since BSP modifies only the training objective, parameter counts and inference times are unchanged across each model pair, so the gains in spectral fidelity come at no additional inference cost. Beyond the training objective, we find that the choice of optimizer can also influence high-frequency fidelity. In particular, SOAP tends to preserve more high-frequency energy than Adam, leading to sharper gradient and Laplacian diagnostics. A focused comparison is provided in Appendix D and Figure D.2. 40 Figure 9: Turbulent jet: Spectral bias in neural operators. Columns show (from left to right) schlieren images of an impinging jet, the corresponding energy spectrum E(k), spatial gradients, and the Laplacian. Rows compare the ground truth against predictions from different neural operator architectures (with and without BSP training). We ob-serve that using Binned Spectral Power (BSP) loss helps mitigate spectral bias, improving agreement with the true energy spectrum across wavenumbers. All the results are based on SOAP optimizer. 

41 Model Field Error Energy-Spectrum Error Barron-Norm Error # params (M) Inference time (s) 

DeepONet 0.0529 0.1566 0.4171 3.8 0.00086 

DeepONet (BSP) 0.0535 0.1675 0.4083 3.8 0.00086 DeepOKAN 0.0538 0.1289 0.3659 3.0 0.00991 DeepOKAN (BSP) 0.0547 0.0416 0.1662 3.0 0.00991 FNO 0.0517 0.0865 0.2641 3.4 0.00433 FNO (BSP) 0.0556 0.0256 0.0828 3.4 0.00433 CNO 0.0512 0.1172 0.3229 3.2 0.01238 CNO (BSP) 0.0543 0.0211 0.0792 3.2 0.01238  

> Table 5: Comparison of neural operator errors and costs for the impinging jet problem.

4.5.2. Earthquake problem 

Here we consider the dynamic response of a six-story reinforced concrete frame building subjected to ground acceleration records from the PEER NGA-West2 database ( https://ngawest2.berkeley.edu/ )1. The govern-ing equation of motion for the linear multi-degree-of-freedom system is 

M¨x + C ˙x + Kx = Mι¨ug(t), (52) where M, C, and K ∈ R504 ×504 are the mass, damping, and stiffness matrices from finite element discretizations, x(t) is the displacement vector, ι is the influence vector distributing ground motion to degrees of freedom, and ¨ug(t)

is the ground acceleration. The system starts at rest with x(0) = ˙ x(0) = 0.For a linear system, the response can be expressed via convolution with the Green’s function, 

x(t) = 

Z t

> 0

¨ug(τ )h(t − τ ) dτ, (53) where the neural operator Gθ approximates the mapping ¨ug(t) 7 → x1(t) from ground acceleration to roof displacement. The dataset comprises 144 earthquake ground motion records, with 100 samples for training and 44 for testing. Each record spans 80 seconds at 50 Hz   

> 1Ground acceleration records are taken from the Pacific Earthquake Engineering Re-search Center (PEER: https://ngawest2.berkeley.edu/ ,https://peer.berkeley. edu/ )

42 sampling (4000 timesteps), with records pre-processed using a Butterworth filter (0.1–24.9 Hz) and resampled to uniform ∆t = 0 .02 s. The structural re-sponse exhibits both low-frequency global modes and high-frequency content from impulsive ground motions, presenting a challenging multi-scale predic-tion problem. The temporal resolution of this problem creates a significantly more demanding spectral learning task than the impinging-jet experiment. For a discrete signal of N points sampled at rate fs, the Nyquist frequency is 

fs/2 and there are N/ 2 discrete frequency bins between zero and the Nyquist limit. The earthquake time series ( N = 4000 , fs = 50 Hz) therefore contains 2000 resolvable frequency bins up to 25 Hz, consistent with the Butterworth filter’s 24.9 Hz cutoff, compared to 64 modes for the impinging jet’s 128-point spatial dimension. This roughly 30 × increase in the number of spectral modes the network must resolve makes the spectral bias problem increasingly challenging. The neural operator architectures used for this problem differ in how they process temporal information. DeepONet and DeepOKAN employ a causal windowing approach where the response at each timestep depends only on past input history, implemented via zero-padding with a shifting window that encodes physical causality; the output state at the current timestep is not affected by input states at future timesteps. For additional details on the causal formulation and dataset, we refer readers to Liu et al. [47]. In contrast, FNO & CNO process the entire input sequence to predict the full output time-series in a single forward pass, and subsequently, the causality was not strictly enforced. All metrics reported are computed in real (physi-cal) space on the de-normalized predictions. We also investigate minimizing the BSP error (see Equation 51) as a strategy to mitigate spectral bias. Computing this term requires computing FFT over the entire predicted sequence, so the BSP loss can only be eval-uated once all timesteps of the predicted outputs are assembled. Training with BSP error minimization was straightforward for FNO & CNO. How-ever, for DeepONet/DeepOKAN with causal training [47], predictions from all timesteps must first be collected before computing BSP. Results are sum-marized in Table 6 and Figure 10. All architectures use SOAP optimizer and causal training for DeepONet/DeepOKAN, with error metrics computed in physical space. 43 Table 6: Architecture and loss comparison for earthquake structural response prediction. SOAP optimizer, causal training for DeepONet/DeepOKAN. 

Model Field Error Log Spectral Error Barron Norm Error 

Baseline BSP Baseline BSP Baseline BSP DeepONet (SIREN) 0.0008 0.0009 0.1257 0.1366 0.0019 0.0027 

DeepONet (Tanh) − 0.0006 − 0.1103 − 0.0012 

DeepOKAN 0.0022 0.0044 0.1230 0.1750 0.0029 0.0077 

FNO 0.0038 0.0041 0.1485 0.0819 0.0045 0.0030 

CNO 0.0133 0.0145 0.1323 0.0758 0.0040 0.0043  

> −Failed to converge.

Consistent with the impinging-jet findings, BSP improves spectral fidelity when the field loss also operates on the full output sequence (FNO, CNO), as illustrated for FNO in Figure 10(b) (see Figure E.4 for all architectures). FNO achieves a 1.8× reduction in log spectral error ( 0.149 → 0.082 ) and CNO a 1.7× reduction ( 0.132 → 0.076 ), with corresponding improvements in Barron norm error. 44 Figure 10: Representative earthquake response predictions illustrating key design choices. Each row shows the time-domain response (left) and energy spectrum (right) for the same test sample. (a) DeepONet (SIREN) achieves the lowest field error (NRMSE = 0 .0008 ; Table 6), followed by DeepOKAN ( 0.0022 ), FNO ( 0.0038 ), and CNO (0.0133 ). (b) BSP improves FNO spectral fidelity, reducing log spectral error from 0.149 

to 0.082 (1.8×), visible in the high-frequency alignment. (c) SOAP enables DeepOKAN convergence (NRMSE = 0 .0022 ), whereas Adam fails across all seeds. (d) SIREN activa-tion enables DeepONet convergence without BSP (NRMSE = 0 .0008 ), while Tanh fails to converge under the same conditions. Black dashed lines denote ground truth; extended comparisons are in Figures E.4–E.6. 

The strongest evidence for BSP’s effect comes from derivative metrics, where 45 the ground truth is dominated by high-frequency content. FNO acceleration error ( d2u/dt 2) drops 2.2× (0.064 → 0.030 ; Table E.2). Similarly, CNO ac-celeration improves more modestly ( 0.032 → 0.026 , 1.2×; Table E.2). Unlike the impinging-jet problem, we did not find the same improvement from BSP for DeepONet and DeepOKAN. DeepONet shows slight degrada-tion across all metrics with BSP: spectral error worsens ( 0.126 → 0.137 ), field error increases ( 0.0008 → 0.0009 ), and Barron norm error rises ( 0.0019 →

0.0027 ). DeepOKAN also exhibits degradation with BSP. As noted ear-lier, BSP requires the full sequence while the causal field loss operates per-timestep, so the two objectives see different views of the prediction. Looking at the cosine similarity between the per loss gradients we see that these com-peting views produce opposing gradient signals: DeepONet (SIREN) exhibits strongly negative cosine similarity between L2 and BSP gradients ( −0.48 by the end of training), while FNO maintains positive alignment ( +0 .28 ). Cru-cially, when the same architectures are trained in non-causal mode (removing the causal/non-causal mismatch), gradient alignment shifts toward positive: DeepONet (SIREN) moves from −0.48 to +0 .13 , see Table E.4 and Fig-ure E.7 in subsection E.4. Nevertheless, we found causality to be necessary to make meaningful predictions for DeepONet/DeepOKAN. When causal-ity is removed and these architectures are trained like FNO and CNO, they mostly fail to converge; this is discussed further in subsection E.3 (Table E.3). The notable exception of BSP’s performance with a causal approach is Deep-ONet with Tanh activation: without BSP the model fails to converge ( 0.024 

NRMSE, 0.468 acceleration error), but BSP enables convergence to the best field accuracy ( 0.0006 ) with acceleration error dropping to 0.030 .Beyond the training objective, optimizer selection and activation function also interact with BSP effectiveness on this dataset. SOAP provides 2.0×

lower NRMSE for DeepONet SIREN and enables convergence for DeepOKAN (Adam fails across all seeds, Figure 10(c)), with strong performance on FNO and CNO as well (see Figure D). Activation function and spectral regu-larization interact non-trivially: SIREN’s periodic activations provide im-plicit spectral bias mitigation sufficient for convergence without BSP, while Tanh requires BSP to converge. This suggests that SIREN and BSP ad-dress overlapping spectral deficiencies, whereas Tanh and BSP provide more complementary mechanisms (see subsection E.2 and Figure 10(d)). Detailed comparisons are provided in Appendix D. 46 5. Conclusions and Outlook 

We presented a systematic investigation of spectral bias in physics-informed neural networks, physics-informed Kolmogorov–Arnold networks, and neu-ral operators. Through a combination of theoretical analysis and extensive numerical experiments across elliptic, hyperbolic, and dispersive PDEs, we demonstrated that spectral bias in scientific machine learning is not solely a representational limitation of neural architectures, but it depends on op-timization dynamics, loss formulation, and their interaction with network expressivity. From a theoretical side, linearized analyses based on neural tangent kernel and Gauss–Newton dynamics clarify why first-order optimization methods preferentially learn low-frequency modes, while higher-frequency components converge slowly. In contrast, quasi-second-order and second-order optimiza-tion strategies substantially reduce the frequency dependence of convergence rates by implicitly preconditioning the training dynamics. Although these arguments rely on idealized assumptions, our empirical results show that their qualitative implications persist well beyond the asymptotic regime. Across a wide range of benchmark problems, we observed that optimiza-tion choice plays a dominant role in mitigating spectral bias in PINNs and PIKANs. First-order optimizers such as Adam often converge to spectrally incomplete solutions that satisfy low-order statistics and PDE residuals in an averaged sense, while failing to capture gradients, curvatures, and localized high-frequency features. Quasi-second-order methods such as SOAP, and in particular fully second-order methods such as SS-Broyden, significantly im-prove both convergence behavior and spectral resolution, enabling accurate recovery of higher-order moments and Barron norms. These improvements are especially pronounced in stiff elliptic and dispersive problems, where spec-tral imbalance has the most severe physical consequences. Architectural choices and activation functions were shown to play a com-plementary but secondary role. Oscillatory activations such as sine functions and SIREN-type parameterizations improve high-frequency representation under first-order optimization and can help escape early stagnation. How-ever, their influence diminishes as optimization strength increases, indicating that representational enhancements alone are insufficient to fully overcome 47 spectral bias without high-order optimization. Similar trends were observed in physics-informed KANs, where polynomial-based univariate functions ex-hibit classical spectral limitations for non-smooth solutions unless combined with strong optimization strategies. For neural operators, our results demonstrate that spectral bias persists even in architectures that explicitly operate in Fourier space. Standard 

L2 training objectives remain dominated by low-frequency content, leading to systematic underestimation of high-wavenumber energy. Spectral-aware loss formulations, such as binned spectral power losses, provide an effective and computationally inexpensive approach to restore spectral balance across operator architectures, including DeepONet, DeepOKAN, FNO, and CNO. These findings highlight that loss design is also critical for achieving spec-trally resolved operator learning. In summary, our study suggests that spectral bias in scientific machine learning should be viewed primarily as a dynamical phenomenon arising from ill-conditioned training objectives, rather than as an intrinsic failure of neural representations. While expressive architectures and carefully chosen activa-tions can improve performance, robust mitigation of spectral bias requires curvature-aware optimization and, in operator settings, spectrally informed loss functions. From a practical standpoint, our results lead to concrete guidelines: strong second-order or quasi-second-order optimizers are essen-tial for physics-informed learning in multiscale PDEs, while spectral losses play a key role in operator learning when high-frequency fidelity is required. Several open questions remain. Extending these findings to large-scale three-dimensional problems, understanding the interaction between spectral bias and adaptive sampling strategies, and developing scalable second-order optimization methods tailored to physics-informed objectives are promising directions for future work. For example, currently, SOAP can use mini-batching but SS-Broyden cannot, so this is a concrete direction to be pursued given the accuracy superiority of this method. 

CRediT authorship contribution statement Siavash Khodakarami: Writing - review & editing, Writing - original draft, Visualization, Validation, Software, Methodology, Investigation, For-48 mal analysis, Data Curation, Conceptualization. Vivek Oommen: Writing - review & editing, Writing - original draft, Visualization, Validation, Soft-ware, Methodology, Investigation, Formal analysis, Data Curation, Concep-tualization. Nazanin Ahmadi Daryakenari: Writing - review & editing, Writing - original draft, Visualization, Validation, Software, Methodology, Investigation, Formal analysis, Data Curation, Conceptualization. Maxim Beekenkamp: Writing - review & editing, Writing - original draft, Visu-alization, Validation, Software, Methodology, Investigation, Formal analysis, Data Curation. George Em Karniadakis Writing - review & editing, Writ-ing - original draft, Supervision, Funding acquisition, Validation, Methodol-ogy, Formal analysis, Conceptualization. 

Declaration of competing interest 

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. 

Acknowledgements 

We would like to acknowledge funding from the Office of Naval Research as part of MURI-METHODS project with grant number N00014242545 and the ONR Vannevar Bush Faculty Fellowship (N00014-22-1-2795). The authors would like to acknowledge the computational resources and services at the Center for Computation and Visualization (CCV), Brown University. We acknowledge that the Schlieren impinging-jet experiments used in this work were conducted at the SMC Lab, Tsinghua University. The corresponding Schlieren dataset originates from Tsinghua University, and the associated intellectual property of this dataset remains with Tsinghua University. 

Data Availability 

All codes and datasets except the Schlieren dataset will be made publicly available at https://github.com/SiaK4/PIN_NO_Spectral_Bias.git upon publication. Please contact Prof. He Feng (hefeng@tsinghua.edu.cn) to access the im-pinging jet Schlieren dataset. 49 References 

[1] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics , 378:686–707, 2019. [2] Nazanin Ahmadi, Qianying Cao, Jay D Humphrey, and George Em Karniadakis. Physics-informed machine learning in biomedical science and engineering. arXiv preprint arXiv:2510.05433 , 2025. [3] Juan Diego Toscano, Vivek Oommen, Alan John Varghese, Zongren Zou, Nazanin Ahmadi Daryakenari, Chenxi Wu, and George Em Karni-adakis. From PINNs to PIKANs: Recent advances in physics-informed machine learning, 2024. Machine Learning for Computational Science and Engineering , 1(1):15, 2025. [4] Shengze Cai, Zhicheng Wang, Sifan Wang, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks for heat transfer problems. Journal of Heat Transfer , 143(6):060801, 2021. [5] Chi Zhao, Feifei Zhang, Wenqiang Lou, Xi Wang, and Jianyong Yang. A comprehensive review of advances in physics-informed neural networks and their applications in complex fluid dynamics. Physics of Fluids ,36(10), 2024. [6] QiZhi He, David Barajas-Solano, Guzel Tartakovsky, and Alexandre M. Tartakovsky. Physics-informed neural networks for multiphysics data as-similation with application to subsurface transport. Advances in Water Resources , 141:103610, 2020. [7] Khemraj Shukla, Patricio Clark Di Leoni, James Blackshire, Daniel Sparkman, and George Em Karniadakis. Physics-informed neural net-work for ultrasound nondestructive quantification of surface breaking cracks. Journal of Nondestructive Evaluation , 39(3):61, 2020. [8] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anand-kumar. Fourier neural operator for parametric partial differential equa-tions. In International Conference on Learning Representations , 2021. 50 [9] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature Machine Intelli-gence , 3(3):218–229, 2021. [10] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spec-tral bias of neural networks. In International Conference on Machine Learning , pages 5301–5310. PMLR, 2019. [11] Siavash Khodakarami, Vivek Oommen, Aniruddha Bora, and George Em Karniadakis. Mitigating spectral bias in neural opera-tors via high-frequency scaling for physical systems. Neural Networks ,193:108027, 2026. [12] Vivek Oommen, Aniruddha Bora, Zhen Zhang, and George Em Kar-niadakis. Integrating neural operators with diffusion models improves spectral representation in turbulence modelling. Proceedings of the Royal Society A , 481(2309):20240819, 2025. [13] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems , 33:7537–7547, 2020. [14] Bo Wang, Heng Yuan, Lizuo Liu, Wenzhong Zhang, and Wei Cai. On spectral bias reduction of multi-scale neural networks for regression problems. Neural Networks , 185:107179, 2025. [15] Qingguo Hong, Jonathan W Siegel, Qinyang Tan, and Jinchao Xu. On the activation function dependence of the spectral bias of neural net-works. arXiv preprint arXiv:2208.04924 , 2022. [16] Bo Wang, Lizuo Liu, and Wei Cai. Multi-scale deeponet (mscale-deeponet) for mitigating spectral bias in learning high frequency op-erators of oscillatory functions. arXiv preprint arXiv:2504.10932 , 2025. [17] Vivek Oommen, Siavash Khodakarami, Aniruddha Bora, Zhicheng Wang, and George Em Karniadakis. Learning turbulent flows with 51 generative models: Super-resolution, forecasting, and sparse flow re-construction. arXiv preprint arXiv:2509.08752 , 2025. [18] Xintao Chai, Wenjun Cao, Jianhui Li, Hang Long, and Xiaodong Sun. Overcoming the spectral bias problem of physics-informed neural net-works in solving the frequency-domain acoustic wave equation. IEEE Transactions on Geoscience and Remote Sensing , 62:1–20, 2024. [19] Omar Sallam and Mirjam Fürth. On the use of fourier features-physics informed neural networks (FF-PINN) for forward and inverse fluid me-chanics problems. Proceedings of the Institution of Mechanical Engi-neers, Part M: Journal of Engineering for the Maritime Environment ,237(4):846–866, 2023. [20] Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, and Lu Lu. A com-prehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering , 403:115671, 2023. [21] Yarong Liu, Hong Gu, Xiangjun Yu, and Pan Qin. Diminishing spectral bias in physics-informed neural networks using spatially-adaptive fourier feature encoding. Neural Networks , 182:106886, 2025. [22] Xiong Xiong, Zhuo Zhang, Rongchun Hu, Chen Gao, and Zichen Deng. Separated-variable spectral neural networks: a physics-informed learn-ing approach for high-frequency pdes. arXiv preprint arXiv:2508.00628 ,2025. [23] Nazanin Ahmadi Daryakenari, Shupeng Wang, and George Karniadakis. CMINNs: Compartment model informed neural networks — unlocking drug dynamics. Computers in Biology and Medicine , 184:109392, 2025. [24] Nazanin Ahmadi Daryakenari, Mario De Florio, Khemraj Shukla, and George Em Karniadakis. AI-Aristotle: A physics-informed framework for systems biology gray-box identification. PLOS Computational Biol-ogy , 20(3):e1011916, 2024. [25] Ameya D. Jagtap, Kenji Kawaguchi, and George Em Karniadakis. Adaptive activation functions accelerate convergence in deep and physics-informed neural networks. Journal of Computational Physics ,404:109136, 2020. 52 [26] Dibyajyoti Chakraborty, Arvind T Mohan, and Romit Maulik. Binned spectral power loss for improved prediction of chaotic systems. arXiv preprint arXiv:2502.00472 , 2025. [27] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in Neural Information Processing Systems , 31, 2018. [28] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Sys-tems , 33:7462–7473, 2020. [29] George AF Seber and Alan J Lee. Linear Regression Analysis . John Wiley & Sons, 2003. [30] Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory ,39(3):930–945, 2002. [31] Khemraj Shukla, Juan Diego Toscano, Zhicheng Wang, Zongren Zou, and George Em Karniadakis. A comprehensive and FAIR comparison between MLP and KAN representations for differential equations and operator networks. Computer Methods in Applied Mechanics and Engi-neering , 431:117290, 2024. [32] Nazanin Ahmadi Daryakenari, Khemraj Shukla, and George Em Kar-niadakis. Representation meets optimization: Training PINNs and PIKANs for gray-box discovery in systems pharmacology. Computers in Biology and Medicine , 201:111393, 2026. [33] Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. IEEE Transactions on Neural Networks , 6(4):911–917, 1995. [34] Bogdan Raonic, Roberto Molinaro, Tobias Rohner, Siddhartha Mishra, and Emmanuel de Bezenac. Convolutional neural operators. In ICLR 2023 workshop on Physics for Machine Learning , 2023. 53 [35] Elham Kiyani, Khemraj Shukla, Jorge F Urbán, Jérôme Darbon, and George Em Karniadakis. Optimizing the optimizer for physics-informed neural networks and kolmogorov-arnold networks. Computer Methods in Applied Mechanics and Engineering , 446:118308, 2025. [36] Sifan Wang, Ananyae Kumar Bhartari, Bowen Li, and Paris Perdikaris. Gradient alignment in physics-informed neural networks: A second-order optimization perspective. arXiv preprint arXiv:2502.00604 , 2025. [37] Jorge Nocedal. Numerical optimization . Springer, 2006. [38] Jorge F. Urbán, Petros Stefanou, and José A. Pons. Unveiling the op-timization process of physics informed neural networks: How accurate and competitive can PINNs be? Journal of Computational Physics ,523:113656, 2025. [39] Juan Diego Toscano, Daniel T Chen, Vivek Oommen, Jérôme Darbon, and George Em Karniadakis. A variational framework for residual-based adaptivity in neural pde solvers and operator learning. arXiv preprint arXiv:2509.14198 , 2025. [40] Norman J Zabusky and Martin D Kruskal. Interaction of" solitons" in a collisionless plasma and the recurrence of initial states. Physical Review Letters , 15(6):240, 1965. [41] Ameya D Jagtap, Kenji Kawaguchi, and George Em Karniadakis. Lo-cally adaptive activation functions with slope recovery for deep and physics-informed neural networks. Proceedings of the Royal Society A ,476(2239):20200334, 2020. [42] Yixuan Wang, Jonathan W Siegel, Ziming Liu, and Thomas Y Hou. On the expressiveness and spectral bias of kans. arXiv preprint arXiv:2410.01803 , 2024. [43] Gary S Settles and Michael J Hargather. A review of recent develop-ments in schlieren and shadowgraph techniques. Measurement Science and Technology , 28(4):042001, 2017. [44] Gary S Settles and Alex Liberzon. Schlieren and bos velocimetry of a round turbulent helium jet in air. Optics and Lasers in Engineering ,156:107104, 2022. 54 [45] Zhibo Wang, Xiangru Li, Luhan Liu, Xuecheng Wu, Pengfei Hao, Xiwen Zhang, and Feng He. Deep-learning-based super-resolution reconstruc-tion of high-speed imaging in fluids. Physics of Fluids , 34(3), 2022. [46] Qian Zhang, Dmitry Krotov, and George Em Karniadakis. Operator learning for reconstructing flow fields from sparse measurements: an energy transformer approach. arXiv preprint arXiv:2501.08339 , 2025. [47] Lizuo Liu, Kamaljyoti Nath, and Wei Cai. A causality-deeponet for causal responses of linear dynamical systems. Communications in Com-putational Physics , 35:1194–1228, 05 2024. [48] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochas-tic optimization. International Conference on Learning Representations (ICLR) , 2015. arXiv:1412.6980. [49] Nikhil Vyas, Depen Morwani, Rosie Zhao, Mujin Kwun, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham Kakade. Soap: Improving and stabilizing Shampoo using Adam. arXiv preprint arXiv:2409.11321 , 2024. [50] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Precondi-tioned stochastic tensor optimization. In International Conference on Machine Learning (ICML) . PMLR, 2018. arXiv:1802.09568. [51] Chenhui Xu, Dancheng Liu, Amir Nassereldine, and Jinjun Xiong. Fp64 is all you need: Rethinking failure modes in physics-informed neural networks. 2025. arXiv:2505.10949. [52] Shun-ichi Amari. Natural gradient works efficiently in learning. Neural Computation , 10(2):251–276, 1998. 

Appendix A Statistical moments 

First moment represents the average or direct component of the distribution. 

μ(t) = 1

N

> N

X

> i=1

ui(t) (A.1) 55 Second moment represents the variance and is directly related to the total spectral energy and therefore still dominated by low-frequency modes in most physical systems. 

σ2(t) = 1

N

> N

X

> i=1

(ui(t) − μ(t)) 2 (A.2) Third moment represents the skewness and asymmetry of the distribution and is related to higher modes. 

γ1(t) = 1

N

> N

X

> i=1

ui(t) − μ(t)

σ(t)

3

(A.3) Fourth moment (Kurtosis) measures the intermittency of the distribution, which correspond to high-frequency spectral content. 

γ2(t) = 1

N

> N

X

> i=1

ui(t) − μ(t)

σ(t)

4

(A.4) 56 Appendix B Loss history for adaptive activations Adam 

Figure B.1: PINN with adaptive Tanh activation function training history for KdV equation. Relative L2 error during training with adaptive activation functions including slope recovery loss term with different weights ( wsr ), trained with Adam. Note that by increasing the weight for the slope recovery term, the errors starts dropping down earlier. 

57 Appendix C Wave equations results 

Table C.1: Wave equation prediction errors. Comparison of PINN performance with different activation functions and optimizers for cases (i)-(iii). The results for case (iv) are shown in Table 3 of the main text. Note that for case (i) with no multi-scale features or high frequencies, using SIREN with w0 = 5 is better than using SIREN with 

w0 = 30 .

PINN Rel. L2

Error Barron Norm Rel. L2 Error log( eF,p =0 ) log( eF,p =2 ) log( eF,p =4 )

Case (i) 

Adam (Tanh) 5.08 × 10 −3 2.94 × 10 −3 -5.19 -1.83 2.27 Adam (SIREN) 6.11 × 10 −3 4.45 × 10 −3 -5.03 -1.58 2.54 SOAP (Tanh) 1.19 × 10 −3 2.86 × 10 −4 -6.44 -4.28 0.80 SOAP (SIREN, w0 = 30 ) 1.59 × 10 −3 2.19 × 10 −4 -5.11 -4.24 0.49 SOAP (SIREN, w0 = 5 ) 8.45 × 10 −4 1.81 × 10 −4 -6.59 -4.20 0.50 SS-Broyden (Tanh) 4.33 × 10 −8 4.99 × 10 −8 -14.83 -9.69 -3.92 

SS-Broyden (SIREN) 2.32 × 10 −8 2.89 × 10 −8 -14.95 -10.06 -4.17 Case (ii) 

Adam (Tanh) 8.71 × 10 −2 6.20 × 10 −2 -2.75 0.85 5.48 Adam (SIREN) 1.72 × 10 −2 1.07 × 10 −2 -4.16 -1.14 3.92 SOAP (Tanh) 1.32 × 10 −3 1.08 × 10 −3 -6.39 -2.95 1.99 SOAP (SIREN) 6.19 × 10 −4 3.95 × 10 −4 -7.05 -3.46 1.51 SS-Broyden (Tanh) 1.16 × 10 −6 8.01 × 10 −7 -12.50 -6.99 -1.39 

SS-Broyden (SIREN) 2.81 × 10 −7 2.68 × 10 −7 -13.71 -8.26 -2.50 Case (iii) 

Adam (Tanh) 0.84 0.68 -1.39 1.96 6.91 Adam (SIREN) 0.18 0.11 -2.73 0.19 5.38 SOAP (Tanh) 1.38 × 10 −3 1.19 × 10 −3 -6.96 -3.3 1.76 SOAP (SIREN) 1.23 × 10 −3 7.42 × 10 −4 -7.08 -3.50 1.45 SS-Broyden (Tanh) 4.01 × 10 −6 2.82 × 10 −6 -12.04 -3.30 1.76 

SS-Broyden (SIREN) 8.37 × 10 −7 7.01 × 10 −7 -13.39 -7.56 -1.66 

Appendix D Effect of the optimizer on spectral bias in neural operators 

In addition to the training objective, the optimizer can materially affect how fast and how well high spatial frequencies are learned. To isolate optimizer effects from architectural and loss-design choices, we repeat the impinging-jet super-resolution experiment using the same data split and model configura-tions as in the main text, and we vary only the optimizer. Figure D.2 vi-sualizes representative reconstructions together with frequency-domain and derivative-based diagnostics. 58 Figure D.2 compares Adam [48] and SOAP [49] for DeepONet and CNO. Across both architectures, Adam recovers the dominant large-scale shock topology but exhibits stronger attenuation of the high-wavenumber tail of the energy spectrum, consistent with visibly over-smoothed reconstructions. SOAP yields closer agreement with the reference spectrum into the high-k

regime and produces sharper gradients and Laplacians, indicating improved recovery of fine-scale density-gradient features that are under-represented by Adam. These observations suggest that optimizer-induced preconditioning can either amplify or suppress the effective learning rate of high-frequency modes, thereby interacting with spectral bias. Conceptually, Adam is a first-order method that applies a diagonal, per-parameter adaptive scaling based on exponential moving averages of the gra-dient and squared gradient [48]. SOAP is a preconditioned method that builds on Shampoo-style curvature information [50] and runs Adam-like up-dates in a rotated (preconditioned) coordinate system, which empirically im-proves stability and convergence in large-scale training [49]. We also explored second-order optimizers like SS-Broyden for neural operators, motivated by their strong performance in PINNs. However, making such optimizers work for data-driven neural operators was a challenge we could not resolve in this work. Two practical differences exist. First, many PINN studies success-fully use full-batch quasi-Newton training because the number of collocation points is typically modest, for example, [1] explicitly uses L-BFGS as a quasi-Newton, full-batch optimizer when the training set is small. Second, preci-sion can be critical in PDE-residual optimization. Recent evidence shows that FP64 can prevent precision-induced stalls of L-BFGS in PINNs [51]. In contrast, neural operators are typically much bigger, commonly trained in mini-batches on large datasets, and are often run in FP32 for through-put. Under these constraints, stable estimation and inversion of Fisher or Gauss-Newton structure typically requires aggressive approximations (sub-sampling, low-rank structure, truncated solves), and the resulting updates can be sensitive to noise, damping, and numerical conditioning. Developing reliable second-order variants tailored to neural operators remains a promis-ing direction for future work, especially if combined with improved precision control, better curvature estimators, and scalable linear-solve strategies [52]. On the earthquake structural response problem SOAP has a stronger ef-fect, and is critical for DeepONet and DeepOKAN architectures. For Deep-ONet with SIREN, SOAP achieves 2.0× lower NRMSE ( 0.0008 vs 0.0015 ) in 59 Figure D.2: Optimizer effects on spectral bias in neural operators. Columns show (from left to right) Schlieren fields, the isotropic energy spectrum E(k), spatial gradients, and the Laplacian. Rows compare the ground truth against DeepONet and CNO trained with Adam and SOAP. SOAP yields improved agreement with the reference spectrum at high wavenumbers and recovers sharper derivative-based features, indicating reduced attenuation of fine scales relative to Adam. 

the baseline configuration, with the advantage persisting under BSP ( 1.4×

lower NRMSE, 0.0009 vs 0.0013 ). DeepOKAN with Adam [48] fails to con-verge entirely, while SOAP achieves 0.0022 NRMSE, highlighting SOAP’s im-portance for architectures with challenging optimization landscapes. These optimizer effects hold for both baseline (L2 only) and BSP configurations, as shown by the paired rows in Table E.2. FNO and CNO show minimal optimizer sensitivity in field accuracy, but SOAP provides spectral improve-ment for FNO BSP ( 0.082 vs 0.098 , 1.2×). Moreover, SOAP’s advantage extends beyond error magnitude to reliability: across repeated runs with dif-ferent random seeds, Adam occasionally fails to converge even for DeepONet SIREN (which converges reliably with SOAP), while DeepOKAN with Adam fails consistently across all seeds. Figure D.3 illustrates these effects for a 60 representative test sample. 

Figure D.3: SOAP vs Adam for earthquake response prediction. Time response and energy spectrum for a representative test sample comparing Adam (left pair) and SOAP (right pair). Adam DeepOKAN fails to converge (near-flat prediction and flat spectral tail), while SOAP enables convergence for all architectures with improved spectral accuracy at high frequencies. 

Appendix E Additional considerations for the earthquake prob-lem 

Table E.2 compares SOAP and Adam across all architectures for both base-line and BSP loss configurations with causal training for DeepONet/DeepOKAN, 61 including both SIREN and Tanh activations for DeepONet.    

> Table E.2: Optimizer, activation, and loss comparison for earthquake response prediction (causal training). −Failed to converge.

Model Field Error Log Spectral Error Barron Norm Error Accel. Error 

Adam SOAP Adam SOAP Adam SOAP Adam SOAP DeepONet (SIREN) 0.0015 0 .0008 0 .0974 0 .1257 0 .0025 0.0019 0.0381 0 .0506 

DeepONet (SIREN, BSP) 0.0013 0 .0009 0 .1026 0 .1366 0 .0020 0.0027 0.0314 0 .0619 

DeepONet (Tanh) − − − − − − − −

DeepONet (Tanh, BSP) 0.0013 0 .0006 0 .1090 0 .1103 0 .0021 0.0012 0.0373 0 .0303 

DeepOKAN − 0.0022 − 0.1230 − 0.0029 − 0.0584 

DeepOKAN (BSP) − 0.0044 − 0.1750 − 0.0077 − 0.1595 

FNO 0.0037 0 .0038 0 .1463 0 .1485 0 .0047 0.0045 0.0672 0 .0645 

FNO (BSP) 0.0042 0 .0041 0 .0983 0 .0819 0 .0034 0.0030 0.0309 0 .0296 

CNO 0.0126 0 .0133 0 .1183 0 .1323 0 .0039 0.0040 0.0281 0 .0325 

CNO (BSP) 0.0197 0 .0145 0 .0774 0 .0758 0 .0059 0.0043 0.0272 0 .0263 

E.1 Extended BSP comparison 

Figure E.4 extends the representative BSP example in Figure 10(b) to all four architectures, showing the full baseline vs BSP comparison. 62 Figure E.4: Effect of BSP on spectral fidelity for earthquake response predic-tion. Time response (columns 1, 3) and energy spectrum (columns 2, 4) for a represen-tative test sample comparing baseline (left pair) and BSP (right pair) training across all four architectures. Note the difference in scale of energy values (y-axis) when comparing these results to the impinging-jet problem. 

E.2 Activation effects (SIREN vs Tanh) 

The interaction between activation function and loss formulation reveals com-plementary mechanisms for spectral bias mitigation. With baseline train-ing, SIREN converges reliably ( 0.0008 NRMSE) while Tanh fails to con-verge ( 0.024 ), demonstrating SIREN’s robustness. However, with BSP, Tanh achieves the best absolute accuracy ( 0.0006 field, 0.110 spectral) across all 63 configurations, outperforming SIREN+BSP ( 0.0009 field, 0.137 spectral). Acceleration error confirms this pattern: Tanh+BSP achieves 0.030 vs SIREN+BSP 

0.062 (2.0× better). This suggests that SIREN’s periodic activations provide implicit spectral bias mitigation that overlaps with BSP’s explicit regular-ization; when both are active, the redundant spectral gradients may inter-fere rather than compound. In contrast, Tanh relies entirely on BSP for high-frequency content, allowing complementary rather than redundant op-timization gradients. The cosine similarity between the per loss gradients (Table E.4) corroborates this interpretation: SIREN exhibits the strongest negative gradient alignment ( −0.48 final cosine similarity), while Tanh shows weaker conflict ( −0.18 ), permitting BSP’s spectral signal to provide net ben-efit. Figure E.5 illustrates these contrasting mechanisms for a representative test sample.   

> Figure E.5: Activation function effects on DeepONet. Time response and energy spectrum comparing Tanh (left pair) and SIREN (right pair) activations for both baseline (top row) and Log-BSP (bottom row) training. Tanh baseline fails to capture the structural response, but BSP enables convergence to the best overall accuracy. SIREN converges reliably without BSP but shows diminishing returns when BSP is added.

E.3 Causal training 

Causal (per-timestep) training [47] proved essential for DeepONet and DeepOKAN on this small dataset. Non-causal (sequence) variants mostly failed to con-verge, and those that did achieve marginal convergence produced field errors 64 an order of magnitude worse than their causal counterparts, as shown in Ta-ble E.3. Figure E.6 illustrates the failure mode of non-causal training for a representative test sample. 

Figure E.6: Causal vs non-causal training for branch-trunk architectures. Time response and energy spectrum comparing non-causal training (left pair) and causal train-ing (right pair) for DeepONet and DeepOKAN. Non-causal DeepOKAN fails to converge entirely, while non-causal DeepONet achieves only marginal convergence with substan-tially degraded spectral fidelity compared to its causal counterpart. 

65 Table E.3: Causal (C) vs non-causal (NC) training for branch-trunk architectures (SOAP optimizer). − Failed to converge (field error ≥ 0.023 ). 

Model Field Error Log Spectral Error Barron Norm Error Accel. Error 

C NC C NC C NC C NC DeepONet (SIREN) 0.0008 0 .0211 0 .1257 0 .0961 0 .0019 0.0054 0 .0506 0.0277 

DeepONet (SIREN, BSP) 0.0009 − 0.1366 − 0.0027 − 0.0619 −

DeepONet (Tanh) − − − − − − − −

DeepONet (Tanh, BSP) 0.0006 − 0.1103 − 0.0012 − 0.0303 −

DeepOKAN 0.0022 − 0.1230 − 0.0029 − 0.0584 −

DeepOKAN (BSP) 0.0044 − 0.1750 − 0.0077 − 0.1595 −

E.4 Gradient alignment 

To investigate whether the structural mismatch between per-timestep L2 and sequence-level BSP produces competing gradient signals, we train each configuration while computing per-loss gradients and measuring their cosine similarity for each batch. Positive values indicate aligned objectives; neg-ative values indicate the two losses pull parameters in opposing directions. Table E.4 summarises the results across all eight model configurations. To isolate the effect of the causal training structure, we also train DeepONet and DeepOKAN in non-causal mode (both L2 and BSP on the same full-sequence output, identical to FNO/CNO). 66 Table E.4: Cosine similarity between L2 field-loss and BSP spectral-loss gradients. Pos-itive values indicate aligned optimization directions; negative values indicate competing gradients. 

Model Training Mode Final Cos Sim 

DeepONet (SIREN) Causal −0.482 

DeepONet (Tanh) Causal −0.181 

DeepOKAN Causal +0 .021 

DeepONet (SIREN) Non-causal +0 .126 

DeepONet (Tanh) Non-causal +0 .107 

DeepOKAN Non-causal +0 .042 

FNO Non-causal +0 .282 

CNO Non-causal −0.185 

Figure E.7 shows the evolution of gradient cosine similarity over the course of training, confirming that these endpoint values reflect sustained trends rather than transient fluctuations. 

Figure E.7: Gradient cosine similarity over training. Cosine similarity between L2 field-loss and BSP spectral-loss gradients as a function of training completion. Dashed lines indicate causal training; solid lines indicate non-causal training. DeepONet (SIREN) under causal training diverges toward strongly negative values, confirming increasing gradient conflict over the course of training. 

Among the non-causal models, FNO shows the strongest positive alignment 67 (+0 .28 by the end of training). Both the field loss and the BSP loss see the same full-sequence forward pass output, so their gradient directions are com-patible. This explains BSP’s clear spectral improvement for FNO reported in Table 6. DeepONet with SIREN shows the strongest gradient conflict under causal training, with cosine similarity reaching −0.48 by the end of training (Fig-ure E.7). The per-timestep L2 loss and the sequence-level BSP produce opposing gradient signals, directly explaining BSP’s ineffectiveness for this configuration. DeepONet with Tanh shows weaker conflict ( −0.18 final), con-sistent with BSP still providing net benefit despite the structural mismatch. The difference may reflect how SIREN’s periodic activations create addi-tional spectral gradient competition beyond the structural mismatch alone. DeepOKAN gradients are near-orthogonal under causal training ( +0 .02 fi-nal), consistent with BSP having minimal net directional effect on this ar-chitecture. The non-causal results isolate the effect of the causal training structure. When the causal/non-causal mismatch is removed and both losses operate on the same full-sequence output, all three branch-trunk architectures shift toward positive alignment: DeepONet (SIREN) moves from −0.48 to +0 .13 ,DeepONet (Tanh) from −0.18 to +0 .11 , and DeepOKAN from +0 .02 to 

+0 .04 . This confirms that the negative gradient alignment is caused by the causal training structure rather than being an intrinsic property of the archi-tectures. Notably, SIREN’s non-causal L2 gradients are slightly more aligned with BSP ( +0 .13 ) than Tanh’s ( +0 .11 ), which is directionally consistent with the overlapping-mechanism interpretation from subsection E.2: SIREN’s pe-riodic activations naturally produce field-loss gradients that partially overlap with BSP’s spectral objective, whereas Tanh’s field-loss gradients are more orthogonal to it. CNO shows negative alignment ( −0.19 ) yet BSP still helps empirically, though its benefit is weaker than FNO’s ( 1.2× spectral improvement vs FNO’s 1.8×). This is potentially due to the model’s inherent regularization (due to its bottleneck structure) making the loss landscape more navigable despite the gradient conflict. More investigation is needed to fully explain why BSP’s improvement is contextual to the model it’s used with. 68