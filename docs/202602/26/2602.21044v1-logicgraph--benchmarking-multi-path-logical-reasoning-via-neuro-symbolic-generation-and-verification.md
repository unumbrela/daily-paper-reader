---
title: "LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification"
title_zh: LogicGraph：通过神经符号生成与验证评估多路径逻辑推理的基准
authors: "Yanrui Wu, Lingling Zhang, Xinyu Zhang, Jiayu Chang, Pengyu Li, Xu Jiang, Jingtao Hu, Jun Liu"
date: 2026-02-24
pdf: "https://arxiv.org/pdf/2602.21044v1"
tags: ["query:sr"]
score: 6.0
evidence: 用于逻辑推理和验证的神经符号框架
tldr: 本研究针对大语言模型在逻辑推理中仅关注单一路径（收敛性推理）的局限，提出了首个多路径逻辑推理基准 LogicGraph。该基准利用神经符号框架，通过逆向逻辑生成和语义实例化，构建了包含高深度多路径及逻辑干扰项的验证问题。实验揭示了模型在探索多样化逻辑路径方面的显著不足，为提升模型发散性推理能力提供了重要参考。
motivation: 现有的 LLM 评估主要侧重于寻找单一正确证明的收敛性推理，忽视了现实世界中逻辑问题往往存在多条有效推导路径的特性。
method: 提出一种神经符号框架，结合逆向逻辑生成与语义实例化，构建出经求解器验证、具有高深度多路径特征及逻辑干扰项的推理基准。
result: 实验显示主流 LLM 倾向于过早锁定单一路径而无法探索替代方案，且这种覆盖率差距随推理深度的增加而显著扩大。
conclusion: LogicGraph 揭示了模型在多路径推理中的“发散差距”，为未来开发具备更强探索和验证能力的逻辑推理模型提供了基准和方向。
---

## 摘要
大语言模型（LLMs）的评估主要强调收敛性逻辑推理，即成功被定义为生成单一的正确证明。然而，许多现实世界的推理问题允许多种有效的推导方式，要求模型探索多样的逻辑路径，而非仅仅局限于单一路径。为了解决这一局限性，我们推出了 LogicGraph，这是首个旨在系统评估多路径逻辑推理的基准。它通过一个利用逆向逻辑生成和语义实例化的神经符号框架构建而成。该流程产生了经求解器验证的推理问题，这些问题具有高深度的多路径推理特征和内在的逻辑干扰项，且每个实例都关联了一组详尽的最小证明集。我们还进一步提出了一个无参考评估框架，以严格评估模型在收敛和发散情境下的性能。对最先进语言模型的实验揭示了一个共同的局限性：模型倾向于过早地锁定在单一路径上，而无法探索其他替代方案，且覆盖率差距随推理深度的增加而显著扩大。LogicGraph 揭示了这种发散性差距，并提供了具有启发性的见解，以激励未来的改进。我们的代码和数据将在 https://github.com/kkkkarry/LogicGraph 发布。

## Abstract
Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.