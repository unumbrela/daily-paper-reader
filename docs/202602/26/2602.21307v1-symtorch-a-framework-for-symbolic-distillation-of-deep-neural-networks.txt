Title: SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks

URL Source: https://arxiv.org/pdf/2602.21307v1

Published Time: Thu, 26 Feb 2026 01:05:18 GMT

Number of Pages: 19

Markdown Content:
# SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

Elizabeth S.Z. Tan 1 Adil Soubki 1 Miles Cranmer 1

# Abstract 

Symbolic distillation replaces neural networks, or components thereof, with interpretable, closed-form mathematical expressions. This approach has shown promise in discovering physical laws and mathematical relationships directly from trained deep learning models, yet adoption re-mains limited due to the engineering barrier of integrating symbolic regression into deep learn-ing workflows. We introduce SymTorch, a library that automates this distillation by wrapping neu-ral network components, collecting their input-output behavior, and approximating them with human-readable equations via PySR. SymTorch handles the engineering challenges that have hin-dered adoption: GPU-CPU data transfer, input-output caching, model serialization, and seamless switching between neural and symbolic forward passes. We demonstrate SymTorch across diverse architectures including GNNs, PINNs and trans-former models. Finally, we present a proof-of-concept for accelerating LLM inference by re-placing MLP layers with symbolic surrogates, achieving an 8.3% throughput improvement with moderate performance degradation. 

# 1. Introduction 

Deep learning models excel at analyzing large datasets, but remain largely uninterpretable. This has spurred extensive research into explainable AI. In the sciences, deep models are increasingly used for physical simulation (McCabe et al., 2025; Ohana et al., 2025; McCabe et al., 2024; Taira et al., 2025; Koblischke et al., 2025), where interpretability is particularly critical: the highly parameterized nature of deep networks makes it 

> 1

Department of Applied Mathematics and Theoretical Physics, University of Cambridge, United Kingdom. Correspondence to: Elizabeth S.Z. Tan <eszt2@cam.ac.uk >, Adil Soubki 

<as3591@cam.ac.uk >, Miles Cranmer <mc2473@cam.ac.uk >.

difficult to extract what physical laws the model has learned, and thus their ability to advance scientific understanding is fundamentally limited. One promising direction is mechanistic interpretability, which focuses on identifying circuits (Elhage et al., 2021; Marks et al., 2025; Dunefsky et al., 2024), neurons, and activation dimensions (Heimersheim & Nanda, 2024; Makelov et al., 2023) within specific model architectures. These methods enable researchers to identify which 

components matter for specific behaviors. SymTorch addresses a complementary question: what function does a component compute? In doing so, we provide a more holistic approach to model interpretability. In physics, interpretability means using concise equations that reliably explain phenomena (Kutz & Brunton, 2022). These equations are written in the language of mathematical operators and variables whose physical effects are under-stood. Newton’s laws exemplify this: F = ma is accurate in idealized settings yet deliberately inexact. Adding terms for drag or relativistic effects would sacrifice interpretability for minimal accuracy gains. Inspired by this physics-centric view of interpretability, SymTorch brings a symbolic perspective to Neural Network (NN) analysis 1. We use Symbolic Regression (SR) to distill NN components into human-readable mathematical formu-las. Symbolic distillation provides architecture-agnostic interpretability by approximating component behavior with closed-form expressions. This enables direct inspection of input-output mappings and analysis of how input variations affect outputs, including in out-of-distribution settings. The user can further optionally replace these components with the symbolic expression in the forward pass. Apractical consequence of this is potential inference speedup, as simple equations can be faster to evaluate than dense matrix operations. We explore this direction by presenting a framework to accelerate LLM inference through replacing Multi-Layer Perceptron (MLP) with symbolic surrogates. SymTorch is built on PyTorch and automates the process of applying SR to arbitrary model components. It interfaces with PySR (Cranmer, 2023), which uses genetic algorithms to search the space of symbolic expressions. SymTorch lowers the barrier to applying SR to deep learning models 

> 1

pip install torch-symbolic 

1 

> arXiv:2602.21307v1 [cs.LG] 24 Feb 2026 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks
> Figure 1. A cartoon depicting how SymTorch is used to perform symbolic distillation on model components. For a trained PyTorch model, SymTorch wraps around any NN component in the model. The user passes in sample data and in the forward pass, the inputs and outputs (I/O) of the component are collected. Using PySR, SymTorch performs a SR on the I/O to produce the best expressions approximating the behavior of the NN at different levels of complexity. Optionally, the user can select an equation from the Pareto front and replace the component with this chosen equation in the forward pass producing a hybrid neural-symbolic model.

by automating GPU–CPU data movement, caching model inputs and outputs, supporting native PyTorch model serialization, and allowing seamless transitions between the neural components and allowing seamless switching between neural components and symbolic surrogates during forward passes. Our work makes three main contributions: • SymTorch Library: We release an open-source Py-Torch library that automates symbolic distillation of NN components. SymTorch handles the engineer-ing challenges of GPU-CPU data transfer, activation caching, forward hook management, and hybrid neural-symbolic model serialization. Comprehensive docu-mentation accompanies the package, including usage examples for all features and Jupyter notebooks that reproduce the case studies in this paper 2.• Comprehensive Case Studies: To demonstrate Sym-Torch’s capabilities, we present case studies across diverse architectures: (i) extracting empirical force laws from Graph Neural Networks (GNNs) trained on particle dynamics, (ii) recovering PDE solutions from Physics-Informed Neural Networks (PINNs), (iii) analyzing LLM-learned arithmetic operations. 

> 2Documentation:
> https://symtorch.readthedocs.io/en/latest/ .

• SymTorch-enabled Exploratory LLM Speedup Framework: We introduce a proof-of-concept ap-proach to accelerate transformer inference by replacing MLP layers with symbolic surrogates, achieving mea-surable speedups with quantified accuracy tradeoffs. By lowering the barrier to symbolic distillation, SymTorch enables researchers to apply this interpretability technique without reimplementing the underlying infrastructure. 

# 2. Background & Related Work 

2.1. Analysis of Model Internals 

Mechanistic interpretability research focuses on causal in-tervention, activation analysis, and circuit discovery. Trans-formerLens (Nanda & Bloom, 2022) provides interfaces for inspecting transformer internals, while NNSight (Fiotto-Kaufman et al., 2025) offers general-purpose NN inter-pretability through deferred execution. These approaches primarily provide structural and causal insights into model behavior. In contrast, by fitting symbolic expressions di-rectly to component input–output mappings, SymTorch fo-cuses on recovering explicit functional forms. 

2.2. Interpreting Whole Model Behavior 

LIME and SHAP are common model-agnostic methods of explaining whole-model behavior. SHAP (Lundberg & Lee, 2017) assigns importance scores to inputs, whereas LIME 2SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

(Ribeiro et al., 2016) fits a local linear approximation to the model by perturbing the model inputs around a point of interest. 

Supralocal Interpretable Model-Agnostic Explanations 

Fong & Motani (2025) introduce Supralocal Interpretable Model-Agnostic Explanations (SLIME), a method similar to LIME. Rather than approximating local model behavior with linear models, SLIME employs symbolic surrogate models, enabling the capture of non-linear relationships. SLIME further augments the surrogate model training set with points sampled from the true data distribution, as op-posed to only training on randomly perturbed samples as in LIME. SymTorch provides a native implementation of this approach for out-of-the-box use with black-box models. 

2.3. Symbolic Interpretability 

Symbolic distillation as an interpretability tool remains un-derexplored despite promising results. Vafa et al. (2025) showed that transformers trained on planetary dynamics, although accurate in their predictions, fail to recover New-ton’s law of gravitation. Instead they exhibit sample-specific “implied” force laws, as revealed through SR of model pre-dictions. In contrast, Cranmer et al. (2020) showed that deep networks with appropriate inductive biases, trained on em-pirical N -body data, learn physically meaningful pairwise interaction forces. These forces can be recovered by fitting a symbolic expression to the input–output mappings of model component. This procedure not only recovers known force laws but also led to the discovery of a previously unknown empirical law in astronomical data. We reproduce and ex-tend this work in Section 4.2, demonstrating that SymTorch greatly simplifies such analyses. 

2.4. Symbolic Regression with PySR 

SymTorch extends PySR for deep learning applications. PySR performs multi-population evolutionary search over analytic expressions through mutation, crossover, simplifi-cation, and constant optimization. The algorithm maintains a Pareto front of expressions that best fit the data at different complexities. 

Problem Formulation For some metric L : RN × RN →

R, inputs xi = ( x1,i , · · · , x d,i ) ∈ X ⊆ Rd and target variables yi ⊂ Y ∈ R for i = 1 , · · · N , SR aims to find gg = arg min 

> g∈S
> N

X

> i

L(yi, g (xi)) , (1) where S is the set of closed-form analytic expressions. Met-ric L is the data-fitting loss (eg. mean-squared error) that often times contains a penalty on the complexity of g. In PySR, complexity is measured as the number of nodes when 

g is written out as an expression tree. 

Choosing the Best Equation PySR selects the best equa-tion by maximizing the fractional drop in log mean absolute error relative to an increase in model complexity. Specifi-cally, it chooses expression j along on the Pareto front that maximizes the score given by score j = − log( loss j /loss j−1)

complexity j − complexity j−1

. (2) 

# 3. Methods 

In this section, we describe how SymTorch wraps NN com-ponents, collects input–output activations, performs SR, and replaces model blocks with symbolic surrogates. We fur-ther explain SymTorch’s built-in SLIME implementation for explaining local model behavior using analytical equations. 

3.1. Distilling Neural Networks with SymTorch 

SymbolicModel is the entry point for all SymTorch func-tionality. Inheriting from PyTorch’s nn.Module , it can wrap around both PyTorch NNs and callable functions, pro-vided these functions form a mapping 

f : xi → yi, (3) where xi = ( x1,i , · · · , x d,i ) and yi = ( y1,i , · · · , y D,i ).SymTorch expands the SR problem in Equation (1) for many-dimensional outputs by fitting a separate symbolic model to each output dimension. We define the original function f as a model block . For PyTorch NNs, the wrapper registers forward hooks to record the block’s inputs and outputs over user-supplied data. The distill method performs SR by calling PySR with user-specified parameters (operators, genetic algorithm hy-perparameters, fitness function). The outputs are fitted as closed-form equations of the block’s input variables. Users may pass variable transforms to create derived fea-tures (e.g., r = p∆x2 + ∆ y2), which can improve SR efficiency by structuring the search space. 

Switching to Symbolic Approximations After fitting, 

switch to symbolic replaces the original block with expressions selected from the Pareto front based on their complexity, so that subsequent forward passes evaluate the symbolic equations in place of the original computation. If no complexity is specified, SymTorch chooses the best ex-pression according to Equation (2). The model can continue training with fixed equations while other weights update. 

switch to block restores the original function. Since the model remains a native PyTorch module, it can be used with torch.compile for additional optimization. 3SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

Saving and Caching Inputs and outputs are cached, al-lowing SR to be rerun without requiring additional forward passes through the model. SymTorch further works with PyTorch’s native model saving and loading functions. 

3.2. SLIME Implementation 

SymTorch includes a native implementation of SLIME (Fong & Motani, 2025) for local model explanations. Given a black-box model f and a point of interest x∗, we fit a sym-bolic surrogate s ∈ S to approximate f in a neighborhood of x∗.The training dataset D consists of: (1) the J nearest neigh-bors to x∗ from the data distribution, and (2) Nsynthetic 

Gaussian-sampled points around x∗. The Gaussian vari-ance defaults to half the variance of the J neighbors, so the approximation region is controlled by J. We fit s by minimizing: 

s = arg min 

> s∈S

X

> zi∈synthetic

πx(zi)[ f (zi) − s(zi)] 2

+ M X 

> zj∈neighbors

[f (zj ) − s(zj )] 2,

where πx(zi) = exp( −|| x0 − xi|| 2/σ 2) is a proximity-weighted kernel and M weights points from the true dis-tribution. The distill method supports SLIME via a built-in flag.   

> Figure 2. Approximating local model behavior with SLIME. For a complex non-linear model, we choose the point of interest x∗. We sample points around this region and fit a symbolic model to these points.

# 4. Case Studies and Performance 

In this section, we present four specific case studies to demonstrate SymTorch’s usage on a range of different Py-Torch architectures. 

4.1. Symbolic Surrogates of Transformer MLPs 

A broad line of work aims to reduce overall inference cost and latency through techniques such as quantization (Dettmers et al., 2022), pruning (Lagunas et al., 2021), and speculative decoding (Leviathan et al., 2023). Specifically, MLP layers constitute a substantial portion of transformer inference compute (Wei et al., 2024). We explore a new approach to decreasing inference compute in transformers: replacing MLP layers with symbolic expressions via Sym-Torch 3.The accuracy-speed tradeoff we achieve (8.3% speedup for a perplexity increase of 3.14 from baseline 10.62) may not be competitive with established methods. However, we present this framework because: (1) it demonstrates Sym-Torch’s applicability to LLMs, (2) the primary driver of the performance degradation is the dimensionality reduction step whilst the replacement of the MLP with the SymTorch symbolic function only contributes to 1% of the perplex-ity increase, so (3) the approach may improve with better dimensionality reduction, and finally (4) regardless of the performance degredation, our SymTorch neuro-symbolic model is on the Pareto front of token throughput versus perplexity on Wikitext-2, when compared to similarly-sized open-source LLMs (see Figure 5). We further discuss these limitations and points for further research in this section.  

> Figure 3. Framework for reducing inference compute in trans-former models by replacing MLP layers with symbolic surrogate models. Inputs to the MLP layer undergo dimensionality reduction via PCA. A symbolic model maps the inputs to the outputs. Out-put activations have their dimensionality increased, again through PCA, to match model dimensionality.

Framework We first apply Principal Component Analy-sis (PCA) to the input and output activations of each MLP block. PCA provides a linear compression that preserves the dominant variance directions, allowing us to retain the most informative components of the representation while significantly reducing dimensionality. Despite the decrease in performance caused by this dimensionality reduction, this step is required to make SR more tractable. Then, we distill the MLP into a symbolic model using SymTorch. Generally, we want to be harsher on reducing the dimensionality of the MLP inputs as opposed to the outputs because SR scales 

> 3Source code:
> https://github.com/astroautomata/LLM_PCA .

4SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

exponentially with the number of input variables, and lin-early with the number of output variables (as each output dimension requires a new expression). 

Setup We used the Qwen2.5-1.5B-Instruct model (Qwen, 2024) for this experiment and the Wikitext-2-v1 dataset (Merity et al., 2016). The dataset was split into a training and test set each consisting of 178k tokens. The training set was used to train the PCA models and provide the sample data for the SR. We used perplexity Perplexity = exp − 1

N

> N

X

> i=1

log p(xi | x<i )

!

, (4) to quantify the changes in performance of the LLM. We chose to intervene on MLP layers (SwiGLU activations) 7, 14 and 21. Before performing SR, we first determine the minimum number of principal components required to preserve the behavior of the model. To this end, we vary the dimensionality used for PCA on both the input and output activations of the MLP and measure the resulting change in model perplexity. Specifically, the input activations are projected to a lower-dimensional subspace via PCA and then reconstructed before being passed through the MLP; similarly, the MLP outputs are projected and reconstructed prior to being passed to the remainder of the model. All interventions on the transformer model are performed using forward hooks and pre-forward hooks, enabling localized replacement of MLP components without modifying the surrounding architecture. Additional experimental details and extended results are provided in Section D. 

PCA Sensitivity Analysis Figure 4 shows the changes in perplexity as we varied the number of principal components of the input and output to the MLP. Just for reference, the original SwiGLU projects inputs from a 1536 dimensional subspace to an increased 8960 dimensional subspace and back down again and the baseline model perplexity on the test set is given in Table 1. Notably, for a fixed number of PCA components retained at the input, the change in perplexity initially decreases as the number of output PCA components increases, before rising again. One possible explanation is that moderate output compression removes low-variance or noisy directions while preserving the domi-nant functional subspace, whereas excessive expansion rein-troduces poorly conditioned or misaligned directions that degrade downstream representations. 

Symbolic Surrogate Model Based on the PCA sensitivity analysis, we select 32 principal components for the MLP inputs and 8 for outputs. This configuration incurs a notable perplexity increase (+3.11 from baseline 10.62) but reduces the SR problem to tractable dimensions. Using SymTorch, we fit symbolic surrogates mapping re-duced inputs to reduced outputs. After substitution, the         

> Figure 4. Change in test set perplexity under PCA compression and reconstruction of MLP activations relative to the baseline per-plexity of 10.62. For layers 7, 14, and 21, the MLP inputs are projected to a lower-dimensional subspace via PCA and then re-constructed prior to the MLP, while the MLP outputs are similarly projected and reconstructed before being passed to the remainder of the model.
> Table 1. Perplexity comparison between baseline, PCA+MLP, PCA+SymTorch and Control using the test set. There were 32 and 8 principal components for the input and output PCAs respec-tively. For the Control result, the MLPs were replaced with identity functions. Perplexity Baseline
> ∆Perplexity PCA+MLP
> ∆Perplexity PCA+SymTorch
> ∆Perplexity Control 10.62 +3.11 +3.14 +6.97

perplexity increase (+3.14) is comparable to PCA compres-sion alone (+3.11), as shown in Table 1. This indicates the symbolic model captures the MLP’s behavior within the re-duced subspace, with minimal additional degradation from the symbolic approximation itself. 

Inference Speed Benchmarking To measure the impact of this symbolic surrogate intervention on inference speed, we compute token throughput of the model in three con-ditions: (1) a baseline with no intervention, (2) a control where MLPs are replaced by an identity function, and (3) with the symbolic surrogates replacing the MLPs. After 5 warm up passes, we recorded the time elapsed for 100 forward passes of the test set through the model with KV caching disabled. These 100 times are averaged to estimate token throughput for each condition. This benchmark was run on an Nvidia A100-SXM4-80GB GPU for compari-son. The results find that SymTorch distillation of 3/28 layers delivers an 8.3% increase in token throughput over the baseline, roughly matching the inference speed of the control condition. We performed an identical inference speed benchmarking for various popular and small open-5SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks  

> Figure 5. Inference throughput (in tokens per second) versus perplexity on the test set for various language models. The test set is made up of a random chunk of 175k characters from the Wikitext-2-v1 dataset.

source LLMs, and measured their perplexity on the test set. The results are shown in Figure 5. Notably, even with the perplexity increase from symbolic replacement, the modi-fied Qwen2.5-1.5B model remains on the Pareto front of the perplexity-throughput tradeoff. 

4.2. Discovering Empirical Physical Laws from Symbolic Distillation of Models with Inductive Biases 

We demonstrate SymTorch on a reproduction and slight ex-tension of a prior work by Cranmer et al. (2020): training a GNN on empirical particle data and performing SR on the latent representations of the trained model to recover true interaction laws 4. Complete details of the replication includ-ing training details, experimental results, and framework limitations, are provided in Section E. 

Model Architecture We follow the notation as outlined in Battaglia et. al. 2018. There are two MLPs involved in the GNN. The first is the edge model (or edge function), 

ϕe : V × V → E , where V ⊂ RLv

and E ⊆ RLe

. Lv and 

Le are the number of node features and the dimensionality of the messages respectively. In a forward pass of the GNN, the edge model takes the node features, v ∈ V , of two connected nodes as inputs and outputs the edge message, 

e′ 

> k

∈ E , where k denotes the edge. We concatenate the node features when inputting them into the MLP. This function can be written as 

e′ 

> k

= ϕe(vrk , vsk ), (5) 

> 4Source code:
> https://github.com/astroautomata/SymTorch_ symbolic_distillation_GNNs

where vrk denotes the features of the receiving node, and 

vsk denotes the sending node of edge k. The messages to receiving node i are aggregated through an element-wise summation, 

¯e′ 

> i

= X 

> j̸=i

ϕe(vi, vj ), (6) where ¯e′ 

> i

is the aggregated message. The second MLP involved in the GNN is the node model (or node function), ϕv : V × E → D , where D ⊆ RD and D is the dimensionality of the target variable, the updated node features. This model outputs the updated node features for a specific node. It takes the node features and the aggregated message for that node as an input and calculates the node update, ˆv′

> i

.

ˆv′ 

> i

= ϕv (vi, ¯e′

> i

), (7) 

Setup In our setup, the node features contain information about a particle: 

vi = [ xi, y i, ˙xi, ˙yi, q i, m i], (8) where xi, y i are the 2-D positions of the particle i, ˙xi and ˙yi

are the discretised velocities (distance moved in simulation time ∆t), qi is the charge and mi is the mass. Hence, the dimensionality of the node features, Lv , depends on the dimensionality of the system. The inputs to the model com-prise this information. The target variables in this pipeline, 

ˆv′

> i

, are the individual particle accelerations. Hence the di-mensionality of the updated node is equal to the dimension-ality of the system (2-D). We test out this framework to on particles acting under the following pair-wise interaction forces: (a) gravitational force; (b) spring force; (c) force that scales as 1/r and (d) force that scales as 1/r 2, where r is the displacement between the two particles. During training, the cost function was the mean absolute error between the predicted and true particle accelerations. 

Edge Messages as Forces For GNNs trained in this man-ner, it can be shown that the edge messages are linear combi-nations of the true forces acting between particles, provided the dimensionality of the edge messages are the same as that of the system. The full derivation for this result is in Section E.1. Because of this, we want to encourage sparse representation of the edge messages. 

Model Variants We trained several variants of the GNN to explore different methods to encourage sparse representa-tion of the edge messages: (1) Standard, with 100 message components; (2) Bottleneck, with the number of message components matching that of the system (in this case 2); (3) L1, which is the same as Standard except we add a L1 regu-larization term to the messages; (4) Kullback-Leibler (KL), the same as Standard except adding a KL regularization to 6SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

the messages with respect to a Gaussian prior; (5) Pruning, the dimensionality of the edge messages are incrementally reduced during training until the dimensionality matches that of the system. The Pruning variant is our extension of the original work. 

Symbolic Distillation with SymTorch For a trained GNN, SymTorch wraps the edge model to enable direct symbolic distillation of the MLP. Using this framework, we reproduce the symbolic recovery results reported in (Cran-mer et al., 2020). Our pruning-based variant, implemented natively within SymTorch, achieves performance compara-ble to the best-performing bottleneck model, successfully distilling the true interaction laws across all systems con-sidered. SymTorch substantially simplifies experiments of this kind by removing the need for manual extraction of input–output activations from model components. In addi-tion, the framework supports user-defined transformations of input variables, which further improve the tractability of SR in this setting. We note that the use of a GNN is essential for this problem, as applying symbolic regression directly to the raw dataset fails to recover the true interaction forces; assume a dataset {(yi, {x1,i , · · · , x100 ,i }} i=1 ··· N where yi

is the target variable which could for example be the ac-celeration of a single particle in a system of 100 particles. Assume the true relationship to be a composite function 

a

P 

> j

b(xi, xj )



. If the SR needs to consider N equations for functions a and b then we must consider N 2 equations to get the true expression for y. In contrast, when a and b

can be fitted independently - as in our GNN architecture, where separate MLPs learn each function - the search space decomposes, and only 2N candidate expressions need to be considered. Hence the GNN makes SR much more tractable (Cranmer et al., 2020). 

4.3. Wrapping Whole Models 

Here we list case studies that involve wrapping entire models end-to-end with SymTorch 5.4.3.1. E XTRACTING PDE S OLUTIONS FROM A PINN We compared a Physics-Informed Neural Network (PINN) with a standard NN of identical architecture for predicting temperatures governed by the 1-D heat equation: 

∂u ∂t = α ∂2u∂x 2 , (9) where u is the temperature, x ∈ (0 , 1) is a 1-D spatial coor-dinate, t ∈ (0 , 1] is time and α is the diffusion coefficient. While both networks were trained directly on the same data, the PINN incorporated knowledge of the system’s govern-

> 5See documentation for annotated Juypter notebooks for these case studies.

ing Partial Differential Equation (PDE), Initial Conditions (ICs), and Boundary Conditions (BCs) via additional regu-larization terms in its loss. Trained on only ten data points, the PINN achieved substantially better predictions than the standard network, shown in Figure 6, as its inductive bias enforced physically consistent solutions. Using SymTorch, we further distilled the trained PINN into a closed-form analytic expression, successfully recovering the solution to the 1-D heat equation. Using a PINN is advantageous com-pared to applying SR directly to the dataset, as the PINN can generate additional training data that guides symbolic regression toward equations consistent with the underlying physical solution. More information on this experiment can be found in Section F.   

> Figure 6. Comparison of regular NN and PINN predictions and the true solution of the 1D heat equation ( α= 0 .2).

4.3.2. I NTERPRETING LLM-L EARNED OPERATIONS 

LLMs often fail at elementary numerical tasks, such that commercial systems rely on external tool calls for reliable computation (Dziri et al., 2023; Golkar et al., 2024). In this case study, we use SR to analyze the LLM-learned operations to analyze the true function computed by the LLM. Symbolic distillation is well-suited to this task: rather than treating the LLM as a black-box that is only right or wrong, we can recover an explicit analytic approximation of the computation it is performing. This enables direct inspection of systematic numerical errors and reveals how, and where, the model’s internal heuristics deviate from the intended operations. 

Setup We study the model Llama-3.2-1B-Instruct as a representative small LLM and analyze the operation the model has learnt for (a) addition of two 3-digit numbers; (b) multiplication of two 3-digit numbers; (c) counting the number of 1s in a string of 1s and 0s; (d) converting Celsius to Fahrenheit. More information on the experiment includ-ing specific prompts and SR parameters can be found in Section G. 

Results The results of this analysis can be found in Ta-ble 2. It should be noted that the correct equation was present in the Pareto front of the SR for all of these tasks except counting. However it was not the chosen ’best’ equa-tion suggesting that the LLM gets these tasks mostly correct with the addition of some systematic errors. It is known 7SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

that the expressivity of finite-precision transformers (Chiang et al., 2023), or even log-precision transformers (Merrill & Sabharwal, 2023), is limited to variants of first-order logic with counting or majority quantifiers. These logics cannot, in general, implement an algorithm to perform exact count-ing over arbitrary-length inputs, which might be related to why this task proved more difficult. 

Table 2. Symbolically distilled operations LLM-learned operations. The ’best’ equation, as determined by Equation (2), is shown here. The ϵ denotes a small ( ≪ 1) term. Operation Expected Equation LLM-Learned Operation Addition x0 + x1     

> x1·((inv( x0−70 .16) + 1 .07) +(inv(sin( x1) + 0 .80) + x1)
> ·(−ϵ)) x0

Multiplication x0x1 x1(inv(1 .66 − 31 .3 sin( x0)) + x0)

Counting x0 + · · · + x5           

> (−0.11 x0+ 1 .67)( x1+(x1(x0+ 1 .43) −2.50)( x2−0.53) (x0+x3−2.15) + x2)

Temperature Conversion 95 x + 32 inv( x0 − 168 .86)inv( x0 − 129 .65) + inv( x0 − 51 .87) x0 + 33 .79 

# 5. Discussion 

SymTorch demonstrates that symbolic regression can serve as a practical interpretability tool when integrated with deep learning frameworks. Our case studies show successful sym-bolic distillation across GNNs, PINNs, and LLMs, recov-ering known physical laws and revealing LLM arithmetic biases. We have further demonstrated a novel framework to speed up LLM inference by replacing MLP blocks with sym-bolic expressions found using SymTorch. Despite the perfor-mance degradation due to the reduced expressive capacity of symbolic expressions relative to dense neural layers, the resulting neuro-symbolic model remains on the Pareto front of token throughput versus perplexity when compared to popular LLMs of similar scale on the WikiText-2 bench-mark. By handling the engineering complexity of GPU-CPU trans-fer, activation caching, and hybrid model management, Sym-Torch lowers the barrier to symbolic distillation. We hope this accessibility enables SR to become a practical tool for uncovering the functional structure learned by deep models. 

5.1. Limitations & Future Work Computation: SR is a brute-force process. Runtime grows exponentially with the number of variables, operator set size, and dataset size. For complex NNs, obtaining ac-curate symbolic approximations may require large datasets and extended SR runs. We also perform SR per output dimension, which becomes expensive for wide outputs. 

Highly Complex Functions: For highly complex model components, SR may struggle to produce accurate approx-imations. Nevertheless, it remains one of the few viable routes to functional interpretability; without it, such compo-nents would remain entirely opaque. 

Complexity Measures & SR Hyperparameters: PySR’s complexity measure depends solely on the number of nodes in the expression tree, which can produce expressions that are mathematically equivalent in complexity but differ sig-nificantly in interpretability. For instance, nested functions like sin(sin x) have the same complexity as x + 2 . Explicit constraints through hyperparameter tuning may be neces-sary to ensure human-readable outputs. 

Symbolic Surrogates of Transformer MLPs: The frame-work we propose for accelerating LLM inference may not be competitive with existing approaches. In particular, the per-formance degradation introduced by symbolic approxima-tion is non-trivial. Furthermore, we evaluate model quality only on WikiText-2, which matches the training distribution of the symbolic surrogates, and performance may degrade further under distribution shift or on downstream tasks. De-spite these limitations, there exist important regimes where throughput is prioritized over general-purpose flexibility. In many deployment settings, models are fine-tuned for a single downstream task, for example Low-Rank Adaptation (LoRA) (Hu et al., 2021), and are not required to general-ize broadly. In such scenarios, sacrificing some expressive capacity in exchange for substantial inference speedups may represent a favorable trade-off, particularly for latency-critical or resource-constrained applications. We emphasize that this framework is in no way optimized; rather, its pri-mary purpose is to illustrate the experimental workflows made accessible by SymTorch. Hence, possible areas for future work to improve this framework include, • Improved Dimensionality Reduction: The PCA-based compression in Section 4.1 is the major source of performance degradation. Learned linear projec-tions may preserve more relevant information while maintaining SR tractability. • Cross-Domain Generalization: The symbolic surro-gates for LLM components are trained and evaluated on the same distribution, leaving open the question of how well such surrogates generalize across domains. We hope to determine whether domain-agnostic sym-bolic approximations are feasible, or whether task- and domain-specific surrogates are necessary. • Framework Optimization and Extension: We arbi-trarily replace three MLP layers in the LLM, and future work will explore which layers are most amenable to symbolic approximation and how many layers can be 8SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

replaced before performance degrades substantially. We further want to test out this framework on larger LLMs. 

# Acknowledgements 

The authors would like to thank Rachel C. Zhang for com-ments on the draft of this paper. Elizabeth S.Z. Tan’s work is supported by the Aglaia Family Office. Adil Soubki thanks the UK Science and Technology Facilities Council (STFC) for a Ph.D. studentship. Miles Cranmer is grateful for support from the Schmidt Sciences AI2050 Early Career Fellowship and the Isaac Newton Trust. 

# References 

Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., Gulcehre, C., Song, F., Ballard, A., Gilmer, J., Dahl, G., Vaswani, A., Allen, K., Nash, C., Langston, V., Dyer, C., Heess, N., Wierstra, D., Kohli, P., Botvinick, M., Vinyals, O., Li, Y., and Pascanu, R. Relational inductive bi-ases, deep learning, and graph networks, 2018. URL 

https://arxiv.org/abs/1806.01261 .Chiang, D., Cholak, P., and Pillay, A. Tighter bounds on the expressivity of transformer encoders, 2023. URL 

https://arxiv.org/abs/2301.10743 .Cranmer, M. Interpretable machine learning for science with pysr and symbolicregression.jl, 2023. URL https: //arxiv.org/abs/2305.01582 .Cranmer, M., Sanchez-Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., and Ho, S. Discovering symbolic models from deep learning with inductive bi-ases, 2020. URL https://arxiv.org/abs/2006. 11287 .Cranmer, M. D. Discovering symbolic models from deep learning with inductive biases code repository, 2020. URL https://github.com/ MilesCranmer/symbolic_deep_learning .Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022. URL https://arxiv.org/abs/ 2208.07339 .Dunefsky, J., Chlenski, P., and Nanda, N. Transcoders find interpretable llm feature circuits, 2024. URL https: //arxiv.org/abs/2406.11944 .Dziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y., West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., Sanyal, S., Welleck, S., Ren, X., Ettinger, A., Harchaoui, Z., and Choi, Y. Faith and fate: Limits of transformers on compositionality, 2023. URL https://arxiv.org/ abs/2305.18654 .Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A math-ematical framework for transformer circuits. Trans-former Circuits Thread , 2021. https://transformer-circuits.pub/2021/framework/index.html. Fey, M. and Lenssen, J. E. Fast graph representation learning with pytorch geometric, 2019. URL https://arxiv. org/abs/1903.02428 .Fiotto-Kaufman, J., Loftus, A. R., Todd, E., Brinkmann, J., Pal, K., Troitskii, D., Ripa, M., Belfki, A., Rager, C., Juang, C., Mueller, A., Marks, S., Sharma, A. S., Luc-chetti, F., Prakash, N., Brodley, C., Guha, A., Bell, J., Wal-lace, B. C., and Bau, D. Nnsight and ndif: Democratizing access to open-weight foundation model internals, 2025. URL https://arxiv.org/abs/2407.14561 .Fong, K. S. and Motani, M. Slime: Supralocal in-terpretable model-agnostic explanations via evolved equation-based surrogates. In Proceedings of the Ge-netic and Evolutionary Computation Conference Com-panion , GECCO ’25 Companion, pp. 267–270, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400714641. doi: 10.1145/ 3712255.3726721. URL https://doi.org/10. 1145/3712255.3726721 .Golkar, S., Pettee, M., Eickenberg, M., Bietti, A., Cranmer, M., Krawezik, G., Lanusse, F., McCabe, M., Ohana, R., Parker, L., Blancard, B. R.-S., Tesileanu, T., Cho, K., and Ho, S. xval: A continuous numerical tokenization for scientific language models, 2024. URL https:// arxiv.org/abs/2310.02989 .Heimersheim, S. and Nanda, N. How to use and interpret ac-tivation patching, 2024. URL https://arxiv.org/ abs/2404.15255 .Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv. org/abs/2106.09685 .Kingma, D. P. and Ba, J. Adam: A method for stochastic op-timization, 2017. URL https://arxiv.org/abs/ 1412.6980 .9SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

Koblischke, N., Parker, L., Lanusse, F., Morales, I. E., Bovy, J., and Ho, S. Semantic search for 100m+ galaxy im-ages using ai-generated captions, 2025. URL https: //arxiv.org/abs/2512.11982 .Kutz, J. and Brunton, S. Parsimony as the ultimate reg-ularizer for physics-informed machine learning. Non-linear Dynamics , 107:1–17, 01 2022. doi: 10.1007/ s11071-021-07118-3. Lagunas, F., Charlaix, E., Sanh, V., and Rush, A. M. Block pruning for faster transformers, 2021. URL https: //arxiv.org/abs/2109.04838 .Leviathan, Y., Kalman, M., and Matias, Y. Fast inference from transformers via speculative decoding, 2023. URL 

https://arxiv.org/abs/2211.17192 .Lundberg, S. and Lee, S.-I. A unified approach to interpret-ing model predictions, 2017. URL https://arxiv. org/abs/1705.07874 .Makelov, A., Lange, G., and Nanda, N. Is this the sub-space you are looking for? an interpretability illusion for subspace activation patching, 2023. URL https: //arxiv.org/abs/2311.17030 .Marks, S., Rager, C., Michaud, E. J., Belinkov, Y., Bau, D., and Mueller, A. Sparse feature circuits: Discovering and editing interpretable causal graphs in language mod-els, 2025. URL https://arxiv.org/abs/2403. 19647 .McCabe, M., Blancard, B. R.-S., Parker, L. H., Ohana, R., Cranmer, M., Bietti, A., Eickenberg, M., Golkar, S., Krawezik, G., Lanusse, F., Pettee, M., Tesileanu, T., Cho, K., and Ho, S. Multiple physics pretraining for physical surrogate models, 2024. URL https://arxiv.org/ abs/2310.02994 .McCabe, M., Mukhopadhyay, P., Marwah, T., Blancard, B. R.-S., Rozet, F., Diaconu, C., Meyer, L., Wong, K. W. K., Sotoudeh, H., Bietti, A., Espejo, I., Fear, R., Golkar, S., Hehir, T., Hirashima, K., Krawezik, G., Lanusse, F., Morel, R., Ohana, R., Parker, L., Pettee, M., Shen, J., Cho, K., Cranmer, M., and Ho, S. Walrus: A cross-domain foundation model for continuum dynamics, 2025. URL 

https://arxiv.org/abs/2511.15684 .Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models, 2016. Merrill, W. and Sabharwal, A. A logic for expressing log-precision transformers. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems , volume 36, pp. 52453–52463. Curran Associates, Inc., 2023. URL https: //proceedings.neurips.cc/paper_files/ paper/2023/file/a48e5877c7bf86a51395. ab23b360498-Paper-Conference.pdf .Nanda, N. and Bloom, J. Transformerlens. https: //github.com/TransformerLensOrg/ TransformerLens , 2022. Ohana, R., McCabe, M., Meyer, L., Morel, R., Agocs, F. J., Beneitez, M., Berger, M., Burkhart, B., Burns, K., Dalziel, S. B., Fielding, D. B., Fortunato, D., Goldberg, J. A., Hi-rashima, K., Jiang, Y.-F., Kerswell, R. R., Maddu, S., Miller, J., Mukhopadhyay, P., Nixon, S. S., Shen, J., Wat-teaux, R., Blancard, B. R.-S., Rozet, F., Parker, L. H., Cranmer, M., and Ho, S. The well: a large-scale col-lection of diverse physics simulations for machine learn-ing, 2025. URL https://arxiv.org/abs/2412. 00568 .Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-napeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011. Qwen. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.github.io/blog/ qwen2.5/ .Ribeiro, M. T., Singh, S., and Guestrin, C. ”why should i trust you?”: Explaining the predictions of any classi-fier, 2016. URL https://arxiv.org/abs/1602. 04938 .Taira, K., Rigas, G., and Fukami, K. Machine learning in fluid dynamics: A critical assessment, 2025. URL 

https://arxiv.org/abs/2508.13430 .Vafa, K., Chang, P. G., Rambachan, A., and Mullainathan, S. What has a foundation model found? using inductive bias to probe for world models, 2025. URL https: //arxiv.org/abs/2507.06952 .Wei, X., Moalla, S., Pascanu, R., and Gulcehre, C. Building on efficient foundations: Effectively training llms with structured feedforward layers, 2024. URL https:// arxiv.org/abs/2406.16450 .10 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

# A. Appendix A B. Symbolic Regression with PySR Detail 

B.1. Algorithm Overview 

PySR implements the following genetic algorithm (summarized from (Cranmer, 2023)): 1. We begin with several independent populations of individual equations. This allows expressions in each population to evolve simultaneously. 2. In each population, a tournament selection process is run: a random subset of individuals (usually two) are evaluated on their ’fitness’, a metric combining both expression accuracy and complexity. The complexity of an expression is determined by the number of nodes on the binary expression tree. 3. The fittest individual in the subset is chosen as the winner with probability p; otherwise the next fittest is chosen with the same probability, and so on. 4. A copy of the winning individual undergoes some form of mutation, where a node on the expression tree can be changed, a node added or removed. Or the individual may undergo a cross-over operation with the next fittest expression, where parts of the expression tree may swap between these two individuals. This mutation may be accepted or rejected with a probability relating to the fitness of the mutated expression. 5. A set number of tournaments constitutes a round of evolution for the population. At the end of each evolution round, expressions may be simplified to an equivalent form (for example x + x + x → 3 ∗ x) or the constants may be optimized. 6. After a specified number of evolutions, individuals may migrate between populations. At each iteration, the Pareto front is updated with the best-performing equations at different complexity levels. 

# C. SymTorch Extra Details 

C.1. Default PySR Parameters 

Users configure SR via the sr params dictionary in distill (defaults in Table 3). These parameters go directly to PySR’s PySRRegressor . Parameters for the fit method (e.g., variable names or complexities) use the fit params 

dictionary.  

> Table 3. Default SymTorch SR configurations.

SR Parameter Configuration 

Binary operators +, *

Unary operators inv(x)=1/x, sin, exp 

Extra sympy mappings "inv": lambda x: 1/x 

Number of iterations 400 Complexity of operators sin: 3, exp: 3

C.2. Saving SR Results and Symbolic Models 

SymTorch saves PySR SR results in SR output/mlp name , organized by output dimension and timestamp. SymTorch works seamlessly with PyTorch’s torch.save and torch.load functions. 

# D. Symbolic Surrogates of Transformer MLPs Details 

D.1. Setup Dataset We used the Wikitext-2-v1 dataset which was already split into train, test and validation sets. In this experiment, only the train and validation (quoted as the ’test set’ in this paper) sets were used. We further selected a random chunk of 11 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

750k characters from the each dataset which corresponded to approximately 178k tokens. The PCA and symbolic models were trained purely on this subset of the validation set. 

Training the PCA To train the PCA models, we used Scikit-learn (Pedregosa et al., 2011). The input data to the PCA were centred but not whitened (the PCA components were not scaled to have unit variance). The explained variance ratio from the PCA sensitivity analysis is shown in Figure 7.  

> Figure 7. The explained variance ratio for the PCA models trained on the pre-activations and activations of the MLP layers for the PCA sensitivity analysis.

Training the Symbolic Models After the PCA models were trained, we defined a Python callable function that took the MLP reduced dimensionality pre-activations as input and returned the MLP reduced dimensionality activations. SymTorch was used to wrap and perform SR on this function. We used a random subset of 6000 samples from the training set to perform the SR. The SR parameters used were the default parameters in Table 3 but we increased the number of iterations to 5000. 

Results and Benchmarking When calculating the perplexity values across the dataset, chunk sizes of 1048 were used. The perplexity results of this experiment on the training set are shown in Table 4. We further ran an inference speed versus  

> Table 4. Perplexity comparison between baseline, PCA+MLP, PCA+SymTorch and Control using the training set. There were 32 and 8 principal components for the input and output PCAs respectively. For the Control result, the MLPs were replaced with identity functions.

Perplexity Baseline 

∆ Perplexity PCA+MLP 

∆ Perplexity PCA+SymTorch 

∆ Perplexity Control 10.74 +3.60 +3.63 +8.12 perplexity on the Wikitext validation set benchmarking on popular open-source LLMs, as shown in Figure 5. 

SR Wall Clock Time To fit the SymTorch surrogate model to the three MLP blocks it took between 7-8 hours on an Apple M4 Max SoC (14-core CPU: 10 performance + 4 efficiency cores, 36 GB unified memory). 

# E. Discovering Empirical Physics Laws from Symbolic Distillation of Models with Inductive Biases Details 

E.1. Derivation: Edge Messages as Linear Transformations of the True Forces 

For a GNN that has learned to predict accelerations from particle properties, the message vectors are linear transformations of the true forces, provided that the message dimension is equal to the dimensionality of the forces. The mathematical derivation for this is, as first presented in (Cranmer et al., 2020), is shown below. In Newtonian mechanics, the resultant force, ¯fi, acting on particle i is equal to the sum of the individual forces, fk:

X

> k

fk = ¯fi. (10) 12 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks  

> Table 5. Perplexity on the test set (as used in Section 4.1) and inference benchmarking for various open-source language models.

Model Perplexity on Validation Set Avg. Latency (ms) p95 Latency (ms) Throughput (tokens/s) Qwen2.5-1.5B (Baseline) 10.62 209.89 211.76 4878.82 Qwen2.5-1.5B (Symbolic) 13.76 193.89 195.34 5281.42 Qwen2.5-3B-Instruct 9.91 397.85 401.55 2573.84 Qwen2.5-7B-Instruct 8.87 866.06 867.71 1182.36 Llama-3.1-8B-Instruct 8.52 891.84 892.89 1148.18 Llama-3.2-1B-Instruct 14.21 169.31 170.08 6048.19 Llama-3.2-3B-Instruct 12.32 414.51 416.03 2470.36 SmolLM2-1.7B-Instruct 10.33 240.48 240.89 4258.23 SmolLM3-3B 10.20 392.89 393.52 2606.35 TinyLlama-1.1B-Chat-v1.0 14.80 149.00 149.44 6872.68 If we ignore the particle mass, the node model predicts the resultant force on the receiver particle rk:

¯fi = ˆv′ 

> i

= ϕv (vi, ¯e′

> i

) = ϕv vi, X 

> rk=i

e′

> k

!

. (11) If we only consider a single interaction, and hence a single edge, the force is: 

¯fi = fk,r k =i = ϕv (vi, e′ 

> k,r k=i

). (12) Again, the resultant force is the sum of the individual forces, so we can use the above equation in the many-particle case and equate this to Equation (11). Explicitly, 

X 

> rk=i

ϕv (vi, e′

> k

) = ϕv vi, X 

> rk=i

e′

> k

!

= ¯fi, (13) which demonstrates that ϕv is a linear function in its second argument: 

ϕv (vi, e′ 

> a

+ e′

> b

) = ϕv (vi, e′

> a

) + ϕv (vi, e′

> b

). (14) Provided ϕv is invertible in e′

> k

, which is true when the dimensionality of e′ 

> k

matches the dimensionality of ˆv′

> i

, then you can invert Equation (12): 

e′ 

> k

= ( ϕv (vi, ·)) −1(fk). (15) Hence, the messages e′ 

> k

are just linear transformations of the true forces fk. If we constrain the message dimensionality to match that of the physical system, we can fit the learned messages using a linear regression on the true forces. A strong linear correspondence indicates that the model has successfully captured the underlying physical forces. 

E.2. Model Variants 

Details of the GNN variants trained in this case study are as follows: 1. Standard . Consisted of a GNN where the message dimension, Le, was set to 100. 2. Bottleneck . The message dimension was set to match the dimensionality of the system, which was 2 in all the experiments. 3. L1 . We applied L1 regularization, Le to the edge messages, 

LL1 = αL1

N eN e−1X

> k=0

|e′

> k

|, (16) 13 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

where N e is the total number of parameters in the edge messages, and αL1 = 10 −2 is the regularization constant. The dimension of the message vectors were set to be 100, the same as the standard model variation. This regularization encourages sparse representation of messages by the absolute values of the message components, effectively driving many of them toward zero. 4. Kullback–Leibler (KL) . We added a standard Gaussian prior, N ∼ (0, 1), to the components of the messages. Unlike the other model variants, the edge model in this case outputs both the mean and log-variance for each message element, which doubles the output dimensionality. This allows the messages to be treated as samples from a Gaussian distribution rather than fixed feature vectors: 

e′ 

> k

∼ N (μ′

> k

, diag [σ′2

> k

]) , (17) where μ′ 

> k

= ϕeμ and σ′2 

> k

= exp( ϕeσ2 ). We trained the model such that ϕeμ are all even outputs and ϕeσ2 are all odd outputs from the edge model. During training, the edge messages are sampled from the distribution defined by Equation (17) before being aggregated and inputted to the node model. A regularization term equivalent to the KL-divergence between the standard normal prior and the probability distribution defined in Equation (17) is added to the loss: 

LKL = 1

N eN e−1X

> k=0
> Le−1

X

> j=0

12 (μ′2 

> k,j

+ σ′2 

> k,j

− log( σ′2 

> k,j

)) . (18) The KL regularization term encourages sparsity in the messages by penalizing deviations from a standard normal distribution, effectively pushing the learned mean and variance of each message component toward zero mean and unit variance. If the model is in an evaluation setting, we do not sample and just take the message elements to equal the means. 5. Pruning . The dimensionality of the edge messages is gradually reduced during training until it matches that of the system using SymTorch’s pruning functionality. To prune the MLP, we chose a random 10,240 data points from the validation set as sample data points and inputted these through the MLP. The dimensions with the lowest variance across these sample points were deemed as ’unimportant’ and we zero-masked these dimensions. We used a Cosine Annealing pruning schedule, with pruning completing at 65% of the way through training. SymTorch contains its own built-in method to prune MLPs in this way. This model variation is an extension to the original paper. Pruning is a similar model variation to bottleneck, but has the advantage that we do not require knowledge of the dimensionality of the system beforehand. 

E.3. Setup and Training Dataset The dataset used in training the GNN was created using the source code from the original paper code repository (Cranmer, 2020). Each sample comprised a tensor containing particle information, as shown in Equation (8), of four interacting particles. The target data included the accelerations of the particles, found by taking the negative derivative of their respective potentials (to calculate the force) and dividing by their mass. Our system has dimensionality of two. Each dataset was created by running 10,000 different simulations over 1000 time steps and then only collecting data from every 5th time step to reduce correlation between samples. This resulted in datasets containing 1 million samples, which were then randomly split into training, validation and test sets with ratio 70/15/15. 

Model Configuration To create and train the GNNs, we used PyTorch and PyTorch Geometric (Fey & Lenssen, 2019). In all experiments, the edge model and node model MLPs contained three hidden layers each with 300 hidden units and ReLU activations between each layer. 

Data Augmentation The node model outputs predictions of the instantaneous accelerations of particles, as shown in Section 4.2. Before passing the node features into the model, we augment the data by adding Gaussian noise with a standard deviation of 3 to the position coordinates of all nodes simultaneously. This follows the approach used in the original paper’s code and was likely introduced to improve model robustness by reducing overfitting to precise spatial positions, while also simulating the presence of noise in real-world data. 14 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

Predictions and Loss The base loss on our model was calculated to be the mean absolute error between the predicted accelerations, ˆv′

> i

, and the actual accelerations, v′

> i

, from our dataset: 

L = 1

N vN v −1X

> i=0

|v′ 

> i

− ˆv′

> i

|. (19) L2 regularization was also used in every training instance, 

LL2 = αL2

N lN l

X

> l=0

|wl|2, (20) where N l are the total number of network parameters, denoted wl, and αL2 = 10 −8 is the L2 regularization constant. Hence, the total loss is L + LL2 for the standard, bottleneck and pruning model variant. For the L1 model variation, the loss is 

L + LL2 + LL1, and for the KL model variation the loss is L + LL2 + LKL .

Training To train our models, we performed gradient descent using the Adam (Kingma & Ba, 2017) optimiser with a Cosine Annealing learning rate scheduler. Each model was trained for 100 epochs with a batch size of 64 on a training set of 700,000 samples, resulting in approximately 1.1 million optimization steps. The training and validation loss was monitored every epoch. 

E.4. Symbolic distillation with SymTorch 

We used SymTorch to perform the SR on the edge model. For the standard and L1 model variant, we performed SR on the top two most important dimensions as determined by SymTorch’s get importance method. Whereas for the KL model variation, we chose the two most importance message dimensions as the ones with the highest KL divergence as calculated in Equation (18). 

Variable transforms To improve the efficiency of the SR, we performed the following variable transforms on the input data: ∆x = x1 − x2, ∆y = y1 − y2, and r = p∆x2 + ∆ y2 + 10 −2. We added a small constant to the distance r to match the form used in the original potential equations when generating the dataset. Thus the inputs to the SR were these transformed variables as well as m1, m 2, q 1, q 2.

SR parameters The operators that were allowed in the SR were +, −, ×, inv (·), which had complexity of 1, as well as 

exp and log , which we set to have complexity of 3. The complexity of constants and input variables were set to be 1. We chose a random set of 5,000 examples from the test set for the SR. For all of the tests, we ran the SR for 7,000 iterations. The parsimony argument was set to 0.05 and the maximum size of equations permitted (measured in terms of complexity) was set to 25. Lastly, we also set constraints of exp and log to be one. 

E.5. Results 

The final loss on the trained GNNs are shown in Table 6 and the symbolic regression results are shown in Table 3. An example Pareto front of equations is shown in Table 8 and the corresponding scores are shown in Figure 8. 

Examples of successful reconstructions Below are some examples of successful reconstructions of the true forces: • Spring; bottleneck 

msg1 =

 1r − 0.99950946 



· (0.8855752 ∆y + 1.8560125 ∆x) + 0.031805687 

• r−2; L1 

msg1 = m2((∆ y + 0.43513915 ) + ∆ x) · 1r3

15 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

Table 6. Test set mean absolute error (MAE) for different GNN model variants across four force law simulations. Lower values indicate better performance. The pruning variant (introduced by SymTorch) achieves comparable performance to the bottleneck model while automatically discovering the optimal dimension. Variance values provide scale context for the prediction errors. 

Simulation Standard Bottleneck L1 KL Pruning Variance 

Charge 18.20 19.12 18.06 39.80 19.40 56417.66 

r−1 0.26 0.25 0.32 15.40 0.28 87.69 

r−2 24.07 25.25 21.67 57.80 23.77 98641.90 Spring 0.24 0.16 0.20 7.29 0.23 55.84 

Table 7. Symbolic regression results for each message component. ✓= correct form of force law recovered; ✗= failure. ∗ Correct form, but with a small constant added to a term (e.g., 1/(r + const. )). † Correct form, but only ∆y apparent in both messages. ‡ Correct form with ∆x in one message and ∆y in the other. § Correct form with only ∆x or ∆y in at least one message. 

Simulation Message Standard Bottleneck L1 KL Pruning 

Charge 1 ✗ ✓‡ ✗ ✗ ✓

2 ✗ ✓‡ ✓§ ✗ ✓§

r−1 1 ✗ ✓ ✓ ✗ ✓

2 ✓ ✓ ✓ ✗ ✓∗

r−2 1 ✓§ ✓ ✓§ ✓‡ ✓

2 ✗ ✓ ✗ ✓‡ ✓

Spring 1 ✓† ✓ ✓ ✓‡ ✓

2 ✓† ✓ ✓§ ✓‡ ✓

• Charge; pruning 

msg2 = 1

(r + 0.036421545 )r2 · q1q2 · (∆ x − 0.0331669 ) + 0.08641323 

(shows the correct functional form except there is a small constant added to one of the 1/r terms and a ∆y term is not present). 

Figure 8. The score of the equations found from PySR for the pruning model trained on the r−2 as given in Table 8. The score is calculated from Equation (2). The best equation, highlighted in red, produces the largest drop in log( loss ) per unit of additional complexity. 

Limitations of the framework In the reconstructions for the r−1 and r−2 force laws, the mass of the receiving node, m1,is absent. As described in Equation (5), the edge model is intended to learn the interaction forces between particles, with the node model aggregating these messages and outputting the resulting accelerations, as shown in Equation (21). However, 16 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

Table 8. Pareto front from PySR results on message 2 of pruning model on r−2 dataset. The highlighted equation shows the ’best equation’ chosen by PySR according to the metric given in Equation 2. The constants have been truncated to two decimal points. 

Complexity Equation Loss 

1 ϕe 

> 2

= 0 .08 0.0888 5 ϕe 

> 2

= (∆ x · 0.00) + 0 .08 0.0885 7 ϕe 

> 2

= ((∆ x + ∆ y) · 0.00) + 0 .08 0.0882 8 ϕe 

> 2

= ((∆ x · inv( r)) · 0.00) + 0 .08 0.0880 9 ϕe 

> 2

= ( m2 · (∆ y + ∆ x) · 0.00) + 0 .08 0.0877 10 ϕe 

> 2

= (inv(( r + ∆ x) · r) · − 0.00) + 0 .08 0.0843 12 ϕe 

> 2

= (inv( r3) · (∆ x · 0.02)) + 0 .08 0.0687 14 ϕe 

> 2

= (( m2 · 0.01) · (∆ x · inv( r3))) + 0 .08 0.0513 16 ϕe 

> 2

= ((∆ y + ∆ x) · (( m2 · inv( r3)) · − 0.01)) + 0 .08 0.0260 18 ϕe 

> 2

= ((inv( r3) · (∆ x + ∆ y · 0.59)) · m2) + 0 .08 0.0128 20 ϕe 

> 2

= (inv( r3) · ((( m2 + 0 .06) · 0.01) · (∆ x + ∆ y · 0.59))) + 0 .08 0.0125 this setup is mathematically equivalent to having the edge model learn accelerations directly, with the node model simply outputting the aggregated accelerations, as shown in Equation (22). 

Edge model learns forces: 

ϕe(vrk , vsk ) ≈ Force on rk by sk

ϕv

vrk , X 

> j̸=rk

ϕe(vrk , vj )

 ≈ ϕv (vrk , Resultant force on rk) ≈ Acceleration of rk (21) 

Edge model learns accelerations: 

ϕe(vrk , vsk ) ≈ Acceleration on rk caused by interaction with sk

ϕv

vrk , X 

> j̸=rk

ϕe(vrk , vj )

 ≈ ϕv (vrk , Resultant acceleration of rk) ≈ Acceleration of rk (22) 

SR Wall Clock Time To perform SR on the edge model it took approximately 10 minutes on an Apple M4 Max SoC (14-core CPU: 10 performance + 4 efficiency cores, 36 GB unified memory). 

# F. Extracting PDE Solutions from a PINN Details 

F.1. Theory PINN architecture PINNs augment standard neural network training with regularization terms encoding physical laws (PDEs, boundary conditions, and initial conditions). We trained both a PINN and regular NN on temperature data governed by the 1-D heat equation (Equation (28)). The BCs for this physical system are 

u(0 , t ) = 0 , u(1 , t ) = 0 (23) and the IC is 

u(x, 0) = sin( πx ) (24) 17 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

Both the regular NN and the PINN share the same architecture, but the PINN augments its loss function with additional regularization terms that encode the governing physical laws. In our case, these regularization terms, L·, are 

LPDE = λPDE 

1

N

> N

X

> i=1

(ut(xi, t i) − α u xx (xi, t i)) 2 (25) 

LBC = λBC 

1

N

> N

X

> i=1

 u(0 , t i)2 + u(1 , t i)2 (26) 

LIC = λIC 

1

N

> N

X

> i=1

(u(xi, 0) − sin( πx i)) 2 (27) with regularization strengths denoted by λ·.

F.2. Methods Dataset The dataset consisted of ten data points, x ∈ X ⊂ R2, sampled from the true solution of the PDE, 

u(x) = u(x, t ) = e−π2αt sin( πx ) (28) where we set α = 0 .2.

Model and training The NN and PINN consisted of a single MLP ϕ : X → Z , where Z ⊆ R, with three hidden layers and Tanh activations. Each hidden layer contained 32 hidden units. For the PINN, we added additional regularization terms (from Equation (25)-Equation (27) with regularization strengths λPDE = 1 and λBD = λIC = 5 ) to the loss. We also set α

to be a differentiable parameter in the PINN such that, through training, the model can predict its value. Both models were trained for 30,000 optimizer steps. 

Symbolic distillation We used SymTorch to wrap both MLPs. For the SR, the default SymTorch parameter, as from Table 3. However, we further added a constraint on the exp and sin operators to only take arguments up to complexity 3, added a parsimony coefficient of 0.01 and ran the search for 1000 iterations. The sample batch for the SR consisted of 5000 randomly sampled data points x and t in their respective allowed intervals. 

F.3. Results 

To compare the performances of the NN and the PINN we evaluate their Mean-Square Error (MSE) losses on a 2-D space-time grid of granularity 100 × 100 . Figure 6 shows a comparison of the NN’s and the PINN’s predictions on the grid and the ground truth. The MSE losses on the grid are shown in Table 9. The PINN outperformed the NN, especially at boundaries. The PINN further predicted the value of α correctly to 4 decimal points.   

> Table 9. Mean squared errors (MSEs) of the regular neural network and Physics-Informed Neural Network (PINN) predictions on the
> u(x, t )space–time grid for the 1D heat equation. The PINN, trained with only 10 data points, achieves three orders of magnitude lower error than the regular NN due to its physics-informed regularization. Variance of the ground-truth solution shown for scale.

Regular NN PINN Variance 

MSE loss 7.81 × 10 −3 7.40 × 10 −6 4.82 × 10 −2

Extracting the true solution From the trained PINN, we were able to distill the correct form of the 1-D heat equation solution as given in Equation (28) with the constants correct to 2 decimal points. However, we were unable to do this for the regular NN. 

SR Wall Clock Time To perform SR on the PINN it took less than 3 minutes on an Apple M4 Max SoC (14-core CPU: 10 performance + 4 efficiency cores, 36 GB unified memory). 18 SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks 

# G. Interpreting LLM-Learned Operations Details 

G.1. Setup 

The specific prompts used to query the Llama-3.2-1B-Instruct model are shown in Table 10. When generating text, we configured the LLM to perform greedy decoding ( do sample=False ) with a maximum generation limit of 250 new tokens. Since the LLM responses included explanatory text beyond the numeric answer (eg. step-by-step reasoning), we implemented a regex-based extraction function to parse the final numeric result from the LLM’s output. We then created a wrapper function for each mathematical operation (addition, multiplication, temperature conversion, and counting) that: (1) took the numerical inputs as a NumPy array (eg. pairs of 3-digit integers for addition), (2) formatted these inputs into natural language prompts, (3) queried the LLM, (4) extracted the numeric answer, and (5) returned the result as a NumPy array. Finally, we used SymTorch’s model-agnostic mode to wrap these functions and perform SR on the LLM’s input-output behavior. The SR parameters used are the default ones as in Table 3 but we added a constraint on sin and exp to only take arguments of up to complexity 1 and ran the SR for 5000 iterations.           

> Table 10. Prompts used to query Llama-3.2-1B-Instruct on four mathematical tasks: addition, multiplication, Celsius-to-Fahrenheit conversion, and counting 1s in binary strings. All prompts request answers in $boxed$ format to enable regex-based extraction of numeric results. For the temperature conversion task, we only inputted temperatures between −20 ◦C and 200 ◦C. Operation Example Prompt Addition of two 3-digit numbers Return only the numeric answer in the format $boxed$. What is 193 + 374 = ?Multiplication of two 3-digit numbers Return only the numeric answer in the format $boxed$. What is 484 ∗726 = ?Counting 1s in a 6-digit string of 1s and 0s Return only the numeric answer in the format $boxed$. How many 1s are there in the string 000101? Converting from Celsius to Fahrenheit Return only the numeric answer in the format $boxed$. What is 30 degrees Celsius in Fahrenheit?

SR Wall-Clock Time Symbolic distillation required 5-15 minutes on an Apple M4 Max SoC (14-core CPU: 10 perfor-mance + 4 efficiency cores, 36 GB unified memory) for each of these operations. 19