---
title: "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data"
title_zh: 行为学习 (BL)：从数据中学习层级优化结构
authors: "Zhenyao Ma, Yue Liang, Dongxu Li"
date: 2026-02-23
pdf: "https://arxiv.org/pdf/2602.20152v1"
tags: ["query:sr"]
score: 8.0
evidence: 从科学数据中学习可解释的符号优化结构
tldr: 受行为科学启发，本文提出行为学习（BL）框架，旨在从数据中学习可解释且可识别的层级优化结构。该框架将效用最大化问题（UMP）作为基础模块，构建可组合的效用函数，支持从单一优化到复杂层级结构的建模。BL不仅在理论上具备通用近似性质和可识别性保证，在实验中也展现出卓越的预测性能、内在可解释性以及对高维数据的扩展能力，为科学领域的优化建模提供了新工具。
motivation: 旨在解决如何从数据中自动学习具有内在可解释性和可识别性的复杂层级优化结构的问题。
method: 通过将效用最大化问题（UMP）参数化为模块化构建块，构建支持层级嵌套的可组合效用函数框架。
result: 理论上证明了框架的通用近似性，实验表明其在保持高预测精度的同时，具备良好的可解释性和高维数据扩展性。
conclusion: BL框架成功统一了预测性能与结构可解释性，为涉及优化行为的科学领域提供了一种通用的机器学习方案。
---

## 摘要
受行为科学启发，我们提出了行为学习（Behavior Learning, BL），这是一种新型的通用机器学习框架，能够从数据中学习可解释且可辨识的优化结构，涵盖从单一优化问题到层级组合的各种形式。它统一了预测性能、内在可解释性和可辨识性，在涉及优化的科学领域具有广泛的适用性。BL 对由内在可解释的模块化组件构建的组合效用函数进行参数化，从而归纳出用于预测和生成的数据分布。每个模块都代表并可以以符号形式表示为效用最大化问题（UMP），这是行为科学中的基础范式，也是一种通用的优化框架。BL 支持从单一 UMP 到层级组合的架构，后者用于建模层级优化结构。其平滑且单调的变体（IBL）保证了可辨识性。在理论上，我们确立了 BL 的通用逼近性质，并分析了 IBL 的 M-估计性质。在实验上，BL 展示了强大的预测性能、内在可解释性以及对高维数据的可扩展性。代码：https://github.com/MoonYLiang/Behavior-Learning；可通过 pip install blnetwork 安装。

## Abstract
Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.

---

## 论文详细总结（自动生成）

这篇论文提出了一种名为**行为学习（Behavior Learning, BL）**的通用机器学习框架，旨在从数据中学习具有内在可解释性和科学依据的层级优化结构。以下是对该论文的深度结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **科学建模的困境**：在社会科学、生物学和物理学等领域，许多现象（如人类决策）难以用精确的公式描述。
*   **性能与可解释性的权衡**：现有的机器学习模型往往在高性能（如深度神经网络，黑盒）和高可解释性（如线性回归，性能受限）之间面临两难。
*   **科学可信度缺失**：大多数可解释模型缺乏“可辨识性”（Identifiability），即同一组数据可能对应多种解释，且模型结构往往与科学理论（如优化理论、微分方程）脱节。
*   **研究动机**：设计一个既能保持深度学习高性能，又具备科学理论支撑（基于优化问题）、内在可解释且数学上可辨识的框架。

### 2. 提出方法论：核心思想与技术细节
*   **核心思想**：将观测到的行为（数据 $y$）视为解决一个或多个相互作用的**效用最大化问题（UMP）**的结果。
*   **UMP 模块化构建块**：
    *   每个模块 $B(x, y)$ 代表一个优化问题，包含三个部分：**效用函数 $U$**（目标）、**不等式约束 $C$**（资源限制）和**等式约束 $T$**（信念一致性）。
    *   公式表达：$B(x, y) = \lambda_0 \phi(U) - \lambda_1 \rho(C) - \lambda_2 \psi(T)$。其中 $\phi$ 是单调递增函数，$\rho$ 和 $\psi$ 是惩罚项。
*   **层级架构**：
    *   **BL(Single)**：学习单个 UMP，可解释性最强。
    *   **BL(Shallow/Deep)**：通过层级嵌套多个 UMP 模块，模拟复杂的层级优化系统（如组织架构或物理系统中的粗粒化过程）。
*   **概率建模与训练**：
    *   使用**条件 Gibbs 分布**建模响应：$p(y|x) \propto \exp(BL(x, y)/\tau)$。
    *   **混合损失函数**：对离散响应使用交叉熵（CE），对连续响应使用去噪得分匹配（DSM）。
*   **可辨识变体 (IBL)**：通过限制惩罚函数的平滑性和单调性，确保模型参数在数学上是唯一的，从而提供科学可信的解释。

### 3. 实验设计
*   **数据集与场景**：
    *   **标准预测任务**：使用了 10 个 OpenML 数据集（涵盖金融、医疗、物理等领域，如 German Credit, Adult Income 等）。
    *   **定性案例研究**：使用 Boston Housing 数据集展示如何将模型还原为符号化的优化公式。
    *   **高维数据扩展性**：在图像（MNIST, Fashion-MNIST）和文本（AG News, Yelp）数据集上进行测试。
    *   **约束执行测试**：验证模型在 64 维能量守恒约束下的表现。
*   **对比方法（Benchmark）**：
    *   包括 MLP、NAM（神经加性模型）、TabNet、LightGBM、随机森林、决策树、逻辑回归、SVGP（高斯过程）等 10 种基准模型。

### 4. 资源与算力
*   **硬件设备**：主要在单张 **NVIDIA L40S GPU** 上运行；部分实验在配备 **NVIDIA RTX 2050 GPU** 的笔记本电脑上完成。
*   **训练时长**：论文提供了训练时间对比表。在相同参数量下，BL 的训练时间略高于 E-MLP（能量基础多层感知机），例如在 MNIST 上 BL 耗时约 110-140 秒，而 E-MLP 约 100-104 秒。

### 5. 实验数量与充分性
*   **实验规模**：
    *   在 10 个标准数据集上进行了 8 个随机种子的重复实验。
    *   针对高维数据，对比了不同深度（1-3 层）的模型表现。
    *   进行了超参数搜索（使用 Optuna 框架，每个数据集 50 次试验）。
*   **充分性评价**：实验设计较为全面，涵盖了从低维表格数据到高维图像/文本数据，从预测精度到 OOD（分布外）检测、校准度（ECE/NLL）以及定性解释分析，能够客观证明该框架的通用性。

### 6. 主要结论与发现
*   **性能卓越**：BL 在保持内在可解释性的同时，预测性能达到了与黑盒模型（如 MLP, LightGBM）相当的第一梯队水平。
*   **内在可解释性**：BL(Deep) 能够成功识别出层级优化结构。在波士顿房价案例中，模型自动学习到的“经济敏感型买家”和“位置敏感型买家”等偏好模式与经典经济学理论高度一致。
*   **可辨识性保证**：理论证明了 IBL 变体具有统计一致性和渐近正态性，能够可靠地恢复底层真实模型。
*   **高维适应性**：BL 能够扩展到高维输入，且在模型校准度和 OOD 鲁棒性上往往优于传统的能量模型。

### 7. 优点（亮点）
*   **科学与 AI 的桥梁**：将机器学习与行为科学中的 UMP 范式统一，使模型输出具有明确的物理/经济含义。
*   **理论完备**：不仅提供了通用近似定理，还解决了可解释模型中罕见的可辨识性问题。
*   **符号化输出**：模型可以直接写成类似“$\max U \text{ s.t. } C \le 0$”的符号形式，透明度极高。
*   **层级解释**：提出了“粗粒化”（Coarse-graining）解释路径，适合建模复杂的社会或物理系统。

### 8. 不足与局限
*   **计算开销**：由于涉及复杂的惩罚项计算和 Gibbs 分布建模，训练时间比普通 MLP 稍长。
*   **基函数选择**：目前主要依赖多项式基函数，虽然可解释性好，但在处理极高阶非线性时可能存在稳定性问题。
*   **理论假设的扩展性**：关于可辨识性的理论假设在超大规模参数化架构下的表现仍需进一步探索。
*   **混合架构探索不足**：目前主要讨论纯 BL 架构，如何将其与预训练黑盒特征提取器（如 Transformer）深度融合尚待研究。

（完）
