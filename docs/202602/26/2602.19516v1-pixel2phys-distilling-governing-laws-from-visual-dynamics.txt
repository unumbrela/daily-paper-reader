Title: Pixel2Phys: Distilling Governing Laws from Visual Dynamics

URL Source: https://arxiv.org/pdf/2602.19516v1

Published Time: Tue, 24 Feb 2026 02:24:39 GMT

Number of Pages: 10

Markdown Content:
# Pixel2Phys: Distilling Governing Laws from Visual Dynamics 

Ruikun Li 2*â€  

, Jun Yao 1,3â€ 

, Yingfan Hua 1, Shixiang Tang 1,4, Biqing Qi 1,Bin Liu 3, Wanli Ouyang 1,4, Yan Lu 1,4â€¡

> 1

Shanghai Artificial Intelligence Laboratory 2Tsinghua University 

> 3

University of Science and Technology of China 4The Chinese University of Hong Kong 

Abstract 

Discovering physical laws directly from high-dimensional visual data is a long-standing human pursuit but remains a formidable challenge for machines, representing a funda-mental goal of scientific intelligence. This task is inherently difficult because physical knowledge is low-dimensional and structured, whereas raw video observations are high-dimensional and redundant, with most pixels carrying lit-tle or no physical meaning. Extracting concise, physically relevant variables from such noisy data remains a key ob-stacle. To address this, we propose Pixel2Phys, a collabo-rative multi-agent framework adaptable to any Multimodal Large Language Model (MLLM). It emulates human scien-tific reasoning by employing a structured workflow to ex-tract formalized physical knowledge through iterative hy-pothesis generation, validation, and refinement. By repeat-edly formulating, and refining candidate equations on high-dimensional data, it identifies the most concise represen-tations that best capture the underlying physical evolution. This automated exploration mimics the iterative workflow of human scientists, enabling AI to reveal interpretable gov-erning equations directly from raw observations. Across di-verse simulated and real-world physics videos, Pixel2Phys discovers accurate, interpretable governing equations and maintaining stable long-term extrapolation where baselines rapidly diverge. 

1. Introduction 

Discovering physical laws from observational data lies at the core of scientific understanding and has historically driven major advances across physics, astronomy, and the natural sciences [1, 7, 39, 44]. As AI systems increasingly participate in scientific workflows [28, 48], the ability to infer governing equations directly from visual observations becomes a critical step toward genuine AI for Science. Hu-

> *Work done during an internship at Shanghai AI Laboratory.
> â€ Equal contribution.
> â€¡Corresponding author (luyan@pjlab.org.cn)

man scientists typically achieve such discoveries by dis-tilling structured, low-dimensional physical variables from complex visual phenomena and formulating compact sym-bolic laws [9, 32]. However, this manual process is slow and labor-intensive, classic examples such as translating Tycho Braheâ€™s astronomical measurements into Keplerâ€™s laws re-quired decades of expert reasoning. Automating this capa-bility, visual equation discovery , would therefore accelerate scientific progress. However, visual equation discovery is extremely chal-lenging because the true physical signals in a video are low-dimensional and concise, yet they are submerged within a large amount of visually irrelevant content such as textures, lighting variations, and background motion []. These redun-dant components dominate the pixel space and obscure the compact physical structure that scientific laws depend on. Prior approaches have attempted to address this difficulty in several ways, but each faces fundamental limitations. (1) Supervised equationâ€“prediction models [2, 24, 35, 48]: learn to directly predict equation from videos, but they require manually organzied equationâ€“video data pairs for training, which are extremely scarce, causing the models to generalize poorly beyond the specific systems they were trained on. (2) Unsupervised latent-coding methods: first learn latent representations from videos using autoencod-ing or predictive objectives [5, 15â€“17, 25, 46], and then ap-ply symbolic regression on these latents. However, because the latent spaces are determined by reconstruction or next-frame prediction optimization process rather than physical structure, they are often non-unique and easily entangle visually salient but physically irrelevant factors. (3) Re-cent Multimodal Large Language Models (MLLMs) com-bine visual understanding with strong symbolic reasoning, suggesting potential for equation discovery [3, 4, 26]. Yet directly prompting an MLLM mainly retrieves and recom-bines prior knowledge from its training corpus; without a structured workflow, it struggles to infer new physical vari-ables or derive laws purely from raw visual data. These challenges highlight the need for a method that can reli-ably extract the physically meaningful structure hidden in-                                                                    

> arXiv:2602.19516v1 [cs.CE] 23 Feb 2026 d
> dt Z=
> âˆ’0.77 âˆ’0.52 âˆ’2.20
> 0.53 1.52 âˆ’2.18
> 0.97 1.79 âˆ’0.75
> Z+
> 19 .18
> 0.65
> âˆ’10 .94
> Eigenvalues:
> Î»1=-0.018
> Î›2,3 = 0.012 Â±2.136i
> slow -drifting spiral
> period â‰ˆ2.940
> Physical Variables Governing Law
> Physical Variables Governing Law
> Future Forecasting Physical Video
> â€¦â€¦
> d
> dt ð‘§ 1=âˆ’0.05 ð‘§ 1
> 2âˆ’sin (ð‘§ 2)
> d
> dt ð‘§ 2=ð‘§ 1âˆ’cos (ð‘§ 2)
> Object Motion Physics Field ab
> c
> z1&z2âˆ†z1&âˆ†z2âˆ†2z1&âˆ†2z2
> d
> dt ð‘§ 1=1âˆ’ð‘§ 1
> 2+ð‘§ 2
> 2ð‘§ 1+ð‘§ 1
> 2+ð‘§ 2
> 2ð‘§ 2+0.1âˆ†z1
> d
> dt ð‘§ 2=âˆ’ð‘§ 1
> 2+ð‘§ 2
> 2ð‘§ 1+1âˆ’ð‘§ 1
> 2+ð‘§ 2
> 2ð‘§ 2+0.1âˆ†z2
> Physical Variables Governing Law

Figure 1. Distill governing laws from videos. (a) Trajectory dynamics of moving objects. (b) Spatiotemporal dynamics of time-varying physical fields. (c) Intrinsic dynamics of physical phenomena. 

side high-dimensional videos and support the discovery of new equations. To address these challenges, we propose Pixel2Phys, a collaborative multi-agent framework that can be paired with any MLLM and organizes visual equation discovery into a structured, iterative scientific workflow (Figure 2a). At the center of the framework is the Plan Agent, which coordinates three specialized agents, the Variable Agent, the Equation Agent, and the Experiment Agent, and deter-mines the refinement strategy across iterations. The Vari-able Agent extracts physical quantities from videos through several complementary defined tools that correspond to different classes of physical systems, enabling it to re-cover object-level motion, field-level evolution, and other low-dimensional structures that often underlie governing equations. The Equation Agent forms equation candidates by dynamically utilizing symbolic regression components, such as adjusting sparsity constraint strength or selecting different symbolic libraries. The Experiment Agent eval-uates each candidate equation by several metrics, such as simulating its predicted dynamics, measuring reconstruc-tion discrepancies, testing temporal extrapolation, and sum-marizing these results into a structured report for the Plan Agent. The Plan Agent integrates these reports and issues targeted instructions, such as enforcing stronger sparsity in the Equation Agent or requiring the Variable Agent to in-corporate dynamical constraints, for a next refinement step. Through this iterative cycle of hypothesis formation, evalu-ation, and adjustment, Pixel2Phys progressively filters away visually irrelevant factors and converges toward variables and equations that best describe the true underlying dynam-ics. Crucially, each step in this workflow is mechanical and fully within the execution capabilities of modern MLLMs, allowing Pixel2Phys to combine the modelâ€™s strong single-step reasoning with structured multi-step coordination, and thereby to discover new variables and governing equa-tions beyond what is present in the modelâ€™s training data. We evaluate our framework on three categories of physics videos with increasing difficulty. The results demonstrate that by distilling accurate and interpretable governing equa-tions, our proposed framework improves extrapolation ac-curacy (RMSE) by 45.35% over baselines. Our contributions can be summarized as follows: â€¢ We propose a novel multi-agent framework for visual equation discovery, where an MLLM planner coordinates specialized agents to parse complex visual dynamics at multiple granularities. â€¢ We design an iterative, co-optimization reasoning process that breaks the circular dependency between visual repre-sentation learning and law discovery. â€¢ Extensive experiments on three challenging categories of physics videos demonstrate that our framework not only discovers physically interpretable governing equa-tions but also improves long-term extrapolation accuracy by 45.35%. 

2. Related Work 

2.1. Inferring Physics from Video 

Existing approaches generally tackle visual dynamics from two perspectives. The first category focuses on implicit neural dynamics , learning latent representations to per-form future prediction. Modules like Neural ODEs [10, 17] and Koopman operators [5] are often integrated to model continuous evolution. Other works decompose video into PDE dynamics [15, 46] or disentangled representations [13, 47]. While effective for prediction, these methods en-capsulate physical laws within black-box networks, lack-ing explicit interpretability. The second category, physics-informed perception , imposes strong inductive biases. Re-searchers have utilized inverse graphics [20] or enforced rigid body constraints [22] to infer specific properties like mass and friction. However, these methods rely heavily on apriori knowledge of the governing equations, limiting their ability to discover unknown laws from unfamiliar physical phenomena. 

2.2. Visual Equation Discovery 

Equation discovery from video is a nascent field that typ-ically follows two paradigms: (1) Pipeline-based ap-proaches [30, 49], which extract explicit trajectories of ob-jects and then apply symbolic regression. These methods rely on pre-trained trackers and struggle with continuous fields (e.g., fluids) where objects are undefined. (2) End-to-end approaches [8, 41], which learn a latent coordi-nate space for equation regression. However, they face a critical circular dependency : extracting a good variable space requires knowledge of the dynamics, while finding the dynamics requires a clean variable space. Consequently, they often settle for sub-optimal solutions in complex vi-sual scenarios. Our work introduces a third paradigm: 

a collaborative, multi-agent framework. By establishing a reasoning-driven feedback loop, we enable variable extrac-tion and equation discovery to mutually refine each other, effectively breaking the circular dependency and extending discovery to diverse visual dynamics. 

3. Problem Fomulation 

In the task of inferring governing laws from high-dimensional data, the goal is to find a compact and accu-rate symbolic expression. We are given a sequence of high-dimensional visual observations (i.e., a video), {X(t)}Nt=1 ,where each frame X(t) âˆˆ RD (D is the pixel space, e.g., 

C Ã— H Ã— W ). We assume this video observes an underly-ing, unknown, low-dimensional dynamical system evolving on an intrinsic manifold [8], Z âŠ† Rd, where d â‰ª D. Our goal is to simultaneously discover both the intrinsic phys-ical variables z(t) âˆˆ Z and their symbolic governing law 

f , which defines the systemâ€™s evolution, dz dt = f (z(t)) .This presents a challenging dual discovery problem, as both the coordinate system z and the function f are unknown. This problem can be decomposed into two coupled sub-problems: â€¢ Physical Variable Extraction. Learning an encoder 

Ï• : RD â†’ Rd that maps the high-dimensional visual observation X(t) to its corresponding intrinsic physical state z(t) = Ï•(X(t)) . To ensure z(t) is an informative representation, this component often includes a decoder 

Ïˆ : Rd â†’ RD , used to enforce a reconstruction con-straint X(t) â‰ˆ Ïˆ(z(t)) .â€¢ Governing Law Distillation. Identifying an inter-pretable, symbolic expression for the dynamics function 

f from a library of candidate functions Î˜ (e.g., polynomi-als, trigonometric terms). This function must accurately model the evolution of the extracted variables z(t).The synergy between these components is the central chal-lenge. The quality of the extracted variables z = Ï•(X) di-rectly dictates the simplicity and accuracy of the distilled law f . Conversely, a simple and sparse law f provides a powerful signal to guide the variable extraction process 

Ï•, compelling it to filter out dynamically irrelevant visual components. Our framework is designed to solve this co-optimization problem, seeking a self-consistent pair of an intrinsic variable space Z and its symbolic law f that fit the observed data and generalize for long-term prediction. 

4. Method 

4.1. Pixel2Phys Method Overview 

The Pixel2Phys framework mimics the collaborative work-flow of a human scientific team to solve the dual discov-ery problem defined in Section 3, involving observing, hy-pothesizing, experimenting, and refining. As illustrated in Figure 2, Pixel2Phys consists of four agents with distinct roles: The Plan Agent acts as the teamâ€™s central coordi-nator, responsible for setting goals, analyzing reports, di-agnosing bottlenecks, and providing instructional prompts 

Iplan to other agents to guide the next iteration. The Vari-able Agent executes visual parsing tasks, i.e., parsing and extracting low-dimensional physical variables z(t) from the high-dimensional observations X(t). The Equation Agent is responsible for distilling the symbolic governing equa-tion f from z(t). Finally, the Experiment Agent validates the quality of the current (Z, f ) pair. In each iteration, the Variable, Equation, and Experiment Agent are required to return a report (denoted as Rvar , Requ , and Rexp ) to the Plan Agent, respectively, containing both quantitative per-formance metrics and qualitative visual diagnostics for the next decision-making step. These agents are instantiated as MLLMs and operate under a unified protocol where they process visual data X and textual prompts P to generate textual responses MLLM (X, P). Algorithm ?? outlines the pseudocode of Pixel2Physâ€™s execution process. 

4.2. Plan Agent: Global Planning in Iterative Rea-soning 

Our framework replaces a simple pipeline with an iterative reasoning loop driven by the Plan Agent. This agent is the central coordinator that orchestrates the co-optimization of the variable space Z and the governing law f .The reasoning loop commences with an initialization step ( k = 0 ). The Plan Agent interprets the userâ€™s task and activates the initial iteration, yielding the candidate pair 

(Z0, f 0) alongside the reports R0

> var

, R0

> equ

, and R0 

> exp

from each agent. In subsequent iterations ( k â‰¥ 1), the Plan Agent aggregates these reports to conduct a two-fold diagnosis. It first inspects the visualizations in Rkâˆ’1 

> exp

to assess qualitative dynamical fidelity, then scrutinizes the specific quantitative metrics with Rkâˆ’1 

> var

and Rkâˆ’1 

> equ

to pinpoint the exact bottle-neck. Based on this analysis, it formulates a instructional prompt Ik 

> plan

to resolve the identified failure mode: â€¢ Variable Refinement. When the diagnosis attributes fail-ure to a poor Z (e.g., high reconstruction errors), the Plan Agent instructs the Variable Agent to re-extract Z. Cru-cially, Plan Agent will provide the equation fkâˆ’1 to acti-vate the physics-informed loss (Section 4.3). Visual Data Variable Agent Equation Agent Experiment Agent Plan Agent Final Equation 

Collaboration 

Variable Agent 

Motion Object 

Physics Field 

Fluid Video Tool Box 

Physical Variables z(t) 

Visual 

Observation 

X(t) 

Equation Agent 

dx = unknown(x,y) 

dy = unknown(x,y)  

> dx dy

= ...  

> Î¾1Î¾2
> 1
> x
> y
> x2
> y2
> xy
> cos(y)

...                 

> Î¾1Î¾2
> [0]
> [0]
> [0]
> [-0.05 ]
> [0]
> [0]
> [0]
> [0]
> [1.00 ]
> [0]
> [0]
> [0]
> [0]
> [-1.00 ]
> sin(y) [-1.00 ][0]

... ...       

> 1xyx2y2xy cos(y) sin(y)

= -0.05 x2 + sin(y) 

x + cos(y) 

Coefficient Matrix 

Operator 

Library 

Operator Subset 

min ||dX - Î˜(X) Îž||+Î·|| Îž ||      

> x, x 2, x 3, 1/x, sin(x), log(x), exp(x), ...
> xy, x 2y, x 3y, ... âˆ†(x), âˆ‡Â² x
> Unary:
> Binary: Spatial:
> dX Î˜(X)

Îž

Physics Variables Governing Law f

Experiment Agent 

dx = -0.05 x2 + sin(y) 

dy = x + cos(y) 

Physics Variables 

Governing Law 

Evaluation Report                

> def f(t,x, y ):
> dx =-0.05 *x** 2+np .sin (y)
> dy =x+np .cos (y)
> return [dx ,dy ]
> sol =solve_ivp (f, t_span, ic, 'BDF' )

ï¼†

Evaluated 

Visual 

Comparison 

Accuracy 

Metrics 

Equation 

Complexity 

Numerical Simulation    

> R2score: _____
> Equation length: _____
> Visual Quality: _____
> Visual Summary:
> 0.967
> 2
> 0.982
> Reference Simulation
> Simulation visually
> aligns well with
> reference trajectory.

Plan Agent      

> dx= -0.05 x2+ sin(y)
> dy=x + cos(y) Evaluation Report
> R2score: _____
> Equation length: _____
> Visual Quality: _____
> Visual Summary:
> 0.967
> 2
> 0.982
> Reference Simulation
> Simulation visually aligns well with
> Reference trajectory.

Reasoning 

Memory User Task    

> <Output> The input video shows a
> moving object. I used the object
> tracking tool to detect its coordinate
> trajectory, which serves as the
> physical variable.
> <Output> The trajectory of
> physical variables is a closed
> curve. I used the SINDy tool
> with a penalty coefficient of 0.5.
> <Output> I performed numerical
> simulation on the discovered
> equation and calculated the R 2
> score against the observed trajectory.
> Here is the visual comparison.
> <Think>The physical variable's trajectory
> has many outliers, suggesting inaccurate
> extraction. Refining the data is prioritized.
> Also, increase the penalty coefficient Î·
> to prevent overfitting.

<Instruction> Refine physical 

variables: Trajectory contains 

outliers. 

<Instruction> Increase 

penalty coefficient Î· in 

next iteration to prevent 

overfitting. 

Object -level 

Pixel -level 

Representation -level Figure 2. The multi-agent collaboration framework of Pixel2Phys. 

â€¢ Equation Refinement. When Z is deemed high-quality but fkâˆ’1 is inaccurate or overly complex, the Plan Agent instructs the Equation Agent to modify its equation search by adjusting the configuration hyperparameters. This iterative refinement loop continues until the Plan Agent determines that Rk

exp satisfies the success criteria (Ap-pendix ?? ). The final (Zk, f k) pair is then returned as the solution. 

4.3. Variable Agent: Variable Extraction from Vi-sual Data 

Given a video sequence X(t), the Variable Agent extracts physical variables z(t) by deploying specific parsing tools. We provide multi-granularity tools for flexible usage to ac-commodate physical information presented at object, pixel, and representation levels. The agent dynamically selects the corresponding tool based on both the videoâ€™s visual prop-erties and the explicit instructional prompt Iplan issued by the Plan Agent. Specifically, we provide object-level tools for videos of moving objects, pixel-level tools for physical fields, and autoencoder-based tools for complex scientific phenomena. 

Object-level Tool. For moving objects, such as celes-tial revolution, the Variable Agent extracts the trajectory 

z(t) = [ x(t), y (t)] . We employ visual foundation mod-els for zero-shot object segmentation and tracking to avoid training on specific shapes of objects. Specifically, we uti-lize Segment Anything [23] to segment potential objects in every frame. The agent then filters out static targets, as de-tailed in Appendix ?? , and records the centroid coordinates of moving objects as z(t).

Pixel-level Tool. For physical fields governed by PDEs (Figure 1b), video frames X(t) are treated as samples of a continuous field u(x, t ) discretized at pixels x [7, 45]. Phys-ical dynamics emerge from local spatial interactions. We equip the Variable Agent with the fixed convolutional ker-nels to compute spatial derivatives directly from the pixel grid, yielding operators such as the Laplacian âˆ† and bi-harmonic âˆ†2. Since the governing equation is valid at any pixel, the agent randomly samples a sparse subset of pixels to efficiently collect the spatial operators as physical vari-ables z(t) = [ u(t), âˆ†u(t), âˆ†2u(t), . . . ].

Representation-level Tool. For complex scientific phe-nomena, underlying dynamics are often obscured by de-vice noise and lighting fluctuations (Figure 1c). We adapt a novel physics-informed autoencoder to compress X(t)

into a latent representation z(t) = Ï•(X(t)) , ensuring the underlying variables remain as simple as possible. This tool operates in two modes depending on the availability of physical priors. First, when the instructional prompt Iplan contains a governing equation f discovered in the previous iteration, the loss function comprises two com-ponents L = Lrecon + Î»eq Leq . The first term is the self-supervised reconstruction error ensuring z retains sufficient observational information. The second term represents the physics consistency loss with coefficient Î»eq defined as 

Leq = âˆ¥F (z) âˆ’ f (z)âˆ¥2, where F(z) denotes the numer-ical derivative via central differences and f (z) represents the symbolic derivative derived from equation f . This con-straint forces the latent space to adhere to the governing equation f , guiding the encoder to focus on physical dy-namics while filtering out textural details and noise. Sec-ond, in the absence of physical information, such as during the cold-start phase, the autoencoder performs only self-supervised reconstruction and functions as a standard au-toencoder. This approach ensures the Variable Agent iden-tifies a latent space Z that balances visual reconstruction with dynamic simplicity. Crucially, it enables the output from the downstream Equation Agent to iteratively refine the extraction of physical variables. Finally, the process concludes by returning the extracted variables z(t) alongside a report Rvar . This report encapsu-lates the specific tools employed and their hyperparameter configurations, such as the model size of SAM and its mask size. 

4.4. Equation Agent: Dynamic Symbolic Regres-sion 

Considering that most true equations dz dt = f (z(t)) exhibit sparsity within a high-dimensional space of candidate func-tions [6, 8], the agent identifies the sparse active terms in this function space through a three-step process. 

Data and Derivative Estimation. Given the discrete time series z(t) organized into a state matrix Z, the agent first es-timates the time derivative Ë™Z numerically. This step utilizes the central difference method [36] F(z) to ensure method-ological consistency, thereby establishing the left-hand side (LHS) of the target equation. 

Candidate Library Construction. Subsequently, the agent constructs a candidate library matrix Î˜( Z), wherein each column represents a potential nonlinear function of the state z(t) that constitutes the right-hand side (RHS) of the governing equation. The candidate library incorporates the following component terms â€¢ Polynomial terms including 1, z, z 2, z 1z2, . . . 

â€¢ Transcendental terms sin( z), cos( z), exp (z), . . . .The equation agent iteratively optimizes the candidate li-brary in the following process. 

Sparse Regression and Law Distillation. The discov-ery task is formulated as solving the overdetermined lin-ear system Ë™Z â‰ˆ Î˜( Z)Îž , where Ë™Z denotes the deriva-tive matrix, Î˜( Z) represents the candidate library, and Îž

is the unknown sparse coefficient matrix. The optimiza-tion objective is formulated as âˆ¥ Ë™Z âˆ’ Î˜( Z)Îž âˆ¥22 + Î»sp âˆ¥Îžâˆ¥1,wherein the second term enforces the sparsity of active terms. To solve for Îž, the agent employs the Sequen-tial Thresholded Least-Squares (STLSQ) algorithm [6] (de-tailed in Appendix ?? ). The sparsity threshold Î»sp is deter-mined via the Plan Agentâ€™s instruction Iplan , enabling active guidance over the parsimony of the discovered law. Ulti-mately, the non-zero entries in Îž = [ Î¾1, Î¾ 2, . . . , Î¾ d] are re-constructed into symbolic equations f .Finally, the process concludes by returning the discov-ered equation f and a report Requ , containing the candidate library and sparsity threshold Î»sp for the decision of the Plan Agent. 

4.5. Experiment Agent: Equation Evaluation and Feedback 

The Experiment Agentâ€™s role is to rigorously validate the quality and consistency of the (Z, f ) pair discovered in the current iteration. It receives the variable time series z(t)

and the symbolic law f and generates a multi-dimensional evaluation report, Rexp , for the Plan Agent. This validation protocol assesses three key aspects. 

Equation Quality. To assess the governing law f , Ex-periment Agent computes two quantitative metrics: (1) R2

score by comparing the numerical derivative F(z) with the symbolic derivative f (z); and (2) Complexity, measured by the number of terms, L0, of the coefficient matrix Îž.

Variable Quality. The Experiment Agent generates phase portraits of the trajectory z(t) to visually characterize the variable space. Concise governing laws typically mani-fest as structured chaotic attractors, whereas noisy or tan-gled trajectories suggest the presence of significant interfer-ence. This image is passed to the Plan Agent, which vi-sually assesses whether the trajectory exhibits a clear, low-dimensional dynamical structure. 

Extrapolation Fidelity. The agent performs a long-term numerical simulation zpred (t) by integrating the discovered law f from an initial condition z(0) . It then (1) computes the Root Mean Square Error (RMSE) between the predicted 

zpred (t) and the extracted variables from held-out future frames zgt (t) over an unseen time horizon; and (2) visual-izes the plots of the predicted and ground-truth trajectories. A self-consistent pair (Z, f ) yields reliable long-term pre-dictions. Finally, the Experiment Agent aggregates all these quan-titative metrics ( R2, L0, RMSE) and plotted figures into the structured report Rexp (see Appendix ?? for structure), a b c dPhysical Trajectories Governing Equations e fLinear Cubic Circular VDP Glider Exp Figure 3. Reasoning results of motion objects: blue line is ground truth; green dashed line is trajectories inferred by PixelsPhys. Table 1. Average performance of motion objects over 5 runs with varying seeds. Best results are in bold.                                                                

> Case Method Terms Found False Positives R2@1000
> Linear AE-SINDy --0.0046 Â±0.0028
> Latent-ODE --0.0154 Â±0.0081
> Coord-Equ Yes 1.100 Â±0.3600 0.8647 Â±0.0554
> Pixel2Phys Yes 00.9913 Â±0.0000
> Cubic AE-SINDy --0.0720 Â±0.0122
> Latent-ODE --0.0039 Â±0.0013
> Coord-Equ No 3.400 Â±1.2800 0.2632 Â±0.1928
> Pixel2Phys Yes 0.3900 Â±0.3620 0.9886 Â±0.0082
> Circular AE-SINDy --0.3647 Â±0.0716
> Latent-ODE --0.0240 Â±0.0028
> Coord-Equ Yes 0.2000 Â±0.0040 0.9903 Â±0.0057
> Pixel2Phys Yes 01.0000 Â±0.0000
> VDP AE-SINDy --0.2483 Â±0.0182
> Latent-ODE --0.0433 Â±0.0102
> Coord-Equ Yes 2.3100 Â±0.6590 0.4920 Â±0.1302
> Pixel2Phys Yes 0.9900 Â±0.0030 0.9954 Â±0.0047
> Glider AE-SINDy --0.0310 Â±0.0030
> Latent-ODE --0.0360 Â±0.0041
> Coord-Equ No 2.1800 Â±0.4900 0.9129 Â±0.0102
> Pixel2Phys No 3.0000 Â±0.0000 0.9995 Â±0.0000

which serves as the foundation for the Plan Agentâ€™s next reasoning step. 

5. Experiments 

We validate Pixel2Phys on three categories of interdisci-plinary videos: object motion (object-level), physical fields (pixel-level), and real-world recordings of scientific phe-nomena (representation-level). We employ GPT-4o as LLM backbone by default. All configurations for Pixel2Phys across experiments are provided in Appendix ?? .

5.1. Discovery of Object-level Dynamics 

Consistent with prior works [19, 30, 41], we utilize videos of object motion to assess the modelâ€™s ability to accurately derive symbolic motion equations from high-dimensional observations. 

5.1.1. Datasets 

We validate on five dynamical systems (Figure 3): Lin-ear , Cubic , Circular , Van Der Pol (VDP) , and Glider .These equations include various characteristics such as lin-ear terms, nonlinear terms, and significant differences in time scales [14], which together form an evaluation bench-mark across different levels of difficulty. Video generation details are provided in Appendix ?? .

5.1.2. Setups 

We categorize the baselines into two classes based on inter-pretability. The first class encodes latent vectors and learns the implicit dynamics, including AE-SINDy [8] and Latent-ODE [10]. The second class explicitly predicts object coor-dinates to infer their governing equations, represented by Coord-Equ [30]. We train on the initial 200 steps and eval-uate using the R2 score on 1,000-step extrapolated coordi-nate trajectories. For AE-SINDy and Latent-ODE, we align their latent predictions to the ground-truth coordinates via Procrustes analysis before evaluation (Appendix ?? ). Addi-tionally, for methods capable of inferring symbolic expres-sions, we evaluate two equation-related metrics: 1) Terms Found: A binary score (Yes/No) indicating if all true terms are correctly identified. 2) False Positives: The number of incorrect terms included in the final equation, measuring parsimony. 

5.1.3. Main Results 

Table 1 and Figure 3 reveal three key insights. First, im-plicit methods (Latent-ODE and AE-SINDy) suffer from extrapolation collapse ( R2 â‰ˆ 0). This empirically vali-dates the necessity of our multi-granularity design, demon-strating that generic representations fail to capture rigid-body physics without explicit object-level parsing. Second, Pixel2Phys demonstrates superior parsimony and robust-ness. Compared to Coord-Equ, our framework significantly reduces false positives and achieves near-perfect long-term prediction, with all discovered symbolic expressions de-Table 2. Average RMSE and VPS (Â± std from 5 runs) of long-term prediction. Best results are in bold. The threshold of VPS is 0.5. 

Method Lambda-Omega Brusselator FitzHughâ€“Nagumo Swiftâ€“Hohenberg RMSE â†“ VPS@0.5 â†‘ RMSE â†“ VPS@0.5 â†‘ RMSE â†“ VPS@0.5 â†‘ RMSE â†“ VPS@0.5 â†‘

Black-box Models FNO [27] 0.68 Â±0.04 477 .00 Â±20 .82 415 .34 Â±81 .70 19 .10 Â±3.96 0.89 Â±0.07 116 .20 Â±6.81 11 .02 Â±4.09 52 .10 Â±9.04 

UNO [33] 0.42 Â±0.05 764 .80 Â±16 .97 423 .64 Â±82 .42 27 .30 Â±4.23 67 .41 Â±5.26 104 .00 Â±13 .73 0.48 Â±0.08 90 .80 Â±5.81 

WNO [40] 96 .98 Â±6.35 41 .20 Â±6.10 68 .67 Â±7.80 9.60 Â±3.67 34 .10 Â±3.09 22 .60 Â±5.24 1.95 Â±0.39 31 .30 Â±8.93 

Symbolic-regression Models PDE-Find [34] 0.67 Â±0.00 492 .00 Â±0.00 1.56 Â±0.00 40 .00 Â±0.00 0.63 Â±0.00 54 .00 Â±0.00 0.19 Â±0.00 200 .00 Â±0.00 

SGA-PDE [11] 0.92 Â±0.10 126 .60 Â±6.51 0.14 Â±0.02 1000 .00 Â±0.00 NaN NaN NaN NaN LLM-PDE [12] 0.55 Â±0.04 438 .40 Â±23 .20 NaN NaN 0.62 Â±0.12 55 .90 Â±4.20 0.69 Â±0.13 34 .30 Â±13 .03 

Pixel2Phys 0.03 Â±0.00 1000 .00 Â±0.00 0.12 Â±0.01 1000 .00 Â±0.00 0.16 Â±0.01 1000 .00 Â±0.00 0.18 Â±0.06 200 .00 Â±0.00 ðœ• ð‘¢                                                                                                                         

> ðœ• ð‘¡ =0.1âˆ†ð‘¢ +1âˆ’ð‘¢ 2âˆ’ð‘£ 2ð‘¢ +ð‘¢ 2+ð‘£ 2ð‘£
> ðœ• ð‘£
> ðœ• ð‘¡ =0.1âˆ†ð‘£ +1âˆ’ð‘¢ 2âˆ’ð‘£ 2ð‘£ âˆ’ð‘¢ 2+ð‘£ 2ð‘¢
> ðœ• ð‘¢
> ðœ• ð‘¡ =1.0âˆ†ð‘¢ âˆ’4.0ð‘¢ +ð‘¢ 2ð‘£ +1.0
> ðœ• ð‘£
> ðœ• ð‘¡ =0.1âˆ†ð‘£ +3.0ð‘¢ âˆ’ð‘¢ 2ð‘£
> ðœ• ð‘¢
> ðœ• ð‘¡ =1.0âˆ†ð‘¢ +ð‘¢ âˆ’ð‘¢ 3âˆ’ð‘£
> ðœ• ð‘£
> ðœ• ð‘¡ =1.0âˆ†ð‘£ +0.5(ð‘¢ âˆ’ð‘£ )
> ðœ• ð‘¢
> ðœ• ð‘¡ =âˆ’2.0âˆ†ð‘¢ âˆ’1.0âˆ†2ð‘¢ âˆ’0.3ð‘¢ +
> 1.0ð‘¢ 2âˆ’1.0ð‘¢ 3
> ðœ• ð‘¢
> ðœ• ð‘¡ =0.102 âˆ†ð‘¢ +0.993 1âˆ’ð‘¢ 2âˆ’ð‘£ 2ð‘¢ +0.996 ð‘¢ 2+ð‘£ 2ð‘£
> ðœ• ð‘£
> ðœ• ð‘¡ =0.102 âˆ†ð‘£ +0.993 1âˆ’ð‘¢ 2âˆ’ð‘£ 2ð‘£ âˆ’0.996 ð‘¢ 2+ð‘£ 2ð‘¢
> ðœ• ð‘¢
> ðœ• ð‘¡ =0.990 âˆ†ð‘¢ âˆ’3.948 ð‘¢ +0.993 ð‘¢ 2ð‘£ +0.998
> ðœ• ð‘£
> ðœ• ð‘¡ =0.094 âˆ†ð‘£ +2.997 ð‘¢ âˆ’0.996 ð‘¢ 2ð‘£
> ðœ• ð‘¢
> ðœ• ð‘¡ =0.998 âˆ†ð‘¢ +0.997 ð‘¢ âˆ’1.015 ð‘¢ 3âˆ’0.997 ð‘£
> ðœ• ð‘£
> ðœ• ð‘¡ =0.982 âˆ†ð‘£ +0.485 ð‘¢ âˆ’0.490 ð‘£
> ðœ• ð‘¢
> ðœ• ð‘¡ =âˆ’1.874 âˆ†ð‘¢ âˆ’0.891 âˆ†2ð‘¢ âˆ’0.256 ð‘¢ +
> 0.908 ð‘¢ 2âˆ’0.923 ð‘¢ 3
> Ground Truth
> Discovered

Lambda â€“Omega Brusselator FitzHugh -Nagumo Swift â€“Hohenberg        

> GT Prediction GT Prediction GT Prediction GT Prediction

Figure 4. Reasoning results of physical fields: equations in blue is ground truth; equations in orange is inferred by PixelsPhys. 

tailed in Appendix ?? . Finally, the Glider case high-lights Pixel2Physâ€™s practicality: although the exact sym-bolic matching is not achieved due to complex trigonomet-ric terms, the near-perfect extrapolation ( R2 = 0 .9995 ) and visual overlap in Figure 3e confirm that Pixel2Phys success-fully captured the underlying physical attractor. Further-more, Pixel2Phys transcends simulated settings, success-fully recovering gravitational laws from real-world videos, despite complex backgrounds and noisy tracking (see Ap-pendix ?? ). 

5.2. Discovery of Pixel-level Dynamics 

In these scenarios, videos represent discrete grid samplings of time-varying fields driven by PDEs [8, 34]. The objec-tive is to capture these pixel-level interactions to derive the underlying PDE equations. 

5.2.1. Datasets 

We conduct experiments on four representative reaction-diffusion equations: Lambdaâ€“Omega (LO) [8], Brusse-lator (Bruss) [29], FitzHughâ€“Nagumo (FHN) [42] and Swiftâ€“Hohenberg (SH) [37]. We solve the differential equa-tions numerically as datasets. The details of the data gener-ation are provided in Appendix ?? .

5.2.2. Setups 

Baselines are categorized into black-box neural operators and symbolic regression methods. Models predict evo-lution over 1,000 steps (200 for SH system) from initial frames. We evaluate performance using root mean square error (RMSE) and valid prediction steps (VPS), defined as the duration where prediction error remains below a specific threshold (details in Appendix ?? ). 

5.2.3. Main Results 

Table 2 reveals that black-box neural operators suffer from severe error accumulation in long-term rollout and result in extremely low valid prediction steps, confirming that im-plicit approximations fail to maintain dynamical stability without physical constraints. Existing symbolic baselines also show significant fragility as SGA-PDE and LLM-PDE frequently fail to converge or yield suboptimal fits marked as NaN. SGA tends to overfit due to the unconstrained search space of genetic algorithms while LLM-PDE lacks visual perception and biases towards over-simplified ex-pressions that miss accurate terms as detailed in the full equation list in Appendix ?? . In contrast, Pixel2Phys con-sistently achieves the lowest RMSE and near-perfect sta-bility across all datasets by integrating precise numerical tools within a reasoning loop. Figure 4 further demonstrates that our framework correctly identifies complex high-order operators like the bi-harmonic term to capture the exact governing mechanism. Moreover, our framework scales to complex real-world PIV datasets, accurately recovering 2D Navier-Stokes components (detailed in Appendix ?? ). 

5.3. Discovery of Representation-level Dynamics 

This category involves real-world scientific recordings, which suffer from low signal-to-noise ratios due to uncon-trolled lighting and sensor noise. Consequently, effective physical components are embedded implicitly. The goal is to discover a compact representation space that filters visual noise to capture these implicit evolution mechanisms. 

5.3.1. Datasets 

We collect six videos: four visualizing KÂ´ armÂ´ an vor-tex streets (fluid dynamics) [31, 38] and two record-ing Belousov-Zhabotinsky reactions (chemical oscilla-tors) [18]. Both represent canonical complex systems governed by low-dimensional attractors but manifested through high-dimensional, noisy visual patterns. All videos are cropped and converted to grayscale (details in Ap-pendix ?? ). 

5.3.2. Setups 

We benchmark against FNO, Latent-ODE, and the ad-vanced video generation model Wan2.2 [43]. Given the limited sequence length (fewer than 300 frames), models are trained on the full sequence and evaluated on their abil-ity to autoregressively reconstruct the entire video from the first frame. For Wan2.2, we freeze pretrained weights and use GPT-4o to generate descriptive text prompts for con-ditioning. In addition to RMSE, we also used vorticity er-ror [21] to evaluate the accuracy of vorticity, whose formula is shown in Appendix ?? .

5.3.3. Main Results 

Figure 6 presents a compelling comparison where the 14B-parameter Wan2.2 generates visually realistic textures, it fails to maintain dynamical consistency, evidenced by the spatial drift of vortices at 1.5s and 2.0s relative to the ground truth. This stems from the qualitative nature of gen-erative prompts (see Appendix ?? ), whereas Pixel2Phys dis-tills quantitative governing laws (e.g., eigenvalues Î»2,3 â‰ˆÂ±2.136 i in Figure 1c) that dictate a strict oscillation pe-riod ( T â‰ˆ 0.294 ) to ensure precise alignment. Visually, our predictions appear less textured, which is attributed to the selective filtering of the co-optimization mechanism. Ir-relevant components like uneven lighting are discarded to extract pure dynamics on the intrinsic manifold. This phys-ical fidelity is further corroborated by the quantitative re-sults in Figure 5, where Pixel2Phys consistently achieves the lowest prediction error, confirming that the discovered parsimonious law successfully captures the dominant high-dimensional behavior despite the filtration of visual noise. FNO Latent ODE LLM-WAN Pixel2Phys     

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> RMSE
> Video
> Flow1
> Flow2
> Flow3
> Flow4
> Chem
> Chem2
> (a) RMSE FNO Latent ODE LLM-WAN Pixel2Phys
> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> Vorticity Error
> Video
> Flow1
> Flow2
> Flow3
> Flow4
> Chem
> Chem2 (b) Vorticity Error

Figure 5. Comparison of prediction errors across all models on physical phenomenon videos. Ground Truth    

> Pixel2Phys
> Wan2.2
> Initial Frame
> 0.5s 1.0s 1.5s 2.0s

Figure 6. Prediction results of PixelsPhys and Wan2.2 for the Wa-ter Flow video. 

5.4. Ablation Study and Robustness 

To verify the necessity of co-optimization, we replace the Plan Agent with a static serial workflow. As detailed in Appendix ?? , the absence of the equation feedback loop results in a highly rugged variable space, failing to distill parsimonious laws. We further test robustness by substitut-ing the LLM backbone with smaller-scale models. Results in Appendix ?? show that Pixel2Phys maintains superior accuracy even with limited reasoning capacity, demonstrat-ing that our collaborative agentic architecture effectively re-duces the dependence on raw model scale. Finally, detailed case studies visualizing the step-by-step reasoning process for each video category are provided in Appendix ?? .

6. Conclusion 

In this work, we present Pixel2Phys, a framework that au-tomates the discovery of governing laws from visual dy-namics. By coordinating specialized agents, our approach replaces static pipelines with an iterative co-optimization process. Crucially, it utilizes preliminary symbolic laws to reversely guide visual variable extraction, effectively re-solving the coupling between variable extraction and law discovery. Experiments show that Pixel2Phys recovers par-simonious equations and achieves robust long-term extrap-olation, marking a solid step towards interpretable visual models. References 

[1] Dimitrios Angelis, Filippos Sofos, and Theodoros E Karaka-sidis. Artificial intelligence in physical sciences: Symbolic regression trends and perspectives. Archives of Computa-tional Methods in Engineering , 30(6):3845â€“3865, 2023. 1 [2] Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aure-lien Lucchi, and Giambattista Parascandolo. Neural sym-bolic regression that scales. In International Conference on Machine Learning , pages 936â€“945. Pmlr, 2021. 1 [3] Abeba Birhane, Atoosa Kasirzadeh, David Leslie, and San-dra Wachter. Science in the age of large language models. 

Nature Reviews Physics , 5(5):277â€“280, 2023. 1 [4] Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature , 624(7992):570â€“578, 2023. 1 [5] Oumayma Bounou, Jean Ponce, and Justin Carpentier. On-line learning and control of complex dynamical systems from sensory input. Advances in Neural Information Processing Systems , 34:27852â€“27864, 2021. 1, 2 [6] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Dis-covering governing equations from data by sparse identifi-cation of nonlinear dynamical systems. Proceedings of the national academy of sciences , 113(15):3932â€“3937, 2016. 5 [7] Qinglong Cao, Ding Wang, Xirui Li, Yuntian Chen, Chao Ma, and Xiaokang Yang. Teaching video diffusion model with latent physical phenomenon knowledge. arXiv preprint arXiv:2411.11343 , 2024. 1, 4 [8] Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences , 116(45):22445â€“22451, 2019. 3, 5, 6, 7 [9] Boyuan Chen, Kuang Huang, Sunand Raghupathi, Ishaan Chandratreya, Qiang Du, and Hod Lipson. Automated dis-covery of fundamental variables hidden in experimental data. 

Nature Computational Science , 2(7):433â€“442, 2022. 1 [10] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equa-tions. Advances in neural information processing systems ,31, 2018. 2, 6 [11] Yuntian Chen, Yingtao Luo, Qiang Liu, Hao Xu, and Dongx-iao Zhang. Symbolic genetic algorithm for discovering open-form partial differential equations (sga-pde). Physical Re-view Research , 4(2):023174, 2022. 7 [12] Mengge Du, Yuntian Chen, Zhongzheng Wang, Longfeng Nie, and Dongxiao Zhang. Large language models for auto-matic equation discovery of nonlinear dynamics. Physics of Fluids , 36(9), 2024. 7 [13] Stathi Fotiadis, Mario Lino Valencia, Shunlong Hu, Stef Garasto, Chris D Cantwell, and Anil Anthony Bharath. Dis-entangled generative models for robust prediction of system dynamics. In International Conference on Machine Learn-ing , pages 10222â€“10248. PMLR, 2023. 2 [14] John Guckenheimer. Dynamics of the van der pol equa-tion. IEEE Transactions on Circuits and Systems , 27(11): 983â€“989, 1980. 6 [15] Vincent Le Guen and Nicolas Thome. Disentangling physi-cal dynamics from unknown factors for unsupervised video prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 11474â€“ 11484, 2020. 1, 2 [16] Irina Higgins, Peter Wirnsberger, Andrew Jaegle, and Alek-sandar Botev. Symetric: Measuring the quality of learnt hamiltonian dynamics inferred from vision. Advances in Neural Information Processing Systems , 34:25591â€“25605, 2021. [17] Florian Hofherr, Lukas Koestler, Florian Bernard, and Daniel Cremers. Neural implicit representations for physical param-eter inference from a single video. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 2093â€“2103, 2023. 1, 2 [18] JL Hudson and JC Mankin. Chaos in the belousovâ€“ zhabotinskii reaction. The Journal of Chemical Physics , 74 (11):6171â€“6177, 1981. 8 [19] Yayati Jadhav and Amir Barati Farimani. Dominant motion identification of multi-particle system using deep learning from video. Neural Computing and Applications , 34(20): 18183â€“18193, 2022. 6 [20] Miguel Jaques, Michael Burke, and Timothy Hospedales. Physics-as-inverse-graphics: Unsupervised physical parameter estimation from video. arXiv preprint arXiv:1905.11169 , 2019. 2 [21] Jinhee Jeong and Fazle Hussain. On the identification of a vortex. Journal of fluid mechanics , 285:69â€“94, 1995. 8 [22] Rama Krishna Kandukuri, Jan Achterhold, Michael Moeller, and Joerg Stueckler. Physical representation learning and parameter identification from video using differentiable physics. International Journal of Computer Vision , 130(1): 3â€“16, 2022. 2 [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. In Proceedings of the IEEE/CVF international confer-ence on computer vision , pages 4015â€“4026, 2023. 4 [24] William La Cava, Bogdan Burlacu, Marco Virgolin, Michael Kommenda, Patryk Orzechowski, FabrÂ´ Ä±cio Olivetti de Franc Â¸a, Ying Jin, and Jason H Moore. Contemporary sym-bolic regression methods and their relative performance. 

Advances in neural information processing systems , 2021 (DB1):1, 2021. 1 [25] Ruikun Li, Huandong Wang, and Yong Li. Learning slow and fast system dynamics via automatic separation of time scales. In Proceedings of the 29th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining , pages 4380â€“4390, 2023. 1 [26] Ruikun Li, Yan Lu, Shixiang Tang, Biqing Qi, and Wanli Ouyang. Mllm-based discovery of intrinsic coordinates and governing equations from high-dimensional data. arXiv preprint arXiv:2505.11940 , 2025. 1 [27] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for para-metric partial differential equations. arXiv preprint arXiv:2010.08895 , 2020. 7 [28] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jian-feng Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177 , 2024. 1 [29] Ryan Lopez and Paul J Atzberger. Gd-vaes: Geomet-ric dynamic variational autoencoders for learning nonlin-ear dynamics and dimension reductions. arXiv preprint arXiv:2206.05183 , 2022. 7 [30] Lele Luan, Yang Liu, and Hao Sun. Distilling governing laws and source input for dynamical systems from videos. arXiv preprint arXiv:2205.01314 , 2022. 3, 6 [31] Bernd R Noack, Konstantin Afanasiev, Marek MorzyÂ´ nski, Gilead Tadmor, and Frank Thiele. A hierarchy of low-dimensional models for the transient and post-transient cylinder wake. Journal of Fluid Mechanics , 497:335â€“363, 2003. 8 [32] Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Gold-blum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. arXiv preprint arXiv:2104.08894 ,2021. 1 [33] Md Ashiqur Rahman, Zachary E Ross, and Kamyar Az-izzadenesheli. U-no: U-shaped neural operators. arXiv preprint arXiv:2204.11127 , 2022. 7 [34] Samuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Data-driven discovery of partial differential equations. Science advances , 3(4):e1602614, 2017. 7 [35] Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, and Chandan Reddy. Transformer-based planning for symbolic regression. Advances in Neural Information Processing Sys-tems , 36:45907â€“45919, 2023. 1 [36] R Charles Swanson and Eli Turkel. On central-difference and upwind schemes. Journal of computational physics , 101 (2):292â€“306, 1992. 5 [37] Jack Swift and Pierre C Hohenberg. Hydrodynamic fluctua-tions at the convective instability. Physical Review A , 15(1): 319, 1977. 7 [38] Kunihiko Taira, Steven L Brunton, Scott TM Dawson, Clarence W Rowley, Tim Colonius, Beverley J McKeon, Oliver T Schmidt, Stanislav Gordeyev, Vassilios Theofilis, and Lawrence S Ukeiley. Modal analysis of fluid flows: An overview. AIAA journal , 55(12):4013â€“4041, 2017. 8 [39] Wassim Tenachi, Rodrigo Ibata, and Foivos I Diakogiannis. Deep symbolic regression for physics guided by units con-straints: toward the automated discovery of physical laws. 

The Astrophysical Journal , 959(2):99, 2023. 1 [40] Tapas Tripura and Souvik Chakraborty. Wavelet neural op-erator for solving parametric partial differential equations in computational mechanics problems. Computer Methods in Applied Mechanics and Engineering , 404:115783, 2023. 7 [41] Silviu-Marian Udrescu and Max Tegmark. Symbolic pre-gression: Discovering physical laws from distorted video. 

Physical Review E , 103(4):043307, 2021. 3, 6 [42] Pantelis R Vlachas, Georgios Arampatzis, Caroline Uhler, and Petros Koumoutsakos. Multiscale simulations of com-plex systems by learning their effective dynamics. Nature Machine Intelligence , 4(4):359â€“366, 2022. 7 [43] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianx-iao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314 , 2025. 8[44] Yiqun Wang, Nicholas Wagner, and James M Rondinelli. Symbolic regression in materials science. MRS Communi-cations , 9(3):793â€“805, 2019. 1 [45] Hao Wu, Fan Xu, Chong Chen, Xian-Sheng Hua, Xiao Luo, and Haixin Wang. Pastnet: Introducing physical inductive biases for spatio-temporal video prediction. In Proceedings of the 32nd ACM international conference on multimedia ,pages 2917â€“2926, 2024. 4 [46] Xinheng Wu, Jie Lu, Zheng Yan, and Guangquan Zhang. Disentangling stochastic pde dynamics for unsupervised video prediction. IEEE Transactions on Neural Networks and Learning Systems , 2023. 1, 2 [47] Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Learning physics constrained dynamics using autoencoders. Advances in Neural Information Pro-cessing Systems , 35:17157â€“17172, 2022. 2 [48] Jie Ying, Haowei Lin, Chao Yue, Yajie Chen, Chao Xiao, Quanqi Shi, Yitao Liang, Shing-Tung Yau, Yuan Zhou, and Jianzhu Ma. A neural symbolic model for space physics. 

Nature Machine Intelligence , pages 1â€“16, 2025. 1 [49] Zitong Zhang, Yang Liu, and Hao Sun. Vision-based dis-covery of nonlinear dynamics for 3d moving target. arXiv preprint arXiv:2404.17865 , 2024. 3