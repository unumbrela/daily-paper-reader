---
title: Ministral 3
title_zh: Ministral 3
authors: "Alexander H. Liu, Kartik Khandelwal, Sandeep Subramanian, Victor Jouault, Abhinav Rastogi, Adrien Sadé, Alan Jeffares, Albert Jiang, Alexandre Cahill, Alexandre Gavaudan, Alexandre Sablayrolles, Amélie Héliou, Amos You, Andy Ehrenberg, Andy Lo, Anton Eliseev, Antonia Calvi, Avinash Sooriyarachchi, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Clémence Lanfranchi, Corentin Barreau, Cyprien Courtot, Daniele Grattarola, Darius Dabert, Diego de las Casas, Elliot Chane-Sane, Faruk Ahmed, Gabrielle Berrada, Gaëtan Ecrepont, Gauthier Guinet, Georgii Novikov, Guillaume Kunsch, Guillaume Lample, Guillaume Martin, Gunshi Gupta, Jan Ludziejewski, Jason Rute, Joachim Studnia, Jonas Amar, Joséphine Delas, Josselin Somerville Roberts, Karmesh Yadav, Khyathi Chandu, Kush Jain, Laurence Aitchison, Laurent Fainsin, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Maarten Buyl, Margaret Jennings, Marie Pellat, Mark Prins, Mathieu Poirée, Mathilde Guillaumin, Matthieu Dinot, Matthieu Futeral, Maxime Darrin, Maximilian Augustin, Mia Chiquier, Michel Schimpf, Nathan Grinsztajn, Neha Gupta, Nikhil Raghuraman, Olivier Bousquet, Olivier Duchenne, Patricia Wang, Patrick von Platen, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Quentin Torroba, Romain Sauvestre, Roman Soletskyi, Rupert Menneer, Sagar Vaze, Samuel Barry, Sanchit Gandhi, Siddhant Waghjale, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Théo Cachet, Theo Simon Sorg, Thibaut Lavril, Thiziri Nait Saada, Thomas Chabal, Thomas Foubert, Thomas Robert, Thomas Wang, Tim Lawson, Tom Bewley, Tom Bewley, Tom Edwards, Umar Jamil, Umberto Tomasini, Valeriia Nemychnikova, Van Phung, Vincent Maladière, Virgile Richard, Wassim Bouaziz, Wen-Ding Li, William Marshall, Xinghui Li, Xinyu Yang, Yassine El Ouahidi, Yihan Wang, Yunhao Tang, Zaccharie Ramzi"
date: 2026-01-13
pdf: "https://arxiv.org/pdf/2601.08584v1"
tags: ["query:sr-llm"]
score: 10.0
evidence: 参数高效稠密语言模型及指令微调变体
tldr: Ministral 3 系列是专为计算和内存受限场景设计的参数高效密集型语言模型，涵盖 3B、8B 和 14B 三种规模。该系列通过级联蒸馏技术构建，提供基础、指令微调和推理三种版本，并具备图像理解能力。模型采用 Apache 2.0 协议开源，旨在为边缘设备提供高性能的通用及复杂问题解决能力。
motivation: 旨在为计算和内存受限的应用场景提供高效且强大的密集型语言模型。
method: 采用级联蒸馏（Cascade Distillation）技术，通过迭代剪枝和结合蒸馏的持续训练来构建模型。
result: 推出了 3B、8B 和 14B 三种规模的模型，每种规模均包含基础、指令和推理变体，并支持图像理解。
conclusion: Ministral 3 系列以 Apache 2.0 协议开源，为受限环境下的复杂任务提供了灵活且高效的解决方案。
---

## 摘要
我们推出了 Ministral 3 系列，这是一组专为计算和内存受限应用设计的参数高效型稠密语言模型，提供 3B、8B 和 14B 三种参数规模。对于每种模型尺寸，我们发布了三个变体：用于通用目的的预训练基础模型、指令微调模型以及用于复杂问题解决的推理模型。此外，我们介绍了通过级联蒸馏（Cascade Distillation）推导 Ministral 3 模型的方法，这是一种结合了迭代剪枝和蒸馏持续训练的技术。每个模型都具备图像理解能力，并全部采用 Apache 2.0 许可证发布。

## Abstract
We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.

---

## 论文详细总结（自动生成）

这是一份关于 **Ministral 3** 论文的结构化深度总结：

### 1. 核心问题与研究动机
*   **核心问题**：如何在计算和内存受限的设备上，通过更少的训练数据和算力，构建出性能媲美大规模模型的高效稠密语言模型？
*   **研究动机**：
    *   **效率挑战**：主流模型（如 Llama 3、Qwen 3）通常需要 15T 到 36T 标记的超大规模预训练，成本极高。
    *   **边缘应用需求**：针对移动端或受限环境，需要参数量小（3B-14B）但推理、指令遵循和多模态能力强的模型。
    *   **知识传承**：探索如何通过“级联蒸馏”将大型预训练模型（如 24B 的 Mistral Small 3.1）的知识高效转移给小型模型。

### 2. 方法论
论文提出了 **级联蒸馏（Cascade Distillation）** 训练方案，其核心是一个迭代的“剪枝-蒸馏-重复”流程：
*   **核心思想**：不从零开始训练，而是从一个强大的父模型（Mistral Small 3.1）出发，通过迭代缩小规模并进行对齐训练。
*   **关键技术细节**：
    1.  **剪枝（Pruning）**：
        *   **层剪枝**：基于输入/输出激活范数比率识别不重要的层。
        *   **隐藏维度剪枝**：利用 PCA（主成分分析）生成旋转矩阵，将模型投影到低维空间。
        *   **FFN 维度剪枝**：根据激活值的平均绝对值计算重要性得分，裁剪 SwiGLU 中的矩阵。
    2.  **蒸馏（Distillation）**：使用父模型的 Logits 进行 KL 散度蒸馏。
    3.  **后训练（Post-training）**：
        *   **指令版（Instruct）**：采用 SFT + **在线直接偏好优化（ODPO）**。ODPO 通过实时采样并利用 Pairwise 奖励模型进行排名，有效减少了模型幻觉和无限循环。
        *   **推理版（Reasoning）**：在预训练基础上进行 SFT（含 CoT 数据）+ **GRPO（组相对策略优化）** + ODPO。
    4.  **多模态与长文本**：集成 410M 的 ViT 视觉编码器；通过 YaRN 和位置缩放技术支持高达 256k 的上下文。

### 3. 实验设计
*   **模型规模**：3B、8B、14B 三种尺寸，每种包含 Base、Instruct、Reasoning 三个版本。
*   **数据集/场景**：
    *   **通用能力**：MMLU-Redux, ARC-Challenge, TriviaQA 等。
    *   **数学与代码**：MATH, GPQA Diamond, MBPP, LiveCodeBench。
    *   **多模态**：MMMU, MathVista。
    *   **对齐与对话**：Arena Hard, WildBench, MM MT-Bench。
*   **对比方法**：主要对比了同规模的 **Qwen 3** 系列和 **Gemma 3** 系列。为了公平性，所有对比模型均在 Mistral 内部的评估框架下重新运行。

### 4. 资源与算力
*   **算力说明**：论文**未明确列出**具体的 GPU 型号、数量或总训练时长（如 H100 GPU 小时数）。
*   **效率描述**：作者强调该方法是“计算高效”的，仅使用了 1T 到 3T 的标记进行持续训练，远低于从头训练所需的规模。

### 5. 实验数量与充分性
*   **实验规模**：论文涵盖了从 3B 到 14B 的 9 个模型，并在数十个主流 Benchmark 上进行了测试。
*   **消融实验**：
    *   对比了不同教师模型（Small vs Medium）的效果。
    *   验证了“后训练后的教师”比“基础版教师”在蒸馏预训练中效果更好。
    *   探讨了 ODPO 对推理模型对话质量的提升。
*   **客观性**：通过重新运行竞争对手的 Benchmark 确保了评估标准的一致性，实验设计较为客观。

### 6. 主要结论与发现
*   **性能卓越**：Ministral 3 14B Base 在参数量减少 40% 的情况下，性能接近其 24B 的父模型。
*   **能力差距（Capacity Gap）**：在预训练蒸馏中，更强的教师（Medium）并不一定能带出更强的学生，但后训练阶段学生能从更强的教师中获益。
*   **教师选择**：使用经过人类偏好对齐（Preference-tuned）的模型作为蒸馏教师，效果显著优于仅经过 SFT 的教师。
*   **推理与冗长平衡**：推理模型虽然在 STEM 任务上极强，但容易产生过度反思和冗长输出，需通过 ODPO 进行行为修正。

### 7. 优点
*   **极高的参数效率**：通过级联蒸馏实现了“以小博大”，在有限参数下达到了极高的 Benchmark 分数。
*   **开源友好**：全系列模型采用 **Apache 2.0** 协议，对开发者和商业应用非常友好。
*   **全能型设计**：原生支持视觉理解和超长上下文（256k），在小尺寸模型中较为罕见。
*   **创新的后训练**：引入 ODPO 和 GRPO 的组合，显著提升了小模型的复杂推理能力。

### 8. 不足与局限
*   **算力细节缺失**：缺乏具体的训练资源消耗数据，难以精确评估其复现成本。
*   **3B 模型的脆弱性**：实验提到 3B 模型在 SFT 阶段容易出现过度冗长和无限循环，对超参数极其敏感，稳定性弱于大尺寸版本。
*   **推理行为的副作用**：推理版模型在追求逻辑严密时，可能会出现不自然的内部独白或回溯行为，影响通用对话体验。

（完）
