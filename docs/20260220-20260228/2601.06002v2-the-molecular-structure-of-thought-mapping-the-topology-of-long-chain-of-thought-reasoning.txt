Title: The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning

URL Source: https://arxiv.org/pdf/2601.06002v2

Published Time: Wed, 14 Jan 2026 02:19:25 GMT

Number of Pages: 36

Markdown Content:
# The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning 

## Qiguang Chen 1,2, Yantao Du 1,†, Ziniu Li 1, Jinhao Liu 2, Songyao Duan 3,

## Jiarui Guo 3, Minghao Liu 4, Jiaheng Liu 5, Tong Yang 3, Ge Zhang 6,

## Libo Qin 7,†, Wanxiang Che 2,†, Wenhao Huang 1

> 1

ByteDance Seed China , 2LARG, SCIR, Harbin Institute of Technology , 3Peking University ,

> 4

2077AI Foundation , 5Nanjing University , 6M-A-P , 7Central South University 

## Abstract 

Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn , a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.  

> Date:

Jan 15, 2026  

> Correspondence:

{qgchen,car}@ir.hit.edu.cn , duyantao@bytedance.com ,lbqin@csu.edu.cn Self -Re fl ection as 

Ionic Bonding 

Self -Exploration as 

Van der Waals Forces 

Step 2 

Step 3 

Deep Reasoning as 

Covalent Bonding 

Concept 

Guess 

Conclusion 

Knowledge 

Condition 

Cause 

Inference 

Knowledge 

Inference 

Knowledge Guess 

Step 1 

Logical Node 

> Folding

O

H

Re fl ected Logical Node 

Previous Logical Node 

Deep Reasoning 

Now, choose Quadratic 

Residue Theory path :…

Self -Exploration 

Given the known condition …

Let’s analysis of prime cases 

using most possible …

Self -Re fl ection 

However, assume m= kn , then 

m²+ 1=k²n²+ 1, k²n² is divisible 

by n, but k²n² + 1 may not be .

Let’s rethink Quadratic …

Therefore, for any positive 

integer n, there exists a positive 

integer m such that …

Figure 1 The hypothesis that stable molecular structure in Long CoT arises from three key “chemical” bonds. 

1

> arXiv:2601.06002v2 [cs.CL] 13 Jan 2026

O

H

H

O

H

O

H

H

O

H

O

H

H

OH

Chain -

Structure 

Reasoning 

Tree /Graph -

Structure 

Reasoning 

Molecular -Structure Reasoning 

Molecule 0 Molecule 1 Molecule 0 +        

> Reasoning
> Chain
> Deep
> Reasoning Self -
> Exploration
> Self -
> Reflection
> Deep Reasoning
> Self -Reflection
> Self -Exploration
> Deep Reasoning
> Self -Reflection
> Self -Exploration
> Normal Operation Figure 2 Comparison of prior chain- or tree-like structures and our molecular structure. Reasoning starts from Mole. 0, uses deep reasoning on strongly related structures, then employs self-exploration for new logic in Mole. 1. When meet errors, reasoning utilize self-reflection to guide chain to optimized Mole. 0 +.

## 1 Introduction 

Recently, large language models (LLMs) have excelled on diverse reasoning tasks via explicit chain-of-thought (CoT) rationales [ 1–4]. Yet, they struggle to cold-start from instruction-tuned or base models into Long CoT models requiring extended multi-step reasoning [ 5, 6]. Notably, Du et al. [7] shows that humans generate Long CoT rationales without imitating DeepSeek-R1 [ 8]. Our preliminary studies reveal that standard supervised fine-tuning and distillation from human or Instruction LLM rationales (using randomly sampled Long CoT examples) fail to reliably instill these skills in LLMs. Models often lose coherence over long trajectories or fail to transfer patterns to novel tasks. This prompts a key question: 

> How do Large Language Models learn and represent effective Long Chain-of-Thought?

To explain this, we posit that they acquire the organization of reasoning trajectories. As shown in Figure 2, prior studies model these as logic nodes in sequences or trees of steps. Yet, our analysis of Long CoT across strong reasoning models reveals a stable distribution of three core behaviors across tasks and architectures: Deep-Reasoning, Self-Reflection, and Self-Exploration [5], which node-centric views fail to capture. This finding triggers a molecular-inspired, distributional view: we model behavior-labeled logic edges as interaction bonds and examine how their global molecular-like structure ensures long-horizon reasoning stability. Specifically, Deep-Reasoning forms dense local clusters of coupled deductions, like covalent bonds; Self-Reflection creates long-range corrective links to prior steps, like hydrogen bonds; and Self-Exploration forges weak bridges between distant clusters, like van der Waals forces. Thus, high-quality Long CoT arises from the stable composition and arrangement of these bond types , guiding effective learning ∗.In this framework, we define semantic isomers as Long CoT trajectories that solve the same tasks and visit similar semantic regions but differ in behavior distributions and transitions. We demonstrate that multiple near-optimal semantic isomers exist per task family, but mixing stable isomers from different strong teachers destabilizes learning, degrading performance and behavior distributions despite matched token statistics. This structurally explains why combining heterogeneous Long CoT traces often fails, beyond token-level distillation. Building on this perspective, we propose Mole-Syn , a structure-aware synthesis framework that first estimates a behavior transition graph from strong reasoning models and then transfers only this behavioral structure to cheaper instruction LLMs via controlled trajectory synthesis, instead of directly copying teacher outputs. This decouples structural transfer from model-specific surface form, enables the generation of Long CoT data that match target behavior distributions from scratch, and yields consistent gains in both Long CoT performance and RL stability across six benchmarks. After that, we analyze the shaping function of each bond in the Long CoT structure. Deep Reasoning bonds encode core logical flow, Self-Reflection bonds support folding pathways to previous steps, and Self-Exploration bonds reinforce long-range consistency checks, enabling targeted bond distributions. Moreover, we discuss why a deteriorated molecular structure is hard to restore, which helps explain how private LLMs protect Long 

> ∗Note: C, H, and O atom references are analogies for molecular structure only.

20      

> 20
> 40
> 60
> 80
> 100
> 0
> 20
> 40
> 60
> 80
> 100
> Accuracy  (%) Accuracy  (%)
> GSM8K MATH -500 AMC2023 OlymBench AIME24 AIME25
> LLaMA3 -
> 8B -Instruct
> +ICL -
> Distill
> +R1 -
> Distill
> +QwQ -
> Distill
> +OSS -
> Distill
> Qwen2.5 -
> 7B -Instruct
> +ICL -
> Distill
> +R1 -
> Distill
> +QwQ -
> Distill
> +OSS -
> Distill
> Qwen2.5 -
> 32B -Instruct
> LLaMA3 -
> 70B -Instruct
> +ICL -
> Distill
> +R1 -
> Distill
> +QwQ -
> Distill
> +OSS -
> Distill
> +ICL -
> Distill
> +R1 -
> Distill
> +QwQ -
> Distill
> +OSS -
> Distill Figure 3

The failure of distillation from weak instruction LLMs with ICL and Human-annotated reasoning traces to acquire Long CoT structures, compared to successful distillation from strong reasoning LLMs. See Appendix Figure 15 for the full result. 

CoT structures from distillation-based imitation. Methods such as summarization and reasoning compression can disrupt Long CoT structure, limiting unauthorized replication of internal reasoning processes. In summary, our contributions are as follows: 

• We model Long CoT as a molecular structure with 3 bonds: deep-reasoning (covalent), self-reflection (hydrogen-bond), and self-exploration (van der Waals), to understand its effective learning. 

• We identify effective Semantic Isomers for Long CoT learning, where only entropy-convergent bonds enable stable learning, while competing structures destabilize learning. 

• We introduce Mole-Syn , which uses distribution-transfer graphs to synthesize these structures, improving Long CoT and stabilizing RL across 6 benchmarks. 

## 2 Preliminary: Cold-start LLMs for Long Chain-of-Thought 

First, we validate whether LLMs exhibit valid Long CoT trajectories suitable for Long CoT learning. We investigate three data sources: distillation from strong reasoning LLMs, distillation from weak instruction LLMs with in-context learning (ICL), and fine-tuning on human reasoning traces. Llama -3.1 -8B -Instruct     

> +Human Distill Data
> + R1 Distill Data
> GSM8K
> MATH -
> 500
> AIME
> 2024
> AIME 2025
> AMC
> 2023
> Olympiad
> Bench
> 20
> 60
> 40
> Figure 4

Performance comparison between human-annotated reasoning traces ( + Human Distill Data ) and R1 distilled reasoning traces ( + R1 Distill Data ). 

> Only distillation from strong reasoning LLMs works.

To identify effective data sources, we curated a syn-thetic set of reasoning traces from three sources. As shown in Figure 3, only distillation from strong rea-soning LLMs enables target models to learn and retain Long CoT structure, improving performance on bench-marks requiring extended reasoning. These results indicate that only high-quality traces reliably support both learning and use of Long CoT structures.  

> Distillation from randomly selected ICL by weak in-struct LLMs does not work.

Instruction LLMs with-out explicit reasoning training lag behind reasoning-specialized models. We test whether LLMs can acquire Long CoT structure by distilling from an instruction LLM using randomly selected ICL demonstrations that emulate Long CoT reasoning. As shown in Figure 3, performance drops sharply. Instruction LLMs can only mimic short CoT traces ( ∼6–8 steps) and fail to extend exploration while preserving intermediate steps and trace coherence. This degradation indicates a limitation of ICL-based distillation rather than robust long-chain imitation. 3Even human-annotated Long-CoT-like traces fail. Inspired by Du et al. [7] , we test whether human step-by-step solutions can induce long CoT. We collect human solutions for complex reasoning tasks and fine-tune LLMs on them. Figure 4 shows that human-trace training does not reproduce the long-CoT gains from distilling strong reasoning models, suggesting that human solutions aid local problem solving but may not reliably encode abstractions for long-horizon reasoning distributions. Takeaway 1 Distillation from strong reasoning LLMs effectively imparts long CoT structures, while ICL from weak instruction models and fine-tuning on human traces yield limited gains. This underscores the importance of high-quality reasoning exemplars for robust long-chain learning in LLMs. 

## 3 Hypothesis: Stable ‘‘Molecular Structure’’ in Long CoT 

To understand these phenomena, as shown in Fig. 1 & 2, effective Long CoT connects nodes in logical space through reasoning behaviors, forming a stable macromolecular structure with mutually supportive components from a global perspective. We formalize a Long CoT trace as a behavior-directed graph G = ( V, E ), where each node v ∈ V represents a reasoning step or edge s = ( u, v ) ∈ E is annotated with a behavior type s → b ∈ {D , R, E} . For a trace corpus C, we estimate the behavior transition PC (b′ | b) over consecutive edges and the marginal distribution 

πC (b). Empirically, strong reasoning teachers produce stable (PC , π C ) across models and tasks. Specifically, these include the following three major bonds †: 

> Deep Reasoning as Covalent Bonds

Deep reasoning forms the bone of the thought process, analogous to covalent bonds defining a molecule’s primary chain. It encodes strong logical dependencies (Step A must justify Step B), maintaining direction and continuity; breaking this bone undermines the following steps and destabilizes the answer. By contrast, “Normal Operation” corresponds to stable local bonds within each step, capturing routine computation and direct semantic expression.  

> Self-Reflection as Hydrogen Bonds

Reflection is a key stabilizer. As proteins gain stability when chains fold and form intra-chain hydrogen bonds, reasoning stabilizes when later steps (e.g., Step 100) test, revise, or reinforce earlier premises (e.g., Step 10). These long-range links constrain drift and hallucination, turning a long sequence into a more self-consistent structure. If later checks fail to align with earlier commitments, the reasoning cannot “fold,” indicating a structural logical error.  

> Self-Exploration as Van der Waals Forces

Exploration resembles transient Van der Waals interactions: it supports abductive and inductive moves by enabling low-commitment associations in semantic space, where concepts can drift, combine, and be probed before stronger constraints are enforced. 

## 4 Verification: Molecular Structure 

4.1 Stable Bond Distribution in Long CoT 

To address this, we first verify whether effective Long CoT traces show stable, macromolecular-like organization across models and tasks. As shown in Fig. 5, traces from multiple LLMs across diverse tasks yield Pearson correlations exceeding 0.9 (p<0.001) for over 2,000 samples. These transfer graphs stabilize with correlations above 0.95 across sampling sizes. This indicates that effective Long CoT structures rely on robust motifs: different models recover similar reasoning topologies across tasks, whereas simple human simulation or ICL cannot emulate the global bond distribution. 

> †Math definitions and proofs are in Appendix C.1.

4(b) Transfer graph on OpenAI -OSS -120B. (c) Transfer graph on QwQ -32B. (a) Transfer graph on DeepSeek -R1 -0528. 

> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> 70
> 60
> 50
> 40
> 30
> 20
> 10

Figure 5 Transfer graph on three different models. Pearson correlation coefficients across models are all greater than 0.9 (p<0.001), when sampling examples > 2,000, transfer graphs will become stable and get over 0.95 Pearson correlation between different sampling sizes. (a) Sparse Crosscoder features with a

non -shared activation ratio (> 0.8)

between the base and SFT models .

(b) Tokens activated by features that 

appear only in the SFT model and not 

in the base model .

(c) Performance of the SFT -trained model when 

keywords in QwQ's reasoning chains are replaced 

with synonyms at different data scales .                                                      

> Base Model Activated Feature At last, "The Golden Compass" is out, [and]
> while it'sgetting decidedly …both of those
> were by dudes from real pplaces (New York
> and California, speci fi cally) .[So] Slate's
> Patrick Radden Keefe is kinda …Marky Mark
> and Russell Crowe are in it probably? [But] it
> was fi lmed in the real -life Boston with real -life
> Bostonian s!…
> I dont have much knowledge of class system in
> Nepal [but ]the class system of Nepalese in
> United States tends to follow the “total number
> of yea rs stay in USA = higher the class ”. It
> might not apply in all cases [but ]mostly does;
> just a thought!!
> SFT Model Activated Feature

Base Model Activated Ratio 

> SFT Model Activated Ratio

0.0 0.2 0.4 0.6 0.8 1.0 This unit is probably too big for you . [But] you         

> might want to consider asmaller less costly …

0.0 

0.8 

0.2 

0.4 

1.0 

0.6 

10 

15 

20 

25 

30 

35 

40 

1K 2K 5K 10K 20K 20K -5epoch 

Origin 

Replace1 

Replace2 

> Accuracy (%)

Data Scale 

Figure 6 The learned features in Long CoT supervised fine-tuning. 

4.2 SFT actually learns these bond structures rather than keywords. 

For the SFT analysis, we consider Llama-3.1-8B-Base that is pre-trained but not instruction-tuned on Long CoT data; Then, we consider Llama-3.1-8B-Base trained on R1-distilled data as Think-SFT model obtained by supervised fine-tuning enriched with Long CoT traces. From a representation-geometry perspective, the sparse auto-encoder analysis shows that Long CoT behavior in the SFT model is concentrated in a small set of discourse-control structures rather than being uniformly distributed across tokens. As shown in Figure 6 (a, b), we train a cross-coder sparse auto-encoder that jointly models hidden states from the base model and the Think-SFT model, and then identify features whose activation on think tokens is at least threefold higher than their average activation rate. Within this highly activated subset, many features are predominantly driven by a few connective keywords, such as “Maybe” ,

“But / so” , and “Alternatively” , indicating that the SFT process has carved out dedicated latents for managing local hypothesis revision, contrastive moves, and branch selection in long CoT traces. Based on previous analyses, we argue that models learn the characteristic reasoning behaviors these keywords represent, not the keywords themselves. Following Chen et al. [5] , we define three categories of Long CoT reasoning behaviors and test this claim using two training datasets derived from QwQ distillation data. In the first, we replace each keyword with one of four alternative variants. In the second, we remove all keywords while preserving the reasoning trajectories. We then fine-tune identical LLMs on each dataset and evaluate their Long CoT reasoning performance. As shown in Figure 6 (c), explicit keywords like "wait" accelerate learning but are not essential. Models trained 5Normal Operation 

> Self -Re fl ection
> Deep Reasoning
> Self -Exploration
> Previous Step Group
> Major Step Group

Figure 7 Verification of the “logical folding” structure on embeddings from Qwen2.5-32B-Instruct and t-SNE-based low-dimensional representations from the OpenAI-OSS-120B-generated reasoning process. Long CoT Long CoT Long CoT 

Long CoT 

q

k

> qk =61.20
> Ed=-3.9e3/ kT
> Dd=3.9e3/ kT

Long CoT Long CoT 

> qk =16.87
> Ee=-1.1e3/ kT
> De=1.1e3/ kT

q k

q

k

(a) Energy levels of different bonds on QwQ -Distilled Models 

(b) Energy levels of different bonds on OSS -Distilled Models   

> qk =34.44
> Er=-2.2e3/ kT
> Dr=2.2e3/ kT
> Deep Reasoning Self -Reflection Self -Exploration Reasoning Chain

q

k

> qk =32.29
> Ed=-2.1e3/ kT ’
> Dd=2.1e3/ kT ’
> qk =9.32
> Ee=-0.6e3/ kT ’
> De=0.6e3/ kT ’

q k

q

k

> qk =21.82
> Er=-1.4e3/ kT ’
> Dr=1.4e3/ kT ’

Figure 8 Energy levels of different bonds across two distilled data. 

without keywords, or with arbitrary alternatives, achieve comparable reasoning performance given sufficient training, provided the underlying reasoning behaviors remain intact. This reveals a fundamental insight: LLMs internalize reasoning structure of reasoning rather than surface lexical cues. Consequently, training data should prioritize the distribution of reasoning behaviors over specific keyword choices to effectively enhance model reasoning capabilities. However, a key open question remains: do these bonds drive Long CoT structure learning, and if so, why do explicit human imitation or random ICL distillation of these markers often fail? 

4.3 ‘‘Logical Bonding-Folding’’ Structure 

To test the hypothesis that Long CoT is analogous to macro-molecular folding, as shown in Fig. 7, we analyzed the topology of CoT in a 3D semantic space. Each trajectory edge was classified as reflection, deep reasoning, or exploration, and its geometric properties were quantified. 

Deep-Reasoning stabilizes logical cluster by Covalent Bonding. Modeled as covalent bonds, as shown in Fig. 7 (a), deep reasoning bonds mainly increase local connectivity, forming stable subdomains. After deep reasoning, 72.56% of steps remained within a group distance of less than 3 in the semantic space (generally, group-group distance >5.6). 

Self-Reflection drives strong folding to previous steps by Hydrogen Bonding. Analogous to hydrogen bonds, as shown in Fig. 7 (b), self-reflection transitions fold later steps back onto earlier, semantically similar clusters rather than extending the chain linearly. Quantitatively, 81.72% of reflection steps reconnected to a previously formed cluster with high semantic similarity. 6Model GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench AVG                                                                      

> LLaMA-3.1-8B-Base [9] 7.58 3.20 0.00 0.00 4.22 1.19 2.70
> +20K R1-Distill-Data 63.38 30.60 0.21 0.42 14.22 8.30 19.52
> +20K OSS-Distill-Data 75.89 54.20 4.38 6.46 37.34 23.85 33.69
> +20K QwQ-Distill-Data 64.53 32.20 2.92 0.42 16.72 8.89 20.95 Llama-3.1-8B-Instruct [9] 75.89 35.20 4.17 1.04 23.59 12.00 25.32
> +20K R1-Distill-Data 79.91 60.60 2.50 3.88 33.13 23.85 33.98
> +20K OSS-Distill-Data 79.00 60.80 10.83 7.71 47.03 30.22 39.27
> +20K QwQ-Distill-Data 82.41 60.80 4.38 8.33 32.97 25.48 35.73
> Table 1

Results across six benchmarks. Full results are reported in Table 5 in the Appendix.  

> Self-Exploration gently links different long-distance clusters by Van der Waals Forces.

In contrast to local stabilization (Fig. 7 (c)), exploration transitions act as loose links between otherwise separated clusters. They show much larger step-to-step distances, with an average trajectory length of 5.32 in the 3D t-SNE projection. Together, these results suggest that effective Long CoT reasoning is not a simple linear chain; instead, it forms a folded, domain-structured topology consistent with the “logical folding” hypothesis. 

4.4 Attention ⇔ Energy Level of Bonds 

In physical chemistry, the behavior probability with energy level Ei at temperature T follows the Boltzmann distribution: 

P (state i) = exp   − Ei/k B T P 

> j

exp   − Ej /k B T  . (1) In Transformers, the attention weight αij of the i-th token to the j-th token is: 

αij = exp  qi · kj /√dk

P 

> l

exp  qi · kl/√dk

 . (2) The correspondence follows by defining attention energy (E) ↔ (−q · k), implies lower Eij and thus higher behavior probability. Formally, our analysis only relies on the observation that attention weights form a Gibbs–Boltzmann distribution over negative logits. We therefore use the term “energy” to denote reparameterized logits and study how their expectations differ across behavior types. We then compare attention energy across Long CoT transition types. Fig. 8 shows distinct distributions for deep reasoning, reflection, and exploration. Deep reasoning exhibits the largest effective chemical-bond energy Dd. Reflection is intermediate, whereas exploration shows the weakest effective bond energy. This ordering and the relative proportions are consistent across models, supporting the hypothesis that a bond-like mechanism broadly links these reasoning behaviors. Takeaway 2 

• Long CoT reasoning exhibits stable structures across models, with reasoning topologies converging. 

• Semantic isomers, reasoning chains with identical concepts but different logical bonds, succeed or fail based on bond structure, not surface keywords. 

• Three distinct logical bonds drive CoT structure: reflection folds back to prior clusters, deep reason-ing creates stable local domains, and exploration bridges distant concepts, each with characteristic attention energy profiles matching Boltzmann-like distributions. 

• SFT learns reasoning structure rather than surface keywords, determining Long CoT capability. 

## 5 Feature: Effective Semantic Isomers 

We now shift to examine the failure of surficial distillation: similar conceptual atoms can be linked by different bonds, producing completely different reasoning chains. We call these as Semantic Isomers . Mathematically, 70.8                                    

> 0.6
> 0.4
> 0.2
> 0.6 0.4 0.2
> Human 1
> Human 2
> Human 3
> Human 4
> DeepSeek -R1
> 0.8
> 0.6
> 0.4
> 0.2
> 0.6 0.4 0.2
> Human 1
> Human 2
> Human 3
> Human 4
> DeepSeek -R1
> 0.0
> Uncertainty (Entropy)
> Cognitive Effort (  Δ Entropy)
> Uncertainty (Entropy)
> Cognitive Effort (  Δ Entropy)
> 0.8
> 0.6
> 0.4
> 0.2
> 0.6 0.4 0.2
> Human 1
> Human 2
> Human 3
> Human 4
> DeepSeek -R1
> 0.0
> Uncertainty (Entropy)
> Cognitive Effort (  Δ Entropy)
> 0.8
> 0.6
> 0.4
> 0.2
> 0.6 0.4 0.2
> Human 1
> Human 2
> Human 3
> Human 4
> DeepSeek -R1
> 0.0
> Uncertainty (Entropy)
> Cognitive Effort (  Δ Entropy)
> 0.8
> 0.6
> 0.4
> 0.2
> 0.8
> 0.6
> 0.4
> 0.2
> Uncertainty (Entropy)
> Cognitive Effort (  Δ Entropy)
> Uncertainty (Entropy)
> Cognitive Effort (  Δ Entropy)
> 0.8
> 0.6
> 0.4
> 0.2
> Uncertainty (Entropy)
> Cognitive Effort (  Δ Entropy)
> 0.8
> 0.6
> 0.4
> 0.2
> Human 1
> Human 2
> Human 3
> Human 4
> DeepSeek -R1
> Human 1
> Human 2
> Human 3
> Human 4
> DeepSeek -R1
> Human 1
> Human 2
> Human 3
> Human 4
> DeepSeek -R1
> Human 1
> Human 2
> Human 3
> Human 4
> DeepSeek -R1
> 0.6 0.4 0.2 0.6 0.4 0.2 0.0 0.6 0.4 0.2 0.0 0.6 0.4 0.2 0.0
> Uncertainty (Entropy)
> Cognitive Effort (  Δ Entropy)

Figure 10 Information flow analysis for humans and reasoning models. 

a semantic isomer of D is any D′ whose (PD′ , π D′ ) lies near (PD , π D ) under a suitable divergence D(· ∥ · ).Then, we will analyze how they form, are learned, and sometimes destabilize in Long CoT. 

5.1 Bond structure of Semantic Isomers is the key to Long-CoT learning 

Well-structured Semantic Isomers can be effective for Long-CoT learning. To assess whether well-structured Semantic Isomers can enhance Long-CoT learning, we constructed a Long CoT dataset by distilling from advanced reasoning LLMs. As shown in Table 1, models exhibit consistent performance gains when trained on these variants (distribution correlation ∼0.9). This suggests that, within a certain range, multiple effective “allotropic” variants of reasoning keys can exist. 

The model has multiple effective Semantic Isomers, and slight differences can significantly affect the results. 

As shown in Table 1, the structural correlation between R1- and OSS-derived chains can reach 0.95, yet for some models performance with R1-based chains drops by more than 10%. This gap indicates the presence of multiple near-optimal Semantic Isomers that are nonetheless fragile, where small changes in the distribution can cause large performance losses. 0

10 

20 

30 

40 

0.1 0.3 0.5 0.7 0.9 

Pearson’s r of Demonstration 

> Accuracy  (%)

R1 -Demo -ICL 

OSS -Demo -ICL 

Llama3 

Qwen2.5 

Figure 9 The performance of LLama-3.1-8B-Instruction under three dif-ferent distillation setups from ICL-augmented Qwen2.5-32B. 

Simulating effective Semantic Isomer structures is the key to ICL dis-tillation. We analyze three ICL setups to simulate Long CoT chains by Qwen2.5-32B-Instruct: demonstrations chosen at random; demon-strations with closely aligned reasoning-key distributions (correlation 

∼0.9); and demonstrations with clearly mismatched distributions (cor-relation < 0.8). As shown in Fig. 9, substantial performance gains emerge only when demonstrations are constructed to match a specific target distribution of reasoning keys, thereby yielding an appropriate “allotrope” of that distribution. 

5.1.1 Not all bonds in isomers are effective. 

To clarify the nature of semantic isomers, we examine which bond structures yield effective reasoning configurations. We hypothesize that functional viability depends on specific bond distributions: despite sharing identical conceptual nodes, incompatible configurations disrupt information exchange. For instance, excessive exploration bonds cause fragmented reasoning, whereas overemphasized deep reasoning bonds create rigid chains unable to adapt to new inputs.Details are provided in Appendix D.3. 8(a) DeepSeek -R1 Semantic Isomers                                 

> with stable molecular structures .
> (b) OpenAI -OSS Semantic Isomers
> with stable molecular structures .
> (c) Learned sub -optimal Semantic Isomers
> with unstable molecular structures .
> Pearson’s r < 0.7
> Sampling Size (t)
> Pearson’s r
> (d) Pearson correlation coefficient
> between transfer distributions .
> 37.9 34.0
> 39.3
> 31.9 31.6
> (e) Performance on different training data mixture strategies .
> Accuracy (%)
> OSS -Distill -Data
> R1 -Distill -Data
> R1 -then -OSS
> OSS -then -R1
> R1 -mix -OSS
> 0.7
> 0.5
> 0.6
> OpenAI -OSS DeepSeek -R1 Figure 11 Conflict Learning between Two Stable Molecular Structures.
> Effective reasoning bond distribution influences the information divergence speed in reasoning dynamics.

To assess this, we compared the reasoning dynamics of R1 models with human cognition in an information phase space [ 10 ]. Mechanistically, LLMs update by maximizing rewards and reducing entropy, whereas human reasoning is additionally constrained by semantic coherence and social feedback. Consequently, machine reasoning converges through accumulated gradient updates, whereas human reasoning stabilizes through iterative self-monitoring and social calibration. As shown in Figure 10, we tracked reasoning unfolding over extended chains in logical deduction tasks. Humans typically exhibit nearly uniform forward information gains (81.3% of cases show changes < 0.1), corresponding to a near-zero slope in phase space. In contrast, R1 models display accelerating informativeness (76.1% of cases show absolute changes > 0.1), progressing from low entropy to rapid convergence. These patterns indicate a fundamental difference in how R1 models and humans integrate information over time.  

> Effective reasoning bonds cause metacognitive oscillation and alignment.

We identify the core rationale for this difference as a "metacognitive oscillation" in LLMs. Their responses alternate between high-entropy divergent exploration (slope > 0.6, ∆entropy > 0.05 ) and stable convergent validation, a dynamic less pronounced in the uniform entropy profiles of humans. Case studies (Figure 10) confirm that R1 models utilize self-reflective revision to adjust reasoning paths against uncertainty. We hypothesize that aligning training objectives with these behavioral structure distributions can narrow the gap between machine and human reasoning dynamics. 

5.2 Conflict between Two Stable Structures 

Understanding how distinct reasoning structures interact reveals fundamental limits of complex cognitive systems. As shown in Fig. 11 (a–c), forcibly fusing stable molecular isomers disrupts their backbone; analogously, combining incompatible reasoning frameworks breaks global logical coherence.  

> Learning two heterogeneous stable structures at the same time will lead to structural chaos in the model.

As shown in Fig. 11 (d), we test this by jointly activating two highly correlated ( r ≈ 0.9) reasoning chains from DeepSeek-R1 and OpenAI-OSS. Despite their similarity, co-activation prevents the model from converging to a single stable behavioral mode: it produces molecular bond distributions that fluctuate across samples and deviate from those characteristic of either OSS or R1. Consistent with this instability, the self-correlation of the jointly activated model does not exceed 0.8. 9Model GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench AVG 

LLaMA-3.1-8B-Base [9] 7.58 3.20 0.00 0.00 4.22 1.19 2.70 

+ 20K Qwen-Distill-Data 62.47 29.40 0.00 0.00 12.81 6.81 18.58 

+ 20K OSS-Distill-Data 75.89 54.20 4.38 6.46 37.34 23.85 33.69 

+ 20K QwQ-Distill-Data 64.53 32.20 2.92 0.42 16.72 8.89 20.95 

+ 20K OSS-M O L E -S Y N 67.85 35.20 1.83 0.83 20.53 11.11 22.89 

+ 20K QwQ-M O L E -S Y N 66.41 35.00 2.08 0.63 20.16 10.37 22.44 Llama-3.1-8B-Instruct [9] 75.89 35.20 4.17 1.04 23.59 12.00 25.32 

+ 20K Qwen-Distill-Data 76.50 39.80 4.38 1.04 25.63 19.70 27.84 

+ 20K OSS-Distill-Data 79.00 60.80 10.83 7.71 47.03 30.22 39.27 

+ 20K QwQ-Distill-Data 82.41 60.80 4.38 8.33 32.97 25.48 35.73 

+ 20K OSS-M O L E -S Y N 83.24 51.80 4.79 1.04 32.50 21.04 32.40 

+ 20K QwQ-M O L E -S Y N 84.31 50.20 5.21 1.67 32.34 20.00 32.29 

Table 2 Performance comparison across six benchmarks. Here, “ ”: distill from instruction LLMs+ Mole-Syn , “ ”: distill from reasoning LLMs, “ ”: distill from instruction LLMs. See Table 6 in Appendix for full results. (b) The reinforcement learning reward                                                       

> curve of Graph -Syn .
> (c) The reinforcement learning length
> scaling curve of Graph -Syn .
> 1200 400 600 800 1000
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> Reinforcement Learning Step s
> Reward
> 1200 400 600 800 1000
> 0
> 2.0
> 4.0
> Reinforcement Learning Step s
> Response Length (K)
> 0200 400 600 800 1000
> 0.51
> 0.57
> 0.63
> 0.69
> Reinforcement Learning Step s
> Accuracy (%)
> 6.0
> (d) The reinforcement learning accuracy
> curve of Graph -Syn on MATH -500 .
> Llama -3.1 -8B -Instruct
> +QwQ -Distill Data +QwQ -Distill Data + RL
> +QwQ -Graph -Syn +QwQ -Graph -Syn + RL
> + 35K QwQ -Graph -Syn + RL
> +QwQ -Distill Data +QwQ -Graph -Syn
> 25
> 30
> 35
> 40
> 45
> 20
> Accuracy (%)
> (a) Analysis of reinforcement learning
> potential of Graph -Syn .

Figure 12 The continual improvement of RL performance with Mole-Syn -initialized models. More details about the RL performance are in Table 8 in Appendix. 

This structural chaos leads to a significant decline in the performance of the model. As shown in Fig. 11 (e), joint activation also causes a marked drop in performance relative to either chain alone. This seemingly paradoxical effect indicates that structural compatibility, rather than mere statistical correlation, governs whether reasoning systems can coexist. The interference pattern suggests that the underlying cognitive architecture is rigid: without careful alignment, attempts to merge such systems yield fragmented, low-utility outputs instead of enhanced capability. Takeaway 3 

• Well-structured isomers perform effectively when their reasoning-key distributions align closely, but small structural shifts cause fragility and sharp performance losses. 

• Co-activating incompatible reasoning structures induces structural chaos that breaks coherence and degrades performance, proving that statistical similarity does not guarantee compatibility. 

## 6 Synthetic Chemistry: Synthesis Long CoT Molecules from Scratch 

LLMs may acquire advanced reasoning partly through exposure to explicit, structured Long CoT reasoning traces. However, it remains unclear how reliably such structures can be induced by prompting an instruction-tuned model, rather than obtained through distillation. 

Mole-Syn Methodology. To address this gap, we propose a synthetic framework, Mole-Syn , that views targeted reasoning traces as macromolecular structures using only instruction LLMs. This method is a random walk on a transition probability graph in Figure 5 composed of 4 reasoning behaviors from stronger reasoning LLMs that support Long CoT. 10 30                                   

> 20
> 10
> 0
> -10
> -20
> -30
> 30
> 20
> 10
> 0
> -10
> -20
> -30 30 20
> 10
> 0
> -10 -20 -30
> AVG Volume= 23.95
> tsne -axis -1
> tsne -axis -2
> tsne -axis -3
> 30
> 20
> 10
> 0
> -10
> -20
> -30
> 30
> 20
> 10
> 0
> -10
> -20
> -30 30 20
> 10
> 0
> -10 -20
> -30
> AVG Volume= 35.20
> tsne -axis -1
> tsne -axis -2
> tsne -axis -3 30
> 20
> 10
> 0
> -10
> -20
> -30
> 30
> 20
> 10
> 0
> -10
> -20
> -30 30 20
> 10
> 0
> -10 -20 -30
> tsne -axis -1
> tsne -axis -2
> tsne -axis -3
> 30
> 20
> 10
> 0
> -10
> -20
> -30
> 30
> 20
> 10
> 0
> -10
> -20
> -30 30 20
> 10
> 0
> -10 -20
> -30
> AVG Volume= 29.22
> tsne -axis -1
> tsne -axis -2
> tsne -axis -3
> tsne -axis -2
> 30
> 20
> 10
> 0
> -10
> -20
> -30
> 30
> 20
> 10
> 0
> -10
> -20
> -30 30 20
> 10
> 0
> -10 -20
> -30
> AVG Volume= 30.12
> tsne -axis -1
> tsne -axis -3 30
> 20
> 10
> 0
> -10
> -20
> -30
> 30
> 20
> 10
> 0
> -10
> -20
> -30 30 20
> 10
> 0
> -10 -20 -30
> tsne -axis -1
> tsne -axis -2
> tsne -axis -3
> (a) The shaping function of Deep Reasoning in semantic space. (c) The shaping function of Self -Exploration in semantic space .
> (b) The shaping function of Self -Reflection in semantic space .
> Coverage Ball with Minimum Volume
> Embedding of Self -Exploration Related Logic Node
> Embedding of Deep Reasoning Related Logic Node
> Embedding of Self -Re fl ection Related Logic Node
> Embedding of Normal Operation Related Logic Node
> Embedding changes before and after training
> AVG Volume= 31.20
> AVG Volume= 23.47
> Expand the Volume
> Reduce the Volume

Figure 13 Roles of individual bonds in reasoning, inferred from semantic-space comparisons between Llama-3.1-8B-Instruct and Llama-3.1-8B-Instruct + QwQ-Mole-Syn . Performance impact is shown in Fig. 16 (Appendix). 

Mole-Syn can synthesize effective bond structures. To test whether Long CoT capabilities can be learned from instruction LLMs, in Table 2, we conduct training on Mole-Syn generated data, which even achieves reasoning performance close to QwQ distillation. This suggests that instruction-driven synthesis can induce useful structural regularities, enabling lower-cost behavior transfer. 

Mole-Syn can further trigger stronger and continually improving RL. We further evaluate Mole-Syn -initialized LLMs’ potential in reinforcement learning (RL). Mole-Syn -initialized LLMs outperform those initialized from base LLMs. In Figure 12 (a), they show steadier fine-tuning gains, indicating stronger immediate reasoning and more reliable RL adaptation. Moreover, Figures 12 (b-d) show that these gains persist over extended RL training, demonstrating durable benefits from synthesized Long CoT structures. This effective use under RL supports their practical utility across diverse cognitive tasks. Takeaway 4 

• Mole-Syn successfully synthesizes Long CoT structures that match transition distributions from capable teacher models without requiring Long CoT distillation data. 

• Transition-based Long CoT datasets achieve stable convergence and near-distillation performance, proving effective reasoning structures emerge purely from instruction-level synthesis at lower cost. 

• Models initialized with synthesized Long CoT weights demonstrate superior and sustained RL performance gains, providing a robust foundation for continual learning in dynamic environments. 

## 7 Function: Shaping Function of Each Bond in Long CoT Structure 

Further, we analyze the shaping function of each bond in Long CoT structure. We suppose that LLM searches for an optimal semantic configuration parallels a protein’s descent along a folding funnel toward a low-energy native state (the solution). 

Deep Reasoning is densing major Structure Formation. As reasoning proceeds, Deep Reasoning drives Primary Structure formation by synthesizing a logical backbone, analogous to Covalent Bonding. As shown in Figure 13 (a), Deep Reasoning densifies the core logical structure: the volume of the smallest covering ball in semantic space decreases by 22% compared with baseline. This stage builds the answer’s skeleton, but it does not yet ensure global stability or correctness. 11 Model GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench AVG                                                                

> Llama-3.1-8B-Instruct 75.89 35.20 4.17 1.04 23.59 12.00 25.32
> + 20K Gemini-Distill-Data 54.89 22.60 1.04 0.63 12.03 7.41 16.43
> + 20K Claude4-Distill-Data 63.76 35.80 1.04 0.83 23.44 13.48 23.06 Qwen-2.5-32B-Base 53.68 33.40 9.17 2.29 35.63 15.85 25.00
> + 20K Gemini-Distill-Data 52.54 20.20 1.88 0.63 21.41 12.44 18.18
> + 20K Claude4-Distill-Data 63.31 37.80 2.92 0.83 28.91 17.78 25.26 Qwen-2.5-32B-Instruct 93.71 81.00 15.60 14.17 69.84 42.22 52.76
> + 20K Gemini-Distill-Data 63.68 32.80 15.00 2.92 35.63 19.11 28.19
> + 20K Claude4-Distill-Data 76.88 54.80 17.71 13.96 55.00 30.96 41.55

Table 3 Distillation Results from Gemini and Claude.        

> Model GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench AVG

Llama-3.1-8B-Instruct 75.89 35.20 4.17 1.04 23.59 12.00 25.32 

+ 20K OSS-Summarized 54.89 22.60 1.04 0.63 12.03 7.41 16.43 

+ 20K QwQ-Summarized 63.76 35.80 1.04 0.83 23.44 13.48 23.06 Qwen-2.5-7B-Instruct 83.24 74.00 12.50 7.08 22.66 38.07 39.59 

+ 20K OSS-Summarized 82.34 72.60 12.50 6.46 21.41 27.70 37.17 

+ 20K QwQ-Summarized 82.71 71.80 11.88 6.25 22.97 25.04 36.77 

Table 4 Results from summarized Long CoT trajectories based on OpenAI-OSS and QwQ distilled trajectories. 

Self-Reflection is densing and stabilize the global logics. After the bone is extended, Self-Reflection stabilizes the structure. Analogous to hydrogen bonds, reflection “folds” the logical chain by cross-linking distant nodes to test consistency, rather than adding new deductive steps. As shown in Figure 13 (b), it consolidates the hydrophobic core and suppresses inconsistent branches, reducing the system volume from 35.2 to 31.2 and moving toward a stable, folded optimal state. 

Self-Exploration expand logical space. Self-Exploration broadens the set of feasible solutions and also increases variability and may introduce less consistent branches, so it improves coverage at the cost of reduced immediate stability. As shown in Figure 13 (c), after learning Long CoT structure, exploration expands the exploration behavior in semantic space from 23.95 to 29.22. Takeaway 5 Long CoT reasoning mimics protein folding through three stages: Deep Reasoning densifies the logical backbone, Self-Exploration expands the search space to avoid local minima, and Self-Reflection converges toward a stable, optimized solution state in semantic space. 

## 8 Deteriorated Molecular Structure Cannot Be Easily Restored 

How Current Private LLMs Protect Their Long CoT from Distillation? Exposing reasoning traces allows LLMs to imitate both answers and procedures. Common defenses always consider compressing intermediate steps. We quantify this by distilling from Gemini-2.5-Pro-Thinking and Claude-4-Sonnet. Table 3 shows that beyond 

∼45% token reduction versus QwQ-32B rationales, distillation causes accuracy drops, showing compression can disrupt Long CoT structure. 

Summarization break reasoning bond distributions to prohibit distillation. To further validate the effectiveness of summarization, we summarized Long-CoT traces from QwQ and R1. Table 4 shows that training on compressed trajectories yields weaker performance than training on full traces and reduces distillation effectiveness by 2%. In Figure 14, summarization shifts reasoning behavior distributions and creates a gap between observable outputs and internal error-bounded transitions, limiting trace inversion and behavioral cloning. However, compression can also protect model architecture and embedded inductive priors from 12 (a) Transfer graph on QwQ -32B.            

> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> 70
> 60
> 50
> 40
> 30
> 20
> 10
> (b) Transfer graph on summarized QwQ -32B.
> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> Normal
> Operation
> Deep
> Reasoning
> Self -
> Reflection
> Self -
> Exploration
> Long CoT
> q
> k
> qk =12.13
> Long CoT
> q
> k
> qk =4.50
> Long CoT
> qk
> qk =15.97
> (c) Energy levels of different bonds on
> Summarized -QwQ -Distilled Models .
> Deep Reasoning
> Self -Reflection
> Self -Exploration
> Reasoning Chain

Figure 14 The reasoning behavior distribution of summarized QwQ traces. 

unauthorized imitation. Takeaway 6 Summarization and reasoning compression effectively protect Long CoT structures from distillation by disrupting structural coherence, preventing unauthorized replication of internal reasoning processes. 

## 9 Related Work 

Chain-of-Thought (CoT) elicits intermediate rationales, improving multi-step reasoning on math and logic tasks. Scaling short CoT to Long CoT is difficult: coherence often degrades, and cold-start gains frequently require targeted training or high-quality trajectories [ 8, 11 ]. A common approach distills stepwise solutions from strong teacher models into weaker students [ 12 , 13 ]. Outcomes hinge on reasoning quality: strong reasoning models transfer useful behaviors [ 14 ], whereas weaker instruction models may mimic format without robust Long CoT capability [7, 15]. Long CoT training elicits three behaviors: deep reasoning, self-reflection, and self-exploration [ 5, 8, 16 ]. Work has studied their roles; Madaan et al. [17] used self-reflection to revise earlier steps, and Shinn et al. [18] combined reflection and exploration to improve robustness. Early studies framed CoT as sequences of these behaviors, emphasizing step-level imitation and local coherence [ 1, 3, 19 ]. Later work used tree- or graph-structured reasoning to capture branching and revisitation [20–23]. Although trees or graphs represent individual Long CoT traces by modeling behaviors as nodes, they do not capture the overall distribution of logical behaviors. In contrast, our approach models Long CoT as a molecular-like structure, with edges encoding stable distributions of reasoning behaviors, to test how their arrangement and interactions support effective learning. 

## 10 Conclusion 

This study provides a mechanistic account of Long CoT learning, conceptualized as molecular-like reasoning structures emerging through Self-Reflection, Deep-Reasoning, and Self-Exploration bonds. We propose a semantic isomer framework through behavior-transition distributions, offering insight into the stability and failure modes of Long CoT learning. Building on these, our Mole-Syn leverages distribution transfer graphs to construct robust Long CoT structures, thereby improving performance and enhancing the stability of RL. 13 Limitations 

While our approach achieves strong performance on several reasoning benchmarks, it still has several limitations: First, limited by cost and scale constraints, our analysis relies on a limited set of teacher models and student backbones, which may introduce bias in observed statistical patterns of Long CoT toward specific architectures or training recipes. Second, we focus on offline distillation and supervised fine-tuning, leaving open how well the method scales in realistic online or interactive settings with RL-like feedback. We can only approximately visualize the geometric characteristics of the inferred bond in information and semantic spaces. However, accurately delineating a universal Long CoT macromolecular structure remains an important future direction. Finally, our behavior analysis relies on an automatically labeled dataset. Even though we provide an initial robustness check, label noise or bias will inevitably appear on estimated distributions. 

## References 

[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022. [2] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [3] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022. [4] Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, and Philip S Yu. Large language models meet nlp: A survey. arXiv preprint arXiv:2405.12819, 2024. [5] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: A survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [6] Xinghao Chen, Zhijing Sun, Guo Wenjin, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, and Xiaoyu Shen. Unveiling the key factors for distilling chain-of-thought reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025 , pages 15094–15119, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.782. URL 

https://aclanthology.org/2025.findings-acl.782/ .[7] Wei Du, Branislav Kisacanin, George Armstrong, Shubham Toshniwal, Ivan Moshkov, Alexan Ayrapetyan, Sadegh Mahdavi, Dan Zhao, Shizhe Diao, Dragan Mašulović, Advaith Avadhanam, Max Wang, Shitij Govil, Sri Yanamandra, Mihir Tandon, Sriram Ananthakrishnan, Vedant Rathi, David Zhang, Joonseok Kang, Leon Luo, Titu Andreescu, Ashmit Dutta, Boris Ginsburg, and Igor Gitman. The challenge of teaching reasoning to LLMs without RL or distillation. In 2nd AI for Math Workshop @ ICML 2025, 2025. URL https://openreview. net/forum?id=fOjo1OHbSK .[8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633–638, 2025. [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv–2407, 2024. [10] Qiguang Chen, Jinhao Liu, Libo Qin, Yimeng Zhang, Yihao Liang, Shangxu Ren, Chengyu Luan, Dengyun Peng, Hanjing Li, Jiannan Guan, et al. The universal landscape of human reasoning. arXiv preprint arXiv:2510.21623, 2025. [11] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. 

14 [12] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori B Hashimoto. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 20286–20332, 2025. [13] Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 million open-source distilled reasoning dataset to empower large language model training. arXiv preprint arXiv:2503.19633, 2025. [14] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [15] Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, and Aaron Courville. Shape of thought: When distribution matters more than correctness in reasoning tasks. arXiv preprint arXiv:2512.22255, 2025. [16] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/ .[17] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:46534–46594, 2023. [18] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:8634–8652, 2023. [19] Qiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. Unlocking the capabilities of thought: A reasoning boundary framework to quantify and optimize chain-of-thought. Advances in Neural Information Processing Systems, 37:54872–54904, 2024. [20] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:11809–11822, 2023. [21] Yao Yao, Zuchao Li, and Hai Zhao. Got: Effective graph-of-thought reasoning in language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 2901–2921, 2024. [22] Mengkang Hu, Yao Mu, Xinmiao Chelsey Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/ forum?id=Glcsog6zOe .[23] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 17682–17690, 2024. [24] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. [25] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [26] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [27] AMC. American mathematics competitions, 2023. URL https://artofproblemsolving.com/wiki/index. php/AMC_Problems_and_Solutions .[28] AIME. American invitational mathematics examination (aime) aime 2024-i & ii, 2024. URL https: //huggingface.co/datasets/Maxwell-Jia/AIME_2024 .

15 [29] AIME. American invitational mathematics examination (aime) 2025-i & ii, 2025. URL https://huggingface. co/datasets/opencompass/AIME2025 .[30] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828–3850, 2024. [31] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [32] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/ qwq-32b-preview/ , 2024. [33] OpenAI. Gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://platform.openai.com/docs/ models/gpt-oss-120b .[34] Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. ROSCOE: A suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=xYlJRpzZtsY .[35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [36] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [37] Thomas Jiralerspong and Trenton Bricken. Model diffing without borders: Unlocking cross-architecture model diffing to reveal hidden ideological alignment in llama and qwen. In Mechanistic Interpretability Workshop at NeurIPS 2025, 2025. URL https://openreview.net/forum?id=ZB84SvrZB8 .[38] Jack Lindsey, Adly Templeton, Jonathan Marcus, Thomas Conerly, Joshua Batson, and Christopher Olah. Sparse crosscoders for cross-layer features and model diffing. 2025. URL https://transformer-circuits.pub/ 2024/crosscoders/index.html .[39] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, et al. A survey on in-context learning. In Proceedings of the 2024 conference on empirical methods in natural language processing, pages 1107–1128, 2024. [40] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [41] Hemish Veeraboina. Aime problem set 1983-2024. 2023. URL https://www.kaggle.com/datasets/ hemishveeraboina/aime-problem-set-1983-2024 .

16 Appendix 

## A General Experimental Setting  

> Target Model.

Unless otherwise specified, we start from a generic instruction-tuned LLM (e.g., Llama-3.1-8B-Instruct [ 9]). The model is standard pre-trained and supervised fine-tuned, but is not specifically optimized for long chain-of-thought reasoning.  

> Data Source.

We randomly sample 20K–35K high-quality Long CoT queries from OpenThoughts-3 [ 24 ] as the primary training corpus. The dataset provides multi-step reasoning traces, averaging over 20 steps per example, and covers diverse mathematical and logical problem types.  

> Evaluation Benchmarks.

We evaluate on 6 challenging mathematical reasoning benchmarks that require multi-step logic: 

• GSM8K [25]: Grade-school math problems requiring multi-step calculations. 

• MATH-500 [26]: High-school and early college-level problems across major math topics. 

• AMC 2023 [27]: High-school competition problems in algebra, geometry, and combinatorics. 

• AIME 2024 [28]: Invitational-level problems with integer final answers. 

• AIME 2025 [29]: Invitational-level problems with integer final answers. 

• OlymBench [30]: A comprehensive Olympiad-level benchmark. Unless stated otherwise, (Overall) Accuracy is the mean accuracy across all benchmarks above.  

> Inference and Metrics.

For evaluation, we use the same sampling temperature as in reinforcement learning, 

= 0 .6, to reduce the SFT–RL mismatch. We report Avg@1 accuracy for GSM8K, MATH-500, and OlymBench. For smaller test sets (AMC 2023 and AIME 2024/2025), we report Avg@16 accuracy. We instruct the model to output the final answer in \boxed{ ·} format to simplify extraction. We then parse boxed answers and match them to reference solutions using standard answer-matching tools (e.g., math-verify ). Except for specifically specified variables, all model training settings are the same. 

## B Detailed Experimental Settings for Preliminary Study 

In this appendix, we detail the experimental setup for the preliminary study on cold-start Long CoT. We compare 3 data construction pipelines. For all pipelines, we fine-tune the model for 1 epoch with a learning rate of 2e − 5 and a global batch size of 128 , using a max sequence length of 16 K or 32 K tokens (we select the better-performing setting between these two lengths). We use Qwen2.5 [ 31 ] and Llama3.1 [ 9] as backbones to cold-start Long CoT across multiple model sizes.  

> Setting 1: Distillation from Strong Reasoning LLMs.

To construct a high-quality synthetic dataset, we use a reasoning LLM as the teacher. Specifically, we distill Long CoT reasoning traces from DeepSeek-R1-671B-0528 [8], QwQ-32B [32], OpenAI-OSS-120B [33].  

> Setting 2: Distillation from Weak Instruction LLMs (ICL-Distill).

To simulate surface imitation via in-context learning in instruction-tuned LLMs, we use a standard instruction model (e.g., Qwen2.5-32B-Instruct) that is not optimized for deep reasoning. We randomly select 1-shot exemplar (an R1-generated Long CoT trace) in the prompt, and generate solutions for the same problems as in Setting 1.   

> Setting 3: Fine-tuning on Human-Annotated Traces.

We utilize 50 R1-generated reasoning traces and 50 human-written step-by-step solutions for supervised fine-tuning to compare against human-annotated reasoning traces. The data are collected from Du et al. [7]. 17 0      

> 20
> 40
> 60
> 80
> 100
> 0
> 20
> 40
> 60
> 80
> 100
> Accuracy  (%) Accuracy  (%)
> GSM8K MATH -500 AMC2023 OlymBench AIME24 AIME25
> LLaMA3 -
> 8B -Instruct
> +ICL -
> Distill
> +R1 -
> Distill
> +QwQ -
> Distill
> +OSS -
> Distill
> Qwen2.5 -
> 7B -Instruct
> +ICL -
> Distill
> +R1 -
> Distill
> +QwQ -
> Distill
> +OSS -
> Distill
> Qwen2.5 -
> 32B -Instruct
> LLaMA3 -
> 70B -Instruct
> +ICL -
> Distill
> +R1 -
> Distill
> +QwQ -
> Distill
> +OSS -
> Distill
> +ICL -
> Distill
> +R1 -
> Distill
> +QwQ -
> Distill
> +OSS -
> Distill Figure 15 The full failure result of ICL from weak instruction-following LLMs and Human-annotated reasoning traces to acquire Long CoT structures, compared to successful distillation from strong reasoning LLMs.

## C Reasoning Bond in Long CoT 

In this section, we provide detailed experimental settings for analyzing reasoning bonds in Long CoT. 

C.1 Mathematical Definition & Analysis of Reasoning Bonds 

C.1.1 Reasoning Behavior Definition. 

Given an input query x, a model generates an output text y containing intermediate reasoning followed by a final answer. We segment y into T step-level units (“steps”) using standard delimiters (e.g., line breaks or bullet markers), following prior work [19, 34]. This yields a Long-CoT trajectory 

τ := ( u1, . . . , u T ), (3) where each ut is a textual step. To analyze the geometry of τ , we map each step ut to a vector ht ∈ Rd. Concretely, we encode ut with a fixed reference encoder and obtain token hidden states from a fixed layer (e.g., the penultimate layer). We then average token hidden states to form a step embedding ht, producing an embedding sequence (h1, . . . , h T ).

Behavior space and labeled transitions. We model Long-CoT as a sequence of step-to-step transitions. For each t ∈ { 1, . . . , T − 1}, define the directed transition (edge) 

et := ( ut → ut+1 ). (4) Let the behavior label set be 

B := {N , D, R, E} , (5) where N denotes Normal Operation, D Deep Reasoning, R Self-Reflection, and E Self-Exploration. An automated classifier (Appendix C.1) assigns a label bt ∈ B to each transition et, yielding a labeled transition sequence (et, b t)T −1 

> t=1

.We formalize a behavior-graph framework for Long CoT, in which molecular-inspired ‘bonds’ correspond to behavior-labeled edges whose distributions can be estimated, compared, and transferred across models. 18 Definition 1 (Deep Reasoning: D). A transition et = ( ut → ut+1 ) is labeled D if the primary intent of ut+1 

is to extend the reasoning chain via non-trivial inference (e.g., multi-step causal, deductive, or analogical reasoning). Operationally, D typically introduces at least one of: (i) new latent assumptions, (ii) intermediate logical variables/nodes, or (iii) derivations that go beyond direct computation or restatement. 

Definition 2 (Self-Reflection: R). A transition et = ( ut → ut+1 ) is labeled R if ut+1 explicitly comments on, audits, or regulates the model’s own reasoning process. This includes expressing uncertainty, revising the solution strategy, identifying or correcting mistakes, or re-examining earlier steps (i.e., tracing back to prior logical nodes). 

Definition 3 (Self-Exploration: E). A transition et = ( ut → ut+1 ) is labeled E if ut+1 intentionally branches into alternative hypotheses or candidate solution paths. Operationally, E raises new possibilities or sub-questions and keeps multiple paths active rather than committing to a single convergent chain. 

Remark (Normal Operation; N ). Any transition not meeting the criteria for D, R, or E is labeled N . This category covers routine progression such as straightforward calculation, paraphrasing, formatting, or direct execution of an already-chosen plan. 

Note: It should be noted that an edge linking logical nodes does not merely connect the step st to the next step st+1 , but also adds edges to previously related nodes. 

C.1.2 Attention Energy Definition of Reasoning Bonds. 

We now replace the molecular analogy with a direct analysis of attention patterns. For a given Transformer layer and head, let qi, k j ∈ Rdk denote the query and key vectors of the i-th and j-th tokens, respectively. The attention weight αij from token i to token j is 

sij = q⊤ 

> i

kj

√dk

, (6) 

αij =exp 

 q⊤ 

> ikj
> √dk

P 

> ℓ

exp 

 q⊤ 

> ikℓ
> √dk

 . (7) Following the standard Gibbs–Boltzmann parametrization, we define the attention energy 

Eij ≜ −sij = − q⊤ 

> i

kj

√dk

. (8) Then, substituting Eq. (8) into Eq. (7) yields 

αij = exp( −Eij )

P 

> ℓ

exp( −Eiℓ ) , (9) which is a Boltzmann distribution over targets j with inverse temperature fixed to 1.‡ That is, the attention weight is Eij Gibbs–Boltzmann distribution (temperature is taken to be 1). In this view, lower Eij corresponds exactly to higher attention weight αij , i.e., stronger dependency from i to j.To relate this to Long CoT behaviors, we use the edge-level labeling procedure from Appendix C.3. Each reasoning transition (edge) in a trajectory is classified as one of three behaviors: Deep Reasoning, Self-Reflection, or Self-Exploration (plus Normal Operation as a local baseline). For each labeled edge b, we aggregate the token-level attention weights between the source and target steps into a single scalar logit, and convert it to an energy value Eb using Eq. (8). This yields three random variables ED , E R, E E corresponding to Deep, Reflection, and Exploration edges, respectively.   

> ‡We do not assume any physical semantics for Eij ; it is simply a convenient reparameterization of the attention logits.

19 C.1.3 Attention-Energy Ordering of Reasoning Bonds 

To analyze ordering among attention energies for different reasoning bonds, model queries, and keys with Rotary Positional Embedding (RoPE) [ 35 ]. In an autoregressive Transformer, attention weights are a softmax of pre-softmax logits; define an attention energy as the negative logit so that higher attention corresponds to lower energy (Boltzmann/Gibbs form). 

RoPE model RoPE can be written as a position-dependent block-diagonal rotation on (pairs of) coordinates. Model this head’s queries/keys as 

qi = R(i)ui, kj = R(j)vj , (10) where R(t) ∈ Rdk ×dk is deterministic and orthogonal. Then 

sij = u⊤ 

> i

R(i)⊤R(j)vj

√dk

= u⊤ 

> i

R(i − j)vj

√dk

, (11) so the score depends only on the relative offset. 

Bond random variables and expectations Let dD := 1 (adjacent), and choose integers 1 < d R < d E . Define bond-energy random variables (random over positions/data) 

ED (i) := Ei,i −1, ER(i) := Ei,i −dR , EE (i) := Ei,i −dE . (12) Define their mean energies (scalars) 

¯ED := E (ED (i)) , ¯ER := E (ER(i)) , ¯EE := E (EE (i)) . (13) Goal: prove ¯ED < ¯ER < ¯EE in expectation, and then with high probability for empirical averages. 

Assumptions Here, we should establish weak dependence under RoPE and state two assumptions on the query/key distributions.  

> A1 (Isotropic, distance-decaying cross-covariance).

There exists a scalar function ρ(d) ≥ 0, strictly decreasing in d, such that for d = |i − j|,

E[uiv⊤ 

> j

] = ρ(d) I. (14)  

> A2 (Positive average alignment of rotation).

Define 

μ(d) := 1

√dk

tr( R(d)) . (15) Assume μ(d) is non-increasing in d and μ(d) ≥ μ > 0 for d ∈ { 1, d R, d E }.

Theorem 1 (Expected bond-energy order under RoPE). Under A1–A2 and 1 < d R < d E ,

¯ED < ¯ER < ¯EE . (16) 

Proof. First, compute the expected logit at distance d. Let j = i − d. Then 

E[si,i −d] = E

 u⊤ 

> i

R(d)vi−d

√dk



(17) 

= 1

√dk

tr  R(d) E[vi−du⊤ 

> i

] . (18) By A1, E[vi−du⊤ 

> i

] = ρ(d)I. Hence 

E[si,i −d] = ρ(d) · 1

√dk

tr( R(d)) = ρ(d)μ(d). (19) 20 By A1, ρ(1) > ρ (dR) > ρ (dE ). By A2, μ(1) ≥ μ(dR) ≥ μ(dE ) ≥ μ > 0. Therefore 

E[si,i −1] > E[si,i −dR ] > E[si,i −dE ]. (20) Finally, since Eij = −sij , taking negatives yields 

¯ED < ¯ER < ¯EE . (21) 

The lower bound between two bond energies. Let bs(d) be the sample mean of si,i −d over N independent draws (or approximately independent blocks), and define bE(d) := −bs(d). Let bED := bE(1) , bER := bE(dR),

bEE := bE(dE ). 

> A3 (Sub-Gaussian logits).

For each fixed distance d ∈ { 1, d R, d E }, the centered logit si,i −d − E[si,i −d] is 

σ2-sub-Gaussian. Then for each d and any ϵ > 0,

Pr   bs(d) − E[s(d)] > ϵ  ≤ 2 exp 



− N ϵ 2

2σ2



. (22) By a union bound over the three distances, all three estimates concentrate simultaneously with probability at least 1 − δ when 

ϵ = σ

r 2 log(6 /δ )

N . (23) If the expected logit gaps satisfy 

E[s(1)] − E[s(dR)] > 2ϵ, E[s(dR)] − E[s(dE )] > 2ϵ, (24) then with probability at least 1 − δ, that satisfies bED < bER < bEE , since bE(d) = −bs(d) preserves strict inequalities after flipping signs. 

Lemma (Finite-sample ordering with high probability). Suppose that the mean scores satisfy ∆DR =

E[sD ] − E[sR] > 0 and ∆RE = E[sR] − E[sE ] > 0. For any 0 < ϵ < min(∆ DR , ∆RE )/2, if 

N ≥ 2σ2

ϵ2 log 4

δ , (25) then, with probability at least 1 − δ, the empirical estimates follow the same ordering: 

Pr( ˆED < ˆER < ˆEE ) ≥ 1 − δ. (26) 

C.1.4 Low-energy edges dominate path aggregation. 

Step-level dependency graph. Let a model-produced chain-of-thought be partitioned into reasoning steps. Represent these steps as nodes in a directed graph G = ( V, B ). For two steps u, v ∈ V , include a directed edge 

b = ( u → v) ∈ B if step v attends to step u with average attention weight above a fixed threshold. (Thus, edges point from an attended-to step to the attending step.) 

Step-level edge energy. Assume token-level energies Eij are defined for token pairs (i, j ). For an edge 

b = ( u → v), define the set of cross-step token pairs 

C(u → v) = {(i, j ) : i ∈ tokens (u), j ∈ tokens (v)}. (27) Define the step-level edge energy as the mean token-level energy across these pairs: 

Eb ≡ Eu→v = 1

|C (u → v)|

X

> (i,j )∈C (u→v)

Eij . (28) 21 Reasoning paths and path energy. A reasoning path p from a source step s to a target step t is a sequence of edges 

p = ( b1, . . . , b L), bℓ = ( uℓ → uℓ+1 ), u1 = s, uL+1 = t. (29) Define the path energy as the additive cost 

E(p) = 

> L

X

> ℓ=1

Ebℓ . (30) 

Soft-min (effective) energy over all paths. Let P(s → t) denote the set of all paths from s to t in G. Following the standard log-sum-exp (soft-min) construction, define the effective energy 

E⋆(s → t) = − log X

> p∈P (s→t)

exp  −E (p). (31) Equivalently, each path receives an (unnormalized) Gibbs weight exp (−E (p)) , so lower-energy paths contribute more strongly to the aggregate dependence from s to t.

Low-energy edges define effective constraints. Consider two paths p and p′ from s to t of equal length L.Assume they are identical except at a single position ℓ∗, where p uses edge bℓ∗ and p′ uses b′ 

> ℓ∗

. If Ebℓ∗ ≤ Eb′ 

> ℓ∗

−δ

for some δ > 0, then 

exp  −E (p)

exp  −E (p′) = exp  E(p′) − E (p) ≥ exp( δ). (32) Thus, holding all other edges fixed, replacing a higher-energy edge by a lower-energy edge amplifies the relative influence of that path by at least a factor exp( δ) in the attention-induced dependency distribution. 

Proof. Because p and p′ differ only at ℓ∗,

E(p) − E (p′) = Ebℓ∗ − Eb′ 

> ℓ∗

. (33) The assumption Ebℓ∗ ≤ Eb′ 

> ℓ∗

− δ implies 

E(p) ≤ E (p′) − δ ⇒ E(p′) − E (p) ≥ δ. (34) Exponentiating yields 

exp  −E (p)

exp  −E (p′) = exp  E(p′) − E (p) ≥ exp( δ), (35) which proves the claim. 

Analysis. Proposition C.1.4 shows that low-energy edges act as effective constraints: they bias the model toward reusing specific multi-step dependency patterns, because any path that swaps in a lower-energy edge gains multiplicative weight in the induced path distribution. If edges labeled as Deep Reasoning consistently exhibit lower energies than alternative behaviors, then multi-hop dependencies that rely more heavily on Deep Reasoning edges will dominate the effective aggregation from premises to conclusions, stabilizing long chain-of-thought structure without invoking an external chemical analogy. 

C.1.5 Long CoT process is looking for a more stable reasoning structure. 

Generally speaking, in chemistry, reactions tend to synthesize more stable, lower-energy compounds. Anal-ogously, we hypothesize that the Long CoT learning process seeks a stable reasoning configuration that minimizes attention energy. We now formalize this intuition under mild ergodicity and bounded-energy assumptions. 22 Step 1. Stationary behavior frequencies. Assume that the reasoning behavior sequence (st)t≥1 forms an irreducible, aperiodic, time-homogeneous Markov chain with transition matrix P and stationary distribution 

π satisfying P ⊤π = π. By the ergodic theorem for finite-state Markov chains, the empirical frequency of each behavior converges almost surely to its stationary probability §:

1

T − 1 

> T−1

X

> t=1

1[st = b] a.s. 

−−−−→  

> T→∞

πb, ∀b ∈ B . (36) 

Step 2. Decomposition of the time-averaged energy. Let Et denote the bond-level attention energy associated with transition et = ( ut → ut+1 ), as defined in Appendix C.4. The trajectory-level average energy can be decomposed by behavior type as 

bET = X

> b∈B

1

T − 1 

> T−1

X

> t=1

1[st = b]

!| {z } 

> Empirical frequency of behavior b

·

PT −1 

> t=1

Et 1[st = b]

PT −1 

> t=1

1[st = b]

!| {z } 

> Mean energy conditional on st=b

. (37) This simply states that the global average equals the weighted sum of per-behavior averages. 

Step 3. Conditional convergence of energy averages. Assume that E[|Et|] < ∞ and, conditional on st = b,the distribution of Et depends on the past only through st (i.e., Et ⊥ (s<t , E <t ) | st). Under this mild conditional-independence assumption, the law of large numbers within each behavior implies that the inner mean converges to the conditional expectation 

1

Nb(T )

X 

> t:st=b

Et → μb = E[Et | st = b]. (38) Combining this with the ergodic frequency limit from Step 1 yields 

bETa.s. 

−−−−→ 

> T→∞

X

> b∈B

πbμb, (39) thus establishing the ergodic low-energy equilibrium. 

Step 4. Exponential routing preference from Gibbs attention. At reasoning step t + 1 , consider a fixed query token i selecting among candidate targets St. The model samples target sj ∈ S t according to the standard Boltzmann (softmax) distribution: 

Pr( St = sj | i) = exp( −Eij )

P 

> sℓ∈S t

exp( −Eiℓ ) . (40) Assume a bounded-deviation condition: for any behavior type b, the corresponding attention energies lie within an interval [μb − ∆, μ b + ∆] . Then for any two behavior classes b, c co-occurring in St,

Pr( St ∈ b | i)Pr( St ∈ c | i) ≥ exp  −(μb + ∆) + ( μc − ∆)  = exp  (μc − μb) − 2∆ . (41) Intuitively, lower-energy behaviors are exponentially favored in the routing distribution. If Deep/Reflection behaviors exhibit mean energies μD , μ R separated from Exploration energy μE by a margin γ > 0, and 2∆ < γ ,then these lower-energy transitions will dominate with exponential advantage exp( γ − 2∆) .Therefore, the model’s attention mechanism inherently biases it toward stable, low-energy Long CoT reasoning structures.   

> §Note: The energy at tstep is related to the previous logical node (node), but has nothing to do with the logical step (edge).

23 C.2 Stable Reasoning Bond Distribution in Long CoT  

> Reasoning Chain Generation

To analyze structural properties across domains, we utilize OpenThoughts-3 [ 24 ], a mixed-domain corpus comprising math, code, and scientific reasoning tasks typical of QwQ-style distillation pipelines. We prompted DeepSeek-R1-671B, OpenAI-o1, and QwQ-32B with original queries to generate Long CoT traces. Generation employed a maximum token length of {16,384, 32,768}, a decoding temperature 

T ∈ [0 , 1] , top-p = 0 .95 , and standard repetition penalties, without additional sampling constraints.  

> Bond Type Annotation

Following Chen et al. [19] , Golovneva et al. [34] , we segmented reasoning traces into step-level units using standard delimiters (e.g., “ \n”, “ \n\n”, “. ”). We then prompted Qwen2.5-32B-Instruct to classify the logical bond for each edge between consecutive steps (ht−1, h t) into one of three types. Validation on a 200-example subset against human annotation yielded a macro-F1 score exceeding 0.85, confirming the reliability of the automated labeling applied to the full corpus. Bond Type Annotation Prompt You are an expert annotator. Classify the CURRENT STEP into exactly one of the following categories of reasoning/behavior: 

• normal operation — Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. 

• deep reasoning — Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. 

• self-reflection — commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. 

• exploration — generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following a single conclusion. Decision rules: (1) If multiple categories seem to overlap, choose the most specific match based on intent: 

– If the text is about reasoning itself → self-reflection. 

– If the text is branching or speculating → exploration. 

– If the text is extending the reasoning chain with deeper causality or hidden steps → deep reasoning. 

– Otherwise, if it’s just direct calculation or straightforward logic → normal operation. (2) Do not label based on correctness of the reasoning — only on the behavioral style of thinking. (3) Ignore surface complexity (e.g., long math steps may still be normal operation if they are straight-forward). (4) If mixed, choose the dominant intent; break ties with this priority: self-reflection > exploration > deep reasoning > normal operation. Output format (strict): Return exactly one line and nothing else: ### Behavior: {normal operation | deep reasoning | self-reflection | exploration} PREVIOUS STEP: {} CURRENT STEP: {}  

> Transfer Distribution Analysis

To construct the reasoning topology (Figure 5), we randomly subsampled 

N ∈ { 0.5k, 1k, 2k, 5k, 10 k, 20 k} examples for each model-task pair. We derived reasoning transfer distributions between sequential behaviors, aggregating transition frequencies into normalized transfer graphs defined over relative step indices or semantic clusters. Stability was assessed via Pearson correlations between graphs 24 from differing models and sample sizes. Averaged across five independent trials, correlations exceeded 0.9 for 

N > 2k and 0.95 when comparing sample sizes above this threshold. 

C.3 Logical Bonding-Folding Structure in Long CoT 

To analyze the distribution and geometric characteristics of reasoning bonds, we embed all reasoning steps into a unified semantic space. This representation enables visualization of the bonding structure and supports the definition of geometric metrics to quantify folding. 

C.3.1 Geometric Embedding and Visualization of Bonding Structure 

To analyze logical folding, we embed complete CoT trajectories into a high-dimensional semantic space using Qwen3-8B [ 36 ]. We encode each reasoning step by averaging the penultimate-layer hidden states of its constituent tokens, representing trajectories as ordered sequences {h1, h 2, . . . , h T }, where T denotes the number of steps. We visualize folding behavior (Figure 7) via t-SNE, employing cosine distance and 5,000 iterations with an early exaggeration of 12.0. These parameters yield stable three-dimensional projections that preserve local neighborhood structure. For clarity, Figure 7 displays only chains and logical clusters relevant to specific behaviors, omitting extraneous points to highlight bonding functions. Moreover, we define “logic cluster” c(ht) by iteratively merging instances located within a distance α of existing clusters. To reduce variability in t-SNE embeddings, we set α proportional to the spread of points: 

α = 0 .02 × (hmax − hmin ), (42) where hmax and hmin are the maximum and minimum pairwise distances among all embedded points in the trajectory. This adaptive threshold ensures that clustering remains sensitive to the specific geometry of each reasoning trace. 

C.3.2 Geometric metrics for folding 

We quantify the geometry of labeled edges ( (t → t+1) ) in the embedding space, where each step is represented by a vector ht ∈ Rd. Euclidean distance is assumed unless otherwise noted.  

> Self-reflection: local movement and reconnection.

To determine if reflective transitions revisit semantic regions, we compute two metrics. First, the local step distance captures instantaneous displacement: 

dt = ∥ht+1 − ht∥2 . (43) Second, for reflection-labeled edges, we measure the return distance to the trajectory history: 

rt = min  

> s<t

∥ht+1 − hs∥2 . (44) A reflection step is defined as reconnecting if the prior state minimizing rt satisfies rt < α , where α is the predefined cutoff. Under this criterion, 81 .72% of reflections return to consolidated clusters rather than drifting to novel areas.  

> Deep reasoning: path length and cluster-level proximity.

Deep reasoning often involves extensive computation that appears local in endpoints but traverses a complex intermediate path. We capture this via the geometric trajectory length. This metric distinguishes direct transitions from extended traversals. Additionally, we quantify semantic movement using the cluster-level graph distance: 

gt = dist G (c(ht), c (ht+1 )) , (45) where dist G computes the shortest distance between clusters in a graph G whose nodes are clusters and edges connect clusters with pairwise distances below α. Empirically, 72 .56% of deep reasoning steps satisfy 

dt < g t < 3, suggesting that deep reasoning typically progresses through nearby logical neighborhoods rather than jumping between remote regions. 25 Self-Exploration: novelty and sustained drift. Self-Exploration entails movement into unvisited regions. We quantify this via novelty distance, defined identically to displacement: 

nt = dt = ∥ht+1 − ht∥2 , (46) where high values of nt implies divergence from previously visited states. To distinguish transient jumps from sustained excursions, we examine the trajectory length ℓt. Self-exploration steps exhibit an average length of 

5.32 , consistent with extended traversals rather than local fluctuations. 

C.4 Attention Energy Levels of Different Logical Bonds 

To analyze attention energy across bond types, we extract attention weights from distilled Llama-3.1-8B-Instruct fine-tuned on QwQ-generated data, using full-precision inference on reasoning queries drawn from evaluation benchmarks. For each generated step, we annotate the reasoning behavior and record attention weights from all layers and heads over preceding tokens. Unless otherwise stated, we analyze the final attention layer, averaging across all heads to enhance the signal-to-noise ratio. We define attention metrics for specific bond types as follows: for self-reflection bonds, we measure attention where queries ( q) from the reflection step attend to keys ( k) in the nearest prior step within the hidden space, identified by minimizing the L2 embedding distance. For deep reasoning bonds, we track attention from the current step to the immediately preceding step. Finally, for self-exploration bonds, we measure attention from the previous step directed toward the exploration step. For each logical bond connecting step i and step j, we construct bond-level statistics from token-level attention (equivalently, token-level energies). First, we select the final token(s) of step j as the target tokens (i.e., the tokens whose outgoing attention is used to characterize how step j relies on prior content); Then, for each selected target token in step j, we extract its attention weights to the final token of step i, and then aggregate across attention heads using the head-wise mean. We convert the aggregated attention logits to energies via Eq. 1, and treat the resulting value as the empirical energy for that bond instance. Applying this procedure across all bonds yields empirical distributions of attention weights and effective energies for reflection, deep-reasoning, and exploration bonds. 

C.5 SFT Learning of Bond Structures 

For the SFT analysis, we consider Llama-3.1-8B-Base that is pre-trained but not instruction-tuned on Long CoT data; Then, we consider Llama-3.1-8B-Base trained on R1-distilled data as Think-SFT model obtained by supervised fine-tuning enriched with Long CoT traces. 

C.5.1 Setting: How does SFT actually learn these bond structures?  

> Cross-coder Sparse Auto-Encoder (SAE) Architecture.

To analyze representational geometry, following Jiralerspong and Bricken [37] , Lindsey et al. [38] , we train a cross-coder sparse auto-encoder (SAE). For each token, the SAE input is the concatenation of hidden states from the base model and the SFT model at aligned token positions. Specifically, the SAE comprises an encoder and a decoder. The encoder is a single linear layer that maps the concatenated hidden state to a sparse-dimensional latent space. We induce sparsity in the latent code using an ℓ1 penalty, calibrated to yield an average activation rate of approximately 1%–3% per latent unit. The decoder is a linear layer trained to reconstruct the original concatenated hidden state from the sparse latent code. We compute token-level feature activations by applying the encoder to each token’s concatenated hidden state. We report only features whose activation probability in the base model is more than 3 × that in the SFT model, or vice versa. Using these features, we manually identify those associated with Long CoT behavior. Specifically, we label think tokens as tokens belonging to explicit reasoning segments in the SFT corpus (as opposed to prompt context or final-answer segments), and we select the features with the strongest enrichment on think tokens. This procedure yields a compact set of discourse-control features preferentially expressed during Long CoT reasoning. Inspecting tokens with the largest contributions to these features reveals strong associations with connective markers such as Maybe”, But/so”, and “Alternatively”. 26 Keyword manipulation dataset construction. We examine whether supervised fine-tuning learns the underly-ing reasoning structure in the data, rather than exploiting superficial lexical cues associated with particular keywords. To this end, we construct two modified versions of the QwQ distillation training corpus. In both versions, whenever a selected keyword occurs (e.g., “wait,” “maybe,” “however”), we replace it with a meaning-preserving alternative while maintaining the local syntax and the intended progression of the reasoning. In the first modified dataset (Keyword-variant 1), each occurrence of a target keyword is randomly replaced by one of four semantically similar alternatives, for example “wait” → “hold on,” “maybe” → “perhaps,” and “however” → “on the other hand.” In the second modified dataset (Keyword-variant 2), we apply the same procedure but use a different set of replacements than those in Keyword-variant 1. This design changes surface realizations while aiming to preserve the underlying reasoning trajectory, enabling a controlled test of sensitivity to lexical form. For reproducibility, we summarize the replacement scheme in a table that lists, for each original keyword, the set of replacements used in Keyword-variant 1 and the distinct set used in Keyword-variant 2, formatted as: original keyword / replacement plan 1 / replacement plan 2. The keywords and corresponding replacements for the Deep Reasoning bond are provided in the boxes below: Keywords for Deep Reasoning otherwise / if not / or else therefore / thus / hence because / since / due to the fact that so / in that case / that means first / to start / firstly next / then / after that finally / in the end / at last then / in that case / as a consequence note / keep in mind / remember notice that / take note / bear in mind important / crucial / significant actually / in fact / really basically / essentially / fundamentally think step by step / work through it step by step / go stepwise let’s reason through this / let’s work through this / let’s think it through carefully / with care / meticulously logically / coherently / consistently rigorously / systematically / by strict logic assumption / premise / starting assumption constraint / restriction / limitation it implies that / it means that / it entails that key insight / central idea / core insight break it down / decompose it / split it up The keywords and corresponding replacements for the Self-Reflection bond are listed in the box below: Keywords for Self-Reflection wait, / hold on, / let’s slow down, but / yet / though however / nevertheless / yet reflect / think back / pause to consider verify / confirm / validate double-check / recheck / verify again reflection / introspection / self-examination 27 Keywords for Self-Reflection (Continued) 

introspect / look inward / self-examine I might be wrong / I could be mistaken / I may be off I could be in error / I might be misreading this / I may be overlooking a detail I’m not sure / I’m uncertain / I’m not certain I’m not fully confident / I can’t say with certainty / I have doubts confidence / certainty / assurance credence / confidence level / subjective probability revise / adjust / update modify / refine / rework reconsider / rethink / take another look check my assumptions / test my assumptions / validate my premises audit my premises / question my starting points / recheck my presuppositions self-critique / self-review / self-audit self-correction / self-check / critical reflection let me check / let me verify / let me double-check alternatively / as an alternative / as another option instead / in lieu of that / in place of that conversely / on the flip side / the other way around I’m struggling with / I’m wrestling with / I’m having a hard time with 

The keywords and corresponding replacements for the Self-Exploration bond are listed in the box below: Keywords for Self-Exploration maybe / perhaps / might be now / at this point / right now let’s / let us / we can probably / likely / presumably seems / appears / looks like maybe not / perhaps not / possibly note I’ll / we’ll / it helps to consider/ think about / look at assume / suppose / let’s say if / provided that / in case explore / look into / examine probe / dig into / unpack consider two cases / split into two cases / handle two scenarios self-exploration / personal exploration / inner exploration self-discovery / discovering myself / learning about myself values / principles / priorities I want / I’d like / I’m aiming to Both datasets preserve the same underlying trajectories and labels as the original SFT corpus. We further ensure that the distributions of problems, answer types, and trajectory lengths remain unchanged. 

## D Semantic Isomer Construction Details 

This section details the construction and analysis of semantic isomers across four key dimensions. We first describe the distillation of well-structured semantic isomers and their simulation via in-context learning (ICL). Subsequently, we examine information flow through metacognitive oscillation analysis and conclude with the specifics of conflict learning between two stable structures. 28 Model GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench AVG 

LLaMA-3.1-8B-Base 7.58 3.20 0.00 0.00 4.22 1.19 2.70 

+ 20K R1-Distill-Data 63.38 30.60 0.21 0.42 14.22 8.30 19.52 

+ 20K OSS-Distill-Data 75.89 54.20 4.38 6.46 37.34 23.85 33.69 

+ 20K QwQ-Distill-Data 64.53 32.20 2.92 0.42 16.72 8.89 20.95 Llama-3.1-8B-Instruct 75.89 35.20 4.17 1.04 23.59 12.00 25.32 

+ 20K R1-Distill-Data 79.91 60.60 2.50 3.96 33.13 23.85 33.99 

+ 20K OSS-Distill-Data 79.00 60.80 10.83 7.71 47.03 30.22 39.27 

+ 20K QwQ-Distill-Data 82.41 60.80 4.38 8.33 32.97 25.48 35.73 Qwen-2.5-7B-Base 40.18 34.20 5.42 0.83 26.72 17.33 20.78 

+ 20K R1-Distill-Data 76.14 24.20 1.20 2.29 10.00 5.33 19.86 

+ 20K OSS-Distill-Data 84.99 68.40 6.04 8.13 46.25 27.70 40.25 

+ 20K QwQ-Distill-Data 78.39 46.80 2.71 1.46 9.84 5.93 24.19 Qwen-2.5-7B-Instruct 83.24 74.00 12.50 7.08 22.66 38.07 39.59 

+ 20K R1-Distill-Data 87.04 74.80 14.17 8.54 46.25 41.48 45.38 

+ 20K OSS-Distill-Data 82.31 74.60 25.42 19.38 54.34 37.63 48.94 

+ 20K QwQ-Distill-Data 85.75 73.80 13.96 16.25 52.97 35.11 46.31 Qwen-2.5-32B-Base 53.68 33.40 9.17 2.29 35.63 15.85 25.00 

+ 20K R1-Distill-Data 76.14 24.20 1.46 2.29 10.00 5.33 19.90 

+ 20K OSS-Distill-Data 89.76 77.20 19.38 17.50 60.63 39.26 50.62 

+ 20K QwQ-Distill-Data 91.59 82.40 19.75 19.62 72.37 41.33 54.51 Qwen-2.5-32B-Instruct 93.71 81.00 15.63 14.17 69.84 42.22 52.76 

+ 20K R1-Distill-Data 93.63 83.60 26.67 22.71 72.56 45.93 57.52 

+ 20K OSS-Distill-Data 86.35 86.20 64.17 53.54 92.34 55.70 73.05 

+ 20K QwQ-Distill-Data 92.65 93.20 59.38 49.38 88.44 61.93 74.16 LLama-3.1-70B-Base 46.78 31.80 3.33 1.88 32.50 13.19 21.58 

+ 20K R1-Distill-Data 73.62 20.40 1.25 2.08 8.75 4.44 18.42 

+ 20K OSS-Distill-Data 89.39 76.80 16.88 17.29 57.97 36.74 49.18 

+ 20K QwQ-Distill-Data 88.86 82.40 19.58 18.33 72.34 39.41 53.49 LLama-3.1-70B-Instruct 84.23 52.20 17.92 3.13 45.63 21.63 37.45 

+ 20K R1-Distill-Data 94.62 80.60 27.29 21.88 64.84 43.11 55.39 

+ 20K OSS-Distill-Data 85.75 83.40 52.50 40.42 84.38 50.81 66.21 

+ 20K QwQ-Distill-Data 93.33 89.00 47.50 36.25 81.41 54.07 66.93 

Table 5 Full results on GSM8K, MATH-500, AIME2024, AIME2025, AMC2023, and OlympiadBench. 

D.1 Distillation of Well-structured Semantic Isomers 

The setting is the same as in Appendix B. We use 8 base and instruct LLMs as the backbone and distill from 3 advanced reasoning LLMs. 

D.2 ICL Simulation of Semantic Isomer Structures 

Inspired by Dong et al. [39] , we study whether demonstration selection in an in-context learning (ICL) setting can approximate the effective semantic isomer structure of a target teacher model (Qwen2.5-32B). 

Demonstration Construction. We built a candidate pool by using QwQ-32B to generate Long-CoT solutions for the training questions in Appendix B. 

Demonstration Selection. For each target question, we select a 1-shot demonstration by comparing the reasoning-key distribution of each candidate trace with that of the target teacher (Qwen2.5-32B) trace for the same question, using Pearson correlation. We consider three strategies: (1) Random: Sample a demonstration uniformly from the pool. (2) High-correlation (Aligned): Choose a demonstration with high correlation (r ≳ 0.9) to the teacher distribution. (3) Low-correlation (Mismatched): Choose a demonstration with low correlation ( r < 0.8), corresponding to structurally incompatible reasoning paths. 

ICL-based Distillation Using the selected 1-shot demonstration, we prompt Qwen2.5-32B-Instruct to generate Long-CoT solutions for the same training questions (Appendix B). We then fine-tune a student model (e.g., 29 Llama3.1-8B-Instruct) on these ICL-generated traces with the same hyperparameters as in Appendix B. For each question, we keep the demonstration fixed across all samples. 

D.3 Information Flow and Metacognitive Oscillation Analysis 

D.3.1 Setting: Information Flow Analysis and Metacognitive Oscillation Quantification  

> Information Flow Analysis in Phase Space (Human vs. R1)

We compare the reasoning dynamics of humans and the R1 model in an information phase space by quantifying information gain, entropy evolution, and the rate of convergence. We use multi-step logical deduction and structured reasoning tasks from Du et al. [7] .For human data, participants wrote their reasoning process step-by-step. For model data, R1 was prompted to produce step-by-step reasoning. We treat each step as a discrete point along a reasoning trajectory. For humans, steps are segmented using natural paragraph breaks or explicit logical transitions identified by the delimiter “ \n\n”. For the model, steps are delineated using explicit reasoning markers in the generated output. For visualization, we report only three representative patterns of information-flow change. Each reasoning step st is mapped to a semantic probability representation pt using a unified semantic probability encoder (Llama-3.1-8B-Instruct). We define step-wise entropy through the tuple (It, ∆It), where 

It denotes the cumulative entropy (or cumulative information measure) up to step t, and ∆It = It − It−1

denotes the instantaneous change. The full reasoning chain thus forms a trajectory in a 2D phase space with coordinates ( It, ∆It). To characterize reasoning dynamics (e.g., uniform gain versus accelerating convergence), we analyze the distribution of ∆It and the local phase-space slope, computed between consecutive points as: 

mt = ∆It − ∆It−1

It − It−1

. (47) when It̸ = It−1. The slope mt captures the rate of change in information gain relative to cumulative information, indicating whether the reasoning process is accelerating, decelerating, or stable at each step.  

> Analysis of Metacognitive Oscillation

We formalize and quantify metacognitive oscillation in LLMs—alternation between high-entropy exploration and low-entropy validation—by relating these dynamic states to the distri-butions of reasoning bonds. Using the phase-space dynamics, we classify each reasoning step into one of two states. (1) High-entropy Exploration: characterized by a steep phase-space slope ( mt > 0.6) together with a substantial entropy increase ( ∆entropy > 0.05 ). (2) Low-entropy Validation: characterized by a near-zero or negative slope ( mt ≈ 0) and and minimal entropy change ( |∆entropy | < 0.05 ). We then measure the frequency and periodicity of transitions between these states, and analyze which reasoning keys (bonds) are most prevalent within each state. 

D.4 Details about Conflict Learning Between Two Stable Structures  

> Setup about performance analysis on different training data mixture strategies.

To test whether simulta-neously training a model on two highly correlated ( r ≈ 0.9) yet structurally distinct reasoning frameworks (from R1 and OSS) leads to “structural chaos.” Specifically, we use OpenAI-OSS-120B [ 33 ] as the OSS model and DeepSeek-R1-671B-0528 [ 8] as the R1 model to generate two sets of Long CoT traces on the same 20K training questions from OpenThoughts-3 [ 24 ]. Based on these generated Long CoT traces, we created three training configurations: (1) OSS-Distill-Data, an R1-only set with 20K samples generated from DeepSeek-R1, (2) R1-Distill-Data, an OSS-only set with 20K samples generated from OpenAI-OSS, and (3) R1-then-OSS, a sequential set with first 10K from DeepSeek-R1 and then 10K from OpenAI-OSS. (4) OSS-then-R1, a sequential set with first 10K from OpenAI-OSS and then 10K from DeepSeek-R1. (5) R1-mix-OSS, a randomly mixed set with 10K from OpenAI-OSS and 10K from DeepSeek-R1.  

> Pearson correlation coefficient between transfer distribution.

To quantify the similarity between two reasoning structures (e.g., R1 and OSS), we compute the Pearson correlation coefficient r between their transfer distributions. Given two transfer matrices P and Q representing the normalized frequencies of 30 reasoning bonds between steps, we flatten these matrices into vectors p and q. The Pearson correlation coefficient is then calculated as: 

r =

Pni=1 (pi − ¯p)( qi − ¯q)

pP ni=1 (pi − ¯p)2pP ni=1 (qi − ¯q)2 , (48) where ¯p and ¯q are the mean values of vectors p and q, respectively, and n is the number of elements in each vector. A high positive correlation (close to 1) indicates similar reasoning structures, while a low or negative correlation indicates dissimilar structures. 

## E Details about Synthetic Long CoT with Mole-Syn 

This section describes the experimental settings used for synthesis, analysis, and reinforcement learning with 

Mole-Syn .

E.1 Supervised-Finetuning with Mole-Syn 

To match the statistical properties of the synthetic chain-of-thought (CoT) to the behavioral patterns of stronger teacher models (Section C.2), we first estimate a reasoning-state transition distribution p(st+1 |st).Specifically, we analyze 20k distilled CoT rationales generated by teacher models (e.g., QwQ-32B and OpenAI-OSS-120B). Each reasoning step st is annotated by LLMs, and we use these annotations to compute an empirical transition matrix ˆP (Figure 5). During synthesis, we initialize the process in an exploration state and then sample transitions between reasoning states according to. The prompts used for each reasoning state are listed below. The prompt for the self-reflection state is as follows: Prompt for Self-Reflection Assume that you are a helpful assistant. You will receive a question and a previously reasoned rationale. If you can directly get the answer, please output the concise answer with \boxed{}. Otherwise, please reflect on the response and provide a self-reflection. Here are some reasoning behavior definitions: 

• normal operation — Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. 

• deep reasoning — Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. 

• self-reflection — commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. 

• exploration — generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following a single conclusion. You should conduct self-reflection behavior now. Please reflect on the response and provide a self-reflection. The prompt for the self-exploration state is as follows: Prompt for Self-Exploration Assume that you are a helpful assistant. You will receive a question and a previously reasoned rationale. If you can directly get the answer, please output the concise answer with \boxed{}. Otherwise, please explore a novel reasoning path in the response. Here are some reasoning behavior definitions: 31 Prompt for Self-Exploration (Continued) 

• normal operation — Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. 

• deep reasoning — Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. 

• self-reflection — commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. 

• exploration — generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following a single conclusion. You should conduct exploration behavior now. Please explore a novel reasoning path in the response. 

The prompt for the normal-operation state is as follows: Prompt for Normal Operation Assume that you are a helpful assistant. You will receive a question and a previously reasoned rationale. If you can directly get the answer, please output the concise answer with \boxed{}. Otherwise, please conduct normal operation on the response. Here are some reasoning behavior definitions: 

• normal operation — Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. 

• deep reasoning — Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. 

• self-reflection — commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. 

• exploration — generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following a single conclusion. You should conduct normal operation behavior now. Please conduct normal operation on the response. The prompt for the deep-reasoning state is as follows: Prompt for Deep Reasoning Assume that you are a helpful assistant. You will receive a question and a previously reasoned rationale. If you can directly get the answer, please output the concise answer with \boxed{}. Otherwise, please further deepen the reasoning on the response. Here are some reasoning behavior definitions: 

• normal operation — Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. 

• deep reasoning — Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. 

• self-reflection — commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. 

• exploration — generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following a single conclusion. You should conduct deep reasoning behavior now. Please further deepen the reasoning on the response. 32 Model GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench AVG 

LLaMA-3.1-8B-Base 7.58 3.20 0.00 0.00 4.22 1.19 2.70 

+ 20K Qwen-Distill-Data 62.47 29.40 0.00 0.00 12.81 6.81 18.58 

+ 20K OSS-Distill-Data 75.89 54.20 4.38 6.46 37.34 23.85 33.69 

+ 20K QwQ-Distill-Data 64.53 32.20 2.92 0.42 16.72 8.89 20.95 

+ 20K OSS-M O L E -S Y N 67.85 35.20 1.83 0.83 20.53 11.11 22.89 

+ 20K QwQ-M O L E -S Y N 66.41 35.00 2.08 0.63 20.16 10.37 22.44 Llama-3.1-8B-Instruct 75.89 35.20 4.17 1.04 23.59 12.00 25.32 

+ 20K Qwen-Distill-Data 76.50 39.80 4.38 1.04 25.63 19.70 27.84 

+ 20K OSS-Distill-Data 79.00 60.80 10.83 7.71 47.03 30.22 39.27 

+ 20K QwQ-Distill-Data 82.41 60.80 4.38 8.33 32.97 25.48 35.73 

+ 20K OSS-M O L E -S Y N 83.24 51.80 4.79 1.04 32.50 21.04 32.40 

+ 20K QwQ-M O L E -S Y N 84.31 50.20 5.21 1.67 32.34 20.00 32.29 Qwen-2.5-7B-Base 40.18 34.20 5.42 0.83 26.72 17.33 20.78 

+ 20K Qwen-Distill-Data 68.69 39.80 4.38 1.04 25.63 19.70 26.54 

+ 20K OSS-Distill-Data 84.99 68.40 6.04 8.13 46.25 27.70 40.25 

+ 20K QwQ-Distill-Data 78.39 46.80 2.71 1.46 9.84 5.93 24.19 

+ 20K QwQ-M O L E -S Y N 81.20 62.20 6.25 3.54 41.88 30.52 37.60 

+ 20K OSS-M O L E -S Y N 83.17 63.80 5.83 1.67 41.56 29.33 37.56 Qwen-2.5-7B-Instruct 83.24 74.00 12.50 7.08 22.66 38.07 39.59 

+ 20K Qwen-Distill-Data 84.31 63.40 6.46 3.13 31.72 29.78 36.46 

+ 20K OSS-Distill-Data 82.31 74.60 25.42 19.38 54.34 37.63 48.94 

+ 20K QwQ-Distill-Data 85.75 73.80 13.96 16.25 52.97 35.11 46.31 

+ 20K QwQ-M O L E -S Y N 89.61 76.00 7.29 3.96 51.88 36.74 44.25 

+ 20K OSS-M O L E -S Y N 88.02 77.80 8.13 5.00 52.81 37.48 44.87 

Table 6 Comparison of strong reasoning LLM distillation versus Mole-Syn using a weak instructed LLM across six benchmarks.                                           

> Model GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench AVG
> Llama-3.1-8B-Instruct 75.89 35.20 4.17 1.04 23.59 12.00 25.32
> + M O L E -S Y N -by-Llama-3.1-8B-Instruct 36.01 23.60 2.71 0.21 13.75 7.41 13.95
> + M O L E -S Y N -by-Llama-3.1-70B-Instruct 82.71 53.60 3.33 0.63 31.72 19.85 31.97
> + M O L E -S Y N -by-Qwen-7B 83.47 51.60 3.33 0.63 32.03 19.85 31.82
> + M O L E -S Y N -by-Qwen-32B 82.41 60.80 4.38 8.33 32.97 25.48 35.73

Table 7 The effectiveness analysis of Mole-Syn methods based on different instruct models. 

Furthermore, to investigate the impact of distinct synthetic backbones, we employed Mole-Syn to distill data from various base models (Figure 7). Models with limited self-reflection or exploration capabilities, such as Llama-3.1-8B-Instruct, struggled to synthesize effective reasoning data. Conversely, combining the Qwen series with Llama-70B yielded robust results across most domains, with the notable exception of tasks requiring deep reasoning. For such tasks (e.g., AIME), performance depended heavily on the model’s intrinsic reasoning depth, following the hierarchy: Qwen-32B outperforms both Qwen-7B and Llama-70B. 

E.2 Reinforcement Learning with Mole-Syn Initialization 

We adopt DAPO [ 40 ] as the reinforcement-learning (RL) framework to fine-tune Llama-3.1-8B-Instruct models initialized from different checkpoints. To isolate the effect of initialization, we hold the RL tasks, reward functions, and all other training hyperparameters constant across experiments. Each model is trained for 1000 steps with a learning rate of 1 × 10 −5, a batch size of 16, a sampling size of 16, and a maximum sequence length of 16384 tokens. We set the clipping parameters to clip-low = 0 .2 and clip-high = 0 .68 . We use MATH [26] and AIME 1989–2023 [41] as training data. We evaluate two RL initialization strategies: (1) QwQ-Distill Data + RL: RL fine-tuning starting from Llama-3.1-8B-Instruct further tuned on QwQ-generated long chain-of-thought data. (2) QwQ-Graph-Syn + RL: RL fine-tuning starting from our Mole-Syn -SFT model. All runs use the same RL tasks, reward functions, and hyperparameters; therefore, performance differences can be attributed to the initialization checkpoint (i.e., 33 Model GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench AVG                                                            

> Llama-3.1-8B-Instruct 75.89 35.20 4.17 1.04 23.59 12.00 25.32
> +20K QwQ-Distill-Data 82.41 60.80 4.38 8.33 32.97 25.48 35.73
> +20K QwQ-Distill-Data +RL 91.51 69.80 8.96 4.17 37.97 25.93 39.72
> +20K QwQ-M O L E -S Y N 84.31 50.20 5.21 1.67 32.34 20.00 32.29
> +20K QwQ-M O L E -S Y N +RL 88.78 70.80 7.50 3.33 39.22 21.04 38.44
> +35K QwQ-M O L E -S Y N +RL 90.30 68.40 10.00 4.38 39.84 24.15 39.51
> Table 8

The reinforcement learning performance based on 3 initialization methods across 6 benchmarks. 

the supervised pretraining/fine-tuning history). 

## F Details of Bond Shaping Function Analysis 

This section provides a detailed description of the experimental settings for the geometric reshaping analyses presented in Fig. 13, corresponding to the three "bond" types in the Long CoT structure: Deep Reasoning (covalent bond), Self-Exploration (exploration bond), and Self-Reflection (stabilizing bond). To analyze the function of different "bonds," we extracted step-level hidden representations from the penulti-mate Transformer layer of both Llama-3.1-8B-Instruct and our trained Mole-Syn model. For each reasoning step t, we computed its vector representation ht via average pooling of its token embeddings. We then used t-SNE to visualize the semantic space and compare the representations horig  

> t

and hmethod  

> t

for the same reasoning step across the two models. 

F.1 Deep Reasoning: Densing the Primary Structure. 

To quantify how Deep Reasoning densifies the “primary structure,” we use the same development-set split as in the main text. We compare two training modes: (1) baseline , which uses standard short CoT prompting by standard Llama-3.1-8B-Instruct; and (2) deep-reasoning-trained , which uses the Long CoT paradigm with the Deep Reasoning transfer probability set to 50%, while the remaining probability mass follows the other behaviors in our default setting. For each input sample, we generate 10K training instances and, under each mode, sample 100 reasoning trajectories from OlymBench. From these trajectories, we extract the step-level baseline representations {h1, . . . , h T } and deep-reasoning-trained representations {˜h1, . . . , ˜hT }.To characterize the compactness of the "primary To characterize the compactness of the “primary structure” in semantic space, we compute the Minimum Enclosing Ball (MEB) over all step representations from the sampled trajectories: 

S = {˜h1, . . . , ˜hTN }, (49) where TN denotes the total number of extracted steps aggregated across N trajectories for the given sample set. We compute the MEB in a shared three-dimensional embedding space obtained by t-SNE, which we use consistently across modes for visualization and for a comparable notion of geometric spread. The procedure is as follows. First, we reduce each step representation from the original high-dimensional embedding space to 

d = 3 using t-SNE. Second, we apply an approximate MEB solver based on Welzl’s algorithm to obtain the center c ∈ R3 and radius r. Third, we define the corresponding volume in d = 3 Euclidean space as: 

V = Cd · rd, (50) where Cd is the unit-ball volume constant (a fixed scaling factor that cancels in relative comparisons). We then compute the arithmetic mean of V over all development-set samples, denoted as Vbase (Baseline) and 

Vdeep (Deep Reasoning). The volume reduction reported in Fig. 13(a) is: 

∆Deep = Vbase − Vdeep 

Vbase 

× 100% . (51) A positive ∆Deep indicates that Deep Reasoning contracts the semantic volume of the core logical backbone while preserving comparable semantic coverage of the task. 34 70                               

> 75
> 80
> 85
> 90
> 0510 15 20 25
> Self -Re fl ection Step
> 10
> 12
> 14
> 16
> 10% 20% 50% 80%
> Deep Reasoning Transfer Probability
> Accuracy  on Hard
> Problem (%)
> 10
> 12
> 14
> 16
> 0510 15
> Deep Reasoning Step
> 65
> 70
> 75
> 80
> 85
> 90
> 5% 10% 20% 50% 80%
> Self -Re fl ection Transfer Probability
> Accuracy  on
> GSM8K(%)
> Deep Reasoning Self -Exploration
> Accuracy  on Hard
> Problem (%)
> Accuracy  on
> GSM8K(%)
> 10
> 12
> 14
> 16
> 5% 10% 20% 50% 80%
> Self -Exploration Transfer Probability
> Accuracy  on Hard
> Problem (%)
> 10
> 12
> 14
> 16
> 0510 15 20 25
> Self -Exploration Step
> Accuracy  on Hard
> Problem (%)
> Self -Re fl ection

Figure 16 Comparative analysis of performance under different reasoning bond ratios. 

F.2 Self-Exploration: Expanding the Logical Space. 

We use the same development-set split and the same two training modes as above: (1) baseline (short CoT or direct answering, without Long CoT) and (2) self-exploration-trained (Long CoT with self-exploration transfer probability >50%, with other behaviors unchanged). For each input sample, we generate 10K training instances and sample 100 reasoning trajectories from OlymBench under each mode, then extract the corresponding step representations. Let Vexp denote the mean MEB volume computed from the step representations used for the self-exploration-trained model. We quantify the relative volume change as: 

∆Exp = Vexp − Vbase 

Vbase 

× 100% . (52) Then, a positive ∆Exp corresponds to an expansion of the explored logical space relative to the baseline. 

F.3 Self-Reflection: Densifying and Stabilizing the Logical Results. 

We follow the same data construction and representation extraction protocol as above. To measure the contraction induced by Self-Reflection, we compute MEB volumes before and after the reflection step, denoted by Vpre and Vpost , respectively. We then report the relative contraction as 

∆Reflect = Vpre − Vpost 

Vpre 

× 100% , (53) where a larger value indicates a stronger “folding” effect in semantic space after reflection. To further examine the “suppression of inconsistent branches,” we conduct an auxiliary clustering analysis (not shown in the main text). Specifically, we cluster the pre- and post-reflection point sets, Spre and Spost ,using an α-threshold grouping method. We observe that, after reflection, intra-cluster distances decrease substantially, while inter-cluster distances remain stable or increase slightly. This pattern suggests that the dominant semantic cluster becomes more compact, whereas inconsistent branches are pruned or weakened. Consistent with these findings, the overall semantic volume decreases from 35.2 to 31.2, indicating that Self-Reflection guides the structure toward a more stable solution manifold. 

## G Analysis of the impact of length and diversity in reasoning behaviors 

To evaluate whether comparable performance gains can be achieved merely by regulating the length and frequency of Self-Reflection steps, we conducted a detailed analysis illustrated in Fig. 13, covering the three distinct "bond" types that constitute the Long CoT framework. 35 G.1 Impact of bond-enhanced behaviors on performance 

We further examined how different reasoning behaviors influence overall model performance (Fig. 16). For complex queries, improvements are primarily driven by reasoning depth rather than the sheer number of steps, once a minimum threshold is achieved. A higher proportion of reflective behaviors tends to enhance performance on challenging tasks but can introduce “overthinking,” which reduces accuracy on simpler ones. Similarly, excessive exploratory behavior on complex tasks often impedes convergence—indeed, 62.7% of AIME cases exhibited extended reasoning without reaching a clear conclusion. In contrast, simple queries generally benefit from more direct exploration, leading to greater accuracy. Across all models, performance consistently peaks at a balanced distribution of behaviors, suggesting the existence of a stable and task-invariant optimal configuration. 

G.2 Effect of bond length on reasoning quality 

We also investigated how varying bond lengths affects performance while maintaining constant behavioral ratios. As shown in Fig. 16, increasing both the number of steps and the length of individual reasoning bonds generally strengthens performance, particularly in deep reasoning scenarios. However, overly long exploration bonds can induce semantic drift and reduce the precision of final outputs. The optimal bond length appears to depend on task difficulty: shorter bonds tend to perform better on simple problems, whereas longer ones are more advantageous for tackling complex challenges. 

## H Details of LLM Structure Reconstruction 

H.1 Reasoning compression or summarization data collection. 

For the analysis about broken structure reconstruction, we treat Gemini-2.5-Pro-Thinking and Claude-4-Sonnet as black-box teacher models that can emit long chain-of-thought (CoT) traces. We query both APIs with identical user prompts and task instructions, and enable their “thinking” content, which are much shorter than other reasoning LLMs with Long CoT. 

H.2 Summarize Reasoning Process Analysis. 

To emulate the protection strategies used in private LLMs, we summarize the full Long CoT traces from QwQ-32B and OpenAI-OSS-120B into concise summaries by Qwen2.5-32B. This procedure removes the long-range reasoning structure of the original CoT, thereby reducing the amount of recoverable step-by-step rationale. The summarization prompt is as follows: Prompt for Summarization You are an expert summarizer. Below is a Long Chain-of-Thought reasoning trace generated by an AI model to solve a complex problem. Your task is to compress this reasoning process into a concise summary. Input Long Chain-of-Thought Trace: [Insert Full Trace Here] Summary: We trained the model on the summarized 20K samples. The corresponding results are reported in Table 4. All other analytical experiments followed the same configuration as in the preceding sections, except for the model and the reported outputs. 36