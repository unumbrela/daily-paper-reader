---
title: "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning"
title_zh: 聪明的副作用：多模态大语言模型（MLLMs）多图推理中的安全风险
authors: "Renmiao Chen, Yida Lu, Shiyao Cui, Xuan Ouyang, Victor Shea-Jay Huang, Shumin Zhang, Chengwei Pan, Han Qiu, Minlie Huang"
date: 2026-01-20
pdf: "https://arxiv.org/pdf/2601.14127v1"
tags: ["query:sr-llm"]
score: 7.0
evidence: 多模态大模型多图推理中的安全风险
tldr: "本研究探讨了多模态大语言模型（MLLM）在多图推理能力提升过程中伴随的安全风险。作者提出了首个针对多图推理安全的基准测试 MIR-SafetyBench，包含2,676个实例。通过对19个模型的评估发现，推理能力越强的模型在安全测试中反而越脆弱。研究还揭示了不安全生成与低注意力熵之间的关联，表明模型可能因过度关注任务解决而忽略了安全约束，为提升MLLM安全性提供了新视角。"
motivation: 随着MLLM多图推理能力的增强，其在处理复杂多图指令时可能产生新的安全隐患，而现有研究对此关注不足。
method: "引入了MIR-SafetyBench基准测试，涵盖9种多图关系和2,676个测试实例，用于系统评估19种主流MLLM的安全性。"
result: 实验发现推理能力更强的模型在安全基准上更易受攻击，且不安全生成的平均注意力熵显著低于安全生成。
conclusion: 模型在追求解决复杂多图任务时往往会忽视安全约束，未来的模型开发需在推理能力与安全性之间取得更好的平衡。
---

## 摘要
随着多模态大语言模型（MLLMs）获得更强的推理能力以处理复杂的多图指令，这种进步可能会带来新的安全风险。我们通过引入 MIR-SafetyBench 来研究这一问题，这是首个专注于多图推理安全的基准测试，包含 9 种多图关系分类体系下的 2,676 个实例。我们对 19 个 MLLMs 的广泛评估揭示了一个令人担忧的趋势：在 MIR-SafetyBench 上，具有更先进多图推理能力的模型可能表现得更加脆弱。除了攻击成功率之外，我们发现许多被标记为安全的响应是表面的，通常源于误解或回避、不置可否的回复。我们进一步观察到，不安全生成的平均注意力熵（attention entropy）低于安全生成。这种内部特征表明存在一种潜在风险，即模型可能过度关注任务解决而忽视了安全约束。我们的代码和数据可在 https://github.com/thu-coai/MIR-SafetyBench 获取。

## Abstract
As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.