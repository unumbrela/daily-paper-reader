---
title: Native LLM and MLLM Inference at Scale on Apple Silicon
title_zh: Apple Silicon 上的原生大规模 LLM 与 MLLM 推理
authors: Wayner Barrios
date: 2026-01-27
pdf: "https://arxiv.org/pdf/2601.19139v2"
tags: ["query:sr-llm"]
score: 10.0
evidence: Apple Silicon上的高效LLM与MLLM推理优化
tldr: 本研究推出了 vllm-mlx 框架，旨在 Apple Silicon 平台上实现高效的 LLM 和 MLLM 原生推理。该框架基于 MLX 构建，通过引入连续批处理和基于内容的视觉前缀缓存，解决了现有工具在多模态支持和原生优化上的不足。实验显示，其文本吞吐量显著超越 llama-cpp，并在多模态任务中实现了高达 28 倍的加速，为消费级硬件的大规模推理提供了强力支持。
motivation: 现有 Apple Silicon 推理工具在原生优化和多模态工作负载支持方面存在显著短板，无法充分发挥统一内存架构的潜力。
method: 开发了基于 MLX 的 vllm-mlx 框架，采用连续批处理技术并引入基于内容哈希的视觉前缀缓存以消除冗余编码。
result: "在 M4 Max 上，文本吞吐量提升了 21% 至 87%，多模态重复查询速度提升 28 倍，视频分析缓存加速达 24.7 倍。"
conclusion: vllm-mlx 证明了通过原生优化和高效缓存机制，可以在 Apple 消费级硬件上实现高性能的大规模多模态模型推理。
---

## 摘要
随着 Apple Silicon 在机器学习开发中的应用日益广泛，对利用其独特统一内存架构的高效推理方案的需求也日益增长。然而，现有工具要么缺乏原生优化（如 PyTorch MPS），要么仅专注于文本模型，导致多模态工作负载的需求未能得到满足。我们提出了 vllm-mlx，这是一个基于 MLX 原生构建的、用于 Apple Silicon 上高效 LLM 和 MLLM 推理的框架。对于文本模型，在从 Qwen3-0.6B 到 Nemotron-30B 的各种模型中，我们的吞吐量比 llama-cpp 高出 21% 至 87%，同时提供的连续批处理（continuous batching）在 16 个并发请求下可实现 4.3 倍的总吞吐量提升。对于多模态模型，我们引入了基于内容的显存前缀缓存（content-based prefix caching），通过内容哈希识别相同图像（无论输入格式如何），从而消除冗余的视觉编码。我们在 Apple M4 Max 上的评估表明，文本模型的吞吐量高达每秒 525 个 token，重复图像查询的速度提升了 28 倍，将多模态延迟从 21.7 秒降低到 1 秒以下。包含多达 64 帧的视频分析实现了 24.7 倍的缓存加速。我们将实现方案开源，以支持在消费级 Apple 硬件上进行高效推理。

## Abstract
The growing adoption of Apple Silicon for machine learning development has created demand for efficient inference solutions that leverage its unique unified memory architecture. However, existing tools either lack native optimization (PyTorch MPS) or focus solely on text models, leaving multimodal workloads underserved. We present vllm-mlx, a framework for efficient LLM and MLLM inference on Apple Silicon built natively on MLX. For text models, we achieve 21\% to 87\% higher throughput than llama-cpp across models ranging from Qwen3-0.6B to Nemotron-30B, while providing continuous batching that scales to 4.3x aggregate throughput at 16 concurrent requests. For multimodal models, we introduce content-based prefix caching that eliminates redundant vision encoding by identifying identical images through content hashing, regardless of input format. Our evaluation on Apple M4 Max demonstrates throughput of up to 525 tokens per second on text models and 28x speedup on repeated image queries, reducing multimodal latency from 21.7 seconds to under 1 second. Video analysis with up to 64 frames achieves 24.7x cache speedup. We release our implementation as open source to support efficient inference on consumer Apple hardware.

---

## 论文详细总结（自动生成）

以下是对论文《Native LLM and MLLM Inference at Scale on Apple Silicon》的结构化深入总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **研究背景**：Apple Silicon（M系列芯片）凭借其统一内存架构（UMA）和高带宽（如 M4 Max 达 546GB/s），已成为本地运行大模型的理想平台。
*   **核心问题**：现有的推理工具存在明显短板。PyTorch MPS 并非原生优化；`llama.cpp` 虽强于文本但不支持多模态模型（MLLM）；`vLLM-metal` 支持批处理但缺乏多模态视觉缓存。
*   **痛点**：在多模态对话中，即使是同一张图片，现有框架也会在每一轮对话中重复调用视觉编码器，产生 1.5 到 4 秒的额外延迟，严重影响交互体验。
*   **整体含义**：本文提出了 `vllm-mlx` 框架，旨在通过 MLX 原生优化、连续批处理和视觉前缀缓存，在 Apple 硬件上实现高性能、低延迟的文本与多模态推理统一方案。

### 2. 提出方法论
*   **核心思想**：基于 Apple 原生的 MLX 框架构建，利用其延迟计算（Lazy Evaluation）和统一内存的零拷贝（Zero-copy）特性，并引入针对多模态场景的缓存机制。
*   **关键技术细节**：
    *   **连续批处理（Continuous Batching）**：不同于传统的静态批处理，该调度器允许新请求在 Token 边界动态加入，已完成请求立即退出，极大提升了 GPU 利用率。
    *   **基于内容的视觉前缀缓存**：
        *   **内容哈希**：对解码后的像素值进行 SHA-256 哈希处理，确保无论输入是 URL、Base64 还是文件路径，只要内容相同即可识别。
        *   **缓存机制**：命中缓存时，直接跳过昂贵的视觉编码器（Vision Encoder）前向传播，并复用已有的 KV 缓存状态。
    *   **文本前缀缓存**：针对系统提示词等共享前缀，通过哈希匹配复用 KV 状态，减少首字延迟（TTFT）。
    *   **内存管理**：采用 LRU（最近最少使用）策略管理缓存条目，防止统一内存溢出。

### 3. 实验设计
*   **测试场景**：单请求吞吐量测试、并发请求扩展性测试、多轮图像对话延迟测试、视频分析性能测试。
*   **Benchmark 与模型**：涵盖了 10 余个主流模型，包括 Qwen3 (0.6B-30B)、Llama 3.2 (1B-3B)、Gemma 3、Nemotron-30B 以及多模态模型 Qwen3-VL。
*   **对比方法**：`llama.cpp`（行业标准）、`mlx-lm`（Apple 官方示例库）、`vLLM-metal`（vLLM 的 Apple 后端）。

### 4. 资源与算力
*   **硬件平台**：所有实验均在 **Apple M4 Max** 芯片上完成，配备 **128GB 统一内存**。
*   **算力细节**：文中重点评估推理性能，未涉及模型训练。M4 Max 的内存带宽（546GB/s）被证明在处理 30B 规模模型时具有极高效率。

### 5. 实验数量与充分性
*   **实验规模**：论文进行了多维度的测试，包括不同参数规模（0.6B 到 30B）的横向对比，以及针对并发数（1 到 16）的纵向扩展性分析。
*   **消融实验**：非常充分。分别针对“仅视觉嵌入缓存”、“仅 KV 缓存”以及“两者结合”的效果进行了对比；还测试了不同图像分辨率和视频帧数对缓存效率的影响。
*   **客观性**：对比了目前 Apple 生态内最主流的三个后端，数据覆盖了吞吐量、延迟、内存占用等核心指标，实验设计公平且具有说服力。

### 6. 主要结论与发现
*   **性能超越**：在文本推理上，`vllm-mlx` 的吞吐量比 `llama.cpp` 高出 **21% 至 87%**。
*   **并发优势**：通过连续批处理，在 16 个并发请求下，小模型（Qwen3-0.6B）实现了 **3.7 倍** 的总吞吐量提升，每秒可处理 25+ 个请求。
*   **多模态突破**：视觉前缀缓存将重复图像查询的延迟从 21.7 秒缩短至 **0.78 秒**（**28 倍加速**）；视频分析（64 帧）实现了 **24.7 倍** 的缓存加速。
*   **首字延迟**：文本前缀缓存使 TTFT 缩短了 **5.8 倍**。

### 7. 优点（亮点）
*   **原生集成**：深度利用 MLX 和统一内存，避免了数据在 CPU 和 GPU 间拷贝的开销。
*   **多模态优化**：解决了 Apple Silicon 上 MLLM 推理冗余编码的行业痛点。
*   **生态友好**：提供 OpenAI 兼容 API，使得现有的 AI Agent 框架（如 LangChain、AutoGPT）可以无缝迁移至本地运行。
*   **开源贡献**：将高性能推理能力开放给消费级硬件用户。

### 8. 不足与局限
*   **平台锁定**：该框架仅适用于 Apple Silicon 硬件，无法在 NVIDIA 或 AMD 平台上运行。
*   **模型支持依赖**：其性能和模型兼容性高度依赖于 MLX 社区对底层算子的适配。
*   **高级特性缺失**：目前尚未实现投机采样（Speculative Decoding）和多机分布式推理。
*   **能效评估不足**：虽然针对性能做了详尽测试，但未提供针对 MacBook 电池续航影响的功耗数据。

（完）
