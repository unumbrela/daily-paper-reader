---
title: "Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks"
title_zh: 双阶段大语言模型推理：自演化数学框架
authors: "ShaoZhen Liu, Xinting Huang, Houwen Peng, Xin Chen, Xinyang Song, Qi Li, Zhenan Sun"
date: 2026-01-09
pdf: "https://arxiv.org/pdf/2601.05616v1"
tags: ["query:sr-llm"]
score: 10.0
evidence: 用于推理的自生成长思维链 (CoT) 数据
tldr: 本研究针对大语言模型在数学推理中过度依赖强化学习的问题，提出了一种双阶段监督微调（SFT）框架。通过多轮对话引导模型生成包含验证、回溯和子目标分解的长思维链数据，并结合难度感知拒绝采样动态优化数据分布。该方法显著提升了模型处理复杂问题的能力，在AIME24等竞赛级任务上表现优异，证明了SFT在激活模型内在推理能力方面的巨大潜力，为高效推理优化提供了新路径。
motivation: 现有研究在提升模型推理能力时过度依赖强化学习，忽视了通过监督微调激活模型内在推理潜力的可能性。
method: 提出两阶段框架：首先通过多轮对话引导生成包含验证与回溯的长思维链，随后利用难度感知拒绝采样优化数据分布。
result: 推理链长度提升4倍以上，在GSM8K、MATH500及AIME24等数学竞赛基准测试中实现了显著的性能突破。
conclusion: 该研究证明了高质量SFT数据能有效激活模型的自我修正能力，是复杂推理任务中一种资源高效的优化方案。
---

## 摘要
近年来，大语言模型（LLMs）在数学解题等复杂推理任务中展现出巨大潜力。然而，现有研究主要依赖强化学习（RL）框架，而忽视了有监督微调（SFT）方法。本文提出了一种新的双阶段训练框架，通过自生成的长思维链（CoT）数据增强模型的自我纠错能力。在第一阶段，采用多轮对话策略引导模型生成包含验证、回溯、子目标分解和逆向推理的思维链数据，并利用预定义规则筛选高质量样本进行有监督微调。第二阶段采用难度感知的拒绝采样机制动态优化数据分布，强化模型处理复杂问题的能力。该方法生成的推理链长度延长了4倍以上，同时保持了强大的可扩展性，证明了SFT能有效激活模型的内在推理能力，并为复杂任务优化提供了一条资源高效的路径。实验结果表明，该方法在包括 GSM8K 和 MATH500 在内的数学基准测试中实现了性能提升，微调后的模型在 AIME24 等竞赛级问题上取得了显著进步。代码将开源。

## Abstract
In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.

---

## 论文详细总结（自动生成）

以下是对论文《Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks》的结构化深入总结：

### 1. 核心问题与整体含义
*   **研究动机**：当前提升大语言模型（LLM）推理能力的研究过度依赖强化学习（RL）框架（如 DeepSeek-R1），而忽视了有监督微调（SFT）在激活模型内在推理潜力方面的作用。
*   **核心问题**：如何仅利用模型自身生成的数据，通过 SFT 激活并强化模型的“自我纠错”、“回溯”、“子目标分解”和“逆向推理”等高级推理能力，从而在不依赖外部高性能模型蒸馏的情况下实现复杂数学问题的解决。

### 2. 方法论
该论文提出了一个**双阶段自演化框架**：
*   **第一阶段：长思维链（CoT）数据构建与微调**
    *   **核心思想**：通过多轮对话引导模型进行自我评价和修正。
    *   **技术细节**：采用“问题 -> 初步回答 -> 自我评估 -> 修正回答 -> 最终总结”的流程。
    *   **数据分类与筛选**：将生成的样本分为四类（对-对、对-错、错-对、错-错）。为了防止模型产生“奖励作弊”（故意先答错再改对），研究者将初次回答正确与错误的样本比例严格控制在 1:1。
*   **第二阶段：难度感知的拒绝采样（Difficulty-Aware Rejection Sampling）**
    *   **核心思想**：解决传统拒绝采样偏向简单题的问题。
    *   **算法流程**：迭代式增加采样次数。若某题在 $n_1$ 次尝试中未解出，则在下一轮增加采样次数（如 $n_2=10, n_3=100$），确保难题也能获得高质量的正确解。
    *   **数据融合**：将第一阶段的多轮对话数据与第二阶段的拒绝采样数据合并，并引入**特定标记（Markers）**（如 `multi-turn marker` 和 `rejection marker`），引导模型在推理时识别并应用不同的逻辑模式。

### 3. 实验设计
*   **基础模型**：Qwen2.5-7B-Instruct。
*   **数据集**：使用 DeepScaleR 数据集中的 15k 个样本（10k 用于长 CoT 合成，5k 用于拒绝采样）。
*   **基准测试（Benchmarks）**：包含 AIME24、AMC23、GSM8K、MATH500、SVAMP、TabMWP 和 Gaokao2023en。
*   **对比方法**：
    *   Qwen2.5-7B-Instruct（原始基座）。
    *   Self-rewarding（自奖励方法）。
    *   Think Twice（双重思考方法）。
    *   Distilled（从 DeepSeek-R1-Distill-Qwen-32B 蒸馏出的数据进行微调）。

### 4. 资源与算力
*   **算力说明**：论文中**未明确列出**具体的 GPU 型号、数量及总训练时长。
*   **工程细节**：提到了使用 vLLM 推理引擎进行加速，推理参数设置为 temperature=0.6, top_p=1.0。

### 5. 实验数量与充分性
*   **实验规模**：在 7 个主流数学基准测试上进行了跨领域评估。
*   **消融实验**：
    *   对比了单阶段与双阶段的效果。
    *   测试了不同数据组合策略（顺序微调 vs 联合微调）。
    *   验证了“标记（Marker）”对性能的影响。
    *   进行了**扩展性分析（Scaling Laws）**，测试了从 0.5k 到 8k 不同数据量下的性能表现。
*   **充分性评价**：实验设计较为全面，涵盖了从基础算术到竞赛级数学（AIME）的难度梯度，且对比了当前主流的自改进方法，实验结果具有较强的说服力。

### 6. 主要结论与发现
*   **SFT 的潜力**：仅通过 SFT 和自生成数据，模型在 AIME24 上的准确率从 6.7% 提升至 16.7%，证明了 SFT 能有效激活长链推理能力。
*   **推理长度增长**：微调后模型的推理链长度增加了 4 到 14 倍（如 GSM8K 从 300 token 增至 1100+ token），展现出类似“深度思考”的行为。
*   **标记的作用**：引入任务标记能显著提升模型在不同场景下的适应性，拒绝采样标记更有利于符号推理，而多轮对话标记有利于交互式解题。
*   **扩展性**：随着训练数据量的增加，模型不仅准确率提升，推理效率（简洁度）也随之优化。

### 7. 优点
*   **资源高效**：无需复杂的强化学习流程，仅靠 SFT 即可实现显著的推理增强。
*   **自给自足**：不依赖更强大的外部模型（如 GPT-4）进行蒸馏，降低了对闭源模型的依赖。
*   **难度均衡**：提出的难度感知拒绝采样有效解决了合成数据中“简单题过剩、难题缺失”的分布偏差问题。

### 8. 不足与局限
*   **模板依赖**：多轮对话的生成高度依赖人工设计的提示词模板，自动化程度有待提高。
*   **推理深度限制**：目前仅探索了两轮对话（Two-turn constraint），对于极高难度的竞赛题，可能需要更深层次的迭代。
*   **算力细节缺失**：缺乏具体的训练资源消耗数据，不便于其他研究者评估复现成本。
*   **模型规模限制**：实验主要集中在 7B 规模模型，尚未验证该框架在超大规模模型上的表现。

（完）
