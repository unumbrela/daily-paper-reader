Title: Native LLM and MLLM Inference at Scale on Apple Silicon

URL Source: https://arxiv.org/pdf/2601.19139v2

Published Time: Fri, 30 Jan 2026 01:33:36 GMT

Number of Pages: 7

Markdown Content:
# Native LLM and MLLM Inference at Scale on Apple Silicon 

## Wayner Barrios 

Wiqonn Technologies Colombia 

wbarriosq@wiqonn.com 

## Abstract 

The growing adoption of Apple Silicon for machine learn-ing development has created demand for efficient inference solutions that leverage its unique unified memory architec-ture. However, existing tools either lack native optimiza-tion (PyTorch MPS) or focus solely on text models, leaving multimodal workloads underserved. We present vllm-mlx, a framework for efficient LLM and MLLM inference on Apple Silicon built natively on MLX. For text models, we achieve 21% to 87% higher throughput than llama.cpp across models ranging from Qwen3-0.6B to Nemotron-30B, while providing continuous batching that scales to 4.3x aggregate through-put at 16 concurrent requests. For multimodal models, we introduce content-based prefix caching that eliminates re-dundant vision encoding by identifying identical images through content hashing, regardless of input format. Our evaluation on Apple M4 Max demonstrates throughput of up to 525 tokens per second on text models and 28x speedup on repeated image queries, reducing multimodal latency from 21.7 seconds to under 1 second. Video analysis with up to 64 frames achieves 24.7x cache speedup. We release our imple-mentation as open source to support efficient inference on consumer Apple hardware. 

Keywords: LLM inference, multimodal LLM, Apple Silicon, MLX, prefix caching, llama.cpp 

## 1 Introduction 

Apple Silicon has rapidly become a significant platform for machine learning development and deployment. With uni-fied memory architectures offering up to 192GB of shared CPU/GPU memory and memory bandwidths of 400+ GB/s ,recent Mac devices provide compelling capabilities for run-ning large language models locally. This has driven growing interest in efficient inference solutions for Apple hardware, particularly for development, privacy-sensitive applications, and edge deployment. However, existing inference solutions for Apple Silicon face significant limitations. PyTorch‚Äôs MPS backend adapts CUDA-style operations to Metal but lacks native optimiza-tion for the unified memory model. llama.cpp provides ex-cellent performance for text models but does not support vision-language models. vLLM-metal [ 14 ], the official vLLM backend for Apple Silicon, provides continuous batching but lacks multimodal support and vision caching. This frag-mented landscape leaves developers without a unified solu-tion for both text and multimodal inference on Apple Silicon. Multimodal models present an additional efficiency chal-lenge. Vision-language models such as Qwen3-VL [ 15 ] and Gemma 3 [ 4 ] must process images through a vision encoder on every request, even when the same image appears across multiple conversation turns. This redundant computation adds 1.5 to 2 seconds of latency per request, severely impact-ing interactive applications. We present a framework for efficient LLM and MLLM inference on Apple Silicon that addresses both challenges. Built natively on MLX [ 2], our system leverages the unified memory architecture for zero-copy operations and provides two key capabilities: (1) efficient text model inference with continuous batching, competitive with llama.cpp, and (2) content-based prefix caching for multimodal models that eliminates redundant vision encoding by identifying identi-cal images through content hashing. We make the following contributions: 

‚Ä¢ A comprehensive benchmark comparing vllm-mlx, mlx-lm, and llama.cpp across models from 0.6B to 30B parameters (Qwen3, Llama 3.2, Gemma 3, Nemotron), demonstrating 21% to 87% higher throughput than llama.cpp on Apple Silicon. 

‚Ä¢ A content-based prefix caching mechanism for vision embeddings that achieves up to 28x speedup on re-peated image queries and 24.7x on video analysis by eliminating redundant encoding. 

‚Ä¢ An open-source implementation with OpenAI-compatible API, continuous batching (4.3x scaling at 16 concur-rent requests), and native MLX backend optimized for Apple Silicon‚Äôs unified memory. Our evaluation on Apple M4 Max demonstrates through-put of up to 525 tokens per second on text models ( Qwen3-0.6B ), with vllm-mlx exceeding both mlx-lm and llama.cpp across all tested configurations. For multimodal workloads, the pre-fix cache reduces latency from 21.7 seconds to 0.78 seconds on cached queries, and text prefix caching achieves 5.8x speedup on shared prompt prefixes. We release our imple-mentation as open source at https://github.com/waybarrios/ vllm-mlx .

> arXiv:2601.19139v2 [cs.LG] 29 Jan 2026 Wayner Barrios

## 2 Background 

2.1 Apple Silicon and Unified Memory 

Apple Silicon processors feature a unified memory architec-ture where CPU, GPU, and Neural Engine share the same physical memory pool. Unlike discrete GPU systems that require explicit data transfers over PCIe, unified memory en-ables zero-copy access to tensors from any processor. The M4 Max, for example, provides up to 128GB of unified memory with 546GB/s bandwidth, comparable to high-end datacenter GPUs. This architecture has important implications for LLM in-ference. KV cache, which grows linearly with context length and can consume tens of gigabytes for long contexts, does not need to be transferred between devices. Similarly, model weights loaded into memory are immediately accessible to both CPU preprocessing and GPU computation without copying. 

2.2 MLX Framework 

MLX [ 2] is Apple‚Äôs machine learning framework designed specifically for Apple Silicon. Unlike PyTorch‚Äôs MPS backend, which adapts CUDA-style operations to Metal, MLX imple-ments operations natively for the unified memory model with lazy evaluation and automatic differentiation. Key advantages of MLX for inference include: (1) true zero-copy operations that exploit unified memory, (2) lazy evaluation that fuses operations and reduces memory allo-cation overhead, and (3) native quantization support with efficient dequantization kernels. The mlx-lm library builds on MLX to provide optimized LLM inference with speculative decoding and KV cache management. 

2.3 LLM Inference on Apple Silicon 

The current landscape for LLM inference on Apple Silicon includes several options. llama.cpp [ 5 ] provides highly op-timized inference through hand-tuned Metal kernels and GGUF quantization, achieving excellent single-stream through-put. However, it lacks continuous batching for serving mul-tiple concurrent requests. PyTorch with MPS backend offers broad model compatibility but suboptimal performance due to its CUDA-centric design. vLLM-metal [ 14 ], the official vLLM backend for Apple Silicon, provides an MLX-based plugin with continuous batching support. Our work differs by adding native multimodal inference with vision-language models and content-based prefix caching that eliminates redundant vision encoding across conversation turns. 

2.4 Serving Challenges on Apple Silicon 

Production LLM serving requires capabilities beyond single-request inference. Continuous batching dynamically groups requests to maximize throughput, allowing new requests to join mid-generation and completed requests to exit without blocking others. OpenAI-compatible APIs enable drop-in Single-Stream 

> Throughput
> Continuous
> Batching
> MLLM
> Support
> Vision
> Caching
> OpenAI
> API
> Memory
> Efficiency
> 1
> 2
> 3
> 4
> 5
> Framework Capability Comparison
> mlx-lm
> llama.cpp
> vLLM-metal
> vllm-mlx (Ours)

Figure 1. Framework capability comparison. the proposed framework (green) provides comprehensive coverage: high throughput matching mlx-lm, continuous batching like vLLM-metal, OpenAI-compatible API, plus unique multi-modal support with vision caching. replacement of cloud services for privacy-sensitive applica-tions. Multimodal serving introduces additional challenges. Vision-language models must encode images before text genera-tion, adding 1.5 to 4 seconds of latency depending on reso-lution. In multi-turn conversations about the same image, this encoding repeats unnecessarily. While datacenter de-ployments address this through GPU memory caching, Ap-ple Silicon‚Äôs unified memory architecture enables a simpler approach: content-based caching that identifies identical im-ages through hashing, storing both vision embeddings and KV cache state for instant reuse. 

## 3 System Design 

3.1 Framework Comparison 

Figure 1 compares the proposed framework against existing inference frameworks for Apple Silicon across six capabil-ity dimensions. While each existing framework excels in specific areas, the proposed framework uniquely provides the complete feature set required for production multimodal inference. 

Key differentiators. Compared to mlx-lm [ 2 ], we add con-tinuous batching, OpenAI-compatible API, and multimodal caching. Compared to llama.cpp [ 5 ], we provide continuous batching (it processes sequentially) and multimodal support with caching. Compared to vLLM-metal [ 14 ], we add content-based vision caching that eliminates redundant encoding, the key contribution for interactive multimodal applications. Native LLM and MLLM Inference at Scale on Apple Silicon 

Algorithm 1 Continuous Batching Scheduler 

Require: Pending queue ùëÑ , active batch ùêµ , max batch size 

ùëÄ  

> 1:

loop  

> 2:

// Admit new requests at token boundaries  

> 3:

while |ùêµ | < ùëÄ and ùëÑ ‚â† ‚àÖ do  

> 4:

ùëü ‚Üê ùëÑ. pop ()  

> 5:

ùêµ. add (ùëü ) 

> 6:

end while  

> 7:

// Generate one token for all active requests  

> 8:

for each request ùëü in ùêµ do  

> 9:

ùë°ùëúùëòùëíùëõ ùëü ‚Üê GenerateToken (ùëü, KVCache [ùëü ])  

> 10:

ùëü . output .append (ùë°ùëúùëòùëíùëõ ùëü ) 

> 11:

end for  

> 12:

// Remove completed requests immediately  

> 13:

for each request ùëü in ùêµ where ùëü . is_complete () do  

> 14:

ùêµ. remove (ùëü ) 

> 15:

yield ùëü . output  

> 16:

end for  

> 17:

end loop 3.2 Text Model Inference 

For text models, the proposed framework wraps mlx-lm with production serving capabilities. 

Continuous Batching. Unlike llama.cpp which processes requests sequentially, our scheduler dynamically batches multiple concurrent requests. New requests join an existing batch at token boundaries, and completed requests exit with-out blocking others. Algorithm 1 describes the core schedul-ing loop. This approach maximizes GPU utilization by keeping the batch full while allowing requests to exit immediately upon completion, unlike traditional batching which waits for all requests to finish. 

Streaming. We implement token-by-token streaming with proper handling of multi-byte UTF-8 sequences and tok-enizer artifacts, ensuring clean output for all languages. 

Text Prefix Caching. For text-only workloads with shared prompt prefixes (e.g., system prompts), we cache and reuse KV states. Algorithm 2 shows our approach: when a new request shares a prefix with a cached entry, we skip the for-ward pass for those tokens and resume generation from the cached state. This reduces Time to First Token (TTFT) by up to 5.8x for prompts with 512-token shared prefixes. 

3.3 Multimodal Inference with Prefix Caching 

For multimodal models, we introduce content-based pre-fix caching to eliminate redundant vision encoding across requests. 

Algorithm 2 Text Prefix Cache Lookup 

Require: Prompt tokens ùëÉ , Cache ùê∂ 

Ensure: KV state, start position  

> 1:

‚Ñéùëéùë†‚Ñé ‚Üê SHA256 (ùëÉ ) 

> 2:

if ‚Ñéùëéùë†‚Ñé ‚àà ùê∂ then  

> 3:

return ùê∂ [‚Ñéùëéùë†‚Ñé ].ùëòùë£ _ùë†ùë°ùëéùë°ùëí , |ùëÉ | ‚ä≤ Full cache hit  

> 4:

end if  

> 5:

for ùëñ = |ùëÉ | down to 1 do  

> 6:

ùëùùëüùëí ùëì ùëñùë• _‚Ñéùëéùë†‚Ñé ‚Üê SHA256 (ùëÉ [1 : ùëñ ])  

> 7:

if ùëùùëüùëí ùëì ùëñùë• _‚Ñéùëéùë†‚Ñé ‚àà ùê∂ then  

> 8:

return ùê∂ [ùëùùëüùëí ùëì ùëñùë• _‚Ñéùëéùë†‚Ñé ].ùëòùë£ _ùë†ùë°ùëéùë°ùëí , ùëñ ‚ä≤ Partial hit  

> 9:

end if  

> 10:

end for  

> 11:

return ‚àÖ, 0 ‚ä≤ Cache miss 

Algorithm 3 Cache-Aware Multimodal Generation 

Require: Request ùëÖ with images {ùêº ùëñ } and text prompt ùëá 

Ensure: Generated response  

> 1:

for each image ùêº ùëñ in request do  

> 2:

‚Ñéùëéùë†‚Ñé ùëñ ‚Üê SHA256 (Decode (ùêº ùëñ ))  

> 3:

if ‚Ñéùëéùë†‚Ñé ùëñ ‚àà Cache then  

> 4:

ùëíùëöùëè ùëñ ‚Üê Cache [‚Ñéùëéùë†‚Ñé ùëñ ].ùëíùëöùëèùëíùëëùëëùëñùëõùëîùë†  

> 5:

ùëòùë£ ‚Üê Cache [‚Ñéùëéùë†‚Ñé ùëñ ].ùëòùë£ _ùë†ùë°ùëéùë°ùëí  

> 6:

skip vision encoder for ùêº ùëñ  

> 7:

else  

> 8:

ùëíùëöùëè ùëñ ‚Üê VisionEncoder (ùêº ùëñ ) 

> 9:

end if  

> 10:

end for  

> 11:

ùëúùë¢ùë°ùëùùë¢ùë° ‚Üê Generate (Concat (ùëíùëöùëè,ùëá ), ùëòùë£ ) 

> 12:

Cache [‚Ñéùëéùë†‚Ñé ] ‚Üê ( ùëíùëöùëè, ùëòùë£ ) ‚ä≤ Store for reuse  

> 13:

return ùëúùë¢ùë°ùëùùë¢ùë° 

Content-Based Hashing. A key challenge is that iden-tical images can arrive in different formats: URLs, base64 strings, or file paths. Our solution computes a SHA-256 hash over decoded pixel values, enabling cache hits regardless of input format. This ensures that the same image always maps to the same cache entry. 

Cache-Aware Generation. Algorithm 3 shows our cache-aware generation process. When a multimodal request ar-rives, we first compute content hashes for all images and check the cache. On hit, we retrieve stored vision embed-dings and KV cache state, skipping both the expensive vision encoder (1.5-4s) and prompt processing. On miss, we process normally and store results for future reuse. 

Memory Management. The cache maintains entries con-taining vision embeddings and KV cache state. We imple-ment LRU eviction to bound memory consumption, with configurable limits (default 512MB). Higher resolution im-ages produce larger cache entries but benefit more from caching due to increased vision encoding cost. Wayner Barrios 1 2 4 8 16     

> Concurrent Requests
> 0
> 250
> 500
> 750
> 1000
> 1250
> 1500
> 1750
> Throughput (tok/s)
> 3.7x
> 2.9x
> 2.6x
> (a) Throughput Scaling
> Qwen3-0.6B
> Qwen3-4B
> Qwen3-8B
> 124816
> Concurrent Requests
> 0
> 5
> 10
> 15
> 20
> 25
> 30
> Requests/sec
> (b) Request Throughput
> Qwen3-0.6B
> Qwen3-4B
> Qwen3-8B

Figure 2. Concurrency scaling on vllm-mlx. (a) Aggregate throughput scales efficiently: Qwen3-0.6B achieves 3.7x higher throughput at 16 concurrent requests. (b) Request throughput (requests/sec) increases with concurrency, show-ing efficient batching. Qwen3-0.6B handles 25+ requests/sec at 16 concurrent connections. 

## 4 Evaluation 

We evaluate the proposed framework on text and multimodal workloads, comparing against established baselines on Apple Silicon. Our experiments answer three questions: (1) How does MLX compare to llama.cpp for text model throughput? (2) What benefits does continuous batching provide? (3) How effective is prefix caching for multimodal workloads? 

Setup. All experiments run on Apple M4 Max with 128GB unified memory. We evaluate 10+ models spanning different architectures: Qwen3 [ 13 ], Llama 3 [ 1], Gemma 3 [ 4 ], GLM-4 [ 11 ], and Nemotron [ 9 ]. All models use 4-bit quantization (Q4_K_M for GGUF, 4-bit for MLX). 

4.1 Text Model Performance 

Table 1 compares throughput across frameworks for text models ranging from 0.6B to 30B parameters. vllm-mlx achieves 21% to 87% higher throughput than llama.cpp, and consis-tently outperforms both vllm-metal [ 14 ] and mlx-lm through optimized continuous batching. For smaller models, vllm-mlx‚Äôs advantage is most pro-nounced (1.87x for Qwen3-0.6B ) due to MLX‚Äôs efficient small-tensor handling. For larger MoE models like Nemotron-30B-A3B ,vllm-mlx maintains a 1.43x advantage over llama.cpp. 

Continuous Batching Scaling. Figure 2 shows how vllm-mlx scales with concurrent requests across three model sizes. Unlike llama.cpp which processes requests sequentially, con-tinuous batching achieves significant throughput scaling. For Qwen3-0.6B , throughput scales from 441 tok/s (single request) to 1642 tok/s (16 concurrent), a 3.7x improvement. Larger models show diminishing returns due to memory bandwidth saturation: Qwen3-8B achieves 2.6x scaling. Re-quest throughput (Figure 2b) also scales efficiently: Qwen3-0.6B handles over 25 requests per second at 16 concurrent con-nections. 

4.2 Multimodal Performance 

Image Prefix Cache. Table 2 demonstrates content-based prefix caching for vision embeddings. In multi-turn conversa-tions about the same image, the cache eliminates redundant vision encoding, reducing latency from 21.7s to under 1s (28x speedup). 

Video Performance. Table 3 shows video benchmark re-sults across different frame configurations. Higher frame counts increase latency but provide richer temporal under-standing. Video caching follows the same content-based approach: identical video frames map to the same cache entries, en-abling speedups for repeated video analysis. 

4.3 Ablation Studies 

We conduct ablation studies on vllm-mlx to understand the contribution of each component. 

Cache Components. Table 4 shows the contribution of vision embeddings vs KV cache to the overall speedup. Vision embedding caching provides 7.8x speedup by elimi-nating the vision encoder forward pass. KV cache reuse adds 2.4x by skipping prompt processing. Combined: 19x speedup. 

Image Resolution Impact. Table 5 shows cache effec-tiveness vs resolution. Higher resolutions benefit more from caching. 

Video Frame Count Impact. Table 6 shows how video caching scales with frame count. Video caching becomes increasingly valuable with more frames: 32-frame videos achieve 24.7x speedup on cache hits despite larger cache entries. 

Text Prefix Caching. For text-only workloads, KV cache reuse also provides significant benefits. Table 7 shows speedup for repeated prompts with shared prefixes. Text prefix caching achieves 5.8x speedup on TTFT by reusing KV cache from previously processed prompts with matching prefixes. 

4.4 Discussion 

Why MLX Outperforms llama.cpp. Our results show vllm-mlx consistently exceeds llama.cpp throughput by 21% to 87%. We attribute this to three factors: (1) MLX‚Äôs na-tive unified memory design enables zero-copy tensor op-erations, avoiding the memory transfer overhead present in llama.cpp‚Äôs Metal backend; (2) MLX‚Äôs lazy evaluation allows operation fusion and reduces kernel launch overhead; (3) our continuous batching scheduler maximizes GPU utilization by processing multiple sequences simultaneously. 

When to Use vllm-mlx. For single-user scenarios requir-ing maximum simplicity, llama.cpp remains a strong choice with its broad model support and minimal dependencies. Native LLM and MLLM Inference at Scale on Apple Silicon 

Table 1. Text model throughput (tok/s) on M4 Max (128GB). All models use 4-bit quantization. Bold indicates best throughput per row. Speedup = Ours / llama.cpp (higher is better). 

Model Ours vllm-metal [14] mlx-lm [2] llama.cpp [5] Speedup 

Qwen3 Family [13] 

Qwen3-0.6B 525.5 365.8 356.2 281.5 1.87x Qwen3-4B 159.0 137.3 128.9 118.2 1.35x Qwen3-8B 93.3 87.1 79.9 76.9 1.21x Qwen3-30B-A3B 109.7 110.3 107.4 89.9 1.17x 

Llama 3.2 Family [1] 

Llama-3.2-1B 461.9 350.9 347.1 331.3 1.39x Llama-3.2-3B 203.6 174.3 167.5 155.8 1.31x 

Other Architectures 

Gemma 3-4B [4] 152.5 117.0 105.4 123.2 1.24x Nemotron-30B-A3B [9] 121.8 ‚Äì 101.6 85.1 1.43x 

Table 2. Multi-turn MLLM latency with prefix caching (Qwen3-VL-8B [ 15 ], 1024 √ó1024 image). Cache stores vision embeddings and KV state. Lower latency ( ‚Üì) and higher speedup ( ‚Üë) are better. 

Turn No Cache With Cache (‚Üì) Speedup (‚Üë)1 (cold) 21.7s 21.7s 1.0x 2 21.7s 1.15s 19x 3+ 21.7s 0.78s 28x Table 3. Video benchmark on Qwen3-VL-4B [ 15 ] (10s test video). More frames improve understanding but increase processing time. Lower time ( ‚Üì) and memory ( ‚Üì) are better; higher tok/s ( ‚Üë) is better.                                

> Config Frames Time (‚Üì)Tok/s (‚Üë)Memory (‚Üì)2 @ 0.5fps 21.8s 83.2 3.2 GB
> 4 @ 1fps 42.4s 62.5 3.8 GB 8 @ 2fps 83.6s 41.7 4.6 GB 16 @ 2fps 16 5.8s 25.9 6.2 GB 32 @ 4fps 32 9.4s 16.0 8.4 GB 64 @ 8fps 64 18.2s 8.2 12.1 GB

However, vllm-mlx is preferred when: (1) serving multiple concurrent users, where continuous batching provides up to 3.7x throughput scaling; (2) multimodal applications with repeated image/video analysis, where prefix caching elim-inates redundant encoding; (3) applications requiring an OpenAI-compatible API for drop-in replacement of cloud services. 

Enabling Local AI Agents. The combination of continu-ous batching and efficient concurrency scaling opens new 

Table 4. Ablation: Cache component contribution (Qwen3-VL-8B [ 15 ], 1024 √ó1024, Turn 2). Lower latency ( ‚Üì) and higher speedup ( ‚Üë) are better. 

Configuration Latency (‚Üì) Speedup (‚Üë)No caching (baseline) 21.7s 1.0x Vision embeddings only 2.8s 7.8x KV cache only 18.2s 1.2x Both (full cache) 1.15s 19x Table 5. Ablation: Cache effectiveness vs image resolution (Qwen3-VL-4B [ 15 ]). Lower cached latency ( ‚Üì) and cache size (‚Üì) are better; higher speedup ( ‚Üë) is better. 

Resolution Cold Cached (‚Üì) Speedup (‚Üë) Cache (‚Üì)224 √ó224 0.8s 0.12s 6.7x 48 MB 

448 √ó448 1.2s 0.14s 8.6x 89 MB 768 √ó768 1.8s 0.15s 12.0x 124 MB 1024 √ó1024 2.1s 0.16s 13.1x 156 MB 

Table 6. Ablation: Video cache effectiveness vs frame count (Qwen3-VL-4B [ 15 ]). Lower cached latency ( ‚Üì) and cache size (‚Üì) are better; higher speedup ( ‚Üë) is better. 

Frames Cold Cached (‚Üì) Speedup (‚Üë) Cache (‚Üì)4 2.4s 0.18s 13.3x 86 MB 

8 3.6s 0.22s 16.4x 142 MB 16 5.8s 0.28s 20.7x 256 MB 32 9.4s 0.38s 24.7x 486 MB possibilities for local AI agent systems. Multi-agent archi-tectures, where several specialized agents collaborate on Wayner Barrios 

Table 7. Ablation: Text prefix caching (Qwen3-4B [ 13 ], 512-token shared prefix). Lower TTFT ( ‚Üì) and higher speedup ( ‚Üë)are better. 

Configuration TTFT (‚Üì) Speedup (‚Üë)No caching (baseline) 245 ms 1.0x Prefix cache hit 42 ms 5.8x 

complex tasks, typically require multiple concurrent LLM calls. Cloud-based deployments incur latency penalties from network round-trips and API rate limits. With vllm-mlx han-dling 25+ requests per second on consumer hardware (Fig-ure 2b), developers can deploy agent swarms entirely on-device. This enables privacy-preserving agent workflows where sensitive data never leaves the local machine, real-time agent collaboration without network latency, and cost-effective development iterations without API charges. The OpenAI-compatible API ensures existing agent frameworks (LangChain [ 3 ], AutoGPT [ 10 ], CrewAI [ 8]) work without modification. 

Limitations and Future Work. Our framework currently supports only Apple Silicon, limiting deployment to macOS environments. Model support depends on MLX community. Future directions include speculative decoding for improved single-stream latency, distributed inference across multiple Apple Silicon devices via network-connected Mac clusters, and energy profiling for battery-powered deployments. We also plan to extend our caching approach to audio modalities for speech-enabled multimodal applications, and explore ten-sor parallelism across the GPU cores available in higher-end Apple Silicon configurations. 

## 5 Related Work 

KV Cache Optimization. Efficient management of the key-value cache has been a focus of recent LLM serving re-search. PagedAttention [ 6 ] introduced memory-efficient KV cache management through paging, enabling higher through-put in vLLM. SGLang [ 16 ] extended this with RadixAttention for prefix caching in text workloads. Our work builds on these foundations, extending prefix caching to multimodal inputs with vision embedding reuse. 

Multimodal Caching. LMCache [ 7 ] recently introduced KV cache reuse for multimodal models in vLLM, achieving significant speedups on NVIDIA GPUs. Their approach uses mm_hashes to identify identical vision inputs. Our work addresses the same problem for Apple Silicon, with a native MLX implementation that leverages unified memory for zero-copy caching. While LMCache operates as an external layer atop vLLM, our approach integrates caching directly into the inference engine. 

Apple Silicon ML Inference. MLX [ 2 ] provides a frame-work for ML on Apple Silicon with native Metal support. The mlx-lm library enables efficient LLM inference with quantiza-tion support. vLLM-metal is a community plugin that brings vLLM to Apple Silicon through a hybrid MLX and PyTorch approach. Our work differs in providing pure MLX inference with integrated multimodal caching, which vLLM-metal does not support. We note that the proposed framework is an in-dependent project that draws architectural inspiration from vLLM but shares no code with it. 

Edge LLM Inference. llama.cpp [ 5 ] has become the stan-dard for efficient LLM inference on consumer hardware, in-cluding Apple Silicon via Metal. MLC-LLM [ 12 ] provides cross-platform deployment with compilation-based optimiza-tion. These systems focus primarily on text models; our work extends efficient inference to multimodal workloads with caching support. 

## 6 Conclusion 

We introduced an efficient framework for multimodal LLM inference on Apple Silicon via content-based prefix caching for vision embeddings. Using content hashing to detect iden-tical images, we cache both vision embeddings and KV cache states, removing redundant vision encoding across requests. Our native MLX implementation exploits unified memory to enable zero-copy cache management. Across repeated image queries, we achieve up to 28 √ó

speedup, cutting latency from 21.7 seconds to under 1 sec-ond. Benchmarks on more than 10 recent models show MLX is a strong Apple Silicon backend, reaching 143 tokens per second on Qwen3-VL-4B. We release our implementation as open source to sup-port further research on efficient multimodal inference on consumer hardware. 

## References                      

> [1] Meta AI. 2024. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783 (2024). [2] Apple. 2023. MLX: Efficient and Flexible Machine Learning on Apple Silicon. https://github.com/ml-explore/mlx .[3] Harrison Chase. 2023. LangChain: Building Applications with LLMs.
> https://github.com/langchain-ai/langchain .[4] Google DeepMind. 2025. Gemma 3 Technical Report. arXiv preprint
> (2025). [5] Georgi Gerganov. 2023. llama.cpp: Port of Facebook‚Äôs LLaMA model in C/C++. https://github.com/ggerganov/llama.cpp .[6] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. Proceedings of the 29th Symposium on Operating Systems Principles (2023). [7] Yuhan Liu et al .2024. LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference. arXiv preprint arXiv:2510.09665
> (2024). [8] Joao Moura. 2024. CrewAI: Framework for Orchestrating Role-Playing AI Agents. https://github.com/crewAIInc/crewAI .

Native LLM and MLLM Inference at Scale on Apple Silicon 

[9] NVIDIA. 2025. Nemotron-3-Nano: Efficient Language Models for Edge Deployment. https://huggingface.co/nvidia/Nemotron-3-Nano-30B-A3B-BF16 .[10] Toran Richards. 2023. AutoGPT: An Autonomous GPT-4 Experiment. 

https://github.com/Significant-Gravitas/AutoGPT .[11] GLM Team. 2024. ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools. arXiv preprint arXiv:2406.12793 

(2024). [12] MLC Team. 2023. MLC-LLM: Universal Deployment of Large Lan-guage Models. https://github.com/mlc-ai/mlc-llm .[13] Qwen Team. 2025. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388 (2025). [14] vLLM Project. 2024. vLLM-Metal: vLLM Backend for Apple Silicon. 

https://github.com/vllm-project/vllm-metal .[15] Peng Wang et al . 2024. Qwen2-VL: Enhancing Vision-Language Model‚Äôs Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191 (2024). [16] Lianmin Zheng et al . 2023. SGLang: Efficient Execution of Structured Language Model Programs. arXiv preprint arXiv:2312.07104 (2023).