---
title: "Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction"
title_zh: Fast KVzip：基于门控 KV 逐出的高效准确 LLM 推理
authors: "Jang-Hyun Kim, Dongyoon Han, Sangdoo Yun"
date: 2026-01-25
pdf: "https://arxiv.org/pdf/2601.17668v1"
tags: ["query:sr-llm"]
score: 10.0
evidence: 通过KV缓存逐出实现高效准确的LLM推理
tldr: "针对大语言模型推理中KV缓存带来的存储压力，本文提出Fast KVzip，一种基于门控机制的高效KV缓存剔除方法。该方法引入轻量级sink-attention门控模块识别关键KV对，通过无需反向传播的训练算法实现任务无关的重构。实验表明，在Qwen和Gemma系列模型上，该方法能剔除高达70%的KV缓存并保持近乎无损的性能，显著提升了长文本和复杂推理任务的推理效率。"
motivation: 现有的KV缓存压缩技术往往在性能下降和计算开销之间存在权衡，难以兼顾效率与精度。
method: 引入轻量级门控模块识别关键KV对，并采用基于前向传播的非反向传播训练算法。
result: "在多个主流模型族上实现高达70%的KV缓存剔除率，且在长文本、代码和数学任务中表现近乎无损。"
conclusion: Fast KVzip 证明了通过简单的门控机制即可在极低开销下实现高效的KV缓存管理，具有极强的通用性。
---

## 摘要
高效的键值（KV）缓存管理对于大语言模型（LLM）的实际部署至关重要，然而现有的压缩技术往往需要在性能下降和计算开销之间进行权衡。我们为权重冻结的 LLM 提出了一种新型的基于门控的 KV 缓存逐出方法，该方法在计算成本极低的情况下实现了高压缩率。我们的方法引入了轻量级的 sink-attention 门控模块来识别并保留关键的 KV 对，并无缝集成到预填充（prefill）和解码（decoding）阶段。所提出的门控训练算法依赖于 LLM 的前向传播，避免了昂贵的反向传播，同时通过任务无关的重构目标实现了强大的任务泛化能力。在 Qwen2.5-1M、Qwen3 和 Gemma3 系列模型上进行的广泛实验表明，我们的方法在逐出高达 70% 的 KV 缓存时，仍能保持近乎无损的性能。实验结果在长文本理解、代码理解和数学推理等广泛任务中保持一致，证明了我们方法的通用性。

## Abstract
Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.

---

## 论文详细总结（自动生成）

以下是对论文《Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction》的结构化深入总结：

### 1. 核心问题与整体含义
*   **研究动机**：随着大语言模型（LLM）处理的上下文长度增加，KV 缓存（KV Cache）所需的内存随序列长度线性增长，成为推理系统的主要瓶颈。
*   **核心挑战**：现有的 KV 缓存压缩技术存在“效率-精度”两难困境。轻量级方法（如 H2O, SnapKV）往往导致性能显著下降；而高精度方法（如 KVzip）在推理时需要进行复杂的上下文重构，带来了巨大的计算开销（预填充时间翻倍），难以在延迟敏感的场景部署。
*   **论文目标**：提出一种既能保持高精度，又几乎不增加推理开销的门控式 KV 逐出方法。

### 2. 方法论
*   **核心思想**：KV 对在未来的利用价值是输入隐藏状态（Hidden States）的内在属性。通过在 Transformer 层中集成一个轻量级的门控模块，可以直接预测每个 KV 对的重要性，从而实现即时逐出。
*   **关键技术细节**：
    *   **Sink-Attention 门控架构**：受“注意力汇聚（Attention Sinks）”启发，设计了一个低秩门控模块。它通过线性投影将隐藏状态映射到低维空间，并结合一组层级特有的、可学习的“汇聚键（Sink Keys）”来计算重要性得分。
    *   **任务无关的训练目标**：冻结 LLM 权重，仅训练门控参数。训练目标是蒸馏（Distill）自 KVzip 的重构得分（即 KV 对在重构完整上下文时获得的最大注意力权重）。这种目标比“预测下一个词”或“特定 QA 任务”具有更强的泛化性。
    *   **算法流程**：
        1.  **预填充阶段**：采用分块预填充（Chunked Prefill），每块处理后根据门控得分剔除低贡献 KV 对，仅保留高分项和最近的滑动窗口项。
        2.  **解码阶段**：设置一个小型缓冲区（如 128 tokens），定期并行触发门控计算并更新缓存，将门控计算开销降低至模型前向传播的 1% 左右。

### 3. 实验设计
*   **实验场景**：涵盖了“预填充密集型”（长文本理解）和“解码密集型”（复杂数学推理）两大场景。
*   **数据集/Benchmark**：
    *   **长文本**：SCBench（包含 9 个子任务，如代码理解、合成键值检索）、OpenAI MRCR（多轮对话检索）、SQuAD（问答）。
    *   **推理**：AIME24、MATH（考察模型在长链条思考下的缓存保留能力）。
    *   **上下文长度**：最高测试至 170K tokens。
*   **对比方法**：KVzip（SOTA 高开销方法）、SnapKV、Expected Attention、DuoAttention、R-KV、TrimKV 等。
*   **测试模型**：Qwen2.5-1M (7B/14B)、Qwen3 (8B/14B/FP8 量化版)、Gemma3-12B（混合滑动窗口注意力模型）。

### 4. 资源与算力
*   **硬件环境**：所有实验均在单张 **NVIDIA H100 80GB GPU** 上完成。
*   **训练效率**：
    *   **训练时长**：对于 14B 规模的模型，门控模块的训练时间**小于 1 小时**（如 Qwen3-8B 仅需 0.59 小时）。
    *   **训练数据**：仅使用来自 FineWeb-Edu 的 1M tokens 数据，数据量极小（约为全集的百万分之一）。
    *   **存储开销**：门控参数量极小，对于 14B 模型仅增加约 0.3GB 的存储需求。

### 5. 实验数量与充分性
*   **实验规模**：论文在 3 个模型族、5 种不同规模/精度的模型上进行了验证，涵盖了从 4K 到 170K 的多种上下文长度。
*   **消融实验**：非常充分。作者对比了不同的门控输入（Hidden States vs Key States）、不同的训练目标（重构 vs 下一个词预测 vs QA 任务）、以及不同的门控架构（线性 vs MLP vs Sink-attention）。
*   **客观性**：实验不仅关注准确率（Accuracy），还详细记录了峰值内存使用量和预填充延迟，并与 2025 年 12 月前的最新基准（如 KVPress）进行了对比，结果显示其在相同预算下性能显著占优。

### 6. 主要结论与发现
*   **高效压缩**：Fast KVzip 在剔除 **60% - 70%** 的 KV 缓存时，仍能保持近乎无损的性能。
*   **推理加速**：相比于不压缩的情况，显著降低了峰值内存和预填充时间；相比于 KVzip，消除了运行时的重构延迟。
*   **泛化性强**：在通用文本上训练的门控模块，在代码、数学、长文本检索等完全不同的下游任务中均表现优异。
*   **层级差异**：定性分析发现，早期层的 KV 缓存通常非常稀疏，而中后期层保留了更多全局信息。

### 7. 优点
*   **极低开销**：通过并行化和轻量级设计，将压缩带来的延迟增加控制在 1% 左右，解决了 KVzip 的痛点。
*   **训练便捷**：无需反向传播通过 LLM 权重，仅需前向传播获取特征，训练极快且对显存要求低。
*   **兼容性好**：证明了该方法对 FP8 量化模型和混合注意力架构（如 Gemma3）同样有效。

### 8. 不足与局限
*   **模型依赖性**：虽然训练很快，但每个不同的模型仍需要单独训练一套门控参数，无法直接跨模型迁移。
*   **极端压缩限制**：当 KV 预算比例低于 20% 时，性能仍会出现较明显的下滑，未能彻底解决极低资源下的精度损失。
*   **静态权重假设**：该方法针对冻结权重的 LLM 设计，若模型在部署后进行频繁的微调（如 LoRA），门控模块可能需要重新训练。

（完）
