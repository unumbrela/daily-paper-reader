# /home/ubuntu/daily-paper/app/main.py
import sqlite3
import os
import json
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import openai

app = FastAPI()

# 允许跨域（方便本地调试，上线后可限制）
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# 数据库与文件路径配置
DB_FILE = os.path.join(os.path.dirname(__file__), "chat.db")
DOCS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "docs")
LLM_API_KEY = os.getenv("LLM_API_KEY")
if not LLM_API_KEY:
    raise RuntimeError("缺少环境变量 LLM_API_KEY，请设置大模型的 API Key")

# 从环境变量读取模型名称和可选的 Base URL
LLM_MODEL = os.getenv("LLM_MODEL", "glm-4.6")
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://open.bigmodel.cn/api/coding/paas/v4")

# 使用 OpenAI 兼容客户端，建议选择支持 200k 上下文长度的模型
CLIENT = openai.OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

# 系统提示词：统一约束回答语法为「Markdown + LaTeX」
SYSTEM_PROMPT = (
    "你是学术讨论助手，负责围绕论文内容进行深入分析与讨论。"
    "我是用的KaTex来进行渲染，请严格使用「Markdown + LaTeX」表达答案"
)

def init_db():
    with sqlite3.connect(DB_FILE) as conn:
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("""
            CREATE TABLE IF NOT EXISTS comments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                paper_id TEXT, role TEXT, content TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
init_db()

class ChatRequest(BaseModel):
    paper_id: str
    question: str  # 前端会传 paper_content，但后端以本地 md/txt 为准读取

@app.get("/api/history")
def get_history(paper_id: str):
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.execute("SELECT role, content, created_at FROM comments WHERE paper_id=? ORDER BY id ASC", (paper_id,))
        return [{"role": r[0], "content": r[1], "time": r[2]} for r in cursor.fetchall()]

@app.post("/api/chat")
def chat(req: ChatRequest):
    # 1. 读取本地 Markdown 文件和预处理后的 txt 文本作为上下文
    md_path = os.path.join(DOCS_DIR, f"{req.paper_id}.md")
    if not os.path.exists(md_path):
        raise HTTPException(status_code=404, detail="Paper not found")
    
    with open(md_path, "r", encoding="utf-8") as f:
        paper_md_content = f.read()

    txt_path = os.path.join(DOCS_DIR, f"{req.paper_id}.txt")
    paper_txt_content = ""
    if os.path.exists(txt_path):
        with open(txt_path, "r", encoding="utf-8") as f:
            paper_txt_content = f.read()

    # 2. 存用户问题
    with sqlite3.connect(DB_FILE) as conn:
        conn.execute("INSERT INTO comments (paper_id, role, content) VALUES (?, ?, ?)", (req.paper_id, "user", req.question))
        # 获取历史用于上下文
        cursor = conn.execute("SELECT role, content FROM comments WHERE paper_id=? ORDER BY id ASC", (req.paper_id,))
        history = cursor.fetchall()

    # 3. 组装 Prompt：PDF 文本 + Markdown + 历史讨论 + 本次问题
    messages = [{
        "role": "system",
        "content": SYSTEM_PROMPT,
    }]

    if paper_txt_content:
        messages.append({
            "role": "user",
            "content": f"### 论文 PDF 提取文本 ###\n{paper_txt_content}",
        })

    messages.append({
        "role": "user",
        "content": f"### 论文 Markdown 内容 ###\n{paper_md_content}",
    })
    
    # 将历史对话转为 API 格式（不包含思考过程 thinking）
    for role, content in history:
        if role == "thinking":
            continue
        api_role = "assistant" if role == "ai" else "user"
        messages.append({"role": api_role, "content": content})

    # 4. 调用 LLM
    try:
        resp = CLIENT.chat.completions.create(
            model=LLM_MODEL,
            messages=messages,
            temperature=0.7,  # 模型温度设为 0.7
            stream=False,
        )
        ai_msg = resp.choices[0].message.content
    except Exception as e:
        ai_msg = f"Error: {str(e)}"

    # 5. 存 AI 回答
    with sqlite3.connect(DB_FILE) as conn:
        conn.execute("INSERT INTO comments (paper_id, role, content) VALUES (?, ?, ?)", (req.paper_id, "ai", ai_msg))

    return {"status": "ok"}


@app.post("/api/chat_stream")
def chat_stream(req: ChatRequest):
    """
    流式返回 AI 回答，用于前端公共研讨区的流式展示。
    """
    # 1. 读取本地 Markdown 文件和预处理后的 txt 文本作为上下文
    md_path = os.path.join(DOCS_DIR, f"{req.paper_id}.md")
    if not os.path.exists(md_path):
        raise HTTPException(status_code=404, detail="Paper not found")

    with open(md_path, "r", encoding="utf-8") as f:
        paper_md_content = f.read()

    txt_path = os.path.join(DOCS_DIR, f"{req.paper_id}.txt")
    paper_txt_content = ""
    if os.path.exists(txt_path):
        with open(txt_path, "r", encoding="utf-8") as f:
            paper_txt_content = f.read()

    # 2. 存用户问题，并取出历史用于上下文
    with sqlite3.connect(DB_FILE) as conn:
        conn.execute(
            "INSERT INTO comments (paper_id, role, content) VALUES (?, ?, ?)",
            (req.paper_id, "user", req.question),
        )
        cursor = conn.execute(
            "SELECT role, content FROM comments WHERE paper_id=? ORDER BY id ASC",
            (req.paper_id,),
        )
        history = cursor.fetchall()

    # 3. 组装 Prompt
    messages = [{
        "role": "system",
        "content": SYSTEM_PROMPT,
    }]

    if paper_txt_content:
        messages.append({
            "role": "user",
            "content": f"### 论文 PDF 提取文本 ###\n{paper_txt_content}",
        })

    messages.append({
        "role": "user",
        "content": f"### 论文 Markdown 内容 ###\n{paper_md_content}",
    })

    for role, content in history:
        if role == "thinking":
            continue
        api_role = "assistant" if role == "ai" else "user"
        messages.append({"role": api_role, "content": content})

    def generate():
        full_answer = ""
        full_thinking = ""
        try:
            stream = CLIENT.chat.completions.create(
                model=LLM_MODEL,
                messages=messages,
                temperature=0.7,
                stream=True,
            )
            for chunk in stream:
                delta = chunk.choices[0].delta

                # 一些模型会在 delta 中提供 “思考 / 推理” 内容（例如 reasoning_content 或 thinking 字段）
                thinking = getattr(delta, "reasoning_content", None) or getattr(delta, "thinking", None)
                if thinking:
                    # 以按行 JSON 的方式输出，前端根据 type 字段区分
                    full_thinking += thinking
                    payload = {"type": "thinking", "content": thinking}
                    yield json.dumps(payload, ensure_ascii=False) + "\n"

                content_piece = getattr(delta, "content", None) or ""
                if not content_piece:
                    continue
                full_answer += content_piece
                # 流式输出答案内容
                payload = {"type": "answer", "content": content_piece}
                yield json.dumps(payload, ensure_ascii=False) + "\n"
        except Exception as e:
            err_msg = f"{str(e)}"
            payload = {"type": "error", "content": err_msg}
            yield json.dumps(payload, ensure_ascii=False) + "\n"
            return

        # 流结束后，把思考过程与完整回答写入数据库
        with sqlite3.connect(DB_FILE) as conn:
            if full_thinking.strip():
                conn.execute(
                    "INSERT INTO comments (paper_id, role, content) VALUES (?, ?, ?)",
                    (req.paper_id, "thinking", full_thinking),
                )
            if full_answer.strip():
                conn.execute(
                    "INSERT INTO comments (paper_id, role, content) VALUES (?, ?, ?)",
                    (req.paper_id, "ai", full_answer),
                )

    return StreamingResponse(generate(), media_type="text/plain")
